<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Courses | Gianluca Quercini</title>
    <link>/courses/</link>
      <atom:link href="/courses/index.xml" rel="self" type="application/rss+xml" />
    <description>Courses</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>Courses</title>
      <link>/courses/</link>
    </image>
    
    <item>
      <title>Tutorials and lab assignments</title>
      <link>/courses/bdia_old/tutorials/cc-tutorials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/cc-tutorials/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;tutorial-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial 1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: MapReduce programming&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;/courses/bdia/tutorials/map-reduce&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;!--
# Tutorial 2

**Title**: MapReduce programming in Python

**Date and time**: Wednesday 6 January 2021, 10:15 AM - 11:45 AM

**Link**: [Click here](https://colab.research.google.com/drive/1W8kyQmEpxiwoP66WG6ZQtNRfQwQQVqSI?usp=sharing){target=&#34;_blank&#34;}

# Tutorial 3

**Title**: Introduction to Spark programming

**Date and time**: Thursday 7 January 2021, 10:15 AM - 11:45 AM

**Link**: [Click here](https://colab.research.google.com/drive/1hQ6h_tNPNnzoAN-s7WI_K9tzoZjgIxzS?usp=sharing){target=&#34;_blank&#34;}


&lt;!-- **Link**: [Click here](https://colab.research.google.com/drive/1qBLJrQZGum4A5m4RrofLq-KRXYug4HQM?usp=sharing)

**Link to the data**: [Click here](/courses/plp/tutorials/data.tgz)

**Link to the data**: [Click here](/courses/plp/tutorials/moviesEmbedded.json)--&gt;
&lt;!--
# Tutorial 4

**Title**: Spark programming using a Spark cluster.

**Date and time**: Thursday 7 January 2021, 1:45 PM - 5 PM

**Link**: [Click here](/courses/plp/tutorials/spark-programming-dce){target=&#34;_blank&#34;}

# Lab assignment 1

**Title**: Advanced Spark programming

**Date and time**: Monday 11 January 2021, 10:15 AM - 11.45 AM

**Link**: [Click here](/courses/plp/tutorials/spark-lab-assignment){target=&#34;_blank&#34;}


# Lab assignment 2

**Title**: MongoDB

**Date and time**: Thursday 21 January 2021, 10:15 AM - 11.45 AM

**Link**: [Click here](/courses/plp/tutorials/mongodb-tutorial){target=&#34;_blank&#34;}

# Tutorial 5

**Title**: Neo4j

**Date and time**: Thursday 28 January 2021, 10:15 AM - 11:45 AM

**Link**: [Click here](/courses/plp/tutorials/neo4j-tutorial){target=&#34;_blank&#34;}
--&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tutorials and lab assignments</title>
      <link>/courses/big-data-marseille/tutorials/cc-tutorials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/tutorials/cc-tutorials/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;tutorial-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial 1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: MapReduce programming&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 3 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;/courses/big-data-marseille/tutorials/map-reduce&#34; target=&#34;_blank&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tutorial-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial 2&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to Spark RDD programming&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 5 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1yYnVu4aowoMCrVuFyX4Uj5afi7k2irvx?usp=sharing&#34; target=&#34;_blank&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;!-- **Link**: [Click here](https://colab.research.google.com/drive/1qBLJrQZGum4A5m4RrofLq-KRXYug4HQM?usp=sharing)

**Link to the data**: [Click here](/courses/plp/tutorials/data.tgz)

**Link to the data**: [Click here](/courses/plp/tutorials/moviesEmbedded.json)--&gt;
&lt;/div&gt;
&lt;div id=&#34;tutorial-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial 3&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to DataFrames and SparkSQL&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 10 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1T3HS6lTQrPrmjCdtNICJSoIHmKSfKFOo?usp=sharing&#34; target=&#34;_blank&#34;&gt;DataFrames&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1M5jsrejgtf9KWNfpuEbr1HxjiMNYXgNt?usp=sharing&#34; target=&#34;_blank&#34;&gt;DataFrames + SQL&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lab-assignment-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lab assignment 1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Apache Spark programming&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 12 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;/courses/big-data-marseille/tutorials/spark-assignment&#34; target=&#34;_blank&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tutorial-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial 4&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Apache Structured Streaming&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 12 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;/courses/big-data-marseille/tutorials/spark-streaming&#34; target=&#34;_blank&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lab-assignment-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lab assignment 2&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: MongoDB&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 19 May 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1ceHXWAcQMklxc2qjtDquAa_mKjuVEda7?usp=sharing&#34; target=&#34;_blank&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Apache Spark — Programming with RDD</title>
      <link>/courses/big-data-marseille/tutorials/spark-assignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/tutorials/spark-assignment/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;&lt;a href=&#34;/courses/big-data-marseille/overview/cluster-connection&#34; target=&#34;_blank&#34;&gt;Refer to this documentation&lt;/a&gt; to learn how to connect and
interact with the cluster.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Assignment submission&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This lab assignment will be &lt;strong&gt;evaluated&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You need to submit a .zip file containing the following files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Source code of the programs that you write.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A PDF document with the answer to the questions that you find in this document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please send me the zip file &lt;strong&gt;by email&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The submission deadline is &lt;strong&gt;Thursday, May 20, 2021 8:00 AM&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We consider a collection of CSV files containing temperature measurements in the following format:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;year,month,day,hours,minutes,seconds,temperature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you can find the files under the directory &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here are the details for each file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contains one measurement per day in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contains one measurement every 2880 seconds in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86.csv&lt;/code&gt; contains one measurement every 86 seconds for the year 1980 alone.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_10.csv&lt;/code&gt; contains one measurement every 10 seconds for the years 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We intend to implement a Spark algorithm to generate pairs &lt;span class=&#34;math inline&#34;&gt;\((y, t_{avg})\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the year and &lt;span class=&#34;math inline&#34;&gt;\(t_{avg}\)&lt;/span&gt; is the average temperature in the year.&lt;/p&gt;
&lt;div id=&#34;first-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; First implementation&lt;/h2&gt;
&lt;p&gt;Copy the file &lt;code&gt;~vialle/DCE-Spark/template_temperatures.py&lt;/code&gt;
to your home directory by typing
the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the file &lt;code&gt;avg_temperatures_first.py&lt;/code&gt;
and write the following function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def avg_temperature(theTextFile): 
    temperatures = theTextFile \ 
                        .map(lambda line: line.split(&amp;quot;,&amp;quot;)) \ 
                        .map(lambda term: (term[0],   [float(term[6])])) \ 
                        .reduceByKey(lambda x, y: x+y) \ 
                        .mapValues(lambda lv: sum(lv)/len(lv)) 
    return temperatures &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the same file, locate the two variables &lt;code&gt;input_path&lt;/code&gt; and &lt;code&gt;output-path&lt;/code&gt;.
and write the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input_path = &amp;quot;hdfs://sar01:9000/data/temperatures/&amp;quot;
output_path = &amp;quot;hdfs://sar01:9000/cpuecm1/cpuecm1_XX/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Don’t forget the / at the end of the file paths and to replace XX with the number at the end of your username&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the script &lt;code&gt;avg_temperatures_first.py&lt;/code&gt; by using &lt;code&gt;temperatures_86400.csv&lt;/code&gt; as an input.
To this extent, use the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should find the output of the program under the folder&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/cpuecm1/cpuecm1_XX/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What’s the execution time?
&lt;ul&gt;
&lt;li&gt;In the output of Spark on the command line you should see a line that mentions something along the following line:&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_2880.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? Does it seem reasonable compared with the execution time that you observed before?
Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the same script by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? How would you justify it, knowing that
the files &lt;code&gt;temperatures_2880.csv&lt;/code&gt; and &lt;code&gt;temperatures_86.csv&lt;/code&gt; have a similar size (11 MB the former, 9 MB the latter)?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;second-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Second implementation&lt;/h2&gt;
&lt;p&gt;Copy the file &lt;code&gt;~vialle/DCE-Spark/template_temperatures.py&lt;/code&gt; to your working directory by typing
the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Based on the observations made in the
previous exercise, write an improved implementation of the
function &lt;code&gt;avg_temperature&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run the script &lt;code&gt;avg_temperatures_second.py&lt;/code&gt; by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What’s the execution time? Compare it with the execution time obtained in the previous exercise and
comment the difference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_10.csv&lt;/code&gt; (3 GB!) as an input.
Do you think that the program takes too long? Why?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends-in-a-social-network&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,B,C,D
B,A,D
C,A,D
D,A,B,C
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We want to obtain the list of the common friends for each pair of individuals&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We use the following input files available in folder &lt;code&gt;hdfs://sar01:9000/data/sn/&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_tiny.csv&lt;/code&gt;. Small social network, that you can use to test your implementation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_10k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_100k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^3\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1m_1m.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Write an implementation in Spark.
&lt;strong&gt;Test your implementation on file &lt;code&gt;sn_tiny.csv&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Run your implementation on the other files and write down the execution times.
Comment on the execution times considering the file sizes, the number of nodes and links
and the number of pairs &lt;span class=&#34;math inline&#34;&gt;\(((A, B), X)\)&lt;/span&gt; generated by the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of
a node in a given graph.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the minimum, maximum and average degree on all the given input files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do these values confirm or invalidate the considerations that you made on the execution times of the
algorithm in the first exercise? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-inverted-index&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Creating an inverted index&lt;/h1&gt;
&lt;p&gt;In folder &lt;code&gt;hdfs://sar01:9000/data/bbc/&lt;/code&gt; you’ll find a collection of 50
articles obtained from the BBC website (2004-2005) organized into five subfolders:
&lt;em&gt;business&lt;/em&gt;, &lt;em&gt;entertainment&lt;/em&gt;, &lt;em&gt;politics&lt;/em&gt;, &lt;em&gt;sport&lt;/em&gt; and &lt;em&gt;technology&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We want to create an &lt;strong&gt;inverted index&lt;/strong&gt;, which associates each word with the list of the
files in which the word occurs.
More specifically, for each word, the inverted index will have a list of the
names of the files (path relative to the folder &lt;code&gt;/data/bbc&lt;/code&gt;) that contain the word.&lt;/p&gt;
&lt;p&gt;The inverted index:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;must not contain the same word twice;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;must not contain any stopwords (the list of stopwords is provided in the
&lt;code&gt;hdfs://sar01:9000/data/stopwords.txt&lt;/code&gt; file);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Words in the inverted index must only contain letters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Words in the inverted index must be lowercase.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write a Spark program to create an inverted index and execute it on the
input folder.
You can use the template available at &lt;code&gt;~vialle/DCE-Spark/template_inverted_index.py&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark DataFrames and Spark SQL</title>
      <link>/courses/bdia/tutorials/spark-sql-assignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/spark-sql-assignment/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;number-of-partitions-in-a-rdd&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Number of partitions in a RDD&lt;/h1&gt;
&lt;p&gt;We consider a set of CSV files that contain temperature measurements over several years.
Each line has the following content: &lt;code&gt;year,month,day,hour,minute,second,temperature&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These files are stored at the following location: &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is the detail for each file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contains one measurement per day between 1980 and 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contains one measurement every 2880 seconds between 1980 and 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86.csv&lt;/code&gt; contains one measurement every 86 seconds for the year 1980 alone.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_10.csv&lt;/code&gt; contains one measurement every 10 seconds between 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Get the file &lt;code&gt;avg_temperatures_rdd.py&lt;/code&gt; by typing the following command:&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;left&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This file contains an efficient implementation of the function that computes the average yearly temperature, as we have seen
in a previous tutorial.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performances-and-number-of-partitions&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Performances and number of partitions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 70.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 80.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;. Replace YOUR_DIRECTORY with &lt;code&gt;bdiaspark2024/bdiaspark2024_XX/&lt;/code&gt; (where XX corresponds to your username number).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observe the instructions at &lt;strong&gt;line 98&lt;/strong&gt; and &lt;strong&gt;line 104&lt;/strong&gt;. They get and show
the number of partitions of the input RDD &lt;code&gt;text_file&lt;/code&gt; and the output RDD &lt;code&gt;temperatures&lt;/code&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Complete Table 1. Run the program on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;. Write down the
execution time and the number of partitions of both RDDs &lt;code&gt;text_file&lt;/code&gt; and &lt;code&gt;temperatures&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Reminder.&lt;/strong&gt; The command to run the program is as follows:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time &lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Number of partitions &lt;br&gt; (&lt;code&gt;text_file&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Number of partitions &lt;br&gt; (&lt;code&gt;temperatures&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
&lt;b&gt;Table 1. Execution time and partition numbers with RDD.&lt;/b&gt;
&lt;/caption&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-of-sparks-operation&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Analysis of Spark’s operation&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Could you understand how Spark determines the number of partitions of the RDD &lt;code&gt;text_file&lt;/code&gt;
by looking at the size of the input files?
&lt;br&gt;
&lt;strong&gt;HINT.&lt;/strong&gt; If you divide the size of the file &lt;code&gt;temperatures_10.csv&lt;/code&gt;
by the number of partitions of the RDD &lt;code&gt;text_file&lt;/code&gt;, which value do you obtain? What does this value represent?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;List the files that are stored in the output folder &lt;code&gt;temperatures_10.rdd.out&lt;/code&gt; under your
HDFS folder. What do you notice? Is there any relation with respect to the number of partitions?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to list the content of the folder in HDFS, you can use the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_XX/temperatures_10.rdd.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-dataframe-api-to-compute-the-average-temperatures&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Using the DataFrame API to compute the average temperatures&lt;/h1&gt;
&lt;p&gt;You’re now going to implement a Spark program to compute the average temperatures by using the Spark DataFrame API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Go through the following steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the code template &lt;code&gt;avg_temperatures_df.py&lt;/code&gt; to your home folder by executing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_df.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;!--* The instructions **at line 109 and 115** show the number of partitions of the RDDs
underlying the dataframes ``df`` (input) and ``df_avg`` (output) respectively.--&gt;
&lt;ul&gt;
&lt;li&gt;The result of the computation will be stored in the folder &lt;code&gt;temperatures_*.df.out&lt;/code&gt;
under your HDFS folder &lt;code&gt;bdiaspark2024/bdiaspark2024_XX&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 78.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 89.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 106.&lt;/strong&gt; Complete the instruction to read from the input CSV file.
Please note that the input CSV files &lt;strong&gt;do not have headers&lt;/strong&gt;. Don’t use schema inference,
just specify your schema manually. As a reminder, the columns are: &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt;,
&lt;code&gt;day&lt;/code&gt;, &lt;code&gt;hour&lt;/code&gt;, &lt;code&gt;minute&lt;/code&gt;, &lt;code&gt;second&lt;/code&gt;, &lt;code&gt;temperature&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 55.&lt;/strong&gt; Complete the definition of function &lt;code&gt;avg_temperature_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute your code on all the input CSV files and &lt;strong&gt;complete Table 2&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 2. RDDs vs. DataFrames.
&lt;/caption&gt;
&lt;/table&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Compare the execution times with the ones obtained with the implementation using the RDDs.
What do you observe? How do you explain the differences?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- ## Effect of the number of partitions

You&#39;re going to study the effect of the number of partitions on the performances of the program.

* In file ``avg_temperatures_df.py`` add the following instruction right below the initialisation of the 
``SparkSession``. The command sets the number of *shuffle* partitions to a certain value `X`.

``
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, X)
``

* Execute the program with different values of `X` (say, 1, 2, 10, 50, 100, 500, 1000, 5000, 100000) **only on file**
``temperatures_86400.csv`` and write down the execution times.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;

* Plot a graph where the x-axis contains the values of `X` and the y-axis contains the execution time.
What do you observe?


* Execute the program on the other files by using small values of `X` (1 and 2). 
Comment on the evolution of the execution times.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div id=&#34;caching-a-dataframe&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Caching a DataFrame&lt;/h2&gt;
&lt;p&gt;You’re now going to discover the advantages of caching a DataFrame.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment the last two lines in file &lt;code&gt;avg_temperatures_df.py&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove the file &lt;code&gt;temperatures_10.df.out&lt;/code&gt; by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_XX/temperatures_10.df.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Execute the code on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.
What is the execution time of each action?
Can you explain in detail what is going on here?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Remove files &lt;code&gt;temperatures_10.df.out&lt;/code&gt; and &lt;code&gt;temperatures_10.df.out.bis&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Cache the DataFrame &lt;code&gt;df_avg&lt;/code&gt; and
execute the code again on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where should you add the cache instruction?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time of each action?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can you explain in detail what is going on here?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-averages-with-sql&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Computing averages with SQL&lt;/h1&gt;
&lt;p&gt;You’re now going to implement the computation of the yearly average temperatures by using SQL on Spark DataFrames.&lt;/p&gt;
&lt;div id=&#34;using-a-view&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Using a view&lt;/h2&gt;
&lt;p&gt;A first option to query a DataFrame with SQL is to create a &lt;strong&gt;view&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;avg_temperatures_sql_view.py&lt;/code&gt; to your home folder by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complete the code in lines 90, 101, 118.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;avg_temperature_sql&lt;/code&gt; (line 57).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all CSV files and &lt;strong&gt;complete Table 3&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What can you tell about the the running times ? Do you find significant differences between using SQL on a view and
DataFrame functions?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL view&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 3. RDDs vs DataFrames with views.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-table&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Using a table&lt;/h2&gt;
&lt;p&gt;A second option to query a DataFrame with SQL is to create a &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;avg_temperatures_sql_table.py&lt;/code&gt; to your home directory by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complete lines 50, 112, 123 and 140.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;avg_temperature_sql&lt;/code&gt; (line 76).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete Table 4.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare the execution times with the ones that you obtained. Discuss the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you test the code for the first time,
it’s possible that your program will return errors (e.g. due to syntax problems).
In this case, it’s imperative to delete any files that may have been created
in the &lt;code&gt;spark-warehouse&lt;/code&gt; directory, as well as the &lt;code&gt;metastore_db&lt;/code&gt; directory, which you’ll find in your local working directory (not on HDFS).&lt;/p&gt;
&lt;/div&gt;
&lt;!--While reading the code in file ``avg_temperatures_sql_table.py``, you probably noticed that we have specified 
the path to a HDFS folder for the **Hive metastore warehouse**. This folder will contain the data of each table 
that we create.--&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-10&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-10) &lt;/strong&gt;&lt;/span&gt;

* Look at the files in your home folder (in the local file system, not HDFS) by using the command ``ls``. 

* Do you notice the presence of new files?

* Can you explain what these files are?

* Execute the code on all CSV files and **complete Table 4**. 

* Compare the execution times against the ones that you obtained in the previous exercises.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::--&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL view&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL table&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 4. RDDs vs DataFrames with views and tables.
&lt;/caption&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the file &lt;code&gt;temperatures_10.sql.table.out&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code again on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
What is the execution time that you obtain now?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-dataframe-api-on-large-files&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Using the DataFrame API on large files&lt;/h1&gt;
&lt;p&gt;We now consider the files stored under &lt;code&gt;hdfs://sar01:9000/data/sales/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These files contain tabular data related to the sale of products in a chain of stores.
We consider two tables: &lt;code&gt;store_sales&lt;/code&gt; and &lt;code&gt;customer&lt;/code&gt;.
In the first table we find information about each sale,
such as the identifier of the product sold,
the identifier of the buyer,
the quantity of purchased product and the price paid by the customer.
For this table, we have 4 files, which only differ in size:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.100.dat&lt;/code&gt;: contains 9.5 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.200.dat&lt;/code&gt;: contains 19 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.400.dat&lt;/code&gt;: contains 38 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.800.dat&lt;/code&gt;: contains 77 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In table &lt;code&gt;customer&lt;/code&gt; we find data about customers, such as first and last names and birth dates.
We only have one file for this table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;customer_10000.dat&lt;/code&gt;: contains 8.3 GiB of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want to test the performances of the &lt;strong&gt;DataFrame API&lt;/strong&gt; on the following queries
(&lt;strong&gt;WARNING. you must write a code that uses DataFrame functions, not SQL!&lt;/strong&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query Q1&lt;/strong&gt;: returns the number of clients. This corresponds to the following SQL query:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;SELECT count(*) 
FROM customer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q2&lt;/strong&gt;: returns the price of the most expensive product. This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT max(ss_list_price) 
FROM store_sales&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q3&lt;/strong&gt;: returns the amount of money spent by each client. This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q4&lt;/strong&gt;: Query Q3 + sort the result so that the client that spent the most money appears on the top.
This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q5&lt;/strong&gt;: Query Q4 + join with the table &lt;code&gt;customer&lt;/code&gt; to get the first and last name of the customers.
This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk 
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;development-of-the-code-using-the-dataframe-api.&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Development of the code using the DataFrame API.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;dataframe_api_benchmark.py&lt;/code&gt; to your home directory by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modify the code by following the instructions in the file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code on file &lt;code&gt;store_sales_100.dat&lt;/code&gt; (the smallest one) to test that your code is bug-free.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once you’re sure that your code is correct, &lt;strong&gt;uncomment lines 82 and 83&lt;/strong&gt;. This will cache the two DataFrames.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all files &lt;code&gt;store-sales_*.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each query is executed 5 times to have a correct estimate of the execution time.
You’ll see that the execution times fluctuate on the first iterations and they stabilize
in the later iterations.
When you write down the execution times, only consider the execution times obtained
at the last iteration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Table 5&lt;/strong&gt; and write down the execution time of each query for each file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why the execution time of the queries Q1 and Q2 is large at the iteration 0?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you think that the difference between the execution times of the queries is reasonable?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you think that the augmentation of the execution times is reasonable given the size of the input files?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File / query&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Read&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q1&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q2&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q3&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q4&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q5&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.100.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.200.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.400.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.800.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 5. Execution times of the queries on the sales dataset.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>/courses/bdia_old/lectures/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/lectures/lectures/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;lecture-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction and MapReduce programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: &lt;a href=&#34;https://centralesupelec.edunao.com/pluginfile.php/166182/course/section/24820/bdia-lecture-1.pdf&#34; target=&#34;_blank&#34;&gt;Available on Edunao&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 2&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Hadoop and storage: HDFS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: &lt;a href=&#34;https://centralesupelec.edunao.com/pluginfile.php/166182/course/section/24820/bdia-lecture-2.pdf&#34; target=&#34;_blank&#34;&gt;Available on Edunao&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 3&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: &lt;a href=&#34;https://centralesupelec.edunao.com/pluginfile.php/166182/course/section/24820/bdia-lecture-3.pdf&#34; target=&#34;_blank&#34;&gt;Available on Edunao&lt;/a&gt;.&lt;/p&gt;
&lt;!--**Spark programming demo**: [Available in Google Colab](https://colab.research.google.com/drive/1bUW3_HVFx3KD5OJZcVCzOEoM26JNIzOF?usp=sharing){target=&#34;_blank&#34;}.--&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 4&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Spark SQL.&lt;/p&gt;
&lt;!--**Spark SQL demo**: [Available in Google Colab](https://colab.research.google.com/drive/1ayEq0dh8aNWB0aIMntKOg4irXOveuoz3?usp=sharing){target=&#34;_blank&#34;}.--&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 5&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Spark Streaming&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 6&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Distributed and NoSQL databases&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-7&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 7&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Document-oriented database systems: MongoDB.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>First Spark program on the DCE</title>
      <link>/courses/bdia/tutorials/first-spark-program/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/first-spark-program/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- The link to the Edunao page where the students can choose a username to connect to the cluster. --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;The &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;“_blank”&#34;&gt;Data Centre d’Enseignement (DCE)&lt;/a&gt; is a set
of computing resources financed by the Metz Eurometropole
by the Eurométropole de Metz, Grand Est, CentraleSupélec and its foundation,
and the Conseil Départemental de Moselle.&lt;/p&gt;
&lt;p&gt;This tutorial is a quick guide to :
* Learn the basic commands for manipulating files stored in HDFS.
* Learn how to run a Spark program on the DCE.&lt;/p&gt;
&lt;p&gt;More information is available on the &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;“_blank”&#34;&gt;official DCE website&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;overview-of-the-dce&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Overview of the DCE&lt;/h1&gt;
&lt;p&gt;The architecture of the DCE is presented in Figure &lt;a href=&#34;#fig:cluster-metz&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cluster-metz&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/plp/tutorials/spark-dce/reseau_dce.drawio.svg&#34; alt=&#34;Architecture of the cluster (source : [DCE documentation](https://dce.pages.centralesupelec.fr/01_cluster_overview/){target=&amp;quot;_blank&amp;quot;})&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Architecture of the cluster (source : &lt;a href=&#34;https://dce.pages.centralesupelec.fr/01_cluster_overview/&#34; target=&#34;_blank&#34;&gt;DCE documentation&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this course, we’ll be using CPU nodes only.
These are divided into two groups: the Sarah cluster and the Kyle cluster.&lt;/p&gt;
&lt;!--# Creating your working directory in HDFS

In this section, you&#39;ll be walked through the procedure to create a directory 
in HDFS that you&#39;ll use as your working directory in the lab sessions.

You user account is: ``bdiaspark2024_X``, where X is between 1 and 
24.

In order to create your working directory in HDFS, type the following command in the terminal:

``
hdfs dfs -mkdir hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X
``

You can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/bdiaspark2024/
with the following command:

``
hdfs dfs -ls hdfs://sar01:9000/bdiaspark2024/
``
--&gt;
&lt;/div&gt;
&lt;div id=&#34;running-a-spark-program&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Running a Spark program&lt;/h1&gt;
&lt;p&gt;Datasets are normally available in the &lt;code&gt;/data/&lt;/code&gt; folder stored in HDFS.
Enter the following command to display the contents of the folder:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/data&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The aim here is to run Spark code to count the number of occurrences of each word in file &lt;code&gt;/data/sherlock.txt&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy file &lt;code&gt;~cpu_vialle/DCE-Spark/template_wc.py&lt;/code&gt; to your home directory through the following command :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_wc.py ./wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Enter the &lt;code&gt;ls&lt;/code&gt; command to check that the &lt;code&gt;wc.py&lt;/code&gt; file is in your home directory.
This file contains the program’s Python code.
This file should also appear in the Visual Studio Code explorer; if not, click on the explorer’s refresh button.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open file &lt;code&gt;wc.py&lt;/code&gt; in Visual Studio Code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the following instruction:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
and replace it with the following :
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://sar01:9000/data/sherlock.txt&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This instruction will create an RDD called &lt;code&gt;text_file&lt;/code&gt; with the contents of the file.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Similarly, locate the following instruction :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;and replace it with the following:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/sherlock.out&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This instruction will create a folder &lt;em&gt;sherlock.out&lt;/em&gt; that your program will produce.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Execute the program &lt;code&gt;wc.py&lt;/code&gt; through the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;When the execution is completed, the output will be available in the folder &lt;code&gt;sherlock.out&lt;/code&gt;.
To verify it, type the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As usual, don’t forget to replace bdiaspark2024_X with your username.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type the following command to print the result:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/sherlock.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Output files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you execute the program and you specify an output file that &lt;strong&gt;already exists&lt;/strong&gt;,
you’ll get an error.
If you really want to overwrite the output file,&lt;br /&gt;
you need to first remove it explicitly with the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Premier programme Spark sur DCE</title>
      <link>/courses/bigdata-fr/dce-first-program/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-fr/dce-first-program/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;Le &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;Data Centre d’Enseignement (DCE)&lt;/a&gt; est un ensemble de
ressources informatiques financées par l’Eurométropole de Metz
par l’Eurométropole de Metz, la Région Grand Est, CentraleSupélec et sa fondation, et le Conseil Départemental de Moselle.&lt;/p&gt;
&lt;p&gt;Ce tutoriel est un guide rapide pour :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apprendre les commandes de base pour manipuler les fichiers stockés dans HDFS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apprendre à exécuter un programme Spark sur le DCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plus d’informations sont disponibles sur le &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;site officiel du DCE&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;présentation-du-dce&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Présentation du DCE&lt;/h1&gt;
&lt;p&gt;L’architecture du DCE est présentée en Figure &lt;a href=&#34;#fig:cluster-metz&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cluster-metz&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/plp/tutorials/spark-dce/reseau_dce.drawio.svg&#34; alt=&#34;Architecture du cluster (source : [documentation du DCE](https://dce.pages.centralesupelec.fr/01_cluster_overview/){target=&amp;quot;_blank&amp;quot;})&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Architecture du cluster (source : &lt;a href=&#34;https://dce.pages.centralesupelec.fr/01_cluster_overview/&#34; target=&#34;_blank&#34;&gt;documentation du DCE&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Dans ce cours, nous allons utiliser uniquement les noeuds CPU. Ces derniers sont divisés en deux groupes: le cluster Sarah et le cluster Kyle.&lt;/p&gt;
&lt;!--# Creating your working directory in HDFS

In this section, you&#39;ll be walked through the procedure to create a directory 
in HDFS that you&#39;ll use as your working directory in the lab sessions.

You user account is: ``asi1_X``, where X is between 1 and 
40.

In order to create your working directory in HDFS, type the following command in the terminal:

``
hdfs dfs -mkdir hdfs://sar01:9000/asi1/asi1_X
``

You can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/asi1/
with the following command:

``
hdfs dfs -ls hdfs://sar01:9000/asi1/
``
--&gt;
&lt;/div&gt;
&lt;div id=&#34;exécution-dun-programme-spark&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Exécution d’un programme Spark&lt;/h1&gt;
&lt;p&gt;Les datasets sont normalement disponibles dans le dossier &lt;code&gt;/data/&lt;/code&gt;
stocké dans HDFS.&lt;/p&gt;
&lt;p&gt;Saisissez la commande suivante pour afifcher le contenu du dossier :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/data&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;L’objectif ici est d’exécuter un code Spark pour compter le nombre d’occurrences de chaque mot dans le fichier &lt;code&gt;/data/sherlock.txt&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;/usr/users/dce-admin/vialle/DCE-Spark/template_wc.py&lt;/code&gt; vers votre dossier personnel en saisissant la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/dce-admin/vialle/DCE-Spark/template_wc.py ./wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Saisissez la commande &lt;code&gt;ls&lt;/code&gt; pour vérifier que le fichier &lt;code&gt;wc.py&lt;/code&gt; figure bien dans votre dossier personnel.
Ce fichier contient le code Python du programme.
Ce fichier devrait aussi apparaître dans l’explorateur de Visual Studio Code ; si cela n’est pas le cas, cliquez sur le bouton d’actualisation de l’explorateur.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ouvrez le fichier &lt;code&gt;wc.py&lt;/code&gt; dans Visual Studio Code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recherchez l’instruction suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
et remplacez-la avec la suivante :
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://sar01:9000/data/sherlock.txt&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cette instruction créera un RDD nommé &lt;code&gt;text_file&lt;/code&gt; avec le contenu du fichier.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;De la même manière, cherchez l’instruction suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;et remplacez-la par la suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://sar01:9000/asi1/asi1_X/sherlock.out&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cette instruction créera un dossier &lt;em&gt;sherlock.out&lt;/em&gt; qui contiendra les fichiers renvoyés par le programme.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exécutez le programme &lt;code&gt;wc.py&lt;/code&gt; à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Lorsque l’exécution est terminée, la sortie sera disponible dans le dossier
&lt;code&gt;sherlock.out&lt;/code&gt;. Pour le vérifier, exécutez la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/asi1/asi1_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Comme d’habitude, n’oubliez pas de remplacer asi1_X par votre nom d’utilisateur.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pour afficher le résultat, exécutez la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/asi1/asi1_X/sherlock.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Fichiers de sortie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous réexécutez le programme en spécifiant un fichier de sortie qui &lt;strong&gt;existe déjà&lt;/strong&gt;,
vous obtiendrez une erreur.
Si vous voulez vraiment écraser le fichier de sortie, vous devez d’abord le supprimer explicitement en tapant la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/asi1/asi1_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Premier programme Spark sur DCE</title>
      <link>/courses/bigdata-mds/labs/first-spark-program/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-mds/labs/first-spark-program/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- The link to the Edunao page where the students can choose a username to connect to the cluster. --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;Le &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;Data Centre d’Enseignement (DCE)&lt;/a&gt; est un ensemble de
ressources informatiques financées par l’Eurométropole de Metz
par l’Eurométropole de Metz, la Région Grand Est, CentraleSupélec et sa fondation, et le Conseil Départemental de Moselle.&lt;/p&gt;
&lt;p&gt;Ce tutoriel est un guide rapide pour :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apprendre les commandes de base pour manipuler les fichiers stockés dans HDFS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apprendre à exécuter un programme Spark sur le DCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plus d’informations sont disponibles sur le &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;site officiel du DCE&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;présentation-du-dce&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Présentation du DCE&lt;/h1&gt;
&lt;p&gt;L’architecture du DCE est présentée en Figure &lt;a href=&#34;#fig:cluster-metz&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cluster-metz&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/plp/tutorials/spark-dce/reseau_dce.drawio.svg&#34; alt=&#34;Architecture du cluster (source : [documentation du DCE](https://dce.pages.centralesupelec.fr/01_cluster_overview/){target=&amp;quot;_blank&amp;quot;})&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Architecture du cluster (source : &lt;a href=&#34;https://dce.pages.centralesupelec.fr/01_cluster_overview/&#34; target=&#34;_blank&#34;&gt;documentation du DCE&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Dans ce cours, nous allons utiliser uniquement les noeuds CPU. Ces derniers sont divisés en deux groupes: le cluster Sarah et le cluster Kyle.&lt;/p&gt;
&lt;!--# Creating your working directory in HDFS

In this section, you&#39;ll be walked through the procedure to create a directory 
in HDFS that you&#39;ll use as your working directory in the lab sessions.

You user account is: ``itpspark24_X``, where X is between 1 and 
25.

In order to create your working directory in HDFS, type the following command in the terminal:

``
hdfs dfs -mkdir hdfs://sar01:9000/itpspark24/itpspark24_X
``

You can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/itpspark24/
with the following command:

``
hdfs dfs -ls hdfs://sar01:9000/itpspark24/
``
--&gt;
&lt;/div&gt;
&lt;div id=&#34;exécution-dun-programme-spark&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Exécution d’un programme Spark&lt;/h1&gt;
&lt;p&gt;Les datasets sont normalement disponibles dans le dossier &lt;code&gt;/data/&lt;/code&gt;
stocké dans HDFS.&lt;/p&gt;
&lt;p&gt;Saisissez la commande suivante pour afifcher le contenu du dossier :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/data&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;L’objectif ici est d’exécuter un code Spark pour compter le nombre d’occurrences de chaque mot dans le fichier &lt;code&gt;/data/sherlock.txt&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;~cpu_vialle/DCE-Spark/template_wc.py&lt;/code&gt; vers votre dossier personnel en saisissant la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_wc.py ./wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Saisissez la commande &lt;code&gt;ls&lt;/code&gt; pour vérifier que le fichier &lt;code&gt;wc.py&lt;/code&gt; figure bien dans votre dossier personnel.
Ce fichier contient le code Python du programme.
Ce fichier devrait aussi apparaître dans l’explorateur de Visual Studio Code ; si cela n’est pas le cas, cliquez sur le bouton d’actualisation de l’explorateur.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ouvrez le fichier &lt;code&gt;wc.py&lt;/code&gt; dans Visual Studio Code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recherchez l’instruction suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
et remplacez-la avec la suivante :
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&#34;hdfs://sar01:9000/data/sherlock.txt&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cette instruction créera un RDD nommé &lt;code&gt;text_file&lt;/code&gt; avec le contenu du fichier.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;De la même manière, cherchez l’instruction suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://...&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;et remplacez-la par la suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&#34;hdfs://sar01:9000/itpspark24/itpspark24_X/sherlock.out&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cette instruction créera un dossier &lt;em&gt;sherlock.out&lt;/em&gt; qui contiendra les fichiers renvoyés par le programme.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exécutez le programme &lt;code&gt;wc.py&lt;/code&gt; à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Lorsque l’exécution est terminée, la sortie sera disponible dans le dossier
&lt;code&gt;sherlock.out&lt;/code&gt;. Pour le vérifier, exécutez la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/itpspark24/itpspark24_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Comme d’habitude, n’oubliez pas de remplacer itpspark24_X par votre nom d’utilisateur.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pour afficher le résultat, exécutez la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/itpspark24/itpspark24_X/sherlock.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Fichiers de sortie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous réexécutez le programme en spécifiant un fichier de sortie qui &lt;strong&gt;existe déjà&lt;/strong&gt;,
vous obtiendrez une erreur.
Si vous voulez vraiment écraser le fichier de sortie, vous devez d’abord le supprimer explicitement en tapant la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/itpspark24/itpspark24_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>DCE tutorial</title>
      <link>/courses/bigdata/documentation/cluster-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/documentation/cluster-connection/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- The link to the Edunao page where the students can choose a username to connect to the cluster. --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;The &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;Data Centre d’Enseignement (DCE)&lt;/a&gt; is a pool of
computing resources that have been financed
by the Eurométropole de Metz, Region Grand Est, CentraleSupélec and
its foundation, and the Conseil Départemental de Moselle.&lt;/p&gt;
&lt;p&gt;This tutorial is a quick guide to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learn how to connect to the DCE.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learn basic commands to manipulate files stored in HDFS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learn how to run a Spark program on the DCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More information are available on the &lt;a href=&#34;https://dce.pages.centralesupelec.fr/&#34; target=&#34;_blank&#34;&gt;DCE official website&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Overview&lt;/h1&gt;
&lt;p&gt;The architecture of the DCE is shown in figure &lt;a href=&#34;#fig:cluster-metz&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cluster-metz&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/plp/tutorials/spark-dce/reseau_dce.drawio.svg&#34; alt=&#34;Cluster architecture (image credit: [DCE documentation](https://dce.pages.centralesupelec.fr/01_cluster_overview/){target=&amp;quot;_blank&amp;quot;})&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Cluster architecture (image credit: &lt;a href=&#34;https://dce.pages.centralesupelec.fr/01_cluster_overview/&#34; target=&#34;_blank&#34;&gt;DCE documentation&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this tutorial we only use the CPU nodes. These are divided in two groups: the cluster Sarah and the cluster Kyle.&lt;/p&gt;
&lt;p&gt;To connect to the DCE you need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A valid username and a password. These are provided by your lab supervisor before the first lab session.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual Studio Code with the extension &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Remote Development&lt;/em&gt;&lt;/a&gt; installed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;connection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Connection&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v126412a91c76zkcgyu5/&#34; target=&#34;_blank&#34;&gt;Watch this video&lt;/a&gt;
to learn how to connect to the DCE with Visual Studio Code.&lt;/p&gt;
&lt;p&gt;Some of the steps shown in the video are explained below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When you connect to the DCE for the first time, or if you need to re-initialise the connection, you’ll be
prompted to enter a SSH command. Type the following (replace &lt;code&gt;your_username&lt;/code&gt; with the username that you received from your lab supervisor):&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;ssh your_username@chome.metz.supelec.fr&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After executing the command, you’ll be prompted to enter your password.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once you’re connected to &lt;code&gt;chome&lt;/code&gt;, open your home folder on &lt;code&gt;chome&lt;/code&gt; in Visual Studio Code, as shown in the video.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;allocating-resources-with-slurm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Allocating resources with slurm&lt;/h1&gt;
&lt;p&gt;You need to allocate computing ressources to run any jobs on the DCE.
The command to do so depends on whether you have a reservation for the resources or not.&lt;/p&gt;
&lt;div id=&#34;with-a-reservation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; With a reservation&lt;/h2&gt;
&lt;p&gt;The resources are reserved by your lab supervisor before any lab session.
Any reservation is given a code.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The code is only valid for a single lab session. When the lab session is over,
the reservation code might not work any longer.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The command to type to allocate the reserved resources is the following (replace &lt;code&gt;code&lt;/code&gt;
with the reservation code given by your lab supervisor ).&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;srun -N 1 -c 2 --reservation [code] -t 04:00:00  --pty bash&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;After running the command, you should be connected to one of the Kyle machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;without-a-reservation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Without a reservation&lt;/h2&gt;
&lt;p&gt;If you don’t have a reservation code, run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;srun -p cpu_inter -t 02:00:00 -N 1 --cpus-per-task=2 --pty bash&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you don’t have a reservation, please use the DCE only between 6PM-8AM in weekdays.
During the weekend, you can allocate resources on the DCE all day.&lt;/p&gt;
&lt;/div&gt;
&lt;!--# Creating your working directory in HDFS

In this section, you&#39;ll be walked through the procedure to create a directory 
in HDFS that you&#39;ll use as your working directory in the lab sessions.

You user account is: ``hpda_X``, where X is between 1 and 
8.

In order to create your working directory in HDFS, type the following command in the terminal:

``
hdfs dfs -mkdir hdfs://sar01:9000/hpda/hpda_X
``

You can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/hpda/
with the following command:

``
hdfs dfs -ls hdfs://sar01:9000/hpda/
``
--&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-a-spark-program&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Running a Spark program&lt;/h1&gt;
&lt;p&gt;The datasets are usually available under directory &lt;code&gt;/data/&lt;/code&gt;
stored in HDFS.&lt;/p&gt;
&lt;p&gt;Type the following command to look at the content of the directory:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/data&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The objective is to run a Spark program that counts the number of occurrences of each word
in file &lt;code&gt;/data/sherlock.txt&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy the file &lt;code&gt;~cpu_vialle/DCE-Spark/template_wc.py&lt;/code&gt; to your home directory by typing the
following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_wc.py ./wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Type the command &lt;code&gt;ls&lt;/code&gt; to verify that the file &lt;code&gt;wc.py&lt;/code&gt; is in your working directory.
This file contains the Python code of the program. The file should also appear in the
Explorer window in Visual Studio Code; if not, click on the Explorer refresh button.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;code&gt;wc.py&lt;/code&gt; in Visual Studio Code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the following instruction:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&amp;quot;hdfs://...&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
and replace it with the following:
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;text_file = sc.textFile(&amp;quot;hdfs://sar01:9000/data/sherlock.txt&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This will create an RDD named &lt;code&gt;text_file&lt;/code&gt; with the content of the file.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Similarly, locate the following instruction:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&amp;quot;hdfs://...&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;and replace it with the following instruction:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;counts.saveAsTextFile(&amp;quot;hdfs://sar01:9000/hpda/hpda_X/sherlock.out&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This will create an output directory &lt;em&gt;sherlock.out&lt;/em&gt; that will contain the files with the output of the program.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run the Python program &lt;code&gt;wc.py&lt;/code&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 wc.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;When the execution is over, the output will be available under the directory
&lt;code&gt;sherlock.out&lt;/code&gt;. To verify it, run the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls -h hdfs://sar01:9000/hpda/hpda_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As usual, remember to replace &lt;code&gt;hpda_X&lt;/code&gt; with your username.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In order to see the result, run the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/hpda/hpda_X/sherlock.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Output files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you rerun the program by specifying an output file that &lt;strong&gt;already exists&lt;/strong&gt;,
you’d get an error.
If you really want to overwrite the output file, you first need to remove it
explicitly by typing the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/hpda/hpda_X/sherlock.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project</title>
      <link>/courses/databases/exam/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/exam/project/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The goal of this project is
to assess your knowledge of the main
notions presented in classroom.&lt;/p&gt;
&lt;p&gt;The project must be implemented by students &lt;strong&gt;working in groups&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This project consists in designing a relational database
for the given application context,
importing the data of a given dataset into a PostgreSQL database and querying the data in SQL.&lt;/p&gt;
&lt;!-- ::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Submission of your project**

The submission site is [available at this link](https://centralesupelec.edunao.com/mod/assign/view.php?id=134588){target=&#34;_blank&#34;}.

Please note that:

* You must be member of a group to submit your work.
* Only one submission per group is possible. Two members of the same 
group cannot submit two different versions.

You must submit **three files**, please follow these instructions:

* A **report in PDF** containing the answers to 
all the questions and exercises, **except the SQL queries** (max 10 pages).
**Please name the file *groupXX_report.pdf***, where XX is your group number.

* A file containing the PostgreSQL database that you generated. 
**Please name the file *groupXX_db.sql***. 
To generate this file, please [watch this video](https://webtv.centralesupelec.fr/permalink/v126412a9e7aca26103s/){target=&#34;_blank&#34;}.

* A file with all the SQL queries. 
**Please name the file *groupXX_queries.sql***. 
Separate each query in the file by a **blank line**.

Example:

```
SELECT *
FROM Airport 

SELECT *
FROM Hotel 
```

**Each group must submit an original work. Edunao is equipped with an 
anti-plagiarism tool that is able to check the originality of each submission.
Submissions that are flagged as too similar by the tool are likely to be 
discarded and will not be evaluated.**

::: --&gt;
&lt;/div&gt;
&lt;div id=&#34;application-context&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Application context&lt;/h1&gt;
&lt;p&gt;We intend to manage the data of a travel reservation system
with clients all over the world.
Upon registration, customers are automatically given a numeric identifier
and they are asked to indicate their first and family names, their gender,
date of birth, a phone number, an email address and their country of residence.&lt;/p&gt;
&lt;p&gt;Any customer can book a trip that includes
the reservation of &lt;strong&gt;one or more flights&lt;/strong&gt; and, possibly, &lt;strong&gt;one or more hotels&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alice wants to fly from Paris,
France to New York City (NYC), USA
and she intends to stay in NYC for 10 days.
Her trip includes two flights: an outbound flight
from Paris to NYC and an inbound flight from NYC to Paris; and an hotel in NYC.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A flight is operated by an airline company,
of which the system keeps its name (e.g., British Airways),
the country where the airline is incorporated and, when available,
its IATA code (e.g., BA, a two-letter code identifying the airline),
its ICAO code (e.g., BAW, a three-letter code identifying the airline)
and alternate name or alias (e.g., British).&lt;/p&gt;
&lt;p&gt;A flight connects two airports,
each having a name (e.g., London Heathrow Airport), and,
possibly, a IATA (e.g., LHR) and ICAO code (e.g., EGLL);
an airport serves a specific location (e.g., London, UK) and its precise position
is given by its geographic coordinates (latitude and longitude).&lt;/p&gt;
&lt;p&gt;A flight connecting two airports at specific departure and arrival times
is identified by a flight number.
Two flights operated by two different airline companies cannot have the same flight number,
but the same flight number can denote two flights operated
by the same airline company on different days.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Emirates flight EK074 leaves Paris,
France at 10 AM and arrives at Dubai, UAE at 7:40 PM (regardless of the departure day).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For each flight booked by a customer,
the system keeps the seat number,
the travel class (e.g., economy or business),
the price and the date of the flight.
Usually, airlines include details on the type of aircraft
they plan to use on their flight schedules;
these details include the name of the aircraft (e.g., Boeing 787-8) and,
when available, the IATA code (e.g., 788, a unique three-letter identifier for the aircraft)
and the ICAO code (e.g., B788, a unique four-letter identifier for the aircraft).&lt;/p&gt;
&lt;p&gt;The system maintains a list of hotels,
with their names, addresses and an average review score,
which is a real number denoting the average grade assigned to
the hotel by its customers.
Customers can write a review for an hotel;
in which case the system stores the text of the review,
the date and its author.
When a customer books an hotel, the system keeps the price paid,
the check-in and check-out dates and whether the breakfast is included.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;design-of-a-relational-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Design of a relational database&lt;/h1&gt;
&lt;p&gt;You’ll now proceed to the definition of a relational database for our travel reservation system.
First, you need to define the &lt;strong&gt;conceptual schema&lt;/strong&gt;
and then you’ll define the tables that compose the database.&lt;/p&gt;
&lt;div id=&#34;the-conceptual-schema&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The conceptual schema&lt;/h2&gt;
&lt;p&gt;Before defining the logical schema of the database, &lt;strong&gt;answer the following questions&lt;/strong&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Can you use the name of the hotel as a primary key? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can you use the flight number as a primary key to identify a flight?
Justify your answer and, in case of a negative answer, propose a solution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Knowing that it is unlikely that two reviews have the same textual content,
would you use it as a primary key? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Knowing that the IATA code uniquely identifies an airport,
would you choose it as a primary key for the entity Airport? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Propose an Entity-Relationship diagram describing the conceptual
model of a relational database for the given application context.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specify all the attributes for each entity and relation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each entity, underline the attributes composing the primary key.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each relation, clearly indicate the minimum and maximum cardinality.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normalization&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Translate the conceptual schema into a collection of tables. For each table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Indicate its name and the names of the columns (but not their types).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Underline the columns that are part of the primary key.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Indicate the entity in the ER diagram to which the table corresponds.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Which &lt;strong&gt;normal form&lt;/strong&gt; are your tables in?&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
For each table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Indicate a &lt;strong&gt;minimal set of functional dependencies&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Indicate the normal form that the table satisfies. Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Normalize&lt;/strong&gt; your tables.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Normalize each table up to the &lt;strong&gt;3rd normal form (3NF)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The dataset&lt;/h2&gt;
&lt;p&gt;You’re given a dataset that you can download by clicking &lt;a href=&#34;/courses/databases/dataset.zip&#34;&gt;here&lt;/a&gt;.
The dataset consists of 7 CSV files:
&lt;em&gt;aircrafts.csv&lt;/em&gt;, &lt;em&gt;airlines.csv&lt;/em&gt;, &lt;em&gt;airports.csv&lt;/em&gt;,
&lt;em&gt;hotels.csv&lt;/em&gt;, &lt;em&gt;customers.csv&lt;/em&gt;,
&lt;em&gt;hotel_bookings.csv&lt;/em&gt;, &lt;em&gt;flight_bookings.csv&lt;/em&gt;.
The separator character in each file is the tab character (’).&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Take some time to look at the content
of the files to understand the structure that your tables will have.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You can open CSV files in Excel.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Look at the dataset.&lt;/strong&gt; Do you think that your logical model is a good one for the data that you’re given?
Don’t hesitate to review your choices before you proceed to creating the physical schema and the actual database.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-physical-schema&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The physical schema&lt;/h2&gt;
&lt;p&gt;You can now define the &lt;strong&gt;physical schema&lt;/strong&gt; of your database.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
Write the SQL code to create the tables. For each table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Indicate the &lt;strong&gt;primary key&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Indicate the &lt;strong&gt;foreign keys&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Indicate NOT NULL and UNIQUE constraints, if needed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To make sure you choose the right data types for the columns,
you can also check the values in the dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;database-creation-and-data-import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Database creation and data import&lt;/h2&gt;
&lt;p&gt;If you followed the normalization steps correctly, it is unlikely that
you can import the data directly from the given CSV files.&lt;/p&gt;
&lt;p&gt;You need a CSV file per table to import the data into your database.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Creating the CSV files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use Excel to rearrange the data from the input CSV files, so as
to create one CSV per table.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Make sure you save the files as CSV with UTF-8 encoding.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://webtv.centralesupelec.fr/videos/create-database-and-import-data-with-pgadmin/&#34; target=&#34;_blank&#34;&gt;Watch this video&lt;/a&gt; to learn how to create the database
in PostgreSQL and import the data.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
Create a PostgreSQL database with the tables that you defined in
the previous exercise. Import the data into the tables.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-queries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running queries&lt;/h2&gt;
&lt;p&gt;Write the following queries in SQL.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Get the average ticket price on Air France flights.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the number of customers by country.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the names of the airports in Paris, France.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the name of the cities (city and country name) with the most airports.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the name of the airline companies that use an Airbus A300.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the identifier, first and family names of all customers who flew to Sydney, Australia.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the identifier, name, city and country of the busiest airport (the one with more outbound and inbound flights).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the average price in the economy class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the average price in the business class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the name, city and country of the destination airport of french customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the destination cities (specify city and country name) preferred by women.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the destination cities (specify city and country name) preferred by men.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the number of customers by country flying to Paris.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the number of hotels by city.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determine the amount of money spent by Tatiana REZE in flights.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deploying to GKE</title>
      <link>/courses/cloudalbert/tutorials/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloudalbert/tutorials/project/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;The goal of this assignment is to deploy a containerized application
to &lt;strong&gt;Google Kubernetes Engine (GKE)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The lab consists of the following activities:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Get a Web application from GitHub.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete the &lt;em&gt;Dockerfile&lt;/em&gt; of the front-end.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy and test the application on your local Kubernetes cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the application to the Google Kubernetes Engine (GKE).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a report describing the application and the deployment procedure.
You’ll find detailed information about the report &lt;a href=&#34;#report&#34;&gt;in the last section&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Submission information&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You have a submit a &lt;strong&gt;single zip file&lt;/strong&gt; containing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The report in PDF format.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;em&gt;Dockerfile&lt;/em&gt; of the front-end.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Submission deadline:&lt;/strong&gt; 5 December 2024, 11:59 PM.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getapp&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Get the application&lt;/h1&gt;
&lt;p&gt;You can download the application
&lt;a href=&#34;https://github.com/gquercini/tripmeal-cloud&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have &lt;strong&gt;git&lt;/strong&gt; installed on your computer you can simply open a terminal and type
the following command:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/gquercini/tripmeal-cloud.git&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;dockerfile-of-the-front-end&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Dockerfile of the front-end&lt;/h1&gt;
&lt;p&gt;The application folder has the same structure as we have seen in the previous labs.
You should be able to understand the role of each file and folder.&lt;/p&gt;
&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The folder &lt;code&gt;web&lt;/code&gt; containing the front-end consists of Python code files, a &lt;code&gt;requirements.txt&lt;/code&gt; file, and two folders &lt;code&gt;static&lt;/code&gt; and &lt;code&gt;templates&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The files and folders contained in the folder &lt;code&gt;web&lt;/code&gt; need to be copied to a Docker image.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Locate the &lt;em&gt;Dockerfile&lt;/em&gt; of the front-end and complete it.
Please note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You need a Python 3.7 environment to run the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure you build a &lt;strong&gt;minimal image&lt;/strong&gt;; don’t add unnecessary files to the image. Also,
choose a base image to have a minimal Python environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure your Dockerfile is written so that it can take full advantage of the build cache.
Specifically, the dependencies should not be installed every time you build an image after a modification of the source code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;local-deployment&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Local deployment&lt;/h1&gt;
&lt;p&gt;You need to deploy and test the application &lt;em&gt;TripMeal&lt;/em&gt; on
a &lt;strong&gt;local Kubernetes cluster&lt;/strong&gt; (either Docker Desktop or Minikube).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Build the images of &lt;strong&gt;all services&lt;/strong&gt; of the application.
Prefix the name of each image with your username, such as:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;your-username/image-name:tag&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now, specify the names of the images in the file &lt;code&gt;tripmeal.yml&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Open the file &lt;code&gt;tripmeal.yml&lt;/code&gt; and add the names of the created Docker images
to the appropriate fields.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Execute the application by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Wait a minute and then check whether the state of the pods with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;If any of the pods is not ready yet, wait a moment and then type the previous command again.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
If one or more pods are in an &lt;strong&gt;error state&lt;/strong&gt;
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;Check the logs with the following commands:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;In case you get Exec format error&lt;/strong&gt;, add to your Dockerfile the following instruction:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;RUN chmod -x path_to_file_app&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;where &lt;code&gt;path_to_file_app&lt;/code&gt; is the path to the file &lt;code&gt;app.py&lt;/code&gt;
&lt;strong&gt;inside the image&lt;/strong&gt;. Rebuild the image.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After fixing the error, before executing the application again, run the following command
to delete all previously created resources:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl delete -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
External IP pending problem
&lt;/summary&gt;
&lt;p&gt;If the external IP address is in &lt;pending state&gt; for the frontend service,
delete the service, change the port number in the &lt;code&gt;.yml&lt;/code&gt; file and restart the service.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If all pods are in running state
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open a web browser window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the public IP and port of your application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confirm that the application works correctly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;After confirming that the application works, you can shut down the application.&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl delete -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-to-gke&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Deploy to GKE&lt;/h1&gt;
&lt;p&gt;In this section, you’re going to deploy your application to GKE.&lt;/p&gt;
&lt;p&gt;Here are the main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Make the code available to a &lt;strong&gt;public&lt;/strong&gt; repository so that it can be easily imported into your cloud environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the Google Cloud Shell to access your cloud environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the application.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The next subsections will describe these steps in greater detail.&lt;/p&gt;
&lt;div id=&#34;upload-the-code-to-github&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Upload the code to GitHub&lt;/h2&gt;
&lt;p&gt;The easiest way to import your application into your cloud environment is to
make it available to a Git repository, such as GitHub or any other Git service
for which you already have an account.
If necessary, the teacher will guide you in this step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-the-google-cloud-shell&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Open the Google Cloud Shell&lt;/h2&gt;
&lt;p&gt;Open the &lt;a href=&#34;https://console.cloud.google.com/welcome?project=gq-cloud-computing&#34; target=&#34;_blank&#34;&gt;Google Cloud Console&lt;/a&gt;.
Make sure you log in with your school account.&lt;/p&gt;
&lt;p&gt;You’ll see in this page an overview of the project created to let you deploy the application to GKE.
In particular, you should take note of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the project number (&lt;em&gt;124930651186&lt;/em&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the project ID (&lt;em&gt;gq-cloud-computing&lt;/em&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You’ll need this information later when you’ll set up the deployment.&lt;/p&gt;
&lt;p&gt;In the top-right corner of the window, you should see a small icon with a prompt (&lt;em&gt;&amp;gt;&lt;/em&gt;).
When you click on it, a Google Cloud Shell will open at the bottom of the screen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It may take few minutes for the shell to up and running.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Deployment&lt;/h2&gt;
&lt;p&gt;You’ll now have to type few commands in the Google Cloud Shell to deploy the application to Kubernetes.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Set the project in the shell session&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Set your Cloud platform project in this session.&lt;/p&gt;
&lt;p&gt;First, create an environment variable PROJECT_ID with the following command:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;export PROJECT_ID=gq-cloud-computing&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;Then configure the project ID with the following command:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud config set project ${PROJECT_ID}&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-repository&#34; class=&#34;section level3&#34; number=&#34;4.3.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1&lt;/span&gt; Create a repository&lt;/h3&gt;
&lt;p&gt;Create a repository where you’ll be storing the Docker images of your application.
The repository is stored in the &lt;em&gt;Artifact Registry&lt;/em&gt;, the Google container registry.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create an environment variable PROJECT_NUMBER whose value is 124930651186.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create an environment variable REPO_NAME whose value is a name of your choice for the container
repository.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create an environment variable APP_NAME whose value is a name of your choice for the
application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create an environment variable APP_VERSION whose value is the version of the app. You can choose
&lt;code&gt;1.0&lt;/code&gt; or &lt;code&gt;latest&lt;/code&gt;, or any other value of your choice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create your repository with the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;gcloud artifacts repositories create ${REPO_NAME} \
   --repository-format=docker \
   --location=europe-west2 \
   --description=&amp;quot;Docker repository&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-docker-images&#34; class=&#34;section level3&#34; number=&#34;4.3.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.2&lt;/span&gt; Building the Docker images&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Download the application from your Git repository with the command &lt;code&gt;git clone&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build and tag the Docker images of your application.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Names of the images&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You have to give your images names that are consistent with the names of images stored
in the container repository.&lt;/p&gt;
&lt;p&gt;In the case of your repository, the name of an image should be as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;europe-west2-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/${APP_NAME}-image-name:${APP_VERSION}&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note: If prompted, authorize Cloud Shell to make Google Cloud API calls.&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Verify that the images have been created with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Add IAM policy bindings (basically, some authorization configurations) to your service account:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;gcloud artifacts repositories add-iam-policy-binding ${REPO_NAME} \
    --location=europe-west2 \
    --member=serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com \
    --role=&amp;quot;roles/artifactregistry.reader&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-the-images-to-the-artifact-registry&#34; class=&#34;section level3&#34; number=&#34;4.3.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.3&lt;/span&gt; Pushing the images to the Artifact registry&lt;/h3&gt;
&lt;p&gt;You must upload the images to a registry so that your GKE cluster can download and run the images.
Here we use the repository that we created above.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Configure the &lt;code&gt;docker&lt;/code&gt; command to authenticate to the Artifact Registry.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud auth configure-docker europe-west2-docker.pkg.dev&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Push the images that you created with the command &lt;code&gt;docker push&lt;/code&gt;. Make
sure that you use the correct image names, as discussed above.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-gke-cluster&#34; class=&#34;section level3&#34; number=&#34;4.3.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.4&lt;/span&gt; Creating a GKE cluster&lt;/h3&gt;
&lt;p&gt;You need now to create a Kubernetes cluster using the Google Kubernetes Engine.
When you create a cluster, you’ll also specify the option to create the network
over which the cluster nodes communicate.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Define a new environment variable CLUSTER_NAME whose value is a name of your choice for the cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define a new environment variable NET_NAME whose value is a name of your choice for the network.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set your compute engine region.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud config set compute/region europe-west2&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create the cluster with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;gcloud container clusters create-auto ${CLUSTER_NAME} \
  --create-subnetwork name=${NET_NAME} \
  --no-enable-master-authorized-networks \
  --enable-private-nodes&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The creation of the cluster might take &lt;strong&gt;several minutes&lt;/strong&gt;.
You may want to get a coffee :)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-the-application&#34; class=&#34;section level3&#34; number=&#34;4.3.5&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.5&lt;/span&gt; Deploy the application&lt;/h3&gt;
&lt;p&gt;You’re now ready to deploy your application!&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In your GitHub repository, modify the file &lt;code&gt;tripmeal.yml&lt;/code&gt; to change the names of the images.
Use the names of the images that you pushed to the the artifact registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Google Cloud Shell, move to the directory that contains your application with the command &lt;code&gt;cd&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the command &lt;code&gt;git pull&lt;/code&gt; to get the updated version of file &lt;code&gt;tripmeal.yml&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ensure that you are connected to your GKE cluster with the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud container clusters get-credentials $CLUSTER_NAME --region europe-west2&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type the following command to deploy your application:&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Wait 30 seconds and then type the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;You should get some information about the state of the deployment.
Keep typing this command until the state of all pods is READY.&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Get the public IP address where your application is available and the port number
and type them in your browser to confirm that you can access and use the application.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;video-of-the-application-use&#34; class=&#34;section level2&#34; number=&#34;4.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Video of the application use&lt;/h2&gt;
&lt;p&gt;Record &lt;strong&gt;a video&lt;/strong&gt; while using the application.
In particular shows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;that you can connect to the website.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;you can create a user.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;you can log in with as new user.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;you can create a recipe.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;you can log out and log in back again.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;shut-down-the-application&#34; class=&#34;section level2&#34; number=&#34;4.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.5&lt;/span&gt; Shut down the application&lt;/h2&gt;
&lt;p&gt;When you’re done using the application, &lt;strong&gt;shut it down and remove all resources&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is very important. Follow these instructions carefully to remove all your resources.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Shut down the application&lt;/strong&gt; by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;kubectl delete -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Delete the Kubernetes cluster&lt;/strong&gt; by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud container clusters delete $CLUSTER_NAME --region europe-west2 --ansync&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Delete ALL the images from the container registry&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud artifacts docker images delete NAME_OF_IMAGE --delete-tags --quiet&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Delete the Docker repository&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;gcloud artifacts repositories delete ${REPO_NAME} --location=europe-west2 --async&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;report&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Report&lt;/h1&gt;
&lt;p&gt;In the report you need to include the following elements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A high-level description of the application. What is the application intended for?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The architecture of the application. How are files organized? How many services is the application composed of?
What is the meaning of each file?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The technologies used in the application. Which programming languages are used for each service?
Which libraries and databases?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In which file can you get the information about the port number of the front end?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the file &lt;code&gt;tripmeal.yml&lt;/code&gt;. Can you explain the content of this file? Can you explain the meaning
of each object created in this file? In particular explain: what is a service and a deployment?
What is a stateful set? Why is a deployment used and for which service?
Why is a stateful set used and for which service?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don’t hesitate to add figures to your report, should you need them to better explain the different notions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Etude de cas: DHCP</title>
      <link>/courses/network/labs/dhcp-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/network/labs/dhcp-lab/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://gaia.cs.umass.edu/kurose_ross/wireshark.php&#34; target=&#34;_blank&#34;&gt;Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dans ce TP,
nous allons jeter un coup d’œil rapide au protocole DHCP.
Rappelons que le DHCP est largement utilisé dans les réseaux locaux câblés
et sans fil des entreprises, des universités et
des particuliers pour attribuer dynamiquement des adresses IP aux hôtes,
ainsi que pour configurer d’autres informations relatives au réseau.&lt;/p&gt;
&lt;div id=&#34;utilisation-de-dhcp-pour-configurer-une-interface-réseau&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Utilisation de DHCP pour configurer une interface réseau&lt;/h1&gt;
&lt;p&gt;Dans cette section, nous forçons notre ordinateur à reconfigurer l’une de ses interfaces réseau à l’aide de DHCP.
La manière pour ce faire dépend du système d’exploitation utilisé.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Si vous travaillez sous Windows&lt;/summary&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Ouvrez une fenêtre de PowerShell et entrez la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;pre&gt;&lt;code&gt;ipconfig /release&lt;/code&gt;&lt;/pre&gt;
&lt;/center&gt;
&lt;p&gt;Cette commande permet à votre PC d’abandonner son adresse IP.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans PowerShell, entrez la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;pre&gt;&lt;code&gt;ipconfig /renew&lt;/code&gt;&lt;/pre&gt;
&lt;/center&gt;
&lt;p&gt;Le protocole DHCP demandera et recevra alors une adresse IP et d’autres informations d’un serveur DHCP.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Après avoir attendu quelques secondes, arrêtez la capture Wireshark.&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Si vous travaillez sous macOS&lt;/summary&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans les paramètres réseau sélectionnez “Avancé” puis “Renouveler le bail dhcp”.
Le protocole DHCP demandera et recevra une adresse IP et d’autres informations du serveur DHCP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Après avoir attendu quelques secondes, arrêtez la capture Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Si vous travaillez sous Linux&lt;/summary&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dans une fenêtre de terminal, entrez les commandes suivantes :&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;pre&gt;&lt;code&gt;sudo ip addr flush en0 
sudo dhclient -r &lt;/code&gt;&lt;/pre&gt;
&lt;/center&gt;
&lt;p&gt;Cette commande supprimera l’adresse IP existante de l’interface et libérera tous les baux d’adresses DHCP existants.
&lt;code&gt;en0&lt;/code&gt; (dans cet exemple) est l’interface sur laquelle vous souhaitez capturer des paquets à l’aide de Wireshark.
Vous pouvez facilement trouver la liste des noms d’interface dans Wireshark en choisissant Capture -&amp;gt; Options.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Démarrez Wireshark, en capturant des paquets dans l’interface que vous avez déconfigurée à l’étape 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans la fenêtre de terminal, entrez la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;pre&gt;&lt;code&gt;sudo dhclient en0&lt;/code&gt;&lt;/pre&gt;
&lt;/center&gt;
&lt;p&gt;où, comme ci-dessus, &lt;code&gt;en0&lt;/code&gt; est l’interface sur laquelle vous capturez actuellement des paquets.
Le protocole DHCP demandera et recevra alors une adresse IP et d’autres informations du serveur DHCP.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Après avoir attendu quelques secondes, arrêtez la capture Wireshark.&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;p&gt;Après avoir arrêté la capture Wireshark à l’étape 4,
vous devriez jeter un coup d’œil dans votre fenêtre Wireshark pour vous assurer que vous avez bien capturé les paquets que nous recherchons.&lt;br /&gt;
Entrez &lt;em&gt;dhcp&lt;/em&gt; dans le champ du filtre d’affichage pour n’afficher que les paquets DHCP.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;questions-sur-dhcp&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Questions sur DHCP&lt;/h1&gt;
&lt;p&gt;Commençons par examiner le message &lt;code&gt;DHCP Discover&lt;/code&gt;.&lt;br /&gt;
Localisez le datagramme IP contenant le premier message &lt;code&gt;Discover&lt;/code&gt; dans votre trace.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Ce message DHCP Discover est-il envoyé en utilisant UDP ou TCP comme protocole de transport sous-jacent ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP source utilisée dans le datagramme IP contenant le message &lt;code&gt;Discover&lt;/code&gt; ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez pourquoi.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de destination utilisée dans le datagramme contenant le message &lt;code&gt;Discover&lt;/code&gt; ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Quelle est la valeur du champ ID de transaction de ce message DHCP Discover ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;
Examinez maintenant le champ des options du message DHCP Discover. Quelles sont
les informations (en plus d’une adresse IP) que le client
suggère ou demande de recevoir du serveur DHCP dans le cadre de cette transaction DHCP ?&lt;/p&gt;
&lt;p&gt;Pour connaître la signification des options,
vous pouvez &lt;a href=&#34;https://www.iana.org/assignments/bootp-dhcp-parameters/bootp-dhcp-parameters.xhtml&#34; target=&#34;_blank&#34;&gt;consulter cette page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Examinons maintenant le message &lt;code&gt;DHCP Offer&lt;/code&gt;.&lt;br /&gt;
Localisez dans votre trace le datagramme IP contenant le message &lt;code&gt;DHCP Offer&lt;/code&gt; qui a été envoyé par un serveur DHCP
en réponse au message &lt;code&gt;DHCP Discover&lt;/code&gt; que vous avez étudié dans les questions précédentes.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;
Comment savez-vous que ce message &lt;code&gt;Offer&lt;/code&gt; est envoyé en réponse au message &lt;code&gt;DHCP Discover&lt;/code&gt; que vous avez étudié dans les questions précédentes ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP source utilisée dans le datagramme IP contenant le message Offer ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de destination utilisée dans le datagramme contenant le message d’offre ?
Cette adresse présente-t-elle des particularités ? Expliquez.&lt;/p&gt;
&lt;p&gt;Regardez attentivement votre trace. La réponse à cette question peut différer de ce que vous avez vu en cours.&lt;br /&gt;
Si vous voulez vraiment approfondir cette question, consultez la &lt;a href=&#34;https://www.ietf.org/rfc/rfc2131.txt&#34; target=&#34;_blank&#34;&gt;RFC 2131, page 24&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.9  &lt;/strong&gt;&lt;/span&gt;
Examinez maintenant le champ des options dans le message d’offre DHCP.
Quelles sont les informations que le serveur DHCP fournit au client DHCP dans le message d’offre DHCP ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Il semblerait qu’une fois le message d’offre DHCP reçu,
le client dispose de toutes les informations dont il a besoin pour continuer.&lt;br /&gt;
Cependant, le client peut avoir reçu des offres de plusieurs serveurs DHCP
et une deuxième phase est donc nécessaire,
avec deux autres messages obligatoires - le message &lt;code&gt;DHCP Request&lt;/code&gt; du client
au serveur et le message &lt;code&gt;DHCP ACK&lt;/code&gt; du serveur au client.
Mais au moins, le client sait qu’il existe au moins un serveur DHCP !&lt;/p&gt;
&lt;p&gt;Examinons le message &lt;code&gt;DHCP Request&lt;/code&gt;,
en nous rappelant que, bien que nous ayons déjà vu un message &lt;code&gt;Discover&lt;/code&gt; dans notre trace,
ce n’est pas toujours le cas lorsqu’un message &lt;code&gt;DHCP Request&lt;/code&gt; est envoyé.&lt;/p&gt;
&lt;p&gt;Localisez le datagramme IP contenant le premier message DHCP Request dans votre trace, et répondez aux questions suivantes.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.10  &lt;/strong&gt;&lt;/span&gt;
Quel est le numéro de port source UDP dans le datagramme IP contenant le premier message DHCP Request de votre trace ?&lt;br /&gt;
Quel est le numéro de port de destination UDP utilisé ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.11  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP source dans le datagramme IP contenant ce message de requête ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.12  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de destination utilisée dans le datagramme contenant ce message ? Cette adresse présente-t-elle des particularités ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.13  &lt;/strong&gt;&lt;/span&gt;
Quelle est la valeur du champ ID de transaction de ce message DHCP Request ? Correspond-elle aux ID de transaction des messages Discover et Offer précédents ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.14  &lt;/strong&gt;&lt;/span&gt;
Examinez maintenant le champ des options dans le message &lt;code&gt;DHCP Discover&lt;/code&gt; et regardez attentivement
la “Parameter Request List”.
La &lt;a href=&#34;https://www.ietf.org/rfc/rfc2131.txt&#34; target=&#34;_blank&#34;&gt;RFC 2131, page 24&lt;/a&gt; indique
que le client peut informer le serveur des paramètres de configuration qui l’intéressent en incluant
l’option “parameter request list”. La partie données de cette option énumère explicitement les options demandées.
Quelles différences voyez-vous entre les entrées de l’option “parameter request list” dans ce message &lt;code&gt;Request&lt;/code&gt; et la même option de liste dans le message &lt;code&gt;Discover&lt;/code&gt; précédent ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Identifiez le datagramme IP contenant le premier message DHCP ACK
et répondez aux questions suivantes.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.15  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP source du datagramme IP contenant ce message ACK ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.16  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de destination utilisée dans le datagramme contenant ce message ACK. Cette adresse a-t-elle quelque chose de particulier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.17  &lt;/strong&gt;&lt;/span&gt;
Quel est le nom de la proprieté du message DHCP ACK qui contient l’adresse IP assignée au client ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.18  &lt;/strong&gt;&lt;/span&gt;
Pour combien de temps (le “bail”) le serveur DHPC a-t-il attribué cette adresse IP au client ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.19  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP (renvoyée par le serveur DHCP au client DHCP dans ce message DHCP ACK)
de la passerelle par défaut ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data modeling in Cassandra</title>
      <link>/courses/gadexed/tutorials/cassandra-data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/cassandra-data-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Sakila management wants us to migrate their database to Cassandra,
they need to compare how Cassandra would compare against MongoDB.
To this extent, your first task is to think of a data model for the new database.&lt;/p&gt;
&lt;p&gt;When designing your data model, you’ll two follow two basic rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Your data model should distribute the data evenly across the nodes of the cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each frequent query, the number of partition to read should be as minimum as possible.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Analyze the Sakila business domain and propose a series of queries
that the Sakila administration might want to run against the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Given the queries that you proposed in the first exercise, draw
a workflow of the application that the Sakila management
will use to query the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
Based on the application workflow, draw the Chebotko diagram (the logical model)
of the Sakila database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--  --&gt;
</description>
    </item>
    
    <item>
      <title>Queries in Cassandra</title>
      <link>/courses/gadexed/tutorials/cassandra-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/cassandra-queries/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;setting-up-the-environment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting up the environment&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download Cassandra &lt;a href=&#34;https://dlcdn.apache.org/cassandra/4.0.4/apache-cassandra-4.0.4-bin.tar.gz&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;from this address&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Move the downloaded archive file to a folder of your choice (other than downloads).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extract the archive. This will create a folder, containing all the files with the Cassandra server.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open a terminal and go the the Cassandra folder.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Run the Cassandra server&lt;/strong&gt; with the following command &lt;code&gt;./bin/cassandra -f&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once the initialization is complete, open a new terminal and go again to the Cassandra folder.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the CQL shell, by typing the following command: &lt;code&gt;./bin/cqlsh&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once you enter the CQL shell, you can start interacting with the Cassandra server to create a new database.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;import-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Import the data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-8/sakila-data-cassandra.zip&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;data here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extract the downloaded archive. This will create a new folder containing the data. Let’s refer to this folder as DATA_FOLDER.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go back to the CQL shell and type the following command &lt;code&gt;source &#39;DATAFOLDER/sakila.cql&#39;&lt;/code&gt; (replace DATAFOLDER with the full path to the data folder). This command will
create the sakila keyspace (database).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the CQL shell, type the command &lt;code&gt;use sakila;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We have 11 tables in this database. You can see the definition with the command &lt;code&gt;describe sakila&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As the last step, we need to import the data. No luck here, we need to import the data table by table with the following commands:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;COPY sakila.actors_by_film FROM &amp;#39;DATAFOLDER/actors_by_film.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.categories_by_film FROM &amp;#39;DATAFOLDER/categories_by_film.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.customers FROM &amp;#39;DATAFOLDER/customers.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.film FROM &amp;#39;DATAFOLDER/film.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.inventory FROM &amp;#39;DATAFOLDER/inventory.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.rentals_by_customer FROM &amp;#39;DATAFOLDER/rentals_by_customer.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.rentals_by_film FROM &amp;#39;DATAFOLDER/rentals_by_film.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.rentals_by_staff FROM &amp;#39;DATAFOLDER/rentals_by_staff.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.rentals FROM &amp;#39;DATAFOLDER/rentals.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.staff FROM &amp;#39;DATAFOLDER/staff.csv&amp;#39; WITH HEADER = TRUE ;

COPY sakila.store FROM &amp;#39;DATAFOLDER/store.csv&amp;#39; WITH HEADER = TRUE ;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;If during the import you get the error ‘too many open files’ close the shell with exit and start it again.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Queries&lt;/h1&gt;
&lt;p&gt;The CQL language used by Cassandra is highly similar to SQL, but there are some key differences.
Here we analyze few of them.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Consider the table &lt;TT&gt;rentals_by_customer&lt;/TT&gt; and try some queries
that use (not all in the same query):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The WHERE condition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aggregating functions and the GROUP BY clause.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The SELECT JSON statement.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to refer to the &lt;a href=&#34;https://cassandra.apache.org/doc/latest/cassandra/cql/dml.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;CQL documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data warehousing</title>
      <link>/courses/gadexed/tutorials/data-warehouse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/data-warehouse/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you will learn basic modeling strategies for a data warehouse.&lt;/p&gt;
&lt;div id=&#34;use-case-scenario&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Use case scenario&lt;/h1&gt;
&lt;p&gt;We intend to set up a data warehouse for the Sakila database that we used in Tutorial 3.&lt;/p&gt;
&lt;p&gt;We suppose that a DVD rental store chain (let’s call it Sakila)
maintains several operational database systems
at their different stores, and the management intends to have a single
point of truth, in order to check business trends. Hence, the idea of
setting up a data warehouse.&lt;/p&gt;
&lt;p&gt;In order to model a schema for the Sakila data warehouse, we need to identify
dimensions and facts.
To this extent, we first identify the business questions that the Sakila
management wants to be answered.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
List some of the business questions that the Sakila management
would like to answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Different sets of questions are possible. Here are some examples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which store generates the most revenue?&lt;/li&gt;
&lt;li&gt;Which customers have rented the most in the past year? (maybe to award fidelity points).&lt;/li&gt;
&lt;li&gt;Which customers have rented the least in the past year? (maybe to offer some discounts to encourage
the customer to rent again).&lt;/li&gt;
&lt;li&gt;Which staffer has processed the most of the rentals in the past year? (staffer of the year).&lt;/li&gt;
&lt;li&gt;Which staffer has processed the least of the rentals in the past year? (maybe a layoff in sight?).&lt;/li&gt;
&lt;li&gt;Which month has the highest revenue? By country, by store?&lt;/li&gt;
&lt;li&gt;Which country generates the least revenue? (is there any concurrent out there that might be soon a threat
in other countries too?)&lt;/li&gt;
&lt;li&gt;Which categories of films are most popular?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We recall here the conceptual model of the Sakila database.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The conceptual schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The conceptual schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Based on the business questions identified in the first exercise,
can you tell what the fact and the dimension tables are?&lt;/p&gt;
&lt;p&gt;Recall that the fact table should contain measures and the dimension tables
should contain the context of the facts.
The dimension table should answer the following questions about a fact: who, what,
when, where.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;From the questions that we identified in the previous exercise,
it is clear that we intend to use the data warehouse to
answer questions about the &lt;strong&gt;rentals&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, our fact table will be &lt;TT&gt;fact_rental&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;As for the dimensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Question 1. suggests the use of the dimension store (where).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Questions 2., 3. suggest the use of the dimension customer (who).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Questions 4., 5. suggest the use of the dimension staff (who).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Question 8. suggests the use of the dimension film (what).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All questions suggests the use of a time dimension (when).&lt;/p&gt;
&lt;p&gt;The time dimension is virtually always present in a data warehouse.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensional-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dimensional modeling&lt;/h1&gt;
&lt;p&gt;Now that we identified the fact and the dimension tables, we incrementally draw the
star diagram for the Sakila data warehouse.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
Draw a first sketch of the star diagram.
Do not specify any attributes in the tables.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-4/sakila-dw-sketch.png&#34; alt=&#34;First sketch of the Sakila data warehouse&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Identify the primary key of the fact table.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The primary key of the fact table is composed of
all the attributes that refer to the dimensions.
So, we write an attribute for each dimension:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;staff_pk&lt;/TT&gt;, foreign key to the dimension &lt;TT&gt;dim_staff&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;film_pk&lt;/TT&gt;, foreign key to the dimension &lt;TT&gt;dim_film&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;period_pk&lt;/TT&gt;, foreign key to the dimension &lt;TT&gt;dim_period&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;store_pk&lt;/TT&gt;, foreign key to the dimension &lt;TT&gt;dim_store&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;customer_pk&lt;/TT&gt;, foreign key to the dimension &lt;TT&gt;dim_customer&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the new schema.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-4/sakila-dw-sketch-keys.png&#34; alt=&#34;First sketch of the Sakila data warehouse with PKs&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
For each dimension table, there is a corresponding table in
the operational database.&lt;/p&gt;
&lt;p&gt;Discuss which attributes you would add to the dimension tables.&lt;/p&gt;
&lt;p&gt;In particular, consider the following points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Are you going to add to a dimension table the&lt;br /&gt;
primary key of the corresponding operational table?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are you going to add to a dimension table attributes that are not
part of the corresponding table?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Each dimension table already has a primary key. However, this is the surrogate
key that identifies each entity in the dimension table.
The business key that is in the corresponding operational table is still
a valuable attribute to add.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, for example, in the table dim_film we’ll add the attribute film_id that is the
primary key in the operational table &lt;TT&gt;film&lt;/TT&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In each dimension table, we typically add all the attributes that are in the
corresponding operational table. However, we also add attributes that
are in the linked tables, if they’re necessary to our analysis.
For example, in the operational database the category of a film is&lt;br /&gt;
in a separate table &lt;TT&gt;film_category&lt;/TT&gt; (as a result of the normalization process).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we decide to integrate these dimensions and keep a normalized schema,
we obtain a &lt;strong&gt;snowflake schema&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-4/sakila-dw-snowflake.png&#34; alt=&#34;Snowflake schema of Sakila data warehouse&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, in a data warehouse we tend to denormalize the dimension tables, to avoid
to incur the cost of joining tables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-4/sakila-dw-sketch-dim-full.png&#34; alt=&#34;Sakila data warehouse with attributes in the dimension tables&#34;&gt;&lt;/p&gt;
&lt;p&gt;One last remark on the attribute &lt;TT&gt;film_categories&lt;/TT&gt; in table &lt;TT&gt;dim_film&lt;/TT&gt;.
The value of this attribute is a list.
One might want to avoid this by using a &lt;strong&gt;one-hot encoding&lt;/strong&gt;.
In other words, if the set of all possible categories is small, we can have one boolean attribute
for each category; a &lt;em&gt;True&lt;/em&gt; value would indicate that the film is in the corresponding category.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
In our schema, the fact table is &lt;strong&gt;factless&lt;/strong&gt;: it only contains a primary key with no
measurements.&lt;/p&gt;
&lt;p&gt;Which measurements would you introduce?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;It all depends on the queries that we intend to ask.
In the queries that we identified at the beginning of the tutorial,
we were interested in the revenue generated by the rentals.
Also, we might want to keep track of the number of rentals of
a given film by a given customer in the given period, and the number of returns.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-4/sakila-dw-final.png&#34; alt=&#34;Final schema of the Sakila data warehouse&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data modeling in MongoDB</title>
      <link>/courses/gadexed/tutorials/mongodb-data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/mongodb-data-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;use-case-scenario&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Use case scenario&lt;/h1&gt;
&lt;p&gt;The Sakila database is serving an increasing number of queries from staff and customers
around the world.
A single monolithic database is not sufficient anymore to serve all
the requests and the company is thinking of distributing the database across
several servers (horizontal scalability).
However, a relational database does not handle horizontal scalability very well, due
to the fact that the data is scattered across numerous tables, as are result of the normalization
process.
Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to
MongoDB.&lt;/p&gt;
&lt;p&gt;For the migration to happen, it is necessary to conceive a suitable data model.
From the first discussions with the Sakila management,
you quickly understand that one of the main use of the database is to
manage (add, update and read) rental information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-of-the-existing-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis of the existing model&lt;/h1&gt;
&lt;p&gt;The existing data model is recalled in the following figure.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The conceptual schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The conceptual schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Determine the cardinality (one-to-one, one-to-many, many-to-many)
of each of the relationships
in which the entity &lt;TT&gt;Rental&lt;/TT&gt; is involved.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;N.B.&lt;/strong&gt; Don’t hesitate to look at the attributes of each entity
in the existing PostgreSQL database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The entity &lt;TT&gt;Rental&lt;/TT&gt; has a relationship with the following tables;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;Inventory&lt;/TT&gt;. A rental refers to a single inventory (essentially, a copy of a DVD).
An inventory might be part of several rentals. This is a &lt;strong&gt;one-to-many relationship&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;Staff&lt;/TT&gt;. A rental is taken care of by a single staff member,
A staff member might take care of several rentals. This is a &lt;strong&gt;one-to-many relationship&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;Customer&lt;/TT&gt;. A rental is made by a single customer. A customer might make
several rentals. This is a &lt;strong&gt;one-to-many relationship&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;TT&gt;Payment&lt;/TT&gt;. A payment is relative to a single rental. A rental is associated with a single
payment. This is a &lt;strong&gt;one-to-one relationship&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Look at the tables &lt;TT&gt;Rental&lt;/TT&gt; and &lt;TT&gt;Customer&lt;/TT&gt; in the PostgreSQL
Sakila database.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimate the size of a row in bytes in both tables.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following considerations will help you in the task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The storage size of a numeric data type is clearly indicated in &lt;a href=&#34;https://www.postgresql.org/docs/8.1/datatype.html#DATATYPE-NUMERIC&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;the PostgreSQL documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The storage size of date/time types is clearly indicated in &lt;a href=&#34;https://www.postgresql.org/docs/8.1/datatype-datetime.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;the PostgreSQL documentation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The columns with type &lt;TT&gt;text&lt;/TT&gt; hold UTF-8 characters. We assume that each character is 1 byte long (although some characters might need more than 1 byte).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The columns with type &lt;TT&gt;boolean&lt;/TT&gt; needs 1 byte storage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We assume that an email address is 25 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We assume that a last name is 7 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We assume that a first name is 6 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In both tables we ignore the columns &lt;TT&gt;last_update&lt;/TT&gt; and &lt;TT&gt;create_date&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;In table &lt;TT&gt;Customer&lt;/TT&gt; we ignore the column &lt;TT&gt;active&lt;/TT&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;For the table &lt;TT&gt;Rental&lt;/TT&gt; we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;rental_id&lt;/TT&gt;, &lt;TT&gt;integer&lt;/TT&gt;, &lt;strong&gt;4 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;rental_date&lt;/TT&gt;, &lt;TT&gt;timestamp with time zone&lt;/TT&gt;, &lt;strong&gt;8 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;inventory_id&lt;/TT&gt;, &lt;TT&gt;integer&lt;/TT&gt;, &lt;strong&gt;4 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;customer_id&lt;/TT&gt;, &lt;TT&gt;smallint&lt;/TT&gt;, &lt;strong&gt;2 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;return_date&lt;/TT&gt;, &lt;TT&gt;timestamp with time zone&lt;/TT&gt;, &lt;strong&gt;8 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;staff_id&lt;/TT&gt;, &lt;TT&gt;smallint&lt;/TT&gt;, &lt;strong&gt;2 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In total, a row in table &lt;TT&gt;Rental&lt;/TT&gt; needs &lt;strong&gt;28 bytes&lt;/strong&gt; of storage.&lt;/p&gt;
&lt;p&gt;For the table &lt;TT&gt;Customer&lt;/TT&gt; we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;customer_id&lt;/TT&gt;, &lt;TT&gt;integer&lt;/TT&gt;, &lt;strong&gt;4 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;store_id&lt;/TT&gt;, &lt;TT&gt;smallint&lt;/TT&gt;, &lt;strong&gt;2 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;first_name&lt;/TT&gt;, &lt;TT&gt;text&lt;/TT&gt;, &lt;strong&gt;6 bytes&lt;/strong&gt; (average).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;last_name&lt;/TT&gt;, &lt;TT&gt;text&lt;/TT&gt;, &lt;strong&gt;7 bytes&lt;/strong&gt; (average).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;email&lt;/TT&gt;, &lt;TT&gt;text&lt;/TT&gt;, &lt;strong&gt;25 bytes&lt;/strong&gt; (average).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;address_id&lt;/TT&gt;, &lt;TT&gt;smallint&lt;/TT&gt;, &lt;strong&gt;2 bytes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Column &lt;TT&gt;activebool&lt;/TT&gt;, &lt;TT&gt;boolean&lt;/TT&gt;, &lt;strong&gt;1 byte&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In total, a row in table &lt;TT&gt;Customer&lt;/TT&gt; needs &lt;strong&gt;47 bytes&lt;/strong&gt; of storage.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;considerations-for-the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Considerations for the new model&lt;/h1&gt;
&lt;p&gt;We need to take some decisions as to the new data model.
The considerations that we made in the previous exercises
will lead us to the right decisions.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How would you model in MongoDB the entities &lt;TT&gt;Rental&lt;/TT&gt;
and &lt;TT&gt;Payment&lt;/TT&gt;, given the cardinalities that you identified in the previous section?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The relationship between the two given entities is &lt;strong&gt;one-to-one&lt;/strong&gt;.
Therefore, we can use a denormalized schema in MongoDB.&lt;/p&gt;
&lt;p&gt;That is, we can create one single collection to store the information about the
two entities.&lt;/p&gt;
&lt;p&gt;We have two options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We create a collection &lt;TT&gt;Payment&lt;/TT&gt;, where each document contains
the attributes of a payment and an embedded document with the details of the rental
the payment refers to.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create a collection &lt;TT&gt;Rental&lt;/TT&gt;, where each document contains
the attributes of a rental and an embedded document with the details of the payment
for the rental.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although both options are perfectly valid, we prefer the second one, as
rentals are our first-class citizens in our database.&lt;/p&gt;
&lt;p&gt;We can also consider the attributes of a payment as attributes of a rental, without
creating an embedded document for the payment.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Suppose that we create a collection &lt;TT&gt;Customer&lt;/TT&gt;
and we embed in each customer document the list of rentals
for that customer.&lt;/p&gt;
&lt;p&gt;How many rentals can we store at most for a given customer, knowing
that the size of a document cannot exceed 16 MB?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We’ve seen before that each rental needs 28 bytes of space.
To make the computation easier, we round this size up to 32
bytes (a power of two).
Considering that 16 MB = &lt;span class=&#34;math inline&#34;&gt;\(16 \times 2^{20}\)&lt;/span&gt;, the maximum number of
rentals that we can store for a given customer is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{16 \times 2^{20}}{32} = 2^{-1} \times 2^{20} = 2^{19}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This gives around 600,000 rentals.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
In our current database we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;599 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;16044 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, each customer has around 27 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compute the size in byte of the two following collections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Collection &lt;TT&gt;Customer&lt;/TT&gt;, where each document contains the
information about a customer and an embedded list with the information
on all the rentals made by the customer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Collection &lt;TT&gt;Rental&lt;/TT&gt;, where each document contains the information
about a rental and an embedded document with the information on the customer
that made the rental.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We previously found out that for each customer we need 47 bytes
and for each rental we need 28 bytes.&lt;/p&gt;
&lt;p&gt;A document that holds the data about a customer and the list of all the
rentals of the customer will need &lt;span class=&#34;math inline&#34;&gt;\(47 + 27 \times 28 = 830\)&lt;/span&gt; bytes.&lt;/p&gt;
&lt;p&gt;Hence, the total size of the &lt;TT&gt;Customer&lt;/TT&gt; collection is
&lt;span class=&#34;math inline&#34;&gt;\(803 \times 599 = 480977\)&lt;/span&gt; bytes, that is 470 KB.&lt;/p&gt;
&lt;p&gt;A document that holds the data about a rental and its customer
needs $ 28 + 47 = 75 $ bytes.&lt;/p&gt;
&lt;p&gt;Hence, the total size of the &lt;TT&gt;Rental&lt;/TT&gt; collection is
&lt;span class=&#34;math inline&#34;&gt;\(75 \times 16044 = 1203300\)&lt;/span&gt; bytes, that is 1,1 MB.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
Discuss advantages and disadvantages of the two following options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;Customer&lt;/TT&gt; with a document for each customer
holding the list of all the rentals of the customer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;A collection &lt;TT&gt;Rental&lt;/TT&gt; with a document for each rental
holding the data on the relative customer.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Advantages of solution 1.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is no redundancy. In fact, a rental is relative to at most one customer, therefore
the data on a rental is not duplicated across different documents.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a result, the size of the collection is smaller than the collection in solution 2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each customer, we retrieve the information on all his/her rentals with only one read operation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages of solution 1.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is a limit (albeit an acceptable one) on the number of rentals that we can store
for each customer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We lose a “rental-centric” view of our data. As a result, if any other document in another
collection (e.g., staff) refers to a rental, all the information about a rental
must be denormalized in that document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Advantages of solution 2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A “rental-centric” view of our data. Aggregating information from different rentals
does not require digging rentals out of several lists.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size of each document is small and will never exceed the 16 MB limit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a result, reading a document from the collection takes less time and memory than
reading a document from the collection in solution 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages of solution 2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is a lot of redundancy. The information about a customer are replicated
each time the customer makes a rental.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a result, the size of the collection is much higher than in solution 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;It seems that one of the two solutions has a higher storage demands
than the other, and therefore the odds seems to be stacked against that solution.&lt;/p&gt;
&lt;p&gt;However….&lt;/p&gt;
&lt;p&gt;We still have to consider two more entities that are
linked to the rentals: staff and inventory.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;
Discuss how you can fit staff and inventory in each of the solutions
presented in the previous exercise.
Discuss advantages and disadvantages of each option that you present.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;strong&gt;Solution 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We need to somehow link staff and inventory to the relative rental.
We have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We embed staff and inventory into each rental document, which, let’s recall it, is
already embedded in an array. This creates redundancy, as a staff member or an inventory item
can appear in more than one rental.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create three separate collections (&lt;TT&gt;Customer&lt;/TT&gt;, &lt;TT&gt;Staff&lt;/TT&gt; and &lt;TT&gt;Inventory&lt;/TT&gt;)
and in each we embed an array with the list of the relative rentals.
The problem is that the data on the rentals are now replicated three times.
This solution is particularly bad, because rentals are frequently written.
When we create a rental, we need to write three documents; when a customer
returns an item, we need to update the return date in three documents.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create four collections (&lt;TT&gt;Customer&lt;/TT&gt;, &lt;TT&gt;Staff&lt;/TT&gt;, &lt;TT&gt;Inventory&lt;/TT&gt; and
&lt;TT&gt;Rentals&lt;/TT&gt;); for each customer, staff and inventory we keep a list of rentals,
each item being the identifier that refers to the appropriate rental.
We fall back to the normalized schema of the PostgreSQL database.
Then, it isn’t clear how this normalized schema will help horizontally scale the database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solution 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have only one collection &lt;TT&gt;Rental&lt;/TT&gt;; in each document, we
have an attribute customer, whose value is an embedded document with all the information
about a customer, an attribute staff, whose value is an embedded document
with all information about a staff member, and an attribute inventory, whose value is
an embedded document with all information about an inventory item.&lt;/p&gt;
&lt;p&gt;This solution has higher storage requirements than the options presented in solution 1,
but it has the clear advantage of being a denormalized schema,
where we control every facet of a rental (customer, staff, inventory).
Moreover, when we create a rental, we only write one document ; when we
update the return date of a rental, we only update one document.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;From the previous exercise, we have a clearer idea as to the best solution
to our case.
We take a closer look at the storage requirements of the adopted solution.
Consider that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The size in byte of a document storing the information of a staff member is around 84000 bytes
(we also store a profile picture).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in byte of a document storing the information of an inventory item is 16 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 8  &lt;/strong&gt;&lt;/span&gt;
If we denote by &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; the number of rentals, what is the size in bytes of the
whole database for the adopted solution?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Let’s recall that the size in bytes of a document storing the information on a customer is 47 bytes.&lt;/p&gt;
&lt;p&gt;The size of the only collection &lt;TT&gt;Rental&lt;/TT&gt; (hence, of the whole database) is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
N_{rentals} \times (47 + 84000 + 16) = N_{rentals} \times 84063
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With 10,000 rentals, the size of the database is 800 MB.
With 100,000 rentals, the size of the database is 7 GB.
With 1,000,000 rentals, the size of the database is 78 GB.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Although the size that we determined in the previous exercise, may not sound
impressive, we still have to store other information
(films, actors….).
If we could save a bit of space, we would be happy.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 9  &lt;/strong&gt;&lt;/span&gt;
Discuss how you could save some space in the adopted solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HINT.&lt;/strong&gt; Do you really need to denormalize all data?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;While modeling data for a MongoDB database, the choice
between a normalized and a denormalized schema
does not need to be a black and white one.&lt;/p&gt;
&lt;p&gt;We can have a denormalized schema, where we embed in the same documents
information that are queried together, while storing in a separate collection
the information that are rarely queried.&lt;/p&gt;
&lt;p&gt;For instance, the profile picture of a staff member might not be
an important piece of information while we’re trying to analyze
which staff members are the more productive ones.&lt;/p&gt;
&lt;p&gt;So, we might add another collection &lt;TT&gt;Staff&lt;/TT&gt; where each
document only contains attributes that are not in embedded documents in the collection
&lt;TT&gt;Rental&lt;/TT&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 10  &lt;/strong&gt;&lt;/span&gt;
Propose a solution for all the entities involved and
estimate the saving in terms of storage requirements.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Here is a solution with three different collections.
The information on inventory are completely denormalized into the
collection &lt;TT&gt;Rental&lt;/TT&gt;. For customers and staff members we only keep
the necessary information and we normalize the information that are less
likely to be queried while analyzing the rentals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-5/mongodb-sakila-first-solution.png&#34; alt=&#34;First schema MongoDB&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let’s try to estimate the savings in terms of storage.
In a document of the collection &lt;TT&gt;Rental&lt;/TT&gt;, we have the following attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rental_id: 4 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rental_date and return_date: 16 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;inventory_id: 4 bytes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;film_id and store_id: 4 bytes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;customer_id: 4 bytes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;customer first and last names: 13 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;country: 9 bytes (on average a country name has 9 characters).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;staff_id: 4 bytes&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;staff first and last name: 13 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In total a document in our collection &lt;TT&gt;Rental&lt;/TT&gt; will have 71 bytes
(a huge improvement wrt the 84000 bytes of our first solution).&lt;/p&gt;
&lt;p&gt;Of course, the size of a document in the &lt;TT&gt;Staff&lt;/TT&gt; collection will be
around 84000 bytes (we still store the profile picture).
However, the number of staff members &lt;span class=&#34;math inline&#34;&gt;\(N_{staff}\)&lt;/span&gt; is much lower
than the number of rentals &lt;span class=&#34;math inline&#34;&gt;\(N_{rentals}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is easy to verify that the size of a document of the collection &lt;TT&gt;Customer&lt;/TT&gt;
is 34 bytes (same information as before except first and last name).
Again, the number of customers &lt;span class=&#34;math inline&#34;&gt;\(N_{customers}\)&lt;/span&gt; is much lower than
&lt;span class=&#34;math inline&#34;&gt;\(N_{rentals}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We assume that each customer has on average 30 rentals and for 100 customers we
have 1 staff member.
We have that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$N_{customer} = N_{rental}/30 $&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$N_{staff} = N_{customer}/100 = N_{rental}/3000 $&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the size of the database will be given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
71\times N_{rentals} + 84000 \times N_{rentals}/3000 + 34 \times N_{rentals}/30 = 100 \times N_{rental}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we want to compare against the first solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With 10,000 rentals, the size of the database is 976 KB (instead of 800 MB).
With 100,000 rentals, the size of the database is 9,5 MB (instead of 7 GB).
With 1,000,000 rentals, the size of the database is 95 MB (instead of 78 GB).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, a big improvement! And we have a denormalized schema that
lets us take advantage of the horizontal scalability of MongoDB.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The new model&lt;/h1&gt;
&lt;p&gt;In this section we intend to obtain a complete model of the Sakila database.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 11  &lt;/strong&gt;&lt;/span&gt;
Consider the model that we obtained at the end of the previous section.
Which data can you further denormalize?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;strong&gt;Collection &lt;TT&gt;Staff&lt;/TT&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The attribute &lt;TT&gt;address_id&lt;/TT&gt; refers to a full address. We can fully
denormalize this information into the documents of the collection.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The attribute &lt;TT&gt;store_id&lt;/TT&gt; refers to the store where the staff member works.
The &lt;TT&gt;Store&lt;/TT&gt; table in the original PostgreSQL database does not have
too many columns. Therefore, it might be reasonable to fully denormalize these data.
However, information on the stores are also linked to customers and to inventory items.
If we need to update, say, the manager of a store, we would need to update
three different documents.
Hence, we prefer to create a collection &lt;TT&gt;Store&lt;/TT&gt; and keep in the documents of the
collection &lt;TT&gt;Staff&lt;/TT&gt; only the city and country of a store (and its identifier of course).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Attribute &lt;TT&gt;Inventory&lt;/TT&gt;&lt;/strong&gt; in collection &lt;TT&gt;Customer&lt;/TT&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Same considerations for the attribute &lt;TT&gt;store_id&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The attribute &lt;TT&gt;film_id&lt;/TT&gt; is the reference to the film the inventory item is relative to.
The table &lt;TT&gt;Film&lt;/TT&gt; in the original PostgreSQL database contains 14 columns.
This high number of attributes advises us against a full denormalization, considering that
a film can be relative to multiple inventory items.
We might only keep the film title and the film categories. The rest of the attributes are kept in documents
of a separate collection &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Collection &lt;TT&gt;Customer&lt;/TT&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Same considerations for the attributes &lt;TT&gt;store_id&lt;/TT&gt; and &lt;TT&gt;address_id&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-5/mongodb-sakila-first-model-further-denormalized.png&#34; alt=&#34;Sakila MongoDB data model further normalized&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 12  &lt;/strong&gt;&lt;/span&gt;
Complete the diagram obtained in the previous exercise so as to obtain
a full data model for the Sakila database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-5/mongodb-sakila-complete.png&#34; alt=&#34;Sakila MongoDB full data model&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Queries in MongoDB</title>
      <link>/courses/gadexed/tutorials/mongodb-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/mongodb-queries/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn to write basic and advanced queries in MongoDB.&lt;/p&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get the data&lt;/h1&gt;
&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-6/sakila-mongodb.zip&#34;&gt;&lt;strong&gt;this archive file&lt;/strong&gt;&lt;/a&gt; and extract it.
You’ll find a file for each collection to import into the database.&lt;/p&gt;
&lt;p&gt;Open MongoDB Compass, create a new database named &lt;TT&gt;sakila&lt;/TT&gt; and
create the different collections while importing the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of the customers of Canadian stores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all rentals made by customers from Iran, where
the amount paid is strictly greater than 10 dollars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last names of the actors who played a role in film 213.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find()&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find({}, {email:1})&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find({&#34;store.country&#34;: &#34;Canada&#34;}, {email:1});&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.rental.find({&#34;customer.country&#34;: &#34;Iran&#34;, amount: {$gt: 10}}, {rental_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({film_id: 213}, {&#34;actors.first_name&#34;:1, &#34;actors.last_name&#34;: 1}).sort({&#34;actors.last_name&#34;: -1});&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;operations-on-arrays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Operations on arrays&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have “Behind the Scenes” as special features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all the films where BURT POSEY played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the film that has exactly 15 actors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({special_features : {$elemMatch: {$eq: &#34;Behind the Scenes&#34;}}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({special_features : {$all: [&#34;Commentaries&#34;, &#34;Deleted Scenes&#34;]}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({&#34;actors.first_name&#34;: &#34;BURT&#34;, &#34;actors.last_name&#34;: &#34;POSEY&#34;}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({actors: {$size : 15}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Aggregation framework&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the total amount of payments across all rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors of each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sort the films by the number of actors (decreasing order).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the average number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the country where the customers have rented the most of the films in the category “Music”.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {&#34;customer.first_name&#34;: &#34;TOMMY&#34;, &#34;customer.last_name&#34;: &#34;COLLAZO&#34;}}, 
                    {$project: {&#34;inventory.film.title&#34;: 1, _id:0}})
&lt;/pre&gt;
One can also express this query with the function find()
&lt;pre&gt;
db.rental.find({&#34;customer.first_name&#34;: &#34;TOMMY&#34;, &#34;customer.last_name&#34;: &#34;COLLAZO&#34;}, {&#34;inventory.film.title&#34;: 1, _id:0});
&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$group: {&#34;_id&#34;: null, total_amount: {$sum: &#34;$amount&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.film.aggregate({$project: {nb_actors: {$size: &#34;$actors&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we don’t put the match condition, we get an error because for some films
the field actors is not defined.&lt;/p&gt;
&lt;pre&gt;db.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, 
                {$project: {film_id: 1, &#34;nb_actors&#34;: {$size: &#34;$actors&#34;}}}, 
                {$sort: {nb_actors: -1}})
&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, 
                {$project: {film_id: 1, &#34;nb_actors&#34;: {$size: &#34;$actors&#34;}}}, 
                {$group: {_id: null, avg_actors: {$avg: &#34;$nb_actors&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$group: {_id: &#34;$customer.customer_id&#34;, count: {$sum: 1}}}, {$sort: {count: -1}})
&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$group: {_id: {cust_id: &#34;$customer.customer_id&#34;, cust_first_name: &#34;$customer.first_name&#34;, 
                                    cust_last_name: &#34;$customer.last_name&#34;}, count: {$sum: 1}}}, 
                            {$sort: {count: -1}}, {$limit :1})
&lt;/pre&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;db.rental.aggregate({&lt;span class=&#34;math inline&#34;&gt;\(match: {&amp;quot;inventory.film.categories.category&amp;quot;: &amp;quot;Music&amp;quot;}}, {\)&lt;/span&gt;group: {_id: &amp;quot;&lt;span class=&#34;math inline&#34;&gt;\(customer.country&amp;quot;, count: {\)&lt;/span&gt;sum: 1}}}, {&lt;span class=&#34;math inline&#34;&gt;\(sort:{count: -1}}, {\)&lt;/span&gt;limit: 1})&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;join-operations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Join Operations&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the language of the film with title “ACE GOLDFINGER”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all the information about the staff member who took care of rental 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {&#34;inventory.film.title&#34;: &#34;ACE GOLDFINGER&#34;}}, 
            {$lookup: {from: &#34;film&#34;, localField: &#34;inventory.film.film_id&#34;, foreignField:&#34;film_id&#34;, as:&#34;film&#34;}}, 
            {$project: {&#34;film.language&#34;: 1}}, {$limit : 1})
&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {rental_id: 2}}, 
                    {$lookup: {from: &#34;staff&#34;, localField: &#34;staff.staff_id&#34;, 
                    foreignField:&#34;staff_id&#34;, as:&#34;staff_member&#34;}}, {$project: {&#34;staff_member&#34;: 1}})
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Neo4j</title>
      <link>/courses/gadexed/tutorials/neo4j/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/neo4j/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;setting-up-the-work-environment.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setting up the work environment.&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download &lt;a href=&#34;https://neo4j.com/download/&#34; target=&#34;_blank&#34;&gt;Neo4j Desktop&lt;/a&gt;
and install it on your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new project by clicking on the button “New”
that you’ll find on the top left side of the window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on “Add Database”, then “Create a Local Database”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Give the database a name (e.g., &lt;em&gt;MovieLens&lt;/em&gt;) and set
a password that you can easily remember; then click on “Create”.
Choose version 4.4.8 for the database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on “Start” and wait for the database to become active.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on the button “Open”. The &lt;em&gt;Neo4j Browser&lt;/em&gt; will pop up.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next section, you’ll have to type a sequence of commands to
import the data.
You’ll write the commands in the text field on top of the
&lt;em&gt;Neo4j Browser&lt;/em&gt; (where you find the prompt &lt;em&gt;neo4j$&lt;/em&gt;).&lt;/p&gt;
&lt;div id=&#34;import-the-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import the data.&lt;/h2&gt;
&lt;p&gt;The dataset consists of
data obtained from &lt;em&gt;MovieLens&lt;/em&gt;,
a recommender system
whose users give movies a rate
between 1 and 5,
based on whether they dislike or love them.
MovieLens uses the rates to recommend
movies that its users might like.
The dataset is modeled as a &lt;strong&gt;directed graph&lt;/strong&gt; and
consists of 100,004 rates on 9,125 movies
across 671 users between January 9th, 1995 and October 16, 2016.
The dataset also contains the names of
the directors and the actors of each movie.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the &lt;strong&gt;nodes&lt;/strong&gt; corresponding to the
&lt;strong&gt;movies&lt;/strong&gt; (label &lt;strong&gt;Movie&lt;/strong&gt;) by
using the following command (it took 31 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv&#39; as row
MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)})
RETURN count(m)
&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create an &lt;strong&gt;index&lt;/strong&gt; on the property &lt;em&gt;movie_id&lt;/em&gt; of
the nodes with label &lt;strong&gt;Movie&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
create index movie_idx for (m:Movie) on (m.movie_id)
&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the &lt;strong&gt;nodes&lt;/strong&gt; corresponding to the
&lt;strong&gt;actors&lt;/strong&gt; (label &lt;strong&gt;Actor&lt;/strong&gt;) by
using the following command (it took 62 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv&#39; as row
MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name})
RETURN count(a)
&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create an &lt;strong&gt;index&lt;/strong&gt; on the property &lt;em&gt;actor_id&lt;/em&gt; of
the nodes with label &lt;strong&gt;Actor&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
create index actor_idx for (a:Actor) on (a.actor_id)
&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the &lt;strong&gt;nodes&lt;/strong&gt; corresponding to the
&lt;strong&gt;directors&lt;/strong&gt; (label &lt;strong&gt;Director&lt;/strong&gt;) by
using the following command (it took 4 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv&#39; as row
MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name})
RETURN count(d)
&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create an &lt;strong&gt;index&lt;/strong&gt; on the property &lt;em&gt;director_id&lt;/em&gt; of
the nodes with label &lt;strong&gt;Director&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
create index director_idx for (d:Director) on (d.director_id)
&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the &lt;strong&gt;nodes&lt;/strong&gt; corresponding to the
&lt;strong&gt;genres&lt;/strong&gt; (label &lt;strong&gt;Genre&lt;/strong&gt;) by
using the following command (it took 197 ms on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv&#39; as row
MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name})
RETURN count(g)
&lt;/pre&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create an &lt;strong&gt;index&lt;/strong&gt; on the property &lt;em&gt;genre_id&lt;/em&gt; of
the nodes with label &lt;strong&gt;Genre&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
create index genre_idx for (g:Genre) on (g.genre_id)
&lt;/pre&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the &lt;strong&gt;nodes&lt;/strong&gt; corresponding to the
&lt;strong&gt;users&lt;/strong&gt; (label &lt;strong&gt;User&lt;/strong&gt;) by
using the following command (it took 347 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv&#39; as row
MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname})
RETURN count(u)
&lt;/pre&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create an &lt;strong&gt;index&lt;/strong&gt; on the property &lt;em&gt;user_id&lt;/em&gt; of
the nodes with label &lt;strong&gt;User&lt;/strong&gt; with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
create index user_idx for (u:User) on (u.user_id)
&lt;/pre&gt;
&lt;ol start=&#34;11&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the links of type &lt;strong&gt;ACTED_IN&lt;/strong&gt;
between actors and movies with the following
command (it took 2.5 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (a:Actor {actor_id: toInteger(row.actor_id)})
MERGE (a)-[r:ACTED_IN]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;
&lt;ol start=&#34;12&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the links of type &lt;strong&gt;DIRECTED&lt;/strong&gt;
between directors and movies with the following
command (it took 688 ms on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (d:Director {director_id: toInteger(row.director_id)})
MERGE (d)-[r:DIRECTED]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the links of type &lt;strong&gt;HAS_GENRE&lt;/strong&gt;
between movies and genres with the following
command (it took 1 second on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (g:Genre {genre_id: toInteger(row.genre_id)})
MERGE (m)-[r:HAS_GENRE]-&gt;(g)
RETURN count(r)
&lt;/pre&gt;
&lt;ol start=&#34;14&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import the links of type &lt;strong&gt;RATED&lt;/strong&gt;
between users and movies with the following
command (it took 5.9 seconds on my computer):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (u:User {user_id: toInteger(row.user_id)})
MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploratory queries&lt;/h1&gt;
&lt;p&gt;If you looked at the commands used to import the data,
you might already have an idea as to the structure of
the graph.
You can get a glimpse on the node labels,
the relationship types and the property keys by clicking on the
button circled in the following figure:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/plp/tutorials/neo4j/neo4j-db-button.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Write and execute the following query:&lt;/p&gt;
&lt;pre&gt;
MATCH (m:Movie {title_en:&#34;Toy Story&#34;}) 
RETURN m;
&lt;/pre&gt;
&lt;p&gt;What do you obtain? What are the properties associated
to a node with label &lt;em&gt;Movie&lt;/em&gt;?
Click once on the node to display its properties.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The requested node is displayed in the Neo4j Browser.
By clicking on the node, we see that the properties are:
&lt;em&gt;movie_id&lt;/em&gt;, &lt;em&gt;title_en&lt;/em&gt;, &lt;em&gt;title_fr&lt;/em&gt;, &lt;em&gt;year&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Double-click on the node displayed as the result of the previous query.
Analyze the neighbouring nodes (their labels and properties)
and the incident links (direction, type and properties).
You can move around the node by dragging it in the window.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;From the interface, we see that the movie Toy Story is rated by 90 users,
has 5 genres, 4 actors and 1 director.
The following observations can be made:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each node with label &lt;em&gt;User&lt;/em&gt; has two properties, &lt;em&gt;user_id&lt;/em&gt; and &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each node with label &lt;em&gt;Genre&lt;/em&gt; has two properties, &lt;em&gt;genre_id&lt;/em&gt; and &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each node with label &lt;em&gt;Director&lt;/em&gt; has two properties, &lt;em&gt;director_id&lt;/em&gt; and &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each node with label &lt;em&gt;Actor&lt;/em&gt; has two properties, &lt;em&gt;actor_id&lt;/em&gt; and &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A relationship of type &lt;em&gt;RATED&lt;/em&gt; has a property &lt;em&gt;rate&lt;/em&gt; and is directed from
a node with label &lt;em&gt;User&lt;/em&gt; to a node with label &lt;em&gt;Movie&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A relationship of type &lt;em&gt;DIRECTED&lt;/em&gt; is directed from
a node with label &lt;em&gt;Director&lt;/em&gt; to a node with label &lt;em&gt;Movie&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A relationship of type &lt;em&gt;HAS_GENRE&lt;/em&gt; is directed from
a node with label &lt;em&gt;Movie&lt;/em&gt; to a node with label &lt;em&gt;Genre&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A relationship of type &lt;em&gt;ACTED_IN&lt;/em&gt; is directed from
a node with label &lt;em&gt;Actor&lt;/em&gt; to a node with label &lt;em&gt;Movie&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
Write and execute the following queries:&lt;/p&gt;
&lt;p&gt;Q1. The genres of the movies in the database.&lt;/p&gt;
&lt;p&gt;Q2. The number of movies in the database.&lt;/p&gt;
&lt;p&gt;Q3. The title of the movies released in 2015.&lt;/p&gt;
&lt;p&gt;Q4. The number of directors by movie. Sort in decreasing order.&lt;/p&gt;
&lt;p&gt;Q5. The names of the directors and the title of the movies that they
directed and in which they also played.&lt;/p&gt;
&lt;p&gt;Q6. The genres of the movies in which Tom Hanks played.&lt;/p&gt;
Q7. The title and the rate of all the movies that
the user with identifier 3 rated.
Sort by rate in decreasing order.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
Q1.
&lt;pre&gt;
MATCH (g:Genre) 
RETURN g.name
&lt;/pre&gt;
Q2.
&lt;pre&gt;
MATCH (n:Movie) 
RETURN count(n)
&lt;/pre&gt;
Q3.
&lt;pre&gt;
MATCH (n:Movie {year:2015}) 
RETURN n.title_en;
&lt;/pre&gt;
Q4.
&lt;pre&gt;
MATCH (n:Movie)&lt;-[:DIRECTED]-(d:Director) 
RETURN n.title_en, count(*) AS nb_directors 
ORDER BY nb_directors DESC
&lt;/pre&gt;
Q5.
&lt;pre&gt;
MATCH (a:Actor)-[:ACTED_IN]-&gt;(m:Movie)&lt;-[:DIRECTED]-(d:Director) 
WHERE a.name=d.name 
RETURN a.name, m.title_en;
&lt;/pre&gt;
Q6.
&lt;pre&gt;
MATCH (:Actor {name:&#34;Tom Hanks&#34;})-[:ACTED_IN]-&gt;(:Movie)-[:HAS_GENRE]-&gt;(g:Genre) 
RETURN DISTINCT (g.name);
&lt;/pre&gt;
Q7.
&lt;pre&gt;
MATCH (n:User {user_id:3})-[r:RATED]-&gt;(m:Movie) 
RETURN m.title_en, r.rate 
ORDER BY r.rate desc;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;query-chaining&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Query chaining&lt;/h2&gt;
&lt;p&gt;Cypher allows the specification of
complex queries composed of
several queries that are concatenated
with the clause &lt;strong&gt;WITH&lt;/strong&gt;.
We are now going to see an example to
obtain the titles of the movies
that have been rated by at least 100 users.&lt;/p&gt;
&lt;p&gt;At a first glance, the following query looks like a good solution:&lt;/p&gt;
&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) WHERE count(u) &gt;= 100
RETURN n.title_en
LIMIT 5;
&lt;/pre&gt;
&lt;p&gt;However, executing this query returns the following error:&lt;/p&gt;
&lt;pre&gt;
Invalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41))
&#34;MATCH (n:Movie)&lt;-[:RATED]-(u:User) WHERE count(u) &gt;= 100&#34;
&lt;/pre&gt;
&lt;p&gt;Similarly to SQL, we cannot use aggregating functions in the clause WHERE.&lt;/p&gt;
&lt;p&gt;A correct formulation of the query requires the use of the clause WITH
to concatenate two queries: the first will count the number of rates
for each movie:&lt;/p&gt;
&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) 
RETURN n, count(u) as nb_rates
&lt;/pre&gt;
&lt;p&gt;The second will take in the output of the first and will
filter all the movies where nb_rates &amp;lt; 100.
In order to chain the two queries, we’ll replace the
RETURN clause in the first query with a WITH clause, as follows:&lt;/p&gt;
&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) 
WITH n, count(u) as nb_rates
WHERE nb_rates &gt;= 100
RETURN n.title_en
&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Write and execute a query to obtain the five movies
that obtained the best average rate among the movies
that have been rated by at least 100 users.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;
MATCH (n:Movie)&lt;-[r:RATED]-(u:User) 
WITH n, avg(r.rate) as avg,  count(u) AS nb_rates 
WHERE nb_rates &gt;= 100 
RETURN n.title_en,  avg 
ORDER BY avg DESC;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;movie-recommendation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Movie recommendation&lt;/h1&gt;
&lt;p&gt;We are now going to see how Neo4j
can be effectively used in a real application
by implementing queries that form the basis of a
simple &lt;strong&gt;movie recommendation system&lt;/strong&gt;.
This system is based on the notion of &lt;strong&gt;collaborative filtering&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This consists in recommending a user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; some films
that s/he hasn’t rated yet and other
users with similar preferences have loved.
In our context, we say that a user loves
a movie if s/he rated the movie at least 3.&lt;/p&gt;
&lt;p&gt;This concept is explained in the following figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/courses/plp/tutorials/neo4j/collaborative-filtering.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; loves 6 movies,
3 of which are also loved by the user &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; (the black nodes);
it is reasonable to think that &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;
may also love the two movies that &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; loved and &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; hasn’t rated yet.&lt;/p&gt;
&lt;p&gt;The principle of collaborative filtering is
based on the computation of a &lt;strong&gt;similarity score&lt;/strong&gt;
between two users.
Several similarity scores are possible in this context;
here, we are going to use the &lt;strong&gt;Jaccard coefficient&lt;/strong&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(L(u)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L(v)\)&lt;/span&gt; be the
sets of movies that &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; love
respectively;
the similarity score &lt;span class=&#34;math inline&#34;&gt;\(J(u,v)\)&lt;/span&gt; between &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
J(u, v) = \frac{|L(u) \cap L(v)|}{|L(u) \cup L(v)|}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to recommend movies to
a target user &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;,
the recommender system computes the
similarity score between &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and all
the other users of the system and
proposes to &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; the movies
that s/he hasn’t rated yet and that the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; most similar users loved.&lt;/p&gt;
&lt;p&gt;We are now going to incrementally write a query to recommend some movies to the target user 3.
The first step consists in determining the value &lt;span class=&#34;math inline&#34;&gt;\(|L(v)|\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
Write and execute the query to obtain
the number of movies that the user 3 loved.
This query must return the target user
and the number of movies that s/he loves.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;
MATCH (target:User {user_id:3})-[r:RATED]-&gt;(m:Movie) 
WHERE r.rate &gt;= 3 
RETURN target, count(m) AS lovedByTarget;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Next, we are going to determine the value &lt;span class=&#34;math inline&#34;&gt;\(|L(u)|\)&lt;/span&gt;,
for all users &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; except &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
Write and execute the query
to obtain the number of movies that each user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; loves,
except the target user 3.
This query must return each user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and the number
of movies that s/he loves.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;
MATCH (other:User)-[r:RATED]-&gt;(m:Movie) 
WHERE other.user_id &lt;&gt; 3 and r.rate &gt;= 3 
RETURN other, count(m) as lovedByOther
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We put the two queries together with the clause WITH.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;
Compose the two previous queries with the clause WITH.
This query must return
the target user 3,
the number of movies that s/he loves,
the other users &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and the number of movies
that they love.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;
MATCH (target:User {user_id:3})-[r:RATED]-&gt;(m:Movie) 
WHERE r.rate &gt;= 3 
WITH target, count(m) AS lovedByTarget
MATCH (other:User)-[r:RATED]-&gt;(m:Movie) 
WHERE other &lt;&gt; target and r.rate &gt;= 3 
RETURN target, lovedByTarget, other, count(m) as lovedByOther
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Now, we need to
determine the value &lt;span class=&#34;math inline&#34;&gt;\(L(u)\cup L(v)\)&lt;/span&gt;,
for each user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, and compute the similarity score with the
Jaccard coefficient.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 8  &lt;/strong&gt;&lt;/span&gt;
Append (by using WITH)
to the query written in the previous exercise
a query that obtains
the number of movies
that any user &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; loved
and that the target user 3 loved too,
and computes the similarity score
between the target user 3 and &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;.
This query must return the five most similar
users to the target user and the similarity scores.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Hint&lt;/summary&gt;
Multiply the numerator of the equation
by 1.0, otherwise Cypher will compute an integer division.
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The following is the query written at the previous exercise,
where the RETURN clause has been replaced with WITH.&lt;/p&gt;
&lt;pre&gt;
MATCH (target:User {user_id:3})-[r:RATED]-&gt;(m:Movie) 
WHERE r.rate &gt;= 3 
WITH target, count(m) AS lovedByTarget
MATCH (other:User)-[r:RATED]-&gt;(m:Movie) 
WHERE other &lt;&gt; target and r.rate &gt;= 3 
WITH target, lovedByTarget, other, count(m) as lovedByOther
&lt;/pre&gt;
&lt;p&gt;We have to append the following query:&lt;/p&gt;
&lt;pre&gt;
MATCH (target)-[r:RATED]-&gt;(m:Movie)&lt;-[r1:RATED]-(other) 
WHERE r.rate &gt;= 3 and r1.rate &gt;= 3 
RETURN other, count(m)*1.0 / (lovedByTarget + lovedByOther - count(m)) as sim 
ORDER BY sim DESC LIMIT 5
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;The last step consists in recommending some movies to the target user.
From the previous query,
take the identifier of the user &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; with
the highest similarity to the target user.
You are going to use this identifier directly in the new query.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;strong&gt;Exercise&lt;/strong&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 9  &lt;/strong&gt;&lt;/span&gt;
Write and execute
the query to obtain the list of
the movies that the user &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; loved
and that the target user hasn’t rated yet. Sort this list by decreasing rate.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Hint&lt;/summary&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, write a query
to obtain the list of the movies
that the target user rated.
In the MATCH clause,
use the variable &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; to indicate a movie that the target user rated.
Conclude the query with:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
RETURN collect(m.title_en) AS movies
&lt;/pre&gt;
&lt;p&gt;The function &lt;em&gt;collect&lt;/em&gt; creates a list called &lt;em&gt;movies&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replace RETURN with WITH in the previous query and add a
second query to select the titles of the movies
&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; that the user &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; loved and the target user
did not rate.
In order to exclude the films that the target user
did not rate, use the following predicate:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
none(x in movies where x=m.title_en)
&lt;/pre&gt;
&lt;p&gt;in the WHERE clause.&lt;/p&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The following is the query written at the previous exercise,
where the RETURN clause has been replaced with WITH.&lt;/p&gt;
&lt;pre&gt;
MATCH (target:User {user_id:3})-[r:RATED]-&gt;(m:Movie)  
WITH collect(m.title_en) as movies  
MATCH (u:User {user_id:129})-[r:RATED]-&gt;(m:Movie)  
WHERE r.rate &gt;=3 AND none(x IN movies WHERE x=m.title_en) 
RETURN m.title_en 
ORDER BY r.rate DESC;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Normalization theory</title>
      <link>/courses/gadexed/tutorials/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/normalization/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to obtain a &lt;strong&gt;non-redundant set of functional dependencies&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to determine the &lt;strong&gt;candidate keys&lt;/strong&gt; of a table given its functional dependencies.&lt;/li&gt;
&lt;li&gt;How to determine the &lt;strong&gt;normal form&lt;/strong&gt; of a table.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;question-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 1&lt;/h1&gt;
&lt;p&gt;We consider a relational table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; with four columns &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; be the following set of functional dependencies:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow BC\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow B\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Derive a minimal set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; of functional dependencies that is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We first need to write the functional dependencies in canonical form
(the right-side of each FD must consist of only one column).&lt;/p&gt;
&lt;p&gt;The FD (2.) can be rewritten using the decomposition axiom.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow B\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow C\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow B\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next, we need to make sure that all functional dependencies are left-irreducible.&lt;/p&gt;
&lt;p&gt;All functional dependencies except the first one is trivially left-irreducible (the determinant consists of
only one column).
The first has two columns in the determinant, therefore we need to check whether we
can eliminate one of the two columns and still preserve an equivalent set of functional dependencies.&lt;/p&gt;
&lt;p&gt;We apply Armstrong’s axioms to compute the closure of this set of functional dependencies.&lt;/p&gt;
&lt;p&gt;From (1.) and (4.), we can apply the pseudotransitivity axiom and we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A \rightarrow B \wedge AB \rightarrow C \implies AA \rightarrow C \implies A \rightarrow C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow C\)&lt;/span&gt; is in the cover, the column &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; in FD (1.) is useless
and can therefore be omitted.&lt;/p&gt;
&lt;p&gt;The set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; consists of the following FDs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow C\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow B\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow C\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow B\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;None of these functional dependencies are redundant.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;question-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 2&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; be a relational table with five columns &lt;span class=&#34;math inline&#34;&gt;\((A, B, C, D, E)\)&lt;/span&gt;.
The following set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; of functional dependencies hold:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow D\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow E\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Specify the candidate keys of the table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;A candidate key is a set of columns that imply all the other columns.&lt;/p&gt;
&lt;p&gt;First, let’s try sets composed of only one column: &lt;span class=&#34;math inline&#34;&gt;\(\{A\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\{B\}\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\{C\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\{D\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{E\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We have the following: (&lt;span class=&#34;math inline&#34;&gt;\(\{X\}^+_{\mathcal{F}}\)&lt;/span&gt; indicates the set of all columns implied by &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{A\}^+_{\mathcal{F}} = \{A\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{B\}^+_{\mathcal{F}} = \{B\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{C\}^+_{\mathcal{F}} = \{A, B, C, D, E\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{D\}^+_{\mathcal{F}} = \{D, E\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{E\}^+_{\mathcal{F}} = \{E\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\{C\}\)&lt;/span&gt; is a candidate key because it implies all the other columns.&lt;/p&gt;
&lt;p&gt;From the functional dependency (1), we obtain that &lt;span class=&#34;math inline&#34;&gt;\(AB\)&lt;/span&gt; implies &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;; therefore,
by transitivity they imply all the other columns.&lt;/p&gt;
&lt;p&gt;In conclusion, we have two candidate keys: &lt;span class=&#34;math inline&#34;&gt;\(\{C\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{A, B\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
We assume that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 1NF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 2NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 3NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in BCNF? Justify your answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 2NF. In fact, all non-prime columns depend
entirely on both candidate keys.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is not in 3NF. In fact, the functional dependency &lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow E\)&lt;/span&gt; is
between two non-prime columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a result, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is not in BCNF either.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
If the table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; from the previous exercise is not in BCNF,
how would you change the schema so that BCNF is satisfied?
For each table, specify the primary key and the foreign keys
linking the tables to each other.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The table is not in 3NF.
The offending functional dependency is &lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow E\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We need to create a new table &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;, where the primary key is
the determinant in the offending functional dependency (&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;).
We then move all columns that are dependent on &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; (only &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; in our case) from &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;.
Note that &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is kept in &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; so that we can use it to link
&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In summary:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R = \{A, B, C, D\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R_1 = \{D, E\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The primary key of &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\{C\}\)&lt;/span&gt; (or, we can also choose &lt;span class=&#34;math inline&#34;&gt;\(\{A, B\}\)&lt;/span&gt; if we want).
The primary key of &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\{D\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The column &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is a foreign key referencing the column &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;question-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 3&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; be a relational table with four columns
&lt;span class=&#34;math inline&#34;&gt;\((A, B, C, D)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\{A, B\}\)&lt;/span&gt; is the only candidate key.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
Identify a minimal set of functional dependencies.
Justify your answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;A candidate key implies all the other columns of the table.
Therefore we have :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[AB \rightarrow C\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[AB \rightarrow D\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both functional dependencies are in canonical form (the right side only consist of one column).
Trivially, both functional dependencies are left-irreducible (we cannot remove any of the columns
in the determinant without losing information).
So, this is a minimal set of functional dependencies.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
Add &lt;span class=&#34;math inline&#34;&gt;\(B \rightarrow D\)&lt;/span&gt; to the set of functional dependencies that you identified
in the previous exercise.
Modify the minimal set of functional dependencies accordingly.
Justify your answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We consider the following functional dependencies:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow D\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B \rightarrow D\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The FD (3) clearly makes FD (2) redundant.
By using the augmentation axiom on (3) we obtain:&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow AD\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By applying the decomposition axiom on (4) we obtain:&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow A\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow D\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which shows that (2) is redundant.&lt;/p&gt;
&lt;p&gt;In summary, we have the following functional dependencies:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[AB\rightarrow C \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[B\rightarrow D \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;
We assume that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 1NF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 2NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 3NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in BCNF? Justify your answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is not in 2NF. In fact, the non-prime column &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is functionally dependent on
only one part of the candidate key.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is not in 3NF, let alone BCNF, because it doesn’t fulfill the first condition, that is
being in 2NF.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 8  &lt;/strong&gt;&lt;/span&gt;
Normalize &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; to the BCNF form.
Justify your choices.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The functional dependency that gives a partial dependency is:
&lt;span class=&#34;math inline&#34;&gt;\(B \rightarrow D\)&lt;/span&gt;.
We create a new table &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;, where the determinant (&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;) of the
offending functional dependency is the primary key.
We move the columns that depend on &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; (here, only &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;) from table
&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; to table &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In summary:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R = (A, B, C)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R_1 = (B, D)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can see that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 3NF because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The only non-prime column is &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, therefore there cannot be any dependency between non-prime columns.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in BCNF because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The only functional dependency &lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt; has a key as its determinant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, for &lt;span class=&#34;math inline&#34;&gt;\(R_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;question-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 4&lt;/h1&gt;
&lt;p&gt;We consider the following table:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Patient (ssn, first_name, last_name, phone_number, insurance_number, 
         insurance_expiration_date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the following set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; of functional dependencies holds:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssn -&amp;gt; first_name, last_name, phone_number, insurance_number, 
       insurance_expiration_date 

insurance_number -&amp;gt; insurance_expiration_date&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 9  &lt;/strong&gt;&lt;/span&gt;Derive a minimal set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; of functional dependencies that is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;First, we need to rewrite the FDs in canonical form.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(ssn \rightarrow first\_name\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(ssn \rightarrow last\_name\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(ssn \rightarrow phone\_number\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(ssn \rightarrow insurance\_number\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(ssn \rightarrow insurance\_expiration\_date\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(insurance\_number \rightarrow insurance\_expiration\_date\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The determinant of each FD is composed of only one column, therefore
it is already left-irreducible.
It is easy to see that all FDs with &lt;TT&gt;ssn&lt;/TT&gt; as a determinant must be
kept (otherwise we lose some information).&lt;/p&gt;
&lt;p&gt;By transitivity from 4. and 6. we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ssn \rightarrow insurance\_expiration\_date\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; is obtained from &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; by removing 5.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 10  &lt;/strong&gt;&lt;/span&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt;, identify the candidate keys in the table &lt;TT&gt;Patient&lt;/TT&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;From the functional dependencies in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;, it’s easy to see that
that the only column that implies all the others
is &lt;TT&gt;ssn&lt;/TT&gt;. Therefore, {&lt;TT&gt;ssn&lt;/TT&gt;} is the only candidate key in
this table.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 11  &lt;/strong&gt;&lt;/span&gt;Specify the normal form of the table &lt;TT&gt;Patient&lt;/TT&gt;. Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It is immediate to verify that the table is 1NF.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table is 2NF because there is only one candidate key, which is composed of
only one column. Therefore, there cannot be any partial dependency.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table is not in 3NF. Indeed, there is a functional dependency
between two non-prime columns:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[insurance\_number \rightarrow insurance\_expiration\_date\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 12  &lt;/strong&gt;&lt;/span&gt;How would you normalize
table &lt;TT&gt;Patient&lt;/TT&gt; to BCNF? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The offending functional dependency is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[insurance\_number \rightarrow insurance\_expiration\_date\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We need to create another table (let’s call it &lt;TT&gt;Insurance&lt;/TT&gt;) that contains
the determinant of the offending functional dependency (insurance_number)
as its primary key.
We need to move the colum (insurance_expiration_date) that depends on insurance_number
from table &lt;TT&gt;Patient&lt;/TT&gt; to table &lt;TT&gt;Insurance&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;In summary:&lt;/p&gt;
&lt;pre&gt;
&lt;TT&gt;Patient&lt;/TT&gt;   (&lt;u&gt;ssn&lt;/u&gt;, first_name, last_name, phone_number, insurance_number)
&lt;TT&gt;Insurance&lt;/TT&gt; (&lt;u&gt;insurance_number&lt;/u&gt;, insurance_expiration_date)
&lt;/pre&gt;
&lt;p&gt;Note that the column &lt;em&gt;insurance_number&lt;/em&gt; in table &lt;TT&gt;Patient&lt;/TT&gt; is foreign key to
the column &lt;em&gt;insurance_number&lt;/em&gt; in table &lt;TT&gt;Insurance&lt;/TT&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Relational data modeling</title>
      <link>/courses/gadexed/tutorials/rel-data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/rel-data-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to &lt;strong&gt;create a conceptual schema&lt;/strong&gt; of a database.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;draw an entity-relationship&lt;/strong&gt; (ER) diagram.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;translate&lt;/strong&gt; a &lt;strong&gt;conceptual model&lt;/strong&gt; into a &lt;strong&gt;logical model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;database-of-a-social-network-platform&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Database of a social network platform&lt;/h1&gt;
&lt;p&gt;A social network platform wants to design a relational database
to store information on its users.
For each user, the platform keeps its nickname,
that uniquely identifies the user in the platform, first and family name,
geographic location (city and country) and email address;
the user can register as many email addresses as s/he wishes.
Any user can share content on the platform; each post is characterized by its content,
date, time and, when available, the geolocation (latitude, longitude).
Optionally, users can tag one or more friends in their posts.&lt;/p&gt;
&lt;p&gt;Two users are linked by a friendship relationship
if both agree on befriending each other;
a user can also follow another user without necessarily befriending her.
For any type of relationship (friendship or follower),
the platform registers the date when the relationship is established.&lt;/p&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-1/social-network-solution.png&#34; alt=&#34;Social network solution&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Translate the conceptual schema into a logical schema.
For each table, underline the primary key and specify the foreign keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The &lt;strong&gt;collection of tables&lt;/strong&gt; is the following:&lt;/p&gt;
&lt;pre&gt;
&lt;TT&gt;UserAccount&lt;/TT&gt;  (&lt;u&gt;nickname&lt;/u&gt;, first_name, last_name, city, country)

&lt;TT&gt;Post&lt;/TT&gt;         (&lt;u&gt;post_id&lt;/u&gt;, content, date, time, lat, long, nickname)

&lt;TT&gt;EmailAddress&lt;/TT&gt; (&lt;u&gt;email_address&lt;/u&gt;, nickname)

&lt;TT&gt;Relationship&lt;/TT&gt; (&lt;u&gt;nickname_src&lt;/u&gt;, &lt;u&gt;nickname_dst&lt;/u&gt;, type, date)

&lt;TT&gt;Tag&lt;/TT&gt;          (&lt;u&gt;post_id&lt;/u&gt;, &lt;u&gt;nickname&lt;/u&gt;)

&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;foreign keys&lt;/strong&gt; are the following:&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Post(nickname)&lt;/TT&gt; → &lt;TT&gt;UserAccount(nickname)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;EmailAddress(nickname)&lt;/TT&gt; → &lt;TT&gt;EmailAddress(nickname)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Relationship(nickname_src)&lt;/TT&gt; → &lt;TT&gt;UserAccount(nickname)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Relationship(nickname_dst)&lt;/TT&gt; → &lt;TT&gt;UserAccount(nickname)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Tag(post_id)&lt;/TT&gt; → &lt;TT&gt;Post(post_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Tag(nickname)&lt;/TT&gt; → &lt;TT&gt;UserAccount(nickname)&lt;/TT&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;database-of-a-banking-system&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Database of a banking system&lt;/h1&gt;
&lt;p&gt;The following figure shows the ER diagram with the
conceptual schema of a banking system database.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:banking-system-db&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-1/banking-system-er.png&#34; alt=&#34;The conceptual schema of the bank database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: The conceptual schema of the bank database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each bank is identified by a unique code and name,
and has one or several branches.
A branch is responsible for opening accounts and
granting loans to customers.
Each account is identified by
a number (&lt;em&gt;acct_nbr&lt;/em&gt;)
and is either a checking or savings account
(property &lt;em&gt;acct_type&lt;/em&gt;).
Each customer is identified by its social
security number (&lt;em&gt;ssn&lt;/em&gt;);
a customer can be granted several loans and open as many accounts as s/he wishes.&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;Which primary key would you choose for the entity Bank? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since no
two banks have the same &lt;em&gt;code_bank&lt;/em&gt; or &lt;em&gt;name&lt;/em&gt;,
either property can be chosen as the primary key
of the entity &lt;TT&gt;Bank&lt;/TT&gt;.
Both can be considered as valid &lt;strong&gt;candidate keys&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Would you consider {&lt;em&gt;code_bank&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;}
as a valid candidate key for the entity &lt;TT&gt;Bank&lt;/TT&gt;? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The answer is &lt;strong&gt;no&lt;/strong&gt;.
While there aren’t any banks that have
the same value for {&lt;em&gt;code_bank&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;},
two subsets ({&lt;em&gt;code_bank&lt;/em&gt;} and {&lt;em&gt;name&lt;/em&gt;}) are candidate keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Complete the
diagram in the figure
by adding the cardinalities to the relations.
Justify your choices when any ambiguity arises.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-1/banking-system-er-complete.png&#34; alt=&#34;Banking system solution&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The &lt;strong&gt;collection of tables&lt;/strong&gt; is the following:&lt;/p&gt;
&lt;pre&gt;
&lt;TT&gt;Bank&lt;/TT&gt;     (&lt;u&gt;code_bank&lt;/u&gt;, name, address)

&lt;TT&gt;Branch&lt;/TT&gt;   (&lt;u&gt;branch_id&lt;/u&gt;, address, code_bank)

&lt;TT&gt;Account&lt;/TT&gt;  (&lt;u&gt;acct_nbr&lt;/u&gt;, acct_type, balance, branch_id, ssn)

&lt;TT&gt;Loan&lt;/TT&gt;     (&lt;u&gt;loan_nbr&lt;/u&gt;, loan_type, amount, branch_id, ssn)

&lt;TT&gt;Customer&lt;/TT&gt; (&lt;u&gt;ssn&lt;/u&gt;, first_name, last_name, telephone, address)

&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;foreign keys&lt;/strong&gt; are the following:&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Branch(code_bank)&lt;/TT&gt; → &lt;TT&gt;Bank(code_bank)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Account(branch_id)&lt;/TT&gt; → &lt;TT&gt;Branch(branch_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Account(ssn)&lt;/TT&gt; → &lt;TT&gt;Customer(ssn)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Loan(branch_id)&lt;/TT&gt; → &lt;TT&gt;Branch(branch_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Loan(ssn)&lt;/TT&gt; → &lt;TT&gt;Customer(ssn)&lt;/TT&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;car-dealership-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Car dealership database&lt;/h1&gt;
&lt;p&gt;We want to design the database of a car dealership.
The dealership sells both new and used cars,
and it operates a service facility.
The database should keep data about the cars
(serial number, make, model, color, whether it is new or used),
the salespeople (first and family name)
and the customers (first and family name, phone number, address).
Also, the following business rules hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A salesperson may sell many cars, but each car is sold by only one salesperson.&lt;/li&gt;
&lt;li&gt;A customer may buy many cars, but each car is bought by only one customer.&lt;/li&gt;
&lt;li&gt;A salesperson writes a single invoice for each car s/he sells.
The invoice is identified by a number and indicates the sale date and the price.&lt;/li&gt;
&lt;li&gt;A customer gets an invoice for each car s/he buys.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a customer takes one or more cars in for repair,
one service ticket is written for each car.
The ticket is identified by a number and
indicates the date on which the car is received from the customer,
as well as the date on which the car should be returned to the customer.
A car brought in for service can be worked on by many mechanics,
and each mechanic may work on many cars.&lt;/p&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/courses/databases/tutorial-1/car-dealership.png&#34; alt=&#34;Car dealership solution&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The &lt;strong&gt;collection of tables&lt;/strong&gt; is the following:&lt;/p&gt;
&lt;pre&gt;
  Car          (&lt;u&gt;serial_number&lt;/u&gt;, make, model, color, is_new)
  Customer     (&lt;u&gt;cust_id&lt;/u&gt;, cust_first_name, cust_last_name, cust_phone) 
  Invoice      (&lt;u&gt;invoice_number&lt;/u&gt;, date, price, car_serial_number, sp_id, cust_id)
  Salesperson  (&lt;u&gt;sp_id&lt;/u&gt;, sp_first_name, sp_last_name)
  Mechanic     (&lt;u&gt;mec_id&lt;/u&gt;, mec_first_name, mec_last_name)
  Ticket       (&lt;u&gt;ticket_number&lt;/u&gt;, date_open, date_return, car_serial_number)
  Repair       (&lt;u&gt;ticket_number&lt;/u&gt;, &lt;u&gt;mec_id&lt;/u&gt;)&lt;/pre&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;foreign keys&lt;/strong&gt; are the following:&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Invoice(cust_id)&lt;/TT&gt; → &lt;TT&gt;Customer(cust_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Invoice(car_serial_number)&lt;/TT&gt; → &lt;TT&gt;Car(serial_number)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Invoice(sp_id)&lt;/TT&gt; → &lt;TT&gt;Salesperson(sp_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Ticket(car_serial_number)&lt;/TT&gt; → &lt;TT&gt;Car(serial_number)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Repair(ticket_number)&lt;/TT&gt; → &lt;TT&gt;Ticket(ticket_number)&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;Repair(mec_id)&lt;/TT&gt; → &lt;TT&gt;Mechanic(mec_id)&lt;/TT&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SQL</title>
      <link>/courses/gadexed/tutorials/sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/gadexed/tutorials/sql/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key integrity constraints in a relational database.&lt;/li&gt;
&lt;li&gt;Basic SQL queries.&lt;/li&gt;
&lt;li&gt;Advanced SQL queries.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;installing-postgresql&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Installing PostgreSQL&lt;/h1&gt;
&lt;p&gt;In this tutorial you’ll be using &lt;strong&gt;PostgreSQL&lt;/strong&gt; as a
relational database management system (RDBMS).&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Instructions for Windows users&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;You can download the latest version of PostgreSQL (14.3)
&lt;a href=&#34;https://www.enterprisedb.com/downloads/postgres-postgresql-downloads&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;from this page&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The installer contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The PostgreSQL server.&lt;/li&gt;
&lt;li&gt;pgAdmin, a graphical administration tool.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You’ll find &lt;strong&gt;detailed installation instructions&lt;/strong&gt; &lt;a href=&#34;https://www.enterprisedb.com/docs/supported-open-source/postgresql/installer/02_installing_postgresql_with_the_graphical_installation_wizard/01_invoking_the_graphical_installer/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;on this page&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Instructions for macOS users&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;The best way to get PostgreSQL is to download and install &lt;strong&gt;Postgres.app&lt;/strong&gt;.
You’ll find more details &lt;a href=&#34;https://postgresapp.com/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;on this page&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition, you need to &lt;a href=&#34;https://www.pgadmin.org/download/&#34; target=&#34;blank&#34;&gt;&lt;strong&gt;install pgAdmin&lt;/strong&gt;&lt;/a&gt;,
ad administration tool that lets you interact with your database through a
graphical interface.&lt;/p&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-postgresql-and-pgadmin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Starting PostgreSQL and pgAdmin&lt;/h1&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Instructions for Windows users&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;Once the installation is completed, a PostgreSQL server should be
automatically started.&lt;/p&gt;
&lt;p&gt;You’ll open a connection to the server through the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute pgAdmin.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the left-side menu, right-click on &lt;em&gt;Servers&lt;/em&gt; and select &lt;em&gt;Create server&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;General&lt;/em&gt; tab, give the server a name of your choice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Connection&lt;/em&gt; tab, write &lt;em&gt;localhost&lt;/em&gt; as &lt;em&gt;host&lt;/em&gt;.
Don’t change the values of the other options.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;What if I get the &lt;em&gt;Unable to connect to server&lt;/em&gt; error?&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;You might want to have a look at the list of the services running on your computer to verify
whether the PostgreSQL server is actually running.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dell.com/support/kbdoc/it-it/000126393/how-to-use-windows-services-to-start-or-stop-services&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;This page&lt;/strong&gt;&lt;/a&gt;
explains you how to open the panel with the list of services.
You should locate PostgreSQL in this panel. If the service is not running, you’ll
need to manually start it.&lt;/p&gt;
&lt;/details&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Instructions for MacOS users&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;You’ll open a connection to the server through the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute pgAdmin.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the left-side menu, right-click on &lt;em&gt;Servers&lt;/em&gt; and select &lt;em&gt;Create server&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;General&lt;/em&gt; tab, give the server a name of your choice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Connection&lt;/em&gt; tab, write &lt;em&gt;localhost&lt;/em&gt; as &lt;em&gt;host&lt;/em&gt;.
Don’t change the values of the other options.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;obtain-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Obtain the data&lt;/h1&gt;
&lt;p&gt;We consider the database of a DVD rental store chain containing data on films, actors,
customers and the transactions of the store.&lt;/p&gt;
&lt;p&gt;This database goes by the name &lt;strong&gt;Sakila&lt;/strong&gt;&lt;br /&gt;
and was developed by Mike Hillyer, a former member of the MySQL team.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The conceptual schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The conceptual schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The tables of the database are &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-structure-tables.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;documented on this page&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sakila has been ported from MySQL to PostgreSQL under the name &lt;strong&gt;pagila&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In order to import the data into PostgreSQL, follow the steps below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the pgAdmin interface, create a new database (right-click on &lt;em&gt;Databases&lt;/em&gt;, select &lt;em&gt;Create database&lt;/em&gt;).
You can call the new database &lt;strong&gt;pagila&lt;/strong&gt; (or any name you want).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-4/pagila-insert-data.sql&#34;&gt;&lt;strong&gt;dump file pagila-insert-data.sql&lt;/strong&gt;&lt;/a&gt;
This file contains all the SQL statements necessary to create the tables of the database
and populate them with some data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Right-click on the new database, select &lt;em&gt;Query tool&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on the second (from the left) small icon on the top menu of the query tool to open a new file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Through the interface, locate &lt;em&gt;pagila-schema.sql&lt;/em&gt; and open it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hit on the F5 button or click on the &lt;em&gt;play&lt;/em&gt; icon on the top menu of the query tool to execute the script.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The previous steps should have created the schema of the database&lt;br /&gt;
and inserted the data.&lt;/p&gt;
&lt;p&gt;You can see the list of tables
by selecting (on the left menu) &lt;em&gt;Schemas&lt;/em&gt;, &lt;em&gt;public&lt;/em&gt;, &lt;em&gt;Tables&lt;/em&gt; (15 tables should appear).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open a new query tool&lt;/strong&gt; to do the exercises.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integrity-constraints&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Integrity constraints&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
insert into film_actor values(1, 25)
&lt;/pre&gt;
&lt;p&gt;What is this statement supposed to do?
What is the reaction of the DBMS?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;This statement should insert a new row in table &lt;TT&gt;film_actor&lt;/TT&gt;,
where the value of &lt;TT&gt;actor_id&lt;/TT&gt; is 1 and the value of
&lt;TT&gt;film_id&lt;/TT&gt; is 25.&lt;/p&gt;
&lt;p&gt;The DBMS returns an error because there is already a row
with these values and the two columns
&lt;TT&gt;film_actor&lt;/TT&gt;, &lt;TT&gt;film_id&lt;/TT&gt; form the primary key.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
Write the statement to delete the film with
&lt;em&gt;film_id&lt;/em&gt;=1 from
the table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;Execute the command. What happens?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;PRE&gt;
delete from film where film_id=1
&lt;/PRE&gt;
&lt;p&gt;The statement is rejected because there are rows in other tables
that reference the row that we intend to delete.
This is the effect of the foreign key constraint.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
Look at the definition of the foreign key constraints
in table &lt;TT&gt;film_actor&lt;/TT&gt; (right-click on the table,
select &lt;em&gt;Constraints&lt;/em&gt;, &lt;em&gt;foreign key&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Is the definition of the foreign key constraint
to table &lt;TT&gt;film&lt;/TT&gt; coherent with the behavior observed
in the previous exercise?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; In order to see the options of a foreign key, click on the
edit button on the left of the constraint. Then look at the
tab &lt;em&gt;Action&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The foreign key linking table &lt;TT&gt;film_actor&lt;/TT&gt; to table
&lt;TT&gt;film&lt;/TT&gt; is defined with the option RESTRICT on delete.
This is coherent with the behavior that we observed
in the previous exercise. Referenced rows cannot be deleted if
there are still referencing rows.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Execute the following query:&lt;/p&gt;
&lt;pre&gt;
SELECT f.film_id as film_id_in_table_film, 
       fa.film_id AS film_id_in_table_film_actor, 
       fa.actor_id as actor_id, f.title as title
FROM film f JOIN film_actor fa ON f.film_id = fa.film_id
WHERE  f.title=&#39;ACE GOLDFINGER&#39;
&lt;/pre&gt;
&lt;p&gt;What does the query? Note down the identifier of the film in both tables
&lt;TT&gt;film&lt;/TT&gt; and &lt;TT&gt;film_actor&lt;/TT&gt;.
(columns &lt;TT&gt;film_id_in_table_film&lt;/TT&gt; and &lt;TT&gt;film_id_in_table_film_actor&lt;/TT&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The query returns the list of all actors in the film titled Ace Goldfinger.
We note that the identifier of the film in both tables is identical (2), as it should
because the query joins the two tables on the equality of these two values.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
Write and execute a statement to set the value 10000 to the identifier of the
film ACE GOLDFINGER in table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;After the modification, execute the query of the previous exercise.
What changed? Explain in details what happened.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The statement to modify the &lt;TT&gt;film_id&lt;/TT&gt; of the given film is as follows:&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;
UPDATE film
SET film_id=10000
WHERE title=‘ACE GOLDFINGER’
&lt;/TT&gt;&lt;/p&gt;
&lt;p&gt;After executing the same query as the previous exercise, we see that
the identifier of the film has changed in the table &lt;TT&gt;film_actor&lt;/TT&gt; too.
This is expected, because the foreign key constraint between the
colum &lt;TT&gt;film_id&lt;/TT&gt; in table &lt;TT&gt;film_actor&lt;/TT&gt; and the column
&lt;TT&gt;film_id&lt;/TT&gt; in table &lt;TT&gt;film&lt;/TT&gt; has the option ON UPDATE CASCADE.
This means that if we modify the identifier of the film in table
&lt;TT&gt;film&lt;/TT&gt;, the modification is propagated to all the referencing columns.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
UPDATE film_actor
SET film_id=2
WHERE film_id=10000
&lt;/pre&gt;
&lt;p&gt;What does? What happens? Explain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The statement intends to set the identifier of the film
titled Ace Goldfinger (in the previous exercise we gave it the identifier 10000)
back to its original value.
However, we execute the statement on the table &lt;TT&gt;film_actor&lt;/TT&gt;.
The action is not allowed, as the identifier 2 does not correspond to
any film in table &lt;TT&gt;film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;The foreign key enforces the referential integrity constraint.
A row cannot refer to a non-existing entity in the referenced table.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the first and last name of all customers of the store with identifier 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select * 
from customer
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-and-paginating&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting and paginating&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 8  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the last and first name of all customers. Sort by last name in ascending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 1., but only return the first 100 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the last and first name of all customers of the store with identifier 1.
Sort by last name in ascending order and by first name in descending order.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select last_name, first_name 
from customer
order by last_name asc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select last_name, first_name 
from customer
order by last_name asc
limit 100
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregating-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Aggregating queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 9  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Count the number of films in the database (expected result: 1000).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How many distinct actor last names are there?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the total amount of payments across all rentals (expected result: 67416.51).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the average, minimum and maximum duration of rental across all films
(expected result: 4.9850000000000000, 3, 7).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of copies of each film in each store (table &lt;TT&gt;inventory&lt;/TT&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as 6., but only returns the pairs (film, store) if the number of copies is
greater than or equal to 3.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select count(*) 
from film
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select  count (distinct last_name) 
from actor
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select sum(amount) 
from payment
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select avg(rental_duration), min(rental_duration), max(rental_duration) 
from film
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, count(*) as nb_actors
from film_actor
group by film_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, store_id, count(*) as nb_films
from inventory
group by film_id, store_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, store_id, count(*) as nb_films
from inventory
group by film_id, store_id
having count(*) &gt;=3
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;join-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Join queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 10  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the actors in the film ACE GOLDFINGER.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return first and last name of each actor and the number of films in which they played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 3., but order by number of films in descending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 4., but only return actors who played a role in at least 10 films.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the identifier, the first and family name of the customers who have rented between 10 and 20
movies in the category &lt;em&gt;Family&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name
from staff join store using(store_id)
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name
from film join film_actor using(film_id) join actor using(actor_id)
where title=&#39;ACE GOLDFINGER&#39;
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
order by nb_films desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
having count(*) &gt;= 10
order by nb_films desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select cust.customer_id, first_name, last_name, count(*) as nb_films
from customer cust join rental using(customer_id) 
  join inventory using(inventory_id)
    join film_category using(film_id)
    join category cat using(category_id)
where cat.name=&#39;Family&#39;
group by customer_id
having count(*) between 5 and 10
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;miscellaneous-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Miscellaneous queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 11  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Which last names are &lt;strong&gt;not&lt;/strong&gt; repeated in table &lt;em&gt;actor&lt;/em&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is a copy of the movie ACADEMY DINOSAUR available for rent from store 1?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the title and the release year of all films in one of the following categories:
&lt;em&gt;Family&lt;/em&gt;, &lt;em&gt;Animation&lt;/em&gt;, &lt;em&gt;Documentary&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_in.asp&#34; target=&#34;_blank&#34;&gt;operator IN&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find all customers (id, last name, first name) whose last name starts with the letter L.
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_like.asp&#34; target=&#34;_blank&#34;&gt;operator LIKE&lt;/a&gt;
&lt;/details&gt;&lt;/li&gt;
&lt;li&gt;Return the total paid by each customer. For each customer, display a single
column containing first and last name and another column with the total amount paid.
Order the result alphabetically by last name&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/func_sqlserver_concat.asp&#34; target=&#34;_blank&#34;&gt;operator CONCAT&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the total revenue from the rentals across the stores in each country.
Order by descending revenue.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;The first and last name of the actor that played in the most films.
If two or more actors are tied, the query must return the names of all of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select last_name
from actor
group by last_name
having count(*) = 1
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select distinct i.inventory_id
from film f join inventory i using(film_id)
join rental r using(inventory_id)
where f.title=&#39;ACADEMY DINOSAUR&#39; 
    and i.store_id=1 
    and r.return_date is not null  
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select  distinct f.title, f.release_year
from film f join film_category using(film_id)
join category cat using(category_id)
where cat.name in (&#39;Family&#39;, &#39;Animation&#39;, &#39;Documentary&#39;)
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select customer_id, first_name, last_name
from customer
where last_name LIKE &#39;L%&#39;
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select concat(first_name, &#39; &#39;, last_name), sum(amount)
from customer join payment using (customer_id)
group by customer_id
order by last_name asc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select country, sum(amount) as revenue
from payment join rental using (rental_id)
  join inventory using (inventory_id)
  join store using (store_id)
  join address using (address_id)
  join city using (city_id)
  join country using (country_id)
group by country_id
order by revenue desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;select actor_id, first_name, last_name, count(&lt;em&gt;)
from film_actor join actor using(actor_id)
group by actor_id
having count(&lt;/em&gt;) =
(select max(nb_films)
from (select count(*) as nb_films
from film_actor
group by actor_id) t)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Etude de cas: HTTP</title>
      <link>/courses/network/labs/http-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/network/labs/http-lab/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://gaia.cs.umass.edu/kurose_ross/wireshark.php&#34; target=&#34;_blank&#34;&gt;Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;L’objectif de ce TD est d’explorer le protocole HTTP et avoir un aperçu des principales caractéristiques des protocoles des couches inférieures.&lt;/p&gt;
&lt;div id=&#34;http-requêtes-et-réponses-simples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; HTTP: requêtes et réponses simples&lt;/h1&gt;
&lt;p&gt;Nous commençons notre exploration du protocole HTTP en téléchargeant un fichier HTML très simple et court et ne contenant pas d’objets intégrés, comme par exemple des images.&lt;/p&gt;
&lt;p&gt;Procédez comme suit :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Démarrez votre navigateur web.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez Wireshark, mais sans commencer encore la capture de paquets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez &lt;em&gt;http&lt;/em&gt; dans la fenêtre de spécification du filtre d’affichage,
afin que seuls les messages HTTP capturés soient affichés plus tard dans la fenêtre de liste de paquets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Commencez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur l’URL suivant : &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file1.html&#34; target=&#34;_blank&#34;&gt;lien vers la page HTML&lt;/a&gt;. Votre navigateur devrait afficher le fichier HTML très simple d’une ligne.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Votre navigateur utilise-t-il la version 1.0, 1.1 ou 2 du protocole HTTP ? Quelle est la version de HTTP utilisée par le serveur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Quelles langues (le cas échéant) votre navigateur indique-t-il qu’il peut accepter sur le serveur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de votre ordinateur ? Quelle est l’adresse IP du serveur &lt;em&gt;gaia.cs.umass.edu&lt;/em&gt; ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Quel est le code d’état renvoyé par le serveur à votre navigateur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Quand le fichier HTML que vous récupérez a-t-il été modifié pour la dernière fois sur le serveur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
Combien d’octets de contenu sont renvoyés à votre navigateur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- ::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-7&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-7) &lt;/strong&gt;&lt;/span&gt;
En inspectant les données brutes dans la fenêtre de contenu des paquets, voyez-vous des en-têtes dans les données qui ne sont pas affichées dans la fenêtre de liste des paquets ?  
Dans l&#39;affirmative, citez-en un.
&lt;/div&gt;\EndKnitrBlock{exercise}

::: --&gt;
&lt;/div&gt;
&lt;div id=&#34;http-requêtes-conditionnelles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; HTTP: requêtes conditionnelles&lt;/h1&gt;
&lt;p&gt;La plupart des navigateurs Web mettent en cache les objets téléchargés (pages HTML, images.. )
et effectuent donc souvent une requête conditionnalle lors de la récupération d’un objet HTTP.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assurez-vous que le cache de votre navigateur est vide.
&lt;a href=&#34;https://www.howtogeek.com/304218/how-to-clear-your-history-in-any-browser/&#34; target=&#34;_blank&#34;&gt;Cette page&lt;/a&gt; vous expliquera comment faire.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Procédez maintenant comme suit :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture des paquets dans Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file2.html&#34; target=&#34;_blank&#34;&gt;ce lien&lt;/a&gt;. Votre navigateur doit afficher un fichier HTML très simple de cinq lignes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez simplement sur le bouton “rafraîchir” de votre navigateur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets Wireshark et saisissez &lt;em&gt;http&lt;/em&gt; dans la fenêtre de spécification du filtre d’affichage.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ideally, you should see an If-Modified-Since header since you’ve just downloaded this page a few seconds ago.  However, depending on the browser you’re using, and the format of the server’s earlier response to your initial GET, your browser may not include an If-Modified-Since even if the document has been downloaded and caches. The Chrome browser is pretty good at regularly using If-Modified-Since.  But Safari and Firefox are much more finicky about when to use If-Modified-Since. Life isn’t always as easy in practice as it is in theory! --&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Examinez le contenu de la première requête HTTP GET envoyée par votre navigateur au serveur. Voyez-vous une ligne “IF-MODIFIED-SINCE” dans la requête HTTP GET ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Examinez le contenu de la réponse du serveur. Le serveur a-t-il explicitement renvoyé le contenu du fichier ? Comment pouvez-vous le savoir ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Inspectez maintenant le contenu de la deuxième requête HTTP GET envoyée par votre navigateur au serveur. Voyez-vous une ligne “IF-MODIFIED-SINCE :” dans la requête HTTP GET ? Si oui, quelles informations suivent l’en-tête “IF-MODIFIED-SINCE :”?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Quel est le code d’état HTTP et la phrase renvoyée par le serveur en réponse à ce deuxième HTTP GET ? Le serveur a-t-il explicitement renvoyé le contenu du fichier ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;récupération-de-documents-longs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Récupération de documents longs&lt;/h1&gt;
&lt;p&gt;Dans nos exemples jusqu’à présent, les documents récupérés étaient des fichiers HTML simples et courts.
Voyons maintenant ce qui se passe lorsque nous téléchargeons un long fichier HTML. Procédez comme suit :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Démarrez votre navigateur web et assurez-vous que le cache de votre navigateur a été vidé, comme nous l’avons vu plus haut.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file3.html&#34; target=&#34;_blank&#34;&gt;ce lien&lt;/a&gt;. Votre navigateur devrait afficher l’assez longue Déclaration des droits des États-Unis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans la fenêtre de listage des paquets, vous devriez voir votre message HTTP GET,
suivi de plusieurs segments TCP.&lt;br /&gt;
Assurez-vous que le filtre d’affichage de Wireshark est désactivé pour que les segments TCP soient affichés dans la liste des paquets.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Pourquoi obtenez-vous autant de segments TCP ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Combien de messages de requête HTTP GET votre navigateur a-t-il envoyés ? Quel numéro de paquet dans la trace contient le message GET pour la Déclaration des droits ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Quel numéro de paquet dans la trace contient le code d’état et la phrase associés à la réponse à la requête HTTP GET ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
Quel est le code d’état et la phrase de la réponse ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Combien de segments TCP contenant des données ont été nécessaires pour transporter la réponse HTTP unique et le texte de la Déclaration des droits ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;documents-html-avec-objets-intégrés&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Documents HTML avec objets intégrés&lt;/h1&gt;
&lt;p&gt;Maintenant que nous avons vu comment Wireshark affiche le trafic de paquets capturés pour de gros fichiers HTML,
nous pouvons examiner ce qui se passe lorsque votre navigateur télécharge un fichier avec des objets intégrés,
c’est-à-dire un fichier qui inclut d’autres objets (dans l’exemple ci-dessous, des fichiers images) qui sont stockés sur un ou plusieurs autres serveurs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Démarrez votre navigateur web, et assurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/HTTP-wireshark-file4.html&#34; target=&#34;_blank&#34;&gt;ce lien&lt;/a&gt;.
Votre navigateur doit afficher un court fichier HTML contenant deux images.
Ces deux images sont référencées dans le fichier HTML de base.&lt;br /&gt;
En d’autres termes, les images elles-mêmes ne sont pas contenues dans le fichier HTML,
mais les URL des images sont contenues dans le fichier HTML téléchargé.
Comme indiqué dans le manuel, votre navigateur devra récupérer ces logos sur les sites web indiqués.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Combien de requêtes HTTP GET votre navigateur a-t-il envoyées ? À quelles adresses Internet ces requêtes GET ont-elles été envoyées ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
Pouvez-vous dire si votre navigateur a téléchargé les deux images en série ou si elles ont été téléchargées en parallèle à partir des deux sites web ? Expliquez.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;authentification-http&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Authentification HTTP&lt;/h1&gt;
&lt;p&gt;Nous essayons maintenant de visiter un site web protégé par un mot de passe et examinons la séquence de messages HTTP échangés pour un tel site.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus, et fermez votre navigateur. Ensuite, redémarrez votre navigateur&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez la capture de paquets dans Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;a href=&#34;http://gaia.cs.umass.edu/wireshark-labs/protected_pages/HTTP-wireshark-file5.html&#34; target=&#34;_blank&#34;&gt;ce lien&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez le nom d’utilisateur (&lt;em&gt;wireshark-students&lt;/em&gt;) et le mot de passe (&lt;em&gt;network&lt;/em&gt;) demandés dans la fenêtre contextuelle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Quelle est la réponse du serveur (code d’état et phrase) au message HTTP GET initial de votre navigateur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Lorsque votre navigateur envoie le message HTTP GET pour la deuxième fois, quel nouveau champ est inclus dans le message HTTP GET ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Le nom d’utilisateur (&lt;em&gt;wireshark-students&lt;/em&gt;) et le mot de passe (&lt;em&gt;network&lt;/em&gt;)
que vous avez saisis sont encodés dans la chaîne de caractères
&lt;em&gt;d2lyZXNoYXJrLXN0dWRlbnRzOm5ldHdvcms=&lt;/em&gt; qui suit l’en-tête “Authorization : Basic”
dans le message HTTP GET du client.&lt;br /&gt;
Bien qu’il puisse sembler que votre nom d’utilisateur et votre mot de passe soient cryptés,
ils sont simplement encodés dans un format connu sous le nom de format Base64.
Le nom d’utilisateur et le mot de passe ne sont pas cryptés !&lt;br /&gt;
D’ailleurs Wireshark décode la chaine dans le champs “Credentials”.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Etude de cas: ICMP</title>
      <link>/courses/network/labs/icmp-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/network/labs/icmp-lab/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://gaia.cs.umass.edu/kurose_ross/wireshark.php&#34; target=&#34;_blank&#34;&gt;Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dans ce TP, nous allons explorer plusieurs aspects du protocole ICMP (Internet Control Message Protocol) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les messages ICMP générés par le programme &lt;code&gt;Ping&lt;/code&gt; ;&lt;/li&gt;
&lt;li&gt;les messages ICMP générés par le programme &lt;code&gt;Traceroute&lt;/code&gt; ;&lt;/li&gt;
&lt;li&gt;le format et le contenu d’un message &lt;code&gt;ICMP&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pour rappel, le protocole ICMP est utilisé par les hôtes et
les routeurs pour se communiquer des informations au niveau de la couche réseau.
L’utilisation la plus courante de l’ICMP
est le signalement d’erreurs.
Par exemple, lors de l’exécution d’une session HTTP,
vous avez peut-être rencontré un message d’erreur tel
que “Destination network unreachable” (réseau de destination inaccessible).
Ce message trouve son origine dans l’ICMP.
À un moment donné, un routeur n’a pas été en mesure de trouver
un chemin vers l’hôte spécifié dans votre requête HTTP.
Ce routeur a créé et envoyé un message ICMP à votre hôte pour lui signaler l’erreur.&lt;/p&gt;
&lt;div id=&#34;icmp-et-ping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; ICMP et Ping&lt;/h1&gt;
&lt;p&gt;Commençons notre étude sur ICMP
en capturant les paquets générés par le programme &lt;code&gt;Ping&lt;/code&gt;.
&lt;code&gt;Ping&lt;/code&gt; est un outil simple qui permet à n’importe qui
(par exemple, un administrateur réseau)
de vérifier si un hôte est actif ou non.
Le programme &lt;code&gt;Ping&lt;/code&gt; de l’hôte source
envoie un paquet ICMP à l’adresse IP cible ;
si la cible est active,
le programme &lt;code&gt;Ping&lt;/code&gt; de l’hôte cible répond
en renvoyant un paquet ICMP à l’hôte source.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ouvrez un terminal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez Wireshark et commencez la capture de paquets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;ping www.ust.hk&lt;/code&gt; dans le terminal pour solliciter une réponse ICMP du serveur Web de l’Université des sciences et technologies de Hong Kong.
Exécutez ensuite le programme Ping en tapant la touche “Entrée”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Au bout d’une dizaine de réponses, arrêtez &lt;code&gt;ping&lt;/code&gt; avec la combinaison de touches CTRL+C.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrêtez la capture de paquets dans Wireshark.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardez la sortie dans le terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Que signfie le mot RTT que vous voyez à chaque réponse ICMP dans le terminal ?
Vous pouvez rechercher l’information sur Internet.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Saisissez &lt;code&gt;icmp&lt;/code&gt; dans la fenêtre de filtres Wireshark pour n’afficher que les paquets ICMP dans Wireshark.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de votre hôte ? Quelle est l’adresse IP de l’hôte de destination ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Quelle est la valeur de TTL utilisée par votre système d’exploitation lors de la préparation
d’un datagramme IP ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Pourquoi un paquet ICMP n’a-t-il pas de numéros de port source et de port de destination ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Examinez l’un des paquets de requête ping envoyés par votre hôte.
Quels sont les numéros de type et de code ICMP ?
Quels sont les autres champs de ce paquet ICMP ?
Combien d’octets représentent les champs somme de contrôle (&lt;em&gt;checksum&lt;/em&gt;),
numéro de séquence (&lt;em&gt;sequence number&lt;/em&gt;) et identifiant (&lt;em&gt;identifier&lt;/em&gt;) ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
Examinez le paquet de réponse ping correspondant.
Quels sont les numéros de type et de code ICMP ?
Quel est le numéro de séquence ? Est-ce que cette valeur correspond à celle que vous avez vue
dans le message ping envoyé par votre ordinateur ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;icmp-et-traceroute&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; ICMP et Traceroute&lt;/h1&gt;
&lt;p&gt;Poursuivons notre étude de ICMP
en capturant les paquets générés par le programme &lt;code&gt;Traceroute&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Traceroute&lt;/code&gt; peut être utilisé pour déterminer le chemin emprunté par un paquet de la source à la destination.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Traceroute&lt;/code&gt; est implémenté de différentes manières sous Unix/Linux/MacOS et sous Windows.
Sous Unix/Linux, la source envoie une série de paquets UDP à la destination cible en utilisant
un numéro de port de destination improbable ;
sous Windows, la source envoie une série de paquets ICMP à la destination cible.
Pour les deux systèmes d’exploitation, le programme envoie le premier paquet avec TTL=1,
le deuxième avec TTL=2, et ainsi de suite.
Rappelons qu’un routeur décrémente la valeur TTL d’un paquet au fur et à mesure que celui-ci le traverse.
Lorsqu’un paquet arrive à un routeur avec TTL=1, le routeur envoie
un paquet d’erreur ICMP à la source.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Si vous travaillez sous Windows&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;Vous pourrez utiliser &lt;code&gt;Traceroute&lt;/code&gt; en ouvrant une fenêtre de PowerShell et saisissant la commande &lt;code&gt;tracert&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Si vous travaillez sous macOS/Linux&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;Vous pourrez utiliser &lt;code&gt;Traceroute&lt;/code&gt; en ouvrant une fenêtre de terminal et saisissant la commande &lt;code&gt;traceroute&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Commencez par ouvrir une fenêtre de terminal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez Wireshark et commencez la capture de paquets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande appropriée de &lt;code&gt;Traceroute&lt;/code&gt; suivie par &lt;code&gt;www.ust.hk&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque le programme &lt;code&gt;Traceroute&lt;/code&gt; se termine, arrêtez la capture de paquets dans Wireshark.
Il est possible que le programme ne termine pas, dans ce cas, arrêtez-le avec CTRL-C.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans Wireshark utilisez le filtre &lt;code&gt;icmp&lt;/code&gt; ou &lt;code&gt;udp&lt;/code&gt; (&lt;code&gt;icmp || udp&lt;/code&gt;)
pour n’afficher que les paquets de &lt;code&gt;Traceroute&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pour obtenir des informations sur des adresses IP, vous pouvez toujours
un moteur de recherche mis à disposition par le RIPE : rendez-vous sur
&lt;a href=&#34;https://stat.ripe.net/app/launchpad&#34; target=&#34;_blank&#34;&gt;cette page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Quelle est l’adresse IP de votre hôte ? Quelle est l’adresse IP de l’hôte cible ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Pouvez-vous determiner la route géographique suivie par le paquet ? Observez notamment les délais associés aux réponses.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Quelle est la valeur du champ &lt;em&gt;Protocol&lt;/em&gt; dans l’en-tête IP ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Examinez le paquet ICMP error dans votre capture d’écran.
Quels champs/propriétés apparaîssent ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-service applications in the Cloud</title>
      <link>/courses/cloud-computing/tutorials/kube-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-computing/tutorials/kube-lab/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Objectives&lt;/h1&gt;
&lt;p&gt;In this lab assignment you will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Build and deploy a multi-service application with &lt;strong&gt;Docker Compose&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy a multi-service application on a &lt;strong&gt;local Kubernetes cluster&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy a multi-service application on a &lt;strong&gt;Kubernetes cluster&lt;/strong&gt; in &lt;strong&gt;Microsoft Azure&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;submission&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Submission&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In order to submit your work, you need to answer all the questions
that you &lt;strong&gt;&lt;a href=&#34;https://centralesupelec.edunao.com/mod/quiz/view.php?id=151760&#34; target=&#34;_blank&#34;&gt;find here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Only one member&lt;/strong&gt; of the group must fill in the questionnaire.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can answer the open questions either in French or in English.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some questions require you to upload files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can pause the test at any moment. Your answers will be stored; you’ll find them when you resume the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After answering the last question, you need to click on the button &lt;em&gt;Terminer le test&lt;/em&gt; (&lt;em&gt;Finish attempt&lt;/em&gt; if you use the interface in English) to submit the questionnaire.
&lt;strong&gt;The submission is final. After submitting, you cannot change your answers anymore.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Deadline: 23 May 2024, 23h59&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;context&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Context&lt;/h1&gt;
&lt;p&gt;We intend to build and deploy a Web application called &lt;em&gt;TripMeal&lt;/em&gt; that let users share their favorite recipes.
You can download the application
&lt;a href=&#34;/courses/cloud-computing/kube-lab/tripmeal.zip&#34;&gt;here&lt;/a&gt;.
The source code has been readapted from
&lt;a href=&#34;https://github.com/DanielAndreasen/TripMeal&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The downloaded file is an archive.
Extracting the archive will create a folder named
&lt;code&gt;tripmeal_sujet&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bulding-and-deploying-with-docker-compose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Bulding and deploying with Docker Compose&lt;/h1&gt;
&lt;p&gt;You’re going to build and deploy the application using Docker Compose.
In the folder &lt;code&gt;tripmeal_sujet&lt;/code&gt; you should see a file named &lt;em&gt;docker-compose.yml&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. What is the file &lt;em&gt;docker-compose.yml&lt;/em&gt; used for?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Look at the files and folders inside the folder &lt;code&gt;tripmeal_sujet&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. How many services does the application &lt;em&gt;TripMeal&lt;/em&gt; have?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; in the directory &lt;code&gt;db&lt;/code&gt; is already implemented.
Open it and read the content to answer the following questions.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. Which DataBase Management System (DBMS) is used in the &lt;em&gt;TripMeal&lt;/em&gt; application?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. What is the version (tag) of the image used for the database management system of &lt;em&gt;TripMeal&lt;/em&gt;?
Find the documentation of the image and use it to answer this question.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Where does Docker get the base image for the database management system from?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dockerfile-in-folder-web&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Dockerfile in folder &lt;code&gt;web&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;If you open folder &lt;code&gt;web&lt;/code&gt;, you’ll see that the Dockerfile is empty.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Complete the &lt;code&gt;Dockerfile&lt;/code&gt; in the directory &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You need a Python 3.7 environment to run the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The application consists of several files and folders. You need to include all of them into a Docker image.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add the following instruction in your Dockerfile:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;RUN chmod -x path_to_file_app&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;path_to_file_app&lt;/code&gt; is the path to the file &lt;code&gt;app.py&lt;/code&gt; &lt;strong&gt;inside the image&lt;/strong&gt;.
This will prevent an &lt;code&gt;Exec format error&lt;/code&gt; from occurring.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Time to make sure that we can build an image from this Dockerfile and we can run a container from that image.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Build an image from the Dockerfile that you’ve just completed using the &lt;code&gt;docker build&lt;/code&gt; command.
You can give the image any name you want. For simplicity, I assume that you call it &lt;code&gt;web-test&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run a container from this image by typing the following command. The command will only work if you type
it from the directory where the file &lt;code&gt;tripmeal.env&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker run --env-file tripmeal.env -p 3000:5000 web-test&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open a Web browser and type the following URL: &lt;code&gt;localhost:3000&lt;/code&gt;. You should see the interface of the application. If that’s not the case, stop the container,
and correct the Dockerfile.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you click on the links &lt;em&gt;New recipe&lt;/em&gt;, &lt;em&gt;All recipes&lt;/em&gt;, &lt;em&gt;Weekly menu&lt;/em&gt; and &lt;em&gt;Login&lt;/em&gt; in the application interface, you will get a connection error to the database server.
This is normal and will be fixed later.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the image passes the test, &lt;strong&gt;upload the Dockerfile to Edunao.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-environment-variables&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; The environment variables&lt;/h2&gt;
&lt;p&gt;When testing the image of the service &lt;code&gt;web&lt;/code&gt;, you may have noticed that we passed the file &lt;code&gt;tripmeal.env&lt;/code&gt; as an argument to the command &lt;code&gt;docker run&lt;/code&gt;.
This file contains the definition of &lt;strong&gt;environment variables&lt;/strong&gt; that are used in the application.
Open this file and look at the environment variables to answer the following questions.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. What does the environment variable SERVER_PORT refer to?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. Which of the following sentences are true?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;When you execute TripMeal, SERVER_PORT will be opened on the network interface of the host computer (your computer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you execute TripMeal, SERVER_PORT will be opened on the network interface of a container.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to access the Web interface of TripMeal, you’ll need to map SERVER_PORT to a port number p; p will be opened on the network interface of the host computer (your computer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you execute TripMeal, you’ll be able to access the Web interface by typing the URL localhost:$SERVER_PORT in your browser.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.9  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao.
When you write the file &lt;code&gt;docker-compose.yml&lt;/code&gt;, you’ll need to give the database service a name.
Which of the environment variables tells you the name that you must use?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;volumes-and-networks&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Volumes and networks&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;TripMeal&lt;/em&gt; application keeps user information and recipe information in a database.
Therefore, you’ll need to define a Docker &lt;strong&gt;volume&lt;/strong&gt; to hold the data.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.10  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. Look at the documentation of the base image of the database management system of the application.
Which directory inside the database image will you attach the volume to?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The containers composing the &lt;em&gt;TripMeal&lt;/em&gt; application need to communicate through a Docker &lt;strong&gt;network&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.11  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. If you don’t explicitly create any network, the containers will be able to communicate anyway.
How do you explain it?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; You can answer this question after writing the Docker compose file and running the application.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.12  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. Why is not creating a network for the application considered a bad idea,
even if the application works?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-compose-file&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; The compose file&lt;/h2&gt;
&lt;p&gt;You’re finally ready to complete the file &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Write the file &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remember that you need to pass the &lt;strong&gt;environment variables&lt;/strong&gt;
to your application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a documentation of Docker Compose you can use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;/courses/cloud-computing/tutorials/tutorial-kube&#34; target=&#34;_blank&#34;&gt;examples that we’ve seen together&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The overview presented on the &lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34;&gt;official Docker website&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also find the full specification of Compose &lt;a href=&#34;https://github.com/compose-spec/compose-spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application and test it. You should be able to see the Web interface of the application, create an account, share a recipe and
look at the list of shared recipes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you experience problems, you might find the command &lt;code&gt;docker-compose logs&lt;/code&gt; useful.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;If you successfully deployed the application, you can continue.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Shut down the application!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Shut down both the application before you move on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.13  &lt;/strong&gt;&lt;/span&gt;
Upload your file &lt;code&gt;docker-compose.yml&lt;/code&gt; to Edunao.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.14  &lt;/strong&gt;&lt;/span&gt;
&lt;u&gt;Use Docker Compose&lt;/u&gt; to push all the images that you built in this section to
your DockerHub registry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the answer to this exercise write the link to the uploaded images.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;local-kubernetes-cluster&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Local Kubernetes cluster&lt;/h1&gt;
&lt;p&gt;We intend to deploy the application &lt;em&gt;TripMeal&lt;/em&gt; on
a &lt;strong&gt;local Kubernetes cluster&lt;/strong&gt; (either Docker Desktop or Minikube).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. For each service of the application &lt;em&gt;TripMeal&lt;/em&gt;, specify
the Kubernetes objects that you need to create and their types.
Justify your answers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Help&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to write the specification of each object, you can refer to the
examples that we discussed together in the second tutorial.
You can also refer
to the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/&#34; target=&#34;_blank&#34;&gt;API documentation on the Kubernetes website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each Kubernetes object:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Configure.&lt;/strong&gt; Write a Yaml configuration file. Remember that you need to pass the environment variables to the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create.&lt;/strong&gt; Create the object in Kubernetes with &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Analyze.&lt;/strong&gt; Execute the command &lt;code&gt;kubectl get all&lt;/code&gt; to confirm that all components run as intended.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Test.&lt;/strong&gt; Play with the application to confirm that is works as intended.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;In case of problems.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can look at the logs of a pod with the following command: &lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Shut down the application!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Shut down the application by removing all the Kubernetes objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Create a new file called &lt;code&gt;tripmeal.yml&lt;/code&gt; that contains the
definition of all the Kubernetes objects of the application, as we have seen
in the &lt;a href=&#34;/courses/cloud-computing/tutorials/tutorial-kube#conclusion&#34; target=&#34;_blank&#34;&gt;conclusion of the tutorial 2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Upload this file to Edunao.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kubernetes-cluster-on-microsoft-azure&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Kubernetes cluster on Microsoft Azure&lt;/h1&gt;
&lt;p&gt;At this point you should have successfully deployed the application on your local Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;You’re now going to create a Kubernetes cluster on &lt;strong&gt;Microsoft Azure&lt;/strong&gt;
and deploy &lt;em&gt;TripMeal&lt;/em&gt; on that cluster.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you haven’t activated it yet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Connect to &lt;a href=&#34;https://azure.microsoft.com/en-us/free/students/&#34; target=&#34;_blank&#34;&gt;this webpage&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;code&gt;Start free&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sign in by using your CentraleSupélec credentials.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Follow the instructions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;During this activity, you’ll need to answer some questions that encourage you to
gain a deeper knowledge of the Azure platform and better understand the commands that you type.
&lt;strong&gt;Feel free to read the documentation on the Azure platform in order to answer the questions.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;login-to-azure-through-cli&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Login to Azure through CLI&lt;/h2&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you get the message &lt;em&gt;command not found&lt;/em&gt; when you type &lt;code&gt;az&lt;/code&gt;, it means that you haven’t installed
the Azure CLI yet.
You’ll find more information on the &lt;a href=&#34;https://centralesupelec.edunao.com/course/view.php?id=8081#section-4&#34; target=&#34;_blank&#34;&gt;Edunao course page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Make sure you run version 2.25 or higher of the Azure CLI, by typing:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az --version&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you use a VM with Multipass you’ll need to install the Azure CLI with the following command:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you use a VM either with Multipass or with VirtualBox you’ll need to re-install kubectl:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type the following command so as the command &lt;code&gt;kubectl&lt;/code&gt; does not refer to &lt;code&gt;minikube kubectl&lt;/code&gt; anymore:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;unalias kubectl&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type the following command to install a new version of &lt;code&gt;kubectl&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;sudo az aks install-cli&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Whether you’re on Windows, macOS or Linux, you need to &lt;strong&gt;open a terminal&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In order to log in to your Azure account, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az login&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A Web page will open in your default Web browser, where you can type your username and
password (your CentraleSupélec credentials).
After authenticating your Azure account, you can go back to the terminal, where you should see
some information about your account, such as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
  {
    &amp;quot;cloudName&amp;quot;: &amp;quot;AzureCloud&amp;quot;,
    &amp;quot;homeTenantId&amp;quot;: &amp;quot;&amp;lt;id&amp;gt;&amp;quot;,
    &amp;quot;id&amp;quot;: &amp;quot;&amp;lt;id&amp;gt;&amp;quot;,
    &amp;quot;isDefault&amp;quot;: true,
    &amp;quot;managedByTenants&amp;quot;: [],
    &amp;quot;name&amp;quot;: &amp;quot;Azure for Students&amp;quot;,
    &amp;quot;state&amp;quot;: &amp;quot;Enabled&amp;quot;,
    &amp;quot;tenantId&amp;quot;: &amp;quot;&amp;lt;id&amp;gt;&amp;quot;,
    &amp;quot;user&amp;quot;: {
      &amp;quot;name&amp;quot;: &amp;quot;&amp;lt;email-address&amp;gt;&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot;
    }
  }
]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-and-use-azure-container-registry&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Deploy and use Azure Container Registry&lt;/h2&gt;
&lt;p&gt;When we run TripMeal on a local Kubernetes cluster, we assumed that our computer
could have a direct access to the Internet and, therefore, to the DockerHub registry, where we
could pull the images of the TripMeal services.
When we run a containerized application in production, we cannot make this assumption
as the production servers have often no direct access to the Internet.&lt;/p&gt;
&lt;p&gt;We need to place the images in a container registry that is in the same context
as our production servers.
Since we’re going to run the application on Azure, we can use the
&lt;strong&gt;Azure Container Registry (ACR)&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;registry-creation&#34; class=&#34;section level3&#34; number=&#34;3.2.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2.1&lt;/span&gt; Registry creation&lt;/h3&gt;
&lt;p&gt;First, we need to create a &lt;strong&gt;resource group&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. What is a &lt;strong&gt;resource group&lt;/strong&gt; in Azure and how is it useful?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The command to create a new resource group is the following
(replace RES_GRP_NAME with a name of your choice).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az group create --name  RES_GRP_NAME --location francecentral&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;francecentral&lt;/code&gt; does not work for your account, the previous command will normally suggest a possible location.
Choose one that is near you.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can verify that the resources are correctly created by looking &lt;a href=&#34;https://portal.azure.com/&#34; target=&#34;_blank&#34;&gt;in the Azure portal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, we need to create a &lt;strong&gt;container registry&lt;/strong&gt; with the following command
(replace RES_GRP_NAME with the name of your resource group and REG_NAME with a name of your choice for the registry).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az acr create --resource-group RES_GRP_NAME --name REG_NAME --sku Basic&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
In the previous command, what does &lt;code&gt;sku&lt;/code&gt; refer to?
Feel free to look up on the Azure website to find the answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Answer on Edunao. Based on the specified &lt;code&gt;sku&lt;/code&gt;, find on the Internet how much the container registry will cost (in euros) per day if
the container registry is instantiated in the region &lt;code&gt;France Central&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;registry-login&#34; class=&#34;section level3&#34; number=&#34;3.2.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2.2&lt;/span&gt; Registry login&lt;/h3&gt;
&lt;p&gt;After creating the registry, we can log into it with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az acr login --name REG_NAME&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;image-tagging&#34; class=&#34;section level3&#34; number=&#34;3.2.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2.3&lt;/span&gt; Image tagging&lt;/h3&gt;
&lt;p&gt;We’re almost ready to push the images that compose the application &lt;em&gt;TripMeal&lt;/em&gt; to the registry.
In order to do that, we need to &lt;strong&gt;tag&lt;/strong&gt; (i.e., rename) our images so that their names are
preceded by the &lt;strong&gt;login server name&lt;/strong&gt; of the registry.&lt;/p&gt;
&lt;p&gt;In order to get the name of the &lt;strong&gt;login server name&lt;/strong&gt;
(that we denote here as &lt;code&gt;acrloginserver&lt;/code&gt;) of your registry, you can type
the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az acr list --resource-group RES_GRP_NAME --query &#34;[].{acrLoginServer:loginServer}&#34; --output table&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your &lt;code&gt;acrloginserver&lt;/code&gt; will be something like &lt;code&gt;xxx.azurecr.io&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Tag the images that correspond to the services of the application &lt;em&gt;TripMeal&lt;/em&gt; so that
their name is similar to: &lt;code&gt;acrloginserver/nameofimage:latest&lt;/code&gt;. You can use the
command &lt;code&gt;docker tag&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-the-images&#34; class=&#34;section level3&#34; number=&#34;3.2.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2.4&lt;/span&gt; Pushing the images&lt;/h3&gt;
&lt;p&gt;We can push the images with the following command (use your image name
instead of &lt;code&gt;xxx.azurecr.io/imagename:latest&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push xxx.azurecr.io/imagename:latest&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Make sure to &lt;strong&gt;type this command for each image&lt;/strong&gt; that you want to push.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It might take few minutes before the images are completely pushed to the registry.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finally, verify that the images are actually in the registry with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az acr repository list --name REG_NAME --output table&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, you can connect to &lt;a href=&#34;https://portal.azure.com/&#34; target=&#34;_blank&#34;&gt;your Azure portal&lt;/a&gt;
and visually browse your resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-a-kubernetes-cluster&#34; class=&#34;section level2&#34; number=&#34;3.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Deploy a Kubernetes cluster&lt;/h2&gt;
&lt;p&gt;We now deploy a &lt;strong&gt;Kubernetes cluster on Azure&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;cluster-creation&#34; class=&#34;section level3&#34; number=&#34;3.3.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3.1&lt;/span&gt; Cluster creation&lt;/h3&gt;
&lt;p&gt;We create the cluster with the following command (replace CLUSTER_NAME
with a name of your choice. As before, RES_GRP_NAME is the name of your resource group
and REG_NAME is the name of the container registry).&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;az aks create \
    --resource-group RES_GRP_NAME \
    --name CLUSTER_NAME \
    --node-count 2 \
    --generate-ssh-keys \
    --attach-acr REG_NAME&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you get an error on your SSH key, then your key is not in the right format.&lt;/p&gt;
&lt;p&gt;Here is how we suggest you to proceed.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open your home directory in Visual Studio.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the hidden folder &lt;code&gt;.ssh&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You should have a file named &lt;code&gt;id_rsa.pub&lt;/code&gt; under folder &lt;code&gt;.ssh&lt;/code&gt;. Open it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the whole content of the file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new file under folder &lt;code&gt;.ssh&lt;/code&gt;. Give it a name of your choice (for instance, &lt;code&gt;id_kube_rsa.pub&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paste the content into the new file, while making sure that you don’t have any whitespace before
the first character.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Save and close the file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the command &lt;code&gt;az aks create&lt;/code&gt; and replace the &lt;code&gt;--generate--ssh-keys&lt;/code&gt; option with &lt;code&gt;--ssh-key-value ~/.ssh/id_kube_rsa.pub&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;The cluster will take a while to start. Time for a coffee!
But before, answer the following question!&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
What is the meaning of the option &lt;code&gt;node-count&lt;/code&gt; in the previous command?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;connect-to-the-cluster&#34; class=&#34;section level3&#34; number=&#34;3.3.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3.2&lt;/span&gt; Connect to the cluster&lt;/h3&gt;
&lt;p&gt;We can configure &lt;code&gt;kubectl&lt;/code&gt; to connect to the newly created cluster.
You need to type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;az aks get-credentials --resource-group RES_GRP_NAME --name CLUSTER_NAME&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now, your Kubernetes cluster should be visible locally.
To verify it, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl config get-contexts&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;em&gt;context&lt;/em&gt; refers to the Kubernetes clusters that &lt;em&gt;kubectl&lt;/em&gt; has access to.
The Kubernetes cluster that you created on Azure should be visible in the output; an asterisk
should appear in front of its name, indicating that it is the &lt;em&gt;current context&lt;/em&gt;
(that is the Kubernetes cluster being currently referenced by &lt;code&gt;kubectl&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see the information about the nodes in the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-the-application&#34; class=&#34;section level2&#34; number=&#34;3.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Deploy the application&lt;/h2&gt;
&lt;p&gt;We’re almost done! The images are in the registry, the Kubernetes cluster is up and running.
The only missing piece of the puzzle is our application &lt;em&gt;TripMeal&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;First thing to do is to slightly modify the file &lt;code&gt;tripmeal.yml&lt;/code&gt; that you created at the end
of the previous section.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Look at the names of the images of each service in that file. How must these names change?
Modify the file accordingly (&lt;strong&gt;no need to upload it on Edunao&lt;/strong&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It is the moment you’ve been waiting for!
Deploy your application by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Look at Kubernetes objects created after this command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Wait for all the components to be up and running.
Get the external IP address of the web service and try to connect to the application,
in the same way you did in the previous section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you can play with the application like you did in your local deployment it means that you reached
the conclusion of this assignment! Bravo!&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Submit a &lt;strong&gt;video&lt;/strong&gt; like that the one that &lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v1261a00b74dbbzwk2vr/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;you can see here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The video must &lt;strong&gt;clearly&lt;/strong&gt; show that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’re connected to &lt;strong&gt;your Azure portal&lt;/strong&gt; (Email address on the top right corner of the portal).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All the passages that you see in the sample video: you need to show the &lt;strong&gt;public IP address&lt;/strong&gt;
of the application that you deployed, use that address to connect to your application and play
with the application to show that it works correctly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The next exercise is &lt;strong&gt;optional&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you don’t intend to do it, &lt;strong&gt;make sure to read the conclusion of this document to take down the application and remove all resources!!&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.6  &lt;/strong&gt;&lt;/span&gt;
The title of this exercise is &lt;em&gt;Cerise sur le gâteau&lt;/em&gt; (i.e., this is &lt;strong&gt;optional&lt;/strong&gt;!).&lt;/p&gt;
&lt;p&gt;At this point, you can connect to &lt;em&gt;TripMeal&lt;/em&gt; by using an IP address.
It would be nice if you could use a URL, like in the real websites.
Can you find a way to assign a URL to your web service? &lt;strong&gt;Describe your procedure.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need a bit of Googling here….&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Make sure you follow these instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take down your application in Kubernetes by using the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl delete -f tripmeal.yml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the context of the &lt;code&gt;kubectl&lt;/code&gt; command so that it points back
to a local Kubernetes cluster. Type the following command, where
CONTEXT_NAME will be &lt;code&gt;docker-desktop&lt;/code&gt; or &lt;code&gt;minikube&lt;/code&gt;, depending on which local Kubernetes cluster
you use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl config use-context CONTEXT_NAME&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Destroy all the resources&lt;/strong&gt; linked to the application
&lt;em&gt;TripMeal&lt;/em&gt; on Microsoft Azure, otherwise you’ll get billed even if you don’t use them!
You can destroy all the resources by simply deleting the resource group to which they belong.
You can do it through the Azure portal or by typing the following command (replace RES_GRP_NAME with the name
of the resource group that you intend to remove).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;az group delete --name RES_GRP_NAME&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can check the balance of your Azure credit &lt;a href=&#34;https://www.microsoftazuresponsorships.com/Balance&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can stop Docker and Kubernetes if you don’t need it anymore.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- https://stackoverflow.com/questions/55271912/flask-cli-throws-oserror-errno-8-exec-format-error-when-run-through-docker--&gt;
&lt;!-- merge 1.2 with 1.3, so that we can ask students to point out exactly the common layers --&gt;
&lt;!-- # Docker networking model

We developed a simple chat room in Python that you can download 
[here](/courses/cloud-computing/kube-lab/chat-room.zip). 
The code has been adapted from [this GitHub project](https://github.com/pricheal/python-client-server/){target=&#34;_blank&#34;}.

Participants use a *client* program to connect to the chat room;
the chat room is managed by a *server* application  that receives the 
client connections and forwards the messages between the users.
The archive contains the following files:

* *client.py*. Implementation of the chat room client.
* *server.py*. Implementation of the chat room server.
* *utils.py*. Library with utility functions used in both *client.py* and *server.py*.

The application is written in Python and it **requires Python 3.7**. 
The application doesn&#39;t need any third-party library, the packages included in a minimal Python environment are enough.




::: {.infobox .exercisebox  data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:client-server-methodology&#34;&gt;&lt;strong&gt;(\#exr:client-server-methodology) &lt;/strong&gt;&lt;/span&gt; 

a. Is a single Docker image for this application enough? Justify your answer.

b. Given your answer to the previous question, how many Docker images do you need to create? 
&lt;/div&gt;\EndKnitrBlock{exercise}


:::



We now build the images for the application.

:::{.infobox .curiosity data-latex=&#34;curiosity&#34;}
**Good to know**

If you need to inspect the file system of an
image that you create (e.g., to verify that the files that you 
copied are actually there), you can open a terminal 
in the image by using the following command:

``
docker run -it --entrypoint sh IMAGE_NAME
``

:::


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-24&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-24) &lt;/strong&gt;&lt;/span&gt;Build the image(s) for the application. For each image:

a. Upload the Dockerfile to Edunao.
b. Write the **exact** command that you used to build the image.
&lt;/div&gt;\EndKnitrBlock{exercise}

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Warning**

The size of the images should be kept as small as possible.
Remember to include in the images only what you need to build and run the application.

:::

If you built more than one image:

c. Do they have some common layers? Which ones?

d. If so, is it something that has an impact on the build time and how?

:::








## Client and server on the same network

We want to execute:

* One instance of the server.

* Two instances of the client.

The server, as well as the clients, will 
**run in Docker containers attached to the same network**.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-25&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-25) &lt;/strong&gt;&lt;/span&gt;
We want to make sure that:

1. The server and the clients can communicate with each other.

2. Other Docker containers **cannot** communicate neither with the server nor with the clients.  

How can you satisfy both requirements in Docker? Explain your solution 
and justify it.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Warning**

Only one instance of the server is running. 
Two instances of the client run at the same time.

1. In order to execute the server, we need to **pass a port number as a parameter**. Choose one in the range [49152–65535].

2. In order to execute the client, we need to **pass as parameters the IP address of the server and the port** which the server listens to (the one that you specified at 1.).

:::




::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:client-server-single-network&#34;&gt;&lt;strong&gt;(\#exr:client-server-single-network) &lt;/strong&gt;&lt;/span&gt;
Execute the server and the two clients by using the network configuration that you
explained in the previous exercise.

* The server writes messages on the terminal. Remember to launch the container with the appropriate 
options in order to actually see those messages.

* Users need to interact with the client, that is: read and write messages. 
Remember to launch the container with the appropriate options in order to actually see those messages.

Write the **exact commands** that you typed for 
both configuring the network and launching the server and the clients.  **Explain these commands.**
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Shut the application down!**

Shut both the clients and the server down  before you move on.

* On the client-side, type ``#quit`` at any moment to exit the chat room.

* Type Ctrl-C to stop the server.

:::


## Client and server on different networks

We want to execute:

* One instance of the server.

* Two instances of the client.

However, **neither client** is connected to the same network as the server.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-26&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-26) &lt;/strong&gt;&lt;/span&gt;
Why can&#39;t you launch the containers with the same settings as 
in Exercise \@ref(exr:client-server-single-network)?

What do you propose as a solution instead?

&lt;/div&gt;\EndKnitrBlock{exercise}

:::






::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-27&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-27) &lt;/strong&gt;&lt;/span&gt;
Execute the server and the two clients by making sure that neither client is connected to the 
same network as the server.

Write the **exact commands** that you typed 
for both configuring the network and launching the server and the clients.
**Explain these commands.**

&lt;/div&gt;\EndKnitrBlock{exercise}

:::



::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Shut the application down!**

Shut both the clients and the server down before you move on.

* On the client-side, type ``#quit`` at any moment to exit the chat room.

* Type Ctrl-C to stop the server.

::: --&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB</title>
      <link>/courses/bdia_old/tutorials/mongodb-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/mongodb-tutorial/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!--

# Introduction 

In this lab assignment you&#39;ll run queries in MongoDB.

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Assignment submission**

This lab assignment will be **evaluated**.

In order to finalize the submission of the assignment, 
submit a report in PDF by using [this link](https://centralesupelec.edunao.com/mod/assign/view.php?id=43066){target=&#34;_blank&#34;}.

The report must not be longer than 4 pages. In the report:

* Indicate your first and family name.

* Describe briefly the MongoDB query language and the aggregation framework.

* Write the queries but not the results of the queries.

The submission deadline is **Thursday, January 28, 2021 8:00 AM**.

:::




# Setting up the work environment.

* Start the **MongoDB server**. Refer 
to the [documentation](/courses/plp/overview/installation-mongodb) 
to find out how to start the server depending on your operating system.

* Launch **MongoDB compass**. You should have installed it 
with the MongoDB server.

* From within MongoDB compass, connect to the local MongoDB server.

# Creating the database

* Download the data by [clicking here](/courses/plp/tutorials/mongodb-data.zip).

* In MongoDB Compass, click on the button &#34;Create database&#34;.

* Give the database a name (e.g., *cinema*) and create a collection 
named *movies*.

* Click on the button *Add data* and select *Import file*.

* Select JSON as a file format and select the file *movies.json*.

* Click on &#34;Import&#34;; 88 documents should be imported into the database.

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Troubleshooting**

In case you experience problems while importing the JSON file, you might want to 
try to import the file *movies.csv*.

:::

# Querying the data with *find*

At the bottom of the MongoDB window, you should find a link 
to open the MongoDB shell (MongoSH Beta).

After opening the shell, write the following command to 
switch to database *cinema*:

```
use cinema
```

To verify that you&#39;re actually connected to the database *cinema*, 
type the following query:

```
db.movies.findOne()
```

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
By using the MongoDB shell, execute the following queries.

Q1. The release year of the movie &#34;Le parrain III&#34;.

Q2. The title of the movies released between  1980 and 1990.

Q3. Same query as b. with the titles must be sorted by alphabetical order.

Q4. The titles of the french movies.

Q5. The title of the  &#34;crime&#34; or  &#34;drama&#34; movies.

Q6. The names and birth dates of the  directors of french movies.

Q7. The title of the movies in which Sofia Coppola played.

Q8. The title and the genre of the movies of which Hitchcock is director.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Querying the data with the aggregation framework

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;
By using the MongoDB shell, execute the following queries 
by using **aggregation pipelines**.

Q1. The number of movies by country. Show by decresing number.

Q2. The name of the actor in the role &#34;Mary Corleone&#34; in the movie &#34;Le parrain III&#34;.

Q3. The number of actors by movie. Sort by decreasing number.

Q4. The average number of actors in a film.

&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Join in MongoDB

In the database *cinema*, create a new collection called *movies_boffice*.
Import the documents in file *moviesBoxOffice.json* into this collection.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;
By using the operator *$lookup* on the collections 
*movies* and *movies_boffice*, find the box office of the 
movie &#34;Le parrain III&#34;.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



--&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB data modeling</title>
      <link>/courses/databases/tutorials/mongodb-data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/mongodb-data-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;use-case-scenario&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Use case scenario&lt;/h1&gt;
&lt;p&gt;We consider a relational database that holds the data of a chain of DVD stores; the &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-preface.html&#34; target=&#34;_blank&#34;&gt;database name is
Sakila&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Sakila database is serving an increasing number of queries from staff and customers
around the world.
A single monolithic database is not sufficient anymore to serve all
the requests and the company is thinking about distributing the database across
several servers (&lt;strong&gt;horizontal scalability&lt;/strong&gt;).
However, a relational database does not handle horizontal scalability very well, due
to the fact that the data is scattered across numerous tables, as the result of the normalization
process.
Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to
MongoDB.&lt;/p&gt;
&lt;p&gt;For the migration to happen, it is necessary to conceive a suitable data model.
From the first discussions with the Sakila management,
you quickly understand that one of the main use of the database is to
manage (add, update and read) rental information.&lt;/p&gt;
&lt;div id=&#34;description-of-the-relational-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Description of the relational model&lt;/h2&gt;
&lt;p&gt;The existing data model is recalled in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:sakila-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The logical schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: The logical schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here is the &lt;strong&gt;description of the tables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;actor&lt;/TT&gt; lists information for all actors. Columns: &lt;em&gt;actor_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;address&lt;/TT&gt; contains address information for customers, staff and stores. Columns: &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;address&lt;/em&gt;, &lt;em&gt;address2&lt;/em&gt;, &lt;em&gt;district&lt;/em&gt;, &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;postal_code&lt;/em&gt;, &lt;em&gt;phone&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;category&lt;/TT&gt; lists the categories that can be assigned to a film. Columns: &lt;em&gt;category_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;city&lt;/TT&gt; contains a list of cities. Columns: &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;city&lt;/em&gt;, &lt;em&gt;country_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;country&lt;/TT&gt; contains a list of countries. Columns: &lt;em&gt;country_id&lt;/em&gt;, &lt;em&gt;country&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; contains a list of all customers. Columns: &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;create_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film&lt;/TT&gt; is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table.
Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;title&lt;/em&gt;, &lt;em&gt;description&lt;/em&gt;, &lt;em&gt;release_year&lt;/em&gt;, &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;original_language_id&lt;/em&gt;, &lt;em&gt;rental_duration&lt;/em&gt;, &lt;em&gt;rental_rate&lt;/em&gt;, &lt;em&gt;length&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_actor&lt;/TT&gt; is used to support a many-to-many relationship between films and actors. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;actor_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_category&lt;/TT&gt; is used to support a many-to-many relationship between films and categories. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;category_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;inventory&lt;/TT&gt; contains one row for each copy of a given film in a given store. Columns: &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;language&lt;/TT&gt; contains possible languages that films can have for their language and original language values. Columns: &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;payment&lt;/TT&gt; records each payment made by a customer, with information such as the amount and the rental being paid for.
Columns: &lt;em&gt;payment_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;amount&lt;/em&gt;, &lt;em&gt;payment_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned.
Columns: &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;staff&lt;/TT&gt; lists all staff members. Columns: &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;picture&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;username&lt;/em&gt;, &lt;em&gt;password&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;store&lt;/TT&gt; lists all stores in the system. Columns: &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;manager_staff_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types-in-mongdb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data types in MongDB&lt;/h1&gt;
&lt;p&gt;A MongoDB document is represented as a JSON record.
However, internally MongoDB serializes the JSON record into a BSON record.
In practice, a BSON record is a binary representation of a JSON record.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Looking at
the &lt;a href=&#34;https://bsonspec.org/spec.html&#34; target=&#34;_blank&#34;&gt;specification of BSON&lt;/a&gt;, can you tell how many &lt;strong&gt;bytes&lt;/strong&gt; do you need to represent:
an integer, a date, a string and a boolean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
The size of a document in MongoDB is limited to 16 MiB.
Can you tell why there is such a limit?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; has four integer columns (&lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;) and
2 dates (&lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; has three integer columns (&lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;), three strings (&lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt; and &lt;em&gt;email&lt;/em&gt;),
one boolean value (&lt;em&gt;active&lt;/em&gt;) and one date (&lt;em&gt;create_date&lt;/em&gt;).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimate the size of a document in both collections.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We make the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On average, each character needs 1.5 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An email address is 20 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A last name is 8 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;A first name is 6 characters long on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;considerations-for-the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Considerations for the new model&lt;/h1&gt;
&lt;p&gt;Denormalization in MongoDB is &lt;a href=&#34;https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1&#34; target=&#34;_blank&#34;&gt;strongly encouraged&lt;/a&gt;
to read and write a record relative to an entity in one single operation.&lt;/p&gt;
&lt;p&gt;In the following exercises, we explore different options and analyze advantages and disadvantages.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Suppose that we create a collection &lt;TT&gt;Customer&lt;/TT&gt;,
where each document includes information about a customer.&lt;/p&gt;
&lt;p&gt;Suppose that we embed in each document the list of rentals
for a customer.&lt;/p&gt;
&lt;p&gt;How many rentals can we store for a given customer, knowing
that the size of a document in MongoDB cannot exceed 16 MiB?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Consider the two following options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;customer&lt;/TT&gt;, where each document contains the
information about a customer and an embedded list with the information
on all the rentals made by the customer. We assume that the average number of rentals per customer is 32.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;rental&lt;/TT&gt;, where each document contains the information
about a rental and an embedded document with the information on the customer
that made the rental.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of a document in the two collections.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we have in our database&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;512 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;16384 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, each customer has around 32 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of the collections &lt;TT&gt;customer&lt;/TT&gt; and
&lt;TT&gt;rental&lt;/TT&gt; described in the previous question.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answers in the previous questions, discuss advantages and disadvantages of the two options:
having a collection &lt;TT&gt;customer&lt;/TT&gt; (solution 1) or a collection &lt;TT&gt;rental&lt;/TT&gt; (solution 2).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Look again at the model in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.
A rental document doesn’t only need to include
information on the customer who made the rental, but also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The staff member who took care of the rental.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The inventory item being rented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The payment information.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Discuss the different ways we can include this information in the collections
&lt;TT&gt;customer&lt;/TT&gt; and &lt;TT&gt;rental&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the discussion, which solution would you retain?
A collection &lt;TT&gt;customer&lt;/TT&gt; or a collection &lt;TT&gt;rental&lt;/TT&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data-model-in-mongodb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; The data model in MongoDB&lt;/h1&gt;
&lt;p&gt;In the last question, we chose the collection that we intend to create.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Give the complete schema (name and type of the properties) of a document in the collection
that you chose in the previous question.&lt;/p&gt;
&lt;p&gt;If the value of any property is an embedded document:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specify the schema of that document too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If any property of an embedded document is an identifier referencing another entity,
use that identifier (don’t try, for now, to further denormalize the schema).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s take a closer look at the storage requirements of the adopted solution.
Consider that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of an inventory item is 12 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information about a payment is 20 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
If we denote by &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; the number of rentals, what is the size in bytes of the
database for the adopted solution?
What do you get if we set &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Although the size that we determined in the previous exercise, may not sound
impressive, we still have to store other information
(films, actors….).
If we could save a bit of space, we would be happy.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Discuss how you could save some space in the adopted solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HINT.&lt;/strong&gt; Do you really need to denormalize all data?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Propose a solution for all the entities involved and
estimate the savings in terms of storage requirements.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The new model&lt;/h1&gt;
&lt;p&gt;In this section we intend to obtain a complete model of the Sakila database.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Consider the model that we obtained at the end of the previous section.
Which data can you further denormalize?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Complete the diagram obtained in the previous exercise so as to obtain
a full data model for the Sakila database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB queries</title>
      <link>/courses/databases/tutorials/mongodb-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/mongodb-queries/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Setup&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download MongoDB Compass &lt;a href=&#34;https://www.mongodb.com/try/download/compass&#34; target=&#34;_blank&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v126418813856thlu0kz/&#34; target=&#34;_blank&#34;&gt;Watch this video&lt;/a&gt; to learn how to start a MongoDB server and connect via MongoDB Compass.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;p&gt;We might want to see a list of common operators &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query/#std-label-query-selectors&#34; target=&#34;_blank&#34;&gt;in this page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of the customers of Canadian stores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all rentals made by customers from Iran, where
the amount paid is strictly greater than 10 dollars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last names of the actors who played a role in film 213.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;operations-on-arrays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Operations on arrays&lt;/h1&gt;
&lt;p&gt;Useful array operators are &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query-array/&#34; target=&#34;_blank&#34;&gt;listed here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have “Behind the Scenes” as special features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all the films where BURT POSEY played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the film that has exactly 15 actors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Aggregation framework&lt;/h1&gt;
&lt;p&gt;A useful reference for the aggregation pipeline &lt;a href=&#34;https://www.mongodb.com/docs/manual/meta/aggregation-quick-reference/&#34; target=&#34;_blank&#34;&gt;can be found here here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the total amount of payments across all rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors of each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sort the films by the number of actors (decreasing order).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the average number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the country where the customers have rented the most of the films in the category “Music”.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;join-operations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Join Operations&lt;/h1&gt;
&lt;p&gt;The join operation is &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/&#34; target=&#34;_blank&#34;&gt;explained here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the language of the film with title “ACE GOLDFINGER”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all the information about the staff member who took care of rental 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB modeling</title>
      <link>/courses/bigdata/tutorials/mongodb-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/mongodb-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;use-case-scenario&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Use case scenario&lt;/h1&gt;
&lt;p&gt;We consider a relational database that holds the data of a chain of DVD stores; the &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-preface.html&#34; target=&#34;_blank&#34;&gt;database name is
Sakila&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Sakila database is serving an increasing number of queries from staff and customers
around the world.
A single monolithic database is not sufficient anymore to serve all
the requests and the company is thinking of distributing the database across
several servers (&lt;strong&gt;horizontal scalability&lt;/strong&gt;).
However, a relational database does not handle horizontal scalability very well, due
to the fact that the data is scattered across numerous tables, as the result of the normalization
process.
Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to
MongoDB.&lt;/p&gt;
&lt;p&gt;For the migration to happen, it is necessary to conceive a suitable data model.
From the first discussions with the Sakila management,
you quickly understand that one of the main use of the database is to
manage (add, update and read) rental information.&lt;/p&gt;
&lt;div id=&#34;description-of-the-relational-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Description of the relational model&lt;/h2&gt;
&lt;p&gt;The existing data model is recalled in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:sakila-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The logical schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: The logical schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here is the &lt;strong&gt;description of the tables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;actor&lt;/TT&gt; lists information for all actors. Columns: &lt;em&gt;actor_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;address&lt;/TT&gt; contains address information for customers, staff and stores. Columns: &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;address&lt;/em&gt;, &lt;em&gt;address2&lt;/em&gt;, &lt;em&gt;district&lt;/em&gt;, &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;postal_code&lt;/em&gt;, &lt;em&gt;phone&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;category&lt;/TT&gt; lists the categories that can be assigned to a film. Columns: &lt;em&gt;category_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;city&lt;/TT&gt; contains a list of cities. Columns: &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;city&lt;/em&gt;, &lt;em&gt;country_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;country&lt;/TT&gt; contains a list of countries. Columns: &lt;em&gt;country_id&lt;/em&gt;, &lt;em&gt;country&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; contains a list of all customers. Columns: &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;create_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film&lt;/TT&gt; is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table.
Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;title&lt;/em&gt;, &lt;em&gt;description&lt;/em&gt;, &lt;em&gt;release_year&lt;/em&gt;, &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;original_language_id&lt;/em&gt;, &lt;em&gt;rental_duration&lt;/em&gt;, &lt;em&gt;rental_rate&lt;/em&gt;, &lt;em&gt;length&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_actor&lt;/TT&gt; is used to support a many-to-many relationship between films and actors. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;actor_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_category&lt;/TT&gt; is used to support a many-to-many relationship between films and categories. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;category_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;inventory&lt;/TT&gt; contains one row for each copy of a given film in a given store. Columns: &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;language&lt;/TT&gt; contains possible languages that films can have for their language and original language values. Columns: &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;payment&lt;/TT&gt; records each payment made by a customer, with information such as the amount and the rental being paid for.
Columns: &lt;em&gt;payment_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;amount&lt;/em&gt;, &lt;em&gt;payment_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned.
Columns: &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;staff&lt;/TT&gt; lists all staff members. Columns: &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;picture&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;username&lt;/em&gt;, &lt;em&gt;password&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;store&lt;/TT&gt; lists all stores in the system. Columns: &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;manager_staff_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types-in-mongdb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data types in MongDB&lt;/h1&gt;
&lt;p&gt;A MongoDB document is represented as a JSON record.
However, internally MongoDB serializes the JSON record into a BSON record.
In practice, a BSON record is a binary representation of a JSON record.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Looking at
the &lt;a href=&#34;https://bsonspec.org/spec.html&#34; target=&#34;_blank&#34;&gt;specification of BSON&lt;/a&gt;, can you tell how many &lt;strong&gt;bytes&lt;/strong&gt; do you need to represent:
an integer, a date, a string and a boolean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
The size of a document in MongoDB is limited to 16 MiB.
Can you tell why there is such a limit?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; has four integer columns (&lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;) and
2 dates (&lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; has three integer columns (&lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;), three strings (&lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt; and &lt;em&gt;email&lt;/em&gt;),
one boolean value (&lt;em&gt;active&lt;/em&gt;) and one date (&lt;em&gt;create_date&lt;/em&gt;).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimate the size of a document in both collections.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We make the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On average, each character needs 1.5 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An email address is 20 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A last name is 8 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;A first name is 6 characters long on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;considerations-for-the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Considerations for the new model&lt;/h1&gt;
&lt;p&gt;Denormalization in MongoDB is &lt;a href=&#34;https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1&#34; target=&#34;_blank&#34;&gt;strongly encouraged&lt;/a&gt;
to read and write a record relative to an entity in one single operation.&lt;/p&gt;
&lt;p&gt;In the following exercises, we explore different options and analyze advantages and disadvantages.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Suppose that we create a collection &lt;TT&gt;Customer&lt;/TT&gt;,
where each document includes information about a customer.&lt;/p&gt;
&lt;p&gt;Suppose that we embed in each document the list of rentals
for a customer.&lt;/p&gt;
&lt;p&gt;How many rentals can we store for a given customer, knowing
that the size of a document in MongoDB cannot exceed 16 MiB?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Consider the two following options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;customer&lt;/TT&gt;, where each document contains the
information about a customer and an embedded list with the information
on all the rentals made by the customer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;rental&lt;/TT&gt;, where each document contains the information
about a rental and an embedded document with the information on the customer
that made the rental.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of a document in the two collections.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we have in our database&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;512 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;16384 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, each customer has around 32 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of the collections &lt;TT&gt;customer&lt;/TT&gt; and
&lt;TT&gt;rental&lt;/TT&gt; described in the previous question.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answers in the previous questions, discuss advantages and disadvantages of the two options:
having a collection &lt;TT&gt;customer&lt;/TT&gt; (solution 1) or a collection &lt;TT&gt;rental&lt;/TT&gt; (solution 2).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Look again at the model in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.
A rental document doesn’t only need to include
information on the customer who made the rental, but also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The staff member who took care of the rental.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The inventory item being rented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The payment information.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Discuss the different ways we can include this information in the collections
&lt;TT&gt;customer&lt;/TT&gt; and &lt;TT&gt;rental&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the discussion, which solution would you retain?
A collection &lt;TT&gt;customer&lt;/TT&gt; or a collection &lt;TT&gt;rental&lt;/TT&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data-model-in-mongodb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; The data model in MongoDB&lt;/h1&gt;
&lt;p&gt;In the last question, we chose the collection that we intend to create.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Give the complete schema (name and type of the properties) of a document in the collection
that you chose in the previous question.&lt;/p&gt;
&lt;p&gt;If the value of any property is an embedded document:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specify the schema of that document too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If any property of an embedded document is an identifier referencing another entity,
use that identifier (don’t try, for now, to further denormalize the schema).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s take a closer look at the storage requirements of the adopted solution.
Consider that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of an inventory item is 12 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information about a payment is 20 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
If we denote by &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; the number of rentals, what is the size in bytes of the
database for the adopted solution?
What do you get if we set &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Although the size that we determined in the previous exercise, may not sound
impressive, we still have to store other information
(films, actors….).
If we could save a bit of space, we would be happy.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Discuss how you could save some space in the adopted solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HINT.&lt;/strong&gt; Do you really need to denormalize all data?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Propose a solution for all the entities involved and
estimate the savings in terms of storage requirements.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The new model&lt;/h1&gt;
&lt;p&gt;In this section we intend to obtain a complete model of the Sakila database.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Consider the model that we obtained at the end of the previous section.
Which data can you further denormalize?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Complete the diagram obtained in the previous exercise so as to obtain
a full data model for the Sakila database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB queries</title>
      <link>/courses/bigdata/tutorials/mongodb-querying/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/mongodb-querying/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;We learn how to write queries in MongoDB on the Sakila database.&lt;/p&gt;
&lt;div id=&#34;initialization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Initialization&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Launch a MongoDB server on your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open MongoDB Compass.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connect to the MongoDB server with the following URI: &lt;code&gt;mongodb://localhost:27017&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-6/sakila-mongodb.zip&#34;&gt;&lt;strong&gt;this archive file&lt;/strong&gt;&lt;/a&gt; and extract it.
Each file corresponds to a collection.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In MongoDB Compass create a new database named &lt;code&gt;sakila&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create the collections &lt;code&gt;customer&lt;/code&gt;, &lt;code&gt;film&lt;/code&gt;, &lt;code&gt;rental&lt;/code&gt;, &lt;code&gt;staff&lt;/code&gt; and &lt;code&gt;store&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Import the data from the downloaded JSON files.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;p&gt;We might want to see a list of common operators &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query/#std-label-query-selectors&#34; target=&#34;_blank&#34;&gt;in this page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of the customers of Canadian stores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all rentals made by customers from Iran, where
the amount paid is strictly greater than 10 dollars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last names of the actors who played a role in film 213.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;operations-on-arrays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Operations on arrays&lt;/h1&gt;
&lt;p&gt;Useful array operators are &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query-array/&#34; target=&#34;_blank&#34;&gt;listed here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have “Behind the Scenes” as special features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all the films where BURT POSEY played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the film that has exactly 15 actors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Aggregation framework&lt;/h1&gt;
&lt;p&gt;A useful reference for the aggregation pipeline &lt;a href=&#34;https://www.mongodb.com/docs/manual/meta/aggregation-quick-reference/&#34; target=&#34;_blank&#34;&gt;can be found here here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the total amount of payments across all rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors of each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sort the films by the number of actors (decreasing order).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the average number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the country where the customers have rented the most of the films in the category “Music”.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;join-operations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Join Operations&lt;/h1&gt;
&lt;p&gt;The join operation is &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/&#34; target=&#34;_blank&#34;&gt;explained here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the language of the film with title “ACE GOLDFINGER”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all the information about the staff member who took care of rental 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>/courses/big-data-marseille/lectures/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/lectures/lectures/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;lecture-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction and MapReduce programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 3 May 2021, 1:45 PAM - 5 PM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: Available &lt;a href=&#34;/courses/big-data-marseille/big-data-1.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 2&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Hadoop and its ecosystem: HDFS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 5 May 2021, 8:30 AM - 10 AM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: Available &lt;a href=&#34;/courses/big-data-marseille/big-data-2.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 3&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Wednesday 5 May 2021, 10 AM - 11:30 AM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: Available &lt;a href=&#34;/courses/big-data-marseille/big-data-3.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spark RDD programming demo&lt;/strong&gt;: Available &lt;a href=&#34;https://colab.research.google.com/drive/1jfCdVVMD_OrTubT0-URqu3SJvQ5mqMfh?usp=sharing&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 4&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Apache Spark’s Structured APIs and Structured Streaming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 10 May 2021, 8:30 AM - 11:30 AM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: Available &lt;a href=&#34;/courses/big-data-marseille/big-data-4.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DataFrames:&lt;/strong&gt; Notebook available &lt;a href=&#34;https://colab.research.google.com/drive/1T3HS6lTQrPrmjCdtNICJSoIHmKSfKFOo?usp=sharing&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL:&lt;/strong&gt; Notebook available &lt;a href=&#34;https://colab.research.google.com/drive/1M5jsrejgtf9KWNfpuEbr1HxjiMNYXgNt?usp=sharing&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 5&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Distributed and NoSQL databases&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 17 May 2021, 10:30 AM - 11:30 AM / 14 PM - 16 PM&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: Available &lt;a href=&#34;/courses/big-data-marseille/big-data-5.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lecture 6&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Document-oriented database systems: MongoDB.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date and time&lt;/strong&gt;: Monday 17 May 2021, 16 PM - 17PM&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notebook&lt;/strong&gt;: Available &lt;a href=&#34;https://colab.research.google.com/drive/14QAq62JurwKy1uwJEV1FGYK2vlJ4ZRh7?usp=sharing&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>References</title>
      <link>/courses/bdia_old/references/cc-references/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/references/cc-references/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Singh, Chanchal, and Manish Kumar. &lt;em&gt;Mastering Hadoop 3: Big data processing at scale to unlock unique business insights&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mehrotra, Shrey, and Akash Grade. &lt;em&gt;Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Karau, Holden, et al. &lt;em&gt;Learning spark: lightning-fast big data analysis.&lt;/em&gt; O’Reilly Media, Inc., 2015&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Giamas, Alex. Mastering MongoDB 4.x: &lt;em&gt;Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. &lt;em&gt;MongoDB: The Definitive Guide: Powerful and Scalable Data Storage&lt;/em&gt;. O’Reilly Media, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scifo, Estelle, &lt;em&gt;Hands-on Graph Analytics with Neo4j&lt;/em&gt;. Packt Publishing Ltd, 2020&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>References</title>
      <link>/courses/big-data-marseille/references/cc-references/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/references/cc-references/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Singh, Chanchal, and Manish Kumar. &lt;em&gt;Mastering Hadoop 3: Big data processing at scale to unlock unique business insights&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mehrotra, Shrey, and Akash Grade. &lt;em&gt;Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Karau, Holden, et al. &lt;em&gt;Learning spark: lightning-fast big data analysis.&lt;/em&gt; O’Reilly Media, Inc., 2015&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Giamas, Alex. Mastering MongoDB 4.x: &lt;em&gt;Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x&lt;/em&gt;. Packt Publishing Ltd, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. &lt;em&gt;MongoDB: The Definitive Guide: Powerful and Scalable Data Storage&lt;/em&gt;. O’Reilly Media, 2019.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scifo, Estelle, &lt;em&gt;Hands-on Graph Analytics with Neo4j&lt;/em&gt;. Packt Publishing Ltd, 2020&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>L&#39;API Spark Core</title>
      <link>/courses/bigdata-fr/spark-core-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-fr/spark-core-api/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;Dans ce TP, vous allez coder des fonctions simples en utilisant l’API Spark Core ; vous analyserez les performances de ces fonctions.&lt;br /&gt;
Nous allons considérer deux problèmes : le calcul des températures moyennes et la recherche d’amis communs dans un réseau social.&lt;/p&gt;
&lt;div id=&#34;calcul-des-moyennes&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Calcul des moyennes&lt;/h1&gt;
&lt;p&gt;Nous considérons une collection de fichiers CSV contenant des mesures de température au format suivant :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;année,mois,jour,heure,minute,seconde,temperature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Vous pouvez trouver les fichiers dans le répertoire &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Voici les détails pour chaque fichier :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contient une mesure par jour pour les années 1980 - 2018.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contient une mesure toutes les 2880 secondes pour les années 1980 - 2018.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86.csv&lt;/code&gt; contient une mesure toutes les 86 secondes pour l’année 1980 uniquement.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; contient une mesure toutes les 10 secondes pour les années 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nous souhaitons implémenter un algorithme Spark pour générer des paires &lt;span class=&#34;math inline&#34;&gt;\((y, t_{avg})\)&lt;/span&gt;, où
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; est l’année et &lt;span class=&#34;math inline&#34;&gt;\(t_{avg}\)&lt;/span&gt; est la température moyenne de cette année.&lt;/p&gt;
&lt;div id=&#34;première-implémentation&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Première implémentation&lt;/h2&gt;
&lt;p&gt;Copiez le fichier &lt;code&gt;/usr/users/dce-admin/vialle/DCE-Spark/template_temperatures.py&lt;/code&gt;
dans votre répertoire personnel via la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/dce-admin/vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Ouvrez le fichier &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt;.&lt;br /&gt;
Le fichier contient l’implémentation de la fonction &lt;code&gt;avg_temperature_slow&lt;/code&gt; qui :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prend en entrée un RDD, où chaque élément est une ligne d’un fichier texte donné ;&lt;/li&gt;
&lt;li&gt;retourne un RDD, où chaque élément est une paire clé-valeur &lt;code&gt;(année, temperature moyenne)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans ce même fichier, localisez les deux variables &lt;code&gt;input_path&lt;/code&gt; et &lt;code&gt;output-path&lt;/code&gt;
et écrivez le code suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input_path = &amp;quot;hdfs://sar01:9000/data/temperatures/&amp;quot;
output_path = &amp;quot;hdfs://sar01:9000/asi1/asi1_X/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remplacez &lt;code&gt;X&lt;/code&gt; par le numéro correspondant à votre compte !&lt;br /&gt;
&lt;strong&gt;N’oubliez pas le / à la fin des chemins de fichiers !&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exécutez les actions suivantes :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lancez le script &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; en utilisant &lt;code&gt;temperatures_86400.csv&lt;/code&gt; comme fichier d’entrée.&lt;br /&gt;
Pour ce faire, utilisez la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Vous devriez trouver la sortie du programme dans le dossier suivant :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/asi1/asi1_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Saisissez la commande suivante pour le vérifier :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/asi1/asi1_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Si vous souhaitez lire le résultat du calcul, vous pouvez exécuter la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/asi1/asi1_X/temperatures_86400.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Dans la sortie Spark sur la ligne de commande, vous devriez voir une ligne ressemblant à ceci :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
   &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Notez le temps d’exécution que vous avez obtenu.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lancez le même script en utilisant le fichier &lt;code&gt;temperatures_2880.csv&lt;/code&gt; comme entrée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Vous semble-t-il raisonnable comparé au temps d’exécution précédent ?
Justifiez votre réponse.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le même script avec &lt;code&gt;temperatures_86.csv&lt;/code&gt; en entrée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Comment l’expliquez-vous, sachant que
les fichiers &lt;code&gt;temperatures_2880.csv&lt;/code&gt; et &lt;code&gt;temperatures_86.csv&lt;/code&gt; ont une taille similaire (11 Mo pour le premier, 9 Mo pour le second) ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deuxième-implémentation&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Deuxième implémentation&lt;/h2&gt;
&lt;p&gt;Nous voulons implémenter une version améliorée du programme.&lt;br /&gt;
Vous pouvez esquisser vos idées sur papier avant d’écrire le code.&lt;/p&gt;
&lt;p&gt;Quand vous êtes prêt, créez une copie de &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; et renommez-la en &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt;, avec la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ./avg_temperatures_slow.py ./avg_temperatures_fast.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Ouvrez le fichier et implémentez la fonction &lt;code&gt;avg_temperature_fast&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;REMARQUE.&lt;/strong&gt; N’oubliez pas de commenter l’appel à
&lt;code&gt;avg_temperature_slow&lt;/code&gt; et de décommenter l’appel à &lt;code&gt;avg_temperature_fast&lt;/code&gt; à la fin du fichier.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le script &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt; en utilisant &lt;code&gt;temperatures_86.csv&lt;/code&gt; comme fichier d’entrée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Comparez-le avec le temps obtenu dans l’exercice précédent et
commentez la différence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le même script en utilisant &lt;code&gt;temperatures_10.csv&lt;/code&gt; (3 Go !) comme fichier d’entrée.&lt;br /&gt;
Pensez-vous que le programme met trop de temps ? Pourquoi ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;amis-communs-dans-un-réseau-social&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Amis communs dans un réseau social&lt;/h1&gt;
&lt;p&gt;Considérons un réseau social représenté par un graphe encodé dans un fichier texte.&lt;br /&gt;
Chaque ligne du fichier est une liste d’identifiants séparés par des virgules.&lt;br /&gt;
Par exemple, la ligne &lt;span class=&#34;math inline&#34;&gt;\(B,D,A\)&lt;/span&gt; signifie que &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; est ami avec &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;br /&gt;
Un extrait du fichier ressemble à ceci :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,D,A
A,B,C,D
D,A,B,C
C,A,D
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nous supposons que la relation d’amitié est symétrique : &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implique &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nous voulons obtenir la liste des amis communs pour chaque paire d’individus&lt;/strong&gt; :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(B, C), [A, D] 
(A, D), [B, C] 
(C, D), [A]
(A, C), [D] 
(B, D), [A] 
(A, B), [D]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Une contrainte supplémentaire :&lt;/strong&gt; nous ne voulons représenter chaque couple qu’une seule fois et éviter de représenter le couple symétrique.&lt;br /&gt;
Autrement dit, si nous produisons &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, nous ne voulons pas produire &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nous utilisons les fichiers d’entrée suivants (disponibles dans le dossier &lt;code&gt;hdfs://sar01:9000/data/social-network/&lt;/code&gt;) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sn_tiny.csv&lt;/code&gt;. Petit réseau social, que vous pouvez utiliser pour tester votre implémentation.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sn_10k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sn_100k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sn_1k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^3\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sn_1m_1m.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; liens.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;implémentation&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Implémentation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Récupérez le modèle de code avec la commande suivante :&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/dce-admin/vialle/DCE-Spark/template_common_friends.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Écrivez une implémentation qui utilise &lt;code&gt;groupByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Écrivez une implémentation qui utilise &lt;code&gt;reduceByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Testez les deux implémentations sur le fichier &lt;code&gt;sn_tiny.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-et-mesures-de-performance&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Tests et mesures de performance&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez les deux implémentations sur les autres fichiers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remplissez un tableau dans lequel vous indiquez : le nom et la taille de chaque fichier,
ainsi que les temps d’exécution mesurés pour les deux implémentations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;degré-minimum-maximum-et-moyen&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Degré minimum, maximum et moyen&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ajoutez une fonction dans le fichier &lt;code&gt;template_common_friends.py&lt;/code&gt; qui retourne un tuple contenant le degré minimum, le degré maximum et le degré moyen
d’un nœud dans le réseau social.
Vous devez utiliser des RDDs pour cela, &lt;strong&gt;n’utilisez pas &lt;code&gt;collect()&lt;/code&gt; pour récupérer le contenu des RDDs et calculer les valeurs dans des listes Python&lt;/strong&gt;.
N’utilisez pas non plus les fonctions &lt;code&gt;max&lt;/code&gt; et &lt;code&gt;min&lt;/code&gt; sur les RDDs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez la fonction pour tous les fichiers d’entrée fournis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau que vous avez créé dans l’exercice précédent en ajoutant le
nombre minimum, maximum et moyen d’amis.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-des-performances&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Analyse des performances&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On suppose que tous les individus dans notre réseau social ont le même nombre d’amis.
Calculez (avec papier et crayon, pas besoin de code) le nombre de
paires intermédiaires &lt;span class=&#34;math inline&#34;&gt;\(((A, B), X)\)&lt;/span&gt; générées par votre code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau en indiquant le nombre de paires intermédiaires pour chaque fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tracez trois graphiques, où l’axe des ordonnées correspond au temps d’exécution du programme, et l’axe des abscisses correspond respectivement à :&lt;br /&gt;
le nombre de paires intermédiaires, le degré moyen et la taille du fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quels graphiques prédisent le mieux l’évolution du temps de calcul ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- # Average and standard deviation

We use the same files as in the first question.
Our objective is to write a Spark program that produces 
triples $(y, t_{\mu}, t_{\sigma})$, where $y$, $t_{\mu}$ and 
$t_{\sigma}$ are the year, the average temperature in the year and the 
standard deviation respectively.

We can express the standard deviation of $n$ values $x_1 \ldots x_n$ with the following formula:

$$
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
$$

Type the following command to get a code template:

&lt;div align=&#39;center&#39;&gt;
``
cp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py
``
&lt;/div&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;

* Define a new function ``avg_stddev_temperature`` in file ``avg_stddev_temp.py``.
   
* Execute the script and observe the results.

&lt;/div&gt;\EndKnitrBlock{exercise}

:::




 --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Advanced Spark programming</title>
      <link>/courses/bdia_old/tutorials/spark-lab-assignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/spark-lab-assignment/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;!--

# Introduction 


The goal of this Spark lab assignment is to write Spark programs and run them on a cluster.
[Refer to this documentation](/courses/plp/overview/cluster-connection){target=&#34;_blank&#34;} to learn how to connect and 
interact with the cluster.




::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Assignment submission**

This lab assignment will be **evaluated**.

In order to finalize the submission of the assignment, 
complete [this form available on Edunao](https://centralesupelec.edunao.com/mod/quiz/view.php?id=47098){target=&#34;_blank&#34;}.

The submission deadline is **Tuesday, March 9, 2021 8:00 AM**.

:::


# Common friends in a social network

Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line $A,B,C,D$ means that $A$ is friend with $B$, $C$ and $D$. 
An excerpt of the file looks like as follows:

```
A,B,C,D
B,A,D
C,A,D
D,A,B,C
...
```

We suppose that the friendship relation is symmetric: $(A, B)$ implies
$(B, A)$.

**We want to obtain the list of the common friends for each pair of individuals**: 

```
(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]
```

As an additional constraint, we want to represent a couple only once and avoid 
to represent the symmetric couple. 
In other words, if we output $(A, B)$, we don&#39;t want to output $(B, A)$.


We use the following input files available in folder ``hdfs://sar01:9000/data/sn/``:

* ``sn_tiny.csv``. Small social network, that you can use to test your implementation.

* ``sn_10k_100k.csv``. Social network with $10^4$ individuals and $10^5$ links. 

* ``sn_100k_100k.csv``. Social network with $10^5$ individuals and $10^5$ links. 

* ``sn_1k_100k.csv``. Social network with $10^3$ individuals and $10^5$ links. 

* ``sn_1m_1m.csv``. Social network with $10^6$ individuals and $10^6$ links. 


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
Write an implementation in Spark.
**Test your implementation on file ``sn_tiny.csv``.**
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;
Run your implementation on the other files and write down the execution times. 
Comment on the execution times considering the file sizes, the number of nodes and links 
and the number of pairs $((A, B), X)$ generated by the algorithm.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;
Execute your implementation on the file ``sn_1m_1m.csv`` by varying the 
number of cores used by the Spark executors. 
You can specify the total number of cores with the option ``--total-executor-cores``
of the command ``spark-submit`` (you can also refer [to the Spark documentation](https://spark.apache.org/docs/latest/submitting-applications.html){target=&#34;_blank&#34;}).

* What is the impact of the number of cores on the execution time? Make a graph and comment.


&lt;/div&gt;\EndKnitrBlock{exercise}

:::


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;

1. By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of 
a node in a given graph.

2. Compute the minimum, maximum and average degree on all the given input files.

3. Do these values confirm or invalidate the considerations that you made on the execution times of the 
algorithm in the first exercise? Justify your answer.


&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Creating an inverted index


In folder ``hdfs://sar01:9000/data/bbc/`` you&#39;ll find a collection of 50 
articles obtained from the BBC website (2004-2005) organized into five subfolders:
*business*, *entertainment*, *politics*, *sport* and *technology*. 

We want to create an **inverted index**, which associates each word with the list of the 
files in which the word occurs. 
More specifically, for each word, the inverted index will have a list of the 
names of the files (path relative to the folder ``/data/bbc``) that contain the word.  

The inverted index:

* must not contain the same word twice; 

* must not contain any stopwords (the list of stopwords is provided in the 
``hdfs://sar01:9000/data/stopwords.txt`` file); 

Moreover:

* Words in the inverted index must only contain letters.

* Words in the inverted index must be lowercase.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;
Write a Spark program to create an inverted index and execute it on the 
input folder.
You can use the template available at ``~vialle/DCE-Spark/template_inverted_index.py``.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

--&gt;
</description>
    </item>
    
    <item>
      <title>Programmes Spark sur DCE</title>
      <link>/courses/bigdata-mds/labs/spark-rdd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-mds/labs/spark-rdd/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;calcul-des-moyennes&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Calcul des moyennes&lt;/h1&gt;
&lt;p&gt;Nous considérons une collection de fichiers CSV contenant des mesures de température au format suivant :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;année, mois, jour, heure, minute, seconde, température&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vous pouvez trouver les fichiers dans le répertoire &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Voici les détails de chaque fichier :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contient une mesure par jour pour les années 1980 - 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contient une mesure toutes les 2880 secondes pour les années 1980 - 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86.csv&lt;/code&gt; contient une mesure toutes les 86 secondes pour la seule année 1980.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; contient une mesure toutes les 10 secondes pour les années 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;première-implémentation&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Première implémentation&lt;/h2&gt;
&lt;p&gt;Copiez le fichier &lt;code&gt;~cpu_vialle/DCE-Spark/template_temperatures.py&lt;/code&gt;
dans votre répertoire personnel en saisissant la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Ouvrez le fichier &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt;.
Le fichier contient l’implémentation de la fonction &lt;code&gt;avg_temperature_slow&lt;/code&gt;
qui:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;prend en arguments un RDD, dont chaque élément est une ligne du fichier donné ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;renvoie un RDD, où chaque élément est une paire clé-valeur &lt;code&gt;(year, avg_temp)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans le même fichier, localisez les deux variables &lt;code&gt;input_path&lt;/code&gt; et &lt;code&gt;output-path&lt;/code&gt;
et écrivez le code suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input_path = &amp;quot;hdfs://sar01:9000/data/temperatures/&amp;quot;
output_path = &amp;quot;hdfs://sar01:9000/itpspark24/itpspark24_X/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remplacez &lt;code&gt;X&lt;/code&gt; par le numéro correspondant à votre compte !
&lt;strong&gt;N’oubliez pas le / à la fin des chemins d’accès aux fichiers !&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faites les actions suivantes :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exécutez le script &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; en utilisantle fichier &lt;code&gt;temperatures_86400.csv&lt;/code&gt; comme argument.
Pour ce faire, utilisez la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Vous devriez trouver la sortie du programme dans le dossier suivant :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/itpspark24/itpspark24_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Tapez la commande suivante pour le vérifier :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/itpspark24/itpspark24_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Si vous souhaitez accéder au résultat du calcul, vous pouvez exécuter la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/itpspark24/itpspark24_X/temperatures_86400.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dans la sortie que Spark affiche dans le terminal, vous devriez voir une ligne qui ressemble à la phrase suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
   &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Notez le temps d’exécution que vous avez obtenu.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le même script en utilisant le fichier &lt;code&gt;températures_2880.csv&lt;/code&gt; comme argument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Semble-t-il raisonnable par rapport au temps d’exécution que vous avez observé auparavant ?
Justifiez votre réponse.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le même script en utilisant le fichier &lt;code&gt;températures_86.csv&lt;/code&gt; comme argument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Comment le justifiez-vous, sachant que
les fichiers &lt;code&gt;temperatures_2880.csv&lt;/code&gt; et &lt;code&gt;temperatures_86.csv&lt;/code&gt; ont une taille similaire (11 Mo pour le premier, 9 Mo pour le second) ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deuxième-implémentation&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Deuxième implémentation&lt;/h2&gt;
&lt;p&gt;Nous voulons mettre en œuvre une meilleure version du programme.
Vous pouvez rédiger vos idées sur papier avant d’écrire du code.&lt;/p&gt;
&lt;p&gt;Lorsque vous êtes prêts, créez une copie du fichier &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; et renommez-la en &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt;, avec la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ./avg_temperatures_slow.py ./avg_temperatures_fast.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Ouvrez le fichier et implémentez la fonction &lt;code&gt;avg_temperature_fast&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notez que&lt;/strong&gt; vous devez commenter l’appel à la fonction &lt;code&gt;avg_temperature_slow()&lt;/code&gt; et décommenter l’appel à la fonction &lt;code&gt;avg_temperature_fast()&lt;/code&gt; à la fin du fichier.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le script &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt; en utilisant le fichier &lt;code&gt;temperatures_86.csv&lt;/code&gt; comme argument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution ? Comparez-le avec le temps d’exécution obtenu dans l’exercice précédent et commentez la différence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le même script en utilisant le fichier &lt;code&gt;températures_10.csv&lt;/code&gt; (3 Go !) comme argument.
Pensez-vous que le programme prend trop de temps ? Justifiez votre réponse.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;amis-communs-dans-un-réseau-social&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Amis communs dans un réseau social&lt;/h1&gt;
&lt;p&gt;Considérons un réseau social décrit par un graphe encodé dans un fichier texte.
Chaque ligne du fichier est une liste d’identifiants séparés par des virgules.
Par exemple, la ligne &lt;span class=&#34;math inline&#34;&gt;\(A,D,C,B\)&lt;/span&gt; signifie que &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; est ami avec &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;.
Un extrait du fichier ressemble à ce qui suit :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,D,A
A,D,C,B
D,A,C,B
C,A,D
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nous supposons que la relation d’amitié est symétrique : &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implique
(B, A)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nous voulons obtenir la liste des amis communs pour chaque paire d’individus&lt;/strong&gt; :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(B, C), [A, D] 
(A, D), [B, C] 
(C, D), [A]
(A, C), [D] 
(B, D), [A] 
(A, B), [D]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comme contrainte supplémentaire, nous voulons représenter un couple une seule fois et éviter de représenter un couple symétrique.
En d’autres termes, si nous produisons &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, nous ne voulons pas produire &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nous utilisons les fichiers suivants, disponibles dans le dossier &lt;code&gt;hdfs://sar01:9000/data/social-network/&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_tiny.csv&lt;/code&gt;. Petit réseau social, que vous pouvez utiliser pour tester votre implémentation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_10k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_100k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1k_100k.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^3\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; liens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1m_1m.csv&lt;/code&gt;. Réseau social avec &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; individus et &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; liens.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;implémentation&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Implémentation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Obtenez un squelette du code à l’aide de la commande suivante :&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_common_friends.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Écrivez une implémentation qui utilise la fonction &lt;code&gt;groupByKey()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Écrivez une implémentation qui utilise la fonction &lt;code&gt;reduceByKey()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Testez les deux implémentations sur le fichier &lt;code&gt;sn_tiny.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-et-mesures-de-performance&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Tests et mesures de performance&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez les deux implémentations sur les autres fichiers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remplissez un tableau dans lequel vous indiquez : le nom et la taille de chaque fichier et
les temps d’exécution mesurés des deux implémentations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;degré-minimum-maximum-et-moyen&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Degré minimum, maximum et moyen&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ajoutez une fonction au fichier &lt;code&gt;template_common_friends.py&lt;/code&gt; qui renvoie un tuple contenant le degré minimum, maximum et moyen d’un noeud dans le réseau social.
Vous devez utiliser des RDDs pour le faire, &lt;strong&gt;n’essayez pas d’utiliser la fonction &lt;code&gt;collect()&lt;/code&gt; pour ensuite
calculer les valeurs sur des listes Python&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez la fonction pour tous les fichiers donnés.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau que vous avez créé dans l’exercice précédent en ajoutant les valeurs suivantes :
le nombre minimum, maximum et moyen d’amis pour chaque individu.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analyses-de-performance&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Analyses de performance&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nous supposons que chaque nœud a un nombre d’amis égal au nombre moyen d’amis.
Calculez (sur papier, pas besoin d’écrire un code pour cela) le nombre de paires intermédiaires &lt;span class=&#34;math inline&#34;&gt;\(((A, B), X)\)&lt;/span&gt; générées par votre code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau en écrivant le nombre de paires intermédiaires pour chaque fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tracez trois graphiques, où l’axe des &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; représente les temps d’exécution du programme et l’axe des &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; représente respectivement
le nombre de paires intermédiaires, le degré moyen et la taille du fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quels graphiques prédisent le mieux l’évolution du temps de calcul ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- # Average and standard deviation

We use the same files as in the first question.
Our objective is to write a Spark program that produces 
triples $(y, t_{\mu}, t_{\sigma})$, where $y$, $t_{\mu}$ and 
$t_{\sigma}$ are the year, the average temperature in the year and the 
standard deviation respectively.

We can express the standard deviation of $n$ values $x_1 \ldots x_n$ with the following formula:

$$
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
$$

Type the following command to get a code template:

&lt;div align=&#39;center&#39;&gt;
``
cp ~cpu_vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py
``
&lt;/div&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercice**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;

* Define a new function ``avg_stddev_temperature`` in file ``avg_stddev_temp.py``.
   
* Execute the script and observe the results.

&lt;/div&gt;\EndKnitrBlock{exercise}

:::




 --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark RDD programming</title>
      <link>/courses/bigdata/tutorials/spark-low-level/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/spark-low-level/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We consider a collection of CSV files containing temperature measurements in the following format:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;year,month,day,hours,minutes,seconds,temperature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you can find the files under the directory &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here are the details for each file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contains one measurement per day in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contains one measurement every 2880 seconds in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86.csv&lt;/code&gt; contains one measurement every 86 seconds for the year 1980 alone.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_10.csv&lt;/code&gt; contains one measurement every 10 seconds for the years 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We intend to implement a Spark algorithm to generate pairs &lt;span class=&#34;math inline&#34;&gt;\((y, t_{avg})\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the year and &lt;span class=&#34;math inline&#34;&gt;\(t_{avg}\)&lt;/span&gt; is the average temperature in the year.&lt;/p&gt;
&lt;div id=&#34;first-implementation&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; First implementation&lt;/h2&gt;
&lt;p&gt;Copy the file &lt;code&gt;/usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py&lt;/code&gt;
to your home directory by typing
the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Open the file &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt;.
The file contains the implementation of the function &lt;code&gt;avg_temperature_slow&lt;/code&gt;
that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;takes in an RDD, where each item is a line of the input text file;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;returns an RDD, where each item is a key-value pair &lt;code&gt;(year, avg_temp)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the same file, locate the two variables &lt;code&gt;input_path&lt;/code&gt; and &lt;code&gt;output-path&lt;/code&gt;
and write the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input_path = &amp;quot;hdfs://sar01:9000/data/temperatures/&amp;quot;
output_path = &amp;quot;hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace &lt;code&gt;X&lt;/code&gt; with the number corresponding to your account!
&lt;strong&gt;Don’t forget the / at the end of the file paths!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Execute the following actions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the script &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; by using &lt;code&gt;temperatures_86400.csv&lt;/code&gt; as an input.
To this extent, use the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;You should find the output of the program under the following folder:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Type the following command to verify it :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;If you want to read the result of the computation, you can execute the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the output of Spark on the command line you should see a line that reads something similar to the following phrase:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
   &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note the execution time that you obtained.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_2880.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? Does it seem reasonable compared with the execution time that you observed before?
Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the same script by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? How would you justify it, knowing that
the files &lt;code&gt;temperatures_2880.csv&lt;/code&gt; and &lt;code&gt;temperatures_86.csv&lt;/code&gt; have a similar size (11 MB the former, 9 MB the latter)?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;second-implementation&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Second implementation&lt;/h2&gt;
&lt;p&gt;We want to implement a better version of the program.
You can draft your ideas on paper before you write any code.&lt;/p&gt;
&lt;p&gt;When you’re ready, create a copy of &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; and rename it as &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt;, with the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ./avg_temperatures_slow.py ./avg_temperatures_fast.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Open the file and implement the function &lt;code&gt;avg_temperature_fast&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt; Remember to comment the call
to &lt;code&gt;avg_temperature_slow&lt;/code&gt; and to uncomment the call to &lt;code&gt;avg_temperature_fast&lt;/code&gt; at the end of the file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run the script &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt; by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What’s the execution time? Compare it with the execution time obtained in the previous exercise and
comment the difference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_10.csv&lt;/code&gt; (3 GB!) as an input.
Do you think that the program takes too long? Why?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends-in-a-social-network&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,A,D
A,B,C,D
D,A,B,C
C,A,D
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We want to obtain the list of the common friends for each pair of individuals&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(B, C), [A, D] 
(A, D), [B, C] 
(C, D), [A]
(A, C), [D] 
(B, D), [A] 
(A, B), [D]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We use the following input files available in folder &lt;code&gt;hdfs://sar01:9000/data/social-network/&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_tiny.csv&lt;/code&gt;. Small social network, that you can use to test your implementation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_10k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_100k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^3\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1m_1m.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Implementation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Get the code template with the following command:&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_common_friends.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Write an implementation that uses a &lt;code&gt;groupByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write an implementation that uses a &lt;code&gt;reduceByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Test both implementations on file &lt;code&gt;sn_tiny.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-and-performance-measures&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Tests and performance measures&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run both implementations on the other files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fill in a table where you indicate: the name and size of each file and
the measured running times of both implementations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-maximum-and-average-degree&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Minimum, maximum and average degree&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add a function to file &lt;code&gt;template_common_friends.py&lt;/code&gt; that returns a tuple containing the minimum, the maximum and the average degree of
a node in the social network. You must use RDDs to do so, &lt;strong&gt;don’t try to &lt;code&gt;collect()&lt;/code&gt; the
content of the RDDs and so compute the values on Python lists&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the function for all the given input files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete the table that you created in the previous exercise by adding the
minimum, maximum and average number of friends.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-analysis&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Performance analysis&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We suppose that each node has a number of friends that is equal to the
average number of friends.
Compute (with pencil or paper, no need to write a code for that) the number of
intermediate pairs &lt;span class=&#34;math inline&#34;&gt;\(((A, B), X)\)&lt;/span&gt; generated by your code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete the table by writing down the number of intermediate pairs for each file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot three graphs, where the y-axis has the program running times and the x-axis
has: the number of intermediate pairs, the average degree and the file size respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which graphs best predict the evolution of the computational time?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- # Average and standard deviation

We use the same files as in the first question.
Our objective is to write a Spark program that produces 
triples $(y, t_{\mu}, t_{\sigma})$, where $y$, $t_{\mu}$ and 
$t_{\sigma}$ are the year, the average temperature in the year and the 
standard deviation respectively.

We can express the standard deviation of $n$ values $x_1 \ldots x_n$ with the following formula:

$$
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
$$

Type the following command to get a code template:

&lt;div align=&#39;center&#39;&gt;
``
cp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py
``
&lt;/div&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;

* Define a new function ``avg_stddev_temperature`` in file ``avg_stddev_temp.py``.
   
* Execute the script and observe the results.

&lt;/div&gt;\EndKnitrBlock{exercise}

:::




 --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark MLlib</title>
      <link>/courses/bigdata-mds/labs/spark-mllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-mds/labs/spark-mllib/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;Dans ce TD, vous apprendrez à utiliser la bibliothèque &lt;strong&gt;Spark MLlib&lt;/strong&gt; pour entraîner, tester et évaluer des modèles d’apprentissage automatique.
Nous utiliserons un jeu de données sur les logements AirBnb dans la région de San Francisco.
Le jeu de données est disponible sur HDFS au chemin suivant :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/prof/cpu_quercini/ml/sf-airbnb-clean.parquet&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Source du jeu de données&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Le jeu de données a été téléchargé à partir &lt;a href=&#34;https://github.com/databricks/LearningSparkV2&#34; target=&#34;_blank&#34;&gt;de ce dépôt GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Le but de ce TD est de prédire le prix par nuit d’un appartement en fonction de toutes les caractéristiques des données.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;L’interpréteur Python utilisé par les exécuteurs du cluster n’a pas le paquet &lt;code&gt;numpy&lt;/code&gt; installé.
Afin d’utiliser un interpréteur qui utilise &lt;code&gt;numpy&lt;/code&gt;, vous devez exécuter la commande suivante dans le terminal :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;export PYSPARK_PYTHON=/usr/bin/python3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ensemble-dentraînement-et-ensemble-de-test&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Ensemble d’entraînement et ensemble de test&lt;/h1&gt;
&lt;p&gt;Afin de entraîner et d’évaluer un modèle d’apprentissage automatique, nous devons
obtenir un &lt;strong&gt;ensemble d’entraînement&lt;/strong&gt; et un &lt;strong&gt;ensemble de test&lt;/strong&gt; à partir de notre jeu de données.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;train_test.py&lt;/code&gt; vers votre dossier personnel (sur les machines du DCE) à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/mllib/train_test.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Complétez la ligne 9&lt;/strong&gt;, en spécifiant votre répertoire personnel sur HDFS, qui est le suivant (&lt;strong&gt;remplacer X par votre numéro de compte&lt;/strong&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/itpspark24/itpspark24_X&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Écrivez le code&lt;/strong&gt; pour lire le fichier donné, qui est stocké sur HDFS au format Parquet, et charger son contenu dans un DataFrame nommé &lt;code&gt;airbnb_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;À partir de la ligne 26&lt;/strong&gt;, ajoutez les instructions nécessaires pour afficher le schéma du DataFrame &lt;code&gt;airbnb_df&lt;/code&gt; et les 5 premières lignes.
Quelle option devriez-vous utiliser pour voir les valeurs dans les colonnes ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;, et vérifiez que les données sont correctement chargées dans le DataFrame.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Puisque le DataFrame a beaucoup de colonnes, nous avons besoin de spécifier l’option &lt;code&gt;vertical=True&lt;/code&gt; lorsqu’on appelle la fonction &lt;code&gt;show()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Nous séparons maintenant le DataFrame &lt;code&gt;airbnb_df&lt;/code&gt; en deux DataFrames distincts, &lt;code&gt;train_df&lt;/code&gt; et &lt;code&gt;test_df&lt;/code&gt;,
contenant respectivement les instances d’entraînement et de test.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Écrivez le code&lt;/strong&gt; pour diviser le jeu de données en un ensemble d’entraînement (80% du jeu de données) et un ensemble de test (20% du jeu de données).
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html&#34; target=&#34;_blank&#34;&gt;En consultant la documentation de l’API DataFrame&lt;/a&gt;, quelle fonction allez-vous utiliser ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A partir de la ligne 36&lt;/strong&gt;, écrivez le code pour afficher le nombre d’instances des deux ensembles obtenus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;. Vérifiez que le nombre d’instances des deux ensembles soit correct.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Il est temps d’enregistrer les ensembles d’entraînement et de test dans deux fichiers HDFS séparés.
Ainsi, nous pourrons les charger chaque fois que nous en aurons besoin pour entraîner un modèle d’apprentissage automatique
pour ce problème.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;À partir de la ligne 47&lt;/strong&gt;. Ecrivez le code pour sauvegarder les ensembles d’entraînement et de test dans deux fichiers au format Parquet sur HDFS.
Les chemins d’accès des deux fichiers sont déjà disponibles dans les variables &lt;code&gt;training_set_file&lt;/code&gt; et &lt;code&gt;test_set_file&lt;/code&gt;
définies respectivement aux lignes 40 et 43.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;préparation-des-caractéristiques&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Préparation des caractéristiques&lt;/h1&gt;
&lt;p&gt;Dans les ensembles d’entraînement et de test, les caractéristiques des données correspondent aux colonnes du DataFrame.
La plupart des algorithmes d’apprentissage automatique définis dans Spark ont besoin d’avoir toutes les caractéristiques dans un seul vecteur.
Nous devons utiliser un &lt;strong&gt;transformateur&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Les transformateurs&lt;/strong&gt; prennent en argument un DataFrame et renvoient un nouveau DataFrame
qui a les mêmes colonnes que le DataFrame en argument et des colonnes supplémentaires spécifiées en tant qu’argument du transformateur.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copiez le fichier &lt;code&gt;train_test_model&lt;/code&gt; vers votre dossier personnel (sur les machines du DCE) à l’aide de la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;cp ~cpu_quercini/mllib/train_test_model.py .&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;read_training_set&lt;/code&gt; à la ligne 22.
Cette fonction charge dans un DataFrame l’ensemble d’entraînement à partir du fichier que vous avez généré à l’exercice précédent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;read_test_function&lt;/code&gt; à la ligne 39.
Cette fonction charge dans un DataFrame l’ensemble de test à partir du fichier que vous avez généré à l’exercice précédent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modifiez les variables aux lignes 105 et 106 pour qu’elles pointent vers les deux fichiers HDFS Parquet qui contiennent les ensembles d’entraînement et de test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décommentez les lignes 109 et 110, afin d’afficher le nombre d’instances d’entraînement et de test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier &lt;code&gt;train_test_model&lt;/code&gt; à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;. Vérifiez que le nombre d’instances d’entraînement et de test corresponde
à ce que vous avez obtenu lors des exercices précédents.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test_model.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Il est maintenant temps d’apprendre à utiliser un transformateur pour mettre toutes les caractéristiques nécessaires dans un vecteur.
Pour l’instant, nous n’utiliserons que la caractéristique « bedrooms » pour prédire le prix.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;get_vector&lt;/code&gt; à la ligne 55. Les commentaires dans le fichier décrivent ce que la fonction doit faire.
Vous devrez utiliser un &lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html#pyspark.ml.feature.VectorAssembler&#34; target=&#34;_blank&#34;&gt;objet VectorAssembler&lt;/a&gt; pour implémenter la fonction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décommentez les lignes 117 et 118 pour appeler la fonction &lt;code&gt;get_vector&lt;/code&gt; et afficher le résultat.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier &lt;code&gt;train_test_model&lt;/code&gt; à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;. Observez le contenu du DataFrame &lt;code&gt;train_vect&lt;/code&gt;. Il devrait avoir une colonne nommée
&lt;code&gt;features&lt;/code&gt; contenant une liste avec une valeur (la valeur de la caractéristique &lt;code&gt;bedrooms&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test_model.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Il est maintenant temps d’entraîner un modèle de régression linéaire sur l’ensemble d’entraînement donné et les caractéristiques sélectionnées.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;train_linear_regression_model&lt;/code&gt; à la ligne 79. Les commentaires du fichier décrivent ce que la fonction est censée faire.
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html&#34; target=&#34;_blank&#34;&gt;En consultant la documentation&lt;/a&gt;,
essayez d’identifier l’objet que vous devez utiliser pour créer un régresseur linéaire et la fonction que vous devez utiliser pour l’entraîner.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décommentez les lignes 121, 124, 125, 126 pour appeler la fonction &lt;code&gt;train_linear_regression_model&lt;/code&gt; et afficher les coefficients appris par le modèle de régression linéaire.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier &lt;code&gt;train_test_model&lt;/code&gt; à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test_model.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Nous pouvons maintenant utiliser le modèle entraîné pour faire des prédictions.
Le modèle renovyé par la fonction que vous avez implémentée dans l’exercice précédent
est un objet de type &lt;code&gt;LinearRegressionModel&lt;/code&gt;.
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html#pyspark.ml.regression.LinearRegressionModel&#34; target=&#34;_blank_&#34;&gt;En consultant la documentation&lt;/a&gt;, nous apprenons qu’il existe une fonction &lt;code&gt;predict&lt;/code&gt;
permettant de faire une prédiction à partir d’une seule instance.
Si nous voulons faire une prédiction sur l’ensemble du jeu de test, nous devons utiliser la fonction &lt;code&gt;transform&lt;/code&gt;.
La fonction prend en argument un DataFrame avec l’ensemble de test et renvoie le même DataFrame
avec une colonne supplémentaire &lt;code&gt;prediction&lt;/code&gt; qui contient la valeur prédite.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Regardez les lignes 130 et 131. Quel DataFrame devons-nous passer à la fonction &lt;code&gt;transform&lt;/code&gt; ? Est-ce que &lt;code&gt;test_df&lt;/code&gt; est un bon choix ? Pourquoi ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez la ligne 130 et décommentez les lignes 130, 131, 132.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier &lt;code&gt;train_test_model&lt;/code&gt; à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt; et observez le résultat.
La valeur prédite est-elle celle à laquelle vous vous attendez étant donné que vous connaissez la formule de la droite de régression ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;test_df&lt;/code&gt; n’est pas un bon choix.
En fait, le DataFrame de l’ensemble de test doit avoir le même format que le DataFrame de l’ensemble d’apprentissage
sur lequel le modèle a été entraîné.&lt;/p&gt;
&lt;p&gt;Donc, nous devons le transformer avec un objet &lt;code&gt;vecAssembler&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/train_test_model.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;pipelines&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Pipelines&lt;/h1&gt;
&lt;p&gt;Dans la section précédente, nous avons appris qu’un ensemble d’entraînement et un ensemble
de test doivent passer par les mêmes étapes de transformation afin d’alimenter un modèle d’apprentissage automatique.&lt;/p&gt;
&lt;p&gt;Lorsque nous avons peu de transformations, il est facile de se souvenir de
celles que nous avons appliquées à un ensemble d’entraînement, afin de les appliquer également à l’ensemble de test.&lt;/p&gt;
&lt;p&gt;Cependant, lorsque nous devons appliquer une série de transformations, dont l’ordre est important,
il est facile de commettre des erreurs.&lt;/p&gt;
&lt;p&gt;Une bonne pratique consiste à utiliser l’API &lt;strong&gt;Pipeline&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Un &lt;strong&gt;pipeline&lt;/strong&gt; est composée d’&lt;strong&gt;étapes&lt;/strong&gt;. Chaque étape peut consister de l’appel à un &lt;strong&gt;transformateur&lt;/strong&gt; ou à un &lt;strong&gt;estimateur&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un &lt;strong&gt;transformateur&lt;/strong&gt; est un objet sur lequel nous appelons la fonction &lt;code&gt;transform&lt;/code&gt; pour obtenir un nouveau DataFrame à partir d’un DataFrame donné.&lt;/li&gt;
&lt;li&gt;Un &lt;strong&gt;estimateur&lt;/strong&gt; est un objet sur lequel nous appelons la fonction &lt;code&gt;fit&lt;/code&gt; pour apprendre un modèle sur un DataFrame donné. Le modèle appris est lui-même
un transformateur.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lorsque la fonction &lt;code&gt;fit&lt;/code&gt; est appelée sur un pipeline, l’ensemble d’apprentissage passe par tous
les transformateurs et les estimateurs dans l’ordre dans lequel ils sont déclarés dans le pipeline ;
enfin, l’estimateur spécifié à la dernière étape est entraîné sur l’ensemble d’apprentissage.
Le modèle renovyé par l’application de la fonction &lt;code&gt;fit&lt;/code&gt; sur le pipeline est lui-même un transformateur.
Si nous invoquons la fonction &lt;code&gt;transform&lt;/code&gt; sur ce modèle sur l’ensemble de test, nous obtenons un DataFrame qui contient une colonne nommée &lt;code&gt;predictions&lt;/code&gt;.
Implicitement, toutes les transformations dans le pipeline seront appliquées à l’ensemble de test aussi, avant de faire les prédictions.&lt;/p&gt;
&lt;p&gt;Copiez le fichier &lt;code&gt;pipeline_example.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/mllib/pipeline_example.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-pipeline.html&#34; target=&#34;_blank&#34;&gt;Regardez la documentation&lt;/a&gt; et
écrivez un code à partir de la ligne 34 pour créer un pipeline qui effectue les mêmes opérations que vous avez écrites dans le fichier &lt;code&gt;train_test_model.py&lt;/code&gt;
pour entraîner et tester un modèle de régression linéaire sur les ensembles d’entraînement et de test donnés.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N’oubliez pas de spécifier &lt;strong&gt;aux lignes 27 et 28&lt;/strong&gt; les chemins d’accès des fichiers d’entraînement et de test stockés sur HDFS .&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~cpu_quercini/mllib_solutions/pipeline_example.py&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;des-caractéristiques-catégorielles-aux-caractéristiques-numériques&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Des caractéristiques catégorielles aux caractéristiques numériques&lt;/h2&gt;
&lt;p&gt;De nombreux modèles d’apprentissage automatique n’acceptent pas des valeurs catégorielles : ils ont besoin que toutes les caractéristiques soient numériques.
La régression linéaire en est un exemple.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;onehot_playground.py&lt;/code&gt; vers votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/mllib/onehot_playground.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Modifiez les lignes 27 et 28 et écrivez les chemins d’accès des fichiers contenant les ensembles d’entraînement et de test.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Commençons par déterminer quelles sont les caractéristiques catégorielles de notre jeu de données.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Complétez la fonction &lt;code&gt;get_categorical_columns&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pour avoir une idée sur comment obtenir les colonnes catégorielles, exécutez le code à l’aide de la commande &lt;code&gt;spark_submit&lt;/code&gt;.
Ceci exécute l’instruction à la ligne 86.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une fois que vous avez terminé l’implémentation, exécutez le fichier à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt; et observez la sortie.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~cpu_quercini/mllib_solutions/onehot_playground.py&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Un exemple de caractéristique catégorielle
dans notre jeu de données est &lt;code&gt;property_type&lt;/code&gt;,
qui accepte des valeurs telles que &lt;em&gt;Apartment&lt;/em&gt;, &lt;em&gt;House&lt;/em&gt;, &lt;em&gt;Condominium&lt;/em&gt;…
Chaque valeur d’une caractéristique catégorielle est également appelée catégorie.&lt;/p&gt;
&lt;p&gt;Une façon de transformer cette caractéristique en une caractéristique
numérique serait d’attribuer un numéro à chaque catégorie
(par exemple, &lt;em&gt;Appartement&lt;/em&gt; correspond à 0, &lt;em&gt;Maison&lt;/em&gt; à 1….).
Cependant, cela introduit implicitement un ordre entre les catégories : la catégorie &lt;em&gt;Maison&lt;/em&gt; vaudrait deux fois plus que la catégorie &lt;em&gt;Appartement&lt;/em&gt; ;
Cela fausserait inévitablement le modèle.&lt;/p&gt;
&lt;p&gt;Une méthode couramment utilisée est l’encodage &lt;strong&gt;one-hot&lt;/strong&gt;. Voyons comment elle fonctionne.
Concentrons-nous uniquement sur la caractéristique &lt;code&gt;type_de_propriété&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Décommentez les lignes 40, 41 et 42. Ces lignes instancient un estimateur appelé &lt;code&gt;StringIndexer&lt;/code&gt;.
Cet estimateur associe des index numériques aux valeurs de &lt;code&gt;property_type&lt;/code&gt;. Les index seront stockés dans une autre colonne
nommée &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décommentez la ligne 46. Le &lt;code&gt;StringIndexer&lt;/code&gt; est appliqué à l’ensemble d’entraînement pour apprendre à associer des index aux valeurs de
&lt;code&gt;property_type&lt;/code&gt;. Le résultat de cette instruction est un nouveau transformateur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décommentez la ligne 49. Le transformateur appris à la ligne 46 est utilisé pour transformer l’ensemble d’entraînement dans un nouveau DataFrame.
Ce DataFrame contient une colonne supplémentaire appelée &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Complétez la fonction &lt;code&gt;count_property_types&lt;/code&gt;.
Suivez les instructions que vous trouvez dans le fichier &lt;code&gt;onehot_playground.py&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Une fois la fonction terminée, décommentez la ligne 53 pour appeler la fonction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt; et observez la sortie. Pouvez-vous deviner comment les index sont assignés aux catégories
de l’élément &lt;code&gt;property_type&lt;/code&gt; ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~cpu_quercini/mllib_solutions/onehot_playground.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On constate que le type de propriété le plus fréquent obtient l’indice 0 ; le deuxième plus fréquent obtiendra la valeur 1…les indices sont donc attribués par fréquence.
Chaque fois que nous exécutons le même code, nous obtenons les mêmes index.
Cela signifie que nous avons un déterminisme sur les différentes exécutions d’un même programme.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Il est temps de découvrir comment fonctionne l’encodage one-hot.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Décommentez les lignes 57, 58, 61, 64, 67. Ces lignes instancient un estimateur appelé &lt;code&gt;OneHotEncoder&lt;/code&gt;.
Cet estimateur utilise les index de la colonne &lt;code&gt;property_type_index&lt;/code&gt; et crée une nouvelle colonne &lt;code&gt;property_type_ohe&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;L’estimateur est entraîné sur l’ensemble d’entraînement et un nouveau transformateur est obtenu (ligne 61).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le transformateur est utilisé sur l’ensemble d’entraînement pour le transformer en un nouveau DataFrame, qui aura une nouvelle colonne &lt;code&gt;property_type_ohe&lt;/code&gt; .&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;La ligne 67 affiche des colonnes sélectionnées de cet ensemble d’entraînement transformé.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le fichier à l’aide de la commande &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pouvez-vous comprendre comment fonctionne l’encodage one-hot ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~cpu_quercini/mllib_solutions/onehot_playground.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;L’encodage one-hot crée un vecteur avec autant de valeurs que le nombre de catégories dans la colonne
&lt;code&gt;property_type&lt;/code&gt;.
Chaque catégorie a un index fixe dans ce vecteur, donné par la colonne &lt;code&gt;property_type_index&lt;/code&gt;.
Si, pour une instance, la valeur de la colonne &lt;code&gt;property_type&lt;/code&gt; est &lt;em&gt;Guest suite&lt;/em&gt; et que l’index de &lt;em&gt;Guest suite&lt;/em&gt; est 3,
alors le vecteur produit par l’encodage one-hot n’a qu’une valeur 1 à l’index 3 et 0 partout ailleurs.
Spark ne stocke qu’une représentation concise de ce vecteur creux : un tuple avec la taille du vecteur,
l’index 3 et la valeur associée à cet index (1).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Maintenant, vous avez tous les ingrédients pour créer un pipeline complet
pour former et tester un modèle de régression linéaire en utilisant toutes les fonctionnalités.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Créez un nouveau fichier &lt;code&gt;full_pipeline.py&lt;/code&gt; et suivez ces instructions :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sélectionnez les caractéristiques catégorielles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Configurez un &lt;code&gt;StringIndexer&lt;/code&gt; et un &lt;code&gt;OneHotEncoder&lt;/code&gt; pour appliquer un encodage one-hot à toutes les caractéristiques catégorielles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extrayez les caractéristiques numériques.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mettez en oeuvre un &lt;code&gt;VectorAssembler&lt;/code&gt; pour mettre dans un seul vecteur les caractéristiques encodées et les caractéristiques numériques.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mettez en oeuvre un modèle de régression linéaire qui prend en arguments toutes les caractéristiques et la variable à prédire (&lt;code&gt;price&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mélangez tous ces ingrédients dans un &lt;code&gt;Pipeline&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Utilisez le &lt;code&gt;Pipeline&lt;/code&gt; pour entraîner et tester le modèle.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~cpu_quercini/mllib_solutions/full_pipeline.py&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;évaluation-du-modèle&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Évaluation du modèle&lt;/h1&gt;
&lt;p&gt;Dans les sections précédentes, nous avons examiné les prédictions pour avoir une vague idée des performances de notre
estimateur.
Afin de quantifier la qualité de notre estimateur, nous avons besoin de mesures d’évaluation.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-tuning.html#train-validation-split&#34; target=&#34;_blank&#34;&gt;Regardez la documentation&lt;/a&gt;
et modifiez le code pour sélectionner un modèle à l’aide d’une recherche par grille (&lt;em&gt;grid search&lt;/em&gt;). Le but de la recherche par grille est
de trouver la valeur optimale des hyperparamètres du modèle.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le corrigé est disponible dans le fichier suivant :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
~cpu_quercini/mllib_solutions/model_evaluation.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark MLlib</title>
      <link>/courses/bigdata/tutorials/mllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/mllib/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;In this tutorial, you’ll learn how to use the Spark MLlib library to train, test and evaluate machine learning models.
We’ll be using a dataset of AirBnb accommodation in the San Francisco area.
The dataset is available in HDFS at the following path:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/prof/cpu_quercini/sf-airbnb-clean.parquet&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Dataset source&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The dataset has been obtained &lt;a href=&#34;https://github.com/databricks/LearningSparkV2&#34; target=&#34;_blank&#34;&gt;from this GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The goal of the exercise is to predict the price per night of an apartment given all the features
in the dataset.&lt;/p&gt;
&lt;!--
::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Changing the Python interpreter of the executors**

The default Python interpreter used by the executors in the cluster does not have the package ``numpy`` installed. 
In order to use one that has ``numpy``, you need to run the following command in the terminal:

&lt;div align=&#34;center&#34;&gt;

```
export PYSPARK_PYTHON=/usr/bin/python3
```

&lt;/div&gt;

:::
--&gt;
&lt;div id=&#34;obtaining-a-training-and-test-set&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Obtaining a training and test set&lt;/h1&gt;
&lt;p&gt;In order to build and evaluate a machine learning model, we need to split our dataset into
a &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;test set&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy the file &lt;code&gt;train_test.py&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/train_test.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Complete Line 9&lt;/strong&gt;, by specifying your directory in HDFS, which is as follows (&lt;strong&gt;replace X with your account number&lt;/strong&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;hdfs://sar01:9000/itpspark23/itpspark23_X&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 23.&lt;/strong&gt; Write the code to read the input file, which is stored in HDFS as a Parquet file, into the DataFrame &lt;code&gt;airbnb_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;From Line 26&lt;/strong&gt;, add the instructions necessary to print the schema of &lt;code&gt;airbnb_df&lt;/code&gt; and the first 5 rows. Which option should you use
to nicely see the values in the columns?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code with &lt;code&gt;spark-submit&lt;/code&gt; and verify that the data is correctly loaded.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We now split &lt;code&gt;airbnb_df&lt;/code&gt; into two separate DataFrames, &lt;code&gt;train_df&lt;/code&gt; and &lt;code&gt;test_df&lt;/code&gt;, containing the training and
test instances respectively.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Uncomment Line 32.&lt;/strong&gt; Write the code to split the dataset into a training and test set. 80% of the instances
must be taken as training instances, while the remaining 20% will be used a test instances.
&lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html&#34; target=&#34;_blank&#34;&gt;Looking at the DataFrame API documentation&lt;/a&gt;, which
function are you going to use?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;From Line 36&lt;/strong&gt;, write the code to print the number of instances in the training and test set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code with &lt;code&gt;spark-submit&lt;/code&gt;. You should have 5780 training instances and 1366 test instances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is time to save the training and test sets to a HDFS file.
This way, we can load them whenever we need them to build a machine learning model
for this problem.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From Line 47&lt;/strong&gt;. Write the code to save the training and test sets to two Parquet files in HDFS.
The paths to the two files are already available in variables &lt;code&gt;training_set_file&lt;/code&gt; and &lt;code&gt;test_set_file&lt;/code&gt;
defined at Lines 40 and 43 respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-features&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Preparing the features&lt;/h1&gt;
&lt;p&gt;In both training and test sets, the features correspond to DataFrame columns.
Most of the machine learning algorithms in Spark need to have all features in one single vector.
We need to use a &lt;strong&gt;transformer&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt; take in a DataFrame and return a new DataFrame that has the same columns as the
input DataFrame and additional columns that are specified as an argument to the transformer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copy the file &lt;code&gt;train_test_model&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/train_test_model.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;read_training_set&lt;/code&gt; at Line 22.
The file reads into a Spark DataFrame the training set Parquet file that you generated in the previous section.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;read_test_function&lt;/code&gt; at Line 39. The file reads into a Spark DataFrame the test set Parquet file that you generated in the previous section.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the variables at Line 105 and 106 so that they point to the two HDFS Parquet files that contain the training and test sets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 109 and 110, so as to print the number of training and test instances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;. Check that the number of training and test instances correspond to what you
got in the first section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is now time to learn how to use a transformer to put all the necessary features into a vector.
For the time being, we’ll only be using the feature &lt;code&gt;bedrooms&lt;/code&gt; to predict the price.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;get_vector&lt;/code&gt; at Line 55. The comments in the file describe what the function is supposed to do.
You’ll need to use a &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-features#vectorassembler&#34; target=&#34;_blank&#34;&gt;VectorAssembler object&lt;/a&gt; to implement the function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 117 and 118 to call the function &lt;code&gt;get_vector&lt;/code&gt; and display the result.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;. Observe the content of the the DataFrame &lt;code&gt;train_vect&lt;/code&gt;. It should have a column named
&lt;code&gt;features&lt;/code&gt; containing a list with one value (the value of feature &lt;code&gt;bedrooms&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is now time to train a linear regression model on the given training set and the selected features.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;train_linear_regression_model&lt;/code&gt; at Line 79. The comments in the file describe what the function is supposed to do.
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-classification-regression.html#regression&#34; target=&#34;_blank&#34;&gt;Looking at the documentation&lt;/a&gt;, identify the object that you need to use
to create a linear regressor and the function that you need to invoke to train the linear regressor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 121, 124, 125, 126 to call the function &lt;code&gt;train_linear_regression_model&lt;/code&gt; and display the coefficients learned by the linear regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can now use the trained model to make some predictions.
The model returned by the function that you implemented in the previous exercise
is an object of type &lt;code&gt;LinearRegressionModel&lt;/code&gt;.
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html#pyspark.ml.regression.LinearRegressionModel&#34; target=&#34;_true&#34;&gt;Looking at the documentation&lt;/a&gt;, we learn that there is a function &lt;code&gt;predict&lt;/code&gt; to
make a prediction given a single instance.
If we want to make a prediction on the whole test set, we should use the function &lt;code&gt;transform&lt;/code&gt;.
The function takes in a DataFrame with the test set and returns the same DataFrame with an additional column &lt;code&gt;prediction&lt;/code&gt; that contains the predicted value.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Look at Lines 130 and 131. Which DataFrame do we need to pass the function &lt;code&gt;transform&lt;/code&gt;? Is &lt;code&gt;test_df&lt;/code&gt; a good choice? Why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete line 130 and uncomment lines 130, 131, 132.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt; and observe the result. Is the predicted value the one that you expect given that you
know the formula of the regression line?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pipelines&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Pipelines&lt;/h1&gt;
&lt;p&gt;In the previous section we learned that a training and test set need to go through the same transformation steps in
order to be fed to a machine learning model.
When we have few transformations, it is easy to remember the ones that we applied to a training set and apply them on the test set.
However, when we apply a series of transformations, the order of which is important, mistakes are around the corner; we may very well apply
different sets of transformations to the training and test instances, which leads to erroneous predictions.&lt;/p&gt;
&lt;p&gt;A good practice is to use the &lt;strong&gt;Pipeline API&lt;/strong&gt;.
A &lt;strong&gt;pipeline&lt;/strong&gt; is composed of &lt;strong&gt;stages&lt;/strong&gt;. Each stage may be a &lt;strong&gt;transformer&lt;/strong&gt; or an &lt;strong&gt;estimator&lt;/strong&gt;.
A &lt;strong&gt;transformer&lt;/strong&gt; is an object on which we call the function &lt;code&gt;transform&lt;/code&gt; to obtain a new DataFrame from an input DataFrame.
An &lt;strong&gt;estimator&lt;/strong&gt; is an object on which we call the function &lt;code&gt;fit&lt;/code&gt; to learn a model on a given DataFrame. The learned model is itself
a transformer.&lt;/p&gt;
&lt;p&gt;When the function &lt;code&gt;fit&lt;/code&gt; is called on a Pipeline, the training set goes through all the transformers and the estimators in the order in which they are declared in the pipeline;
the estimator specified in the last stage is trained on the training set.
The model returned by applying the function &lt;code&gt;fit&lt;/code&gt; on the pipeline is itself a transformer.
If we invoke the function &lt;code&gt;transform&lt;/code&gt; on that model on the test set, we obtain a DataFrame that contains a column named &lt;code&gt;predictions&lt;/code&gt;.
Implicitly, all the transformations in the pipeline will be applied to the test set too, before making the predictions.&lt;/p&gt;
&lt;p&gt;Copy the file &lt;code&gt;pipeline_example.py&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/pipeline_example.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-pipeline.html&#34; target=&#34;_blank&#34;&gt;Look at the documentation&lt;/a&gt; and
write a code from Line 34 to create a pipeline that does the same operations as in file &lt;code&gt;train_test_model.py&lt;/code&gt;
to train and test a linear regression model on the given training and test sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don’t forget to specify the paths to the training and test set files on HDFS at Lines 27 and 28.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;from-categorical-to-numerical-features&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; From categorical to numerical features&lt;/h2&gt;
&lt;p&gt;Many machine learning models do not handle categorical values: they need all features to be numerical.
Linear regression is such an example.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;onehot_playground.py&lt;/code&gt; to your home directory in the cluster by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/onehot_playground.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Change lines 27 and 28 and write the paths to the files with the training and test sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;First, let’s find out which features are categorical in our dataset.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Complete the function &lt;code&gt;get_categorical_columns&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In order to get an idea as to how to get the categorical columns, execute the code
with &lt;code&gt;spark_submit&lt;/code&gt;. This execute the instruction at Line 86.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once you’re done with the implementation, execute the file with &lt;code&gt;spark-submit&lt;/code&gt; and observe the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;One example of a categorical feature in our dataset is &lt;code&gt;property_type&lt;/code&gt;, which takes values such as &lt;em&gt;Apartment&lt;/em&gt;, &lt;em&gt;House&lt;/em&gt;, &lt;em&gt;Condominium&lt;/em&gt;…
Each value of a categorical feature is also referred to as a category.&lt;/p&gt;
&lt;p&gt;One way to turn this feature into a numerical one would be to assign a number to each category (e.g., &lt;em&gt;Apartment&lt;/em&gt; corresponds to 1, &lt;em&gt;House&lt;/em&gt; to 2….).
However, this implicitly introduces an order among the categories: the category &lt;em&gt;House&lt;/em&gt; would be worth twice as much as the category &lt;em&gt;Apartment&lt;/em&gt;;
this would inevitably bias the trained model.&lt;/p&gt;
&lt;p&gt;A commonly used method is &lt;strong&gt;one-hot encoding&lt;/strong&gt;. Let’s find out how it works.
Let’s only focus on the feature &lt;code&gt;property_type&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 40, 41 and 42. These lines instantiate an estimator that is called &lt;code&gt;StringIndexer&lt;/code&gt;.
This estimator associates numeric indexes to the values of &lt;code&gt;property_type&lt;/code&gt;. The indexes will be stored in another column
named &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Line 46. The &lt;code&gt;StringIndexer&lt;/code&gt; is applied to the training set to learn how to associate indexes to the values of
&lt;code&gt;property_type&lt;/code&gt;. The result of the instruction is a new transformer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Line 49. The transformer learned on Line 46 is used to transform the training set into a new DataFrame.
This DataFrame contains an additional column called &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Complete the function &lt;code&gt;count_property_types&lt;/code&gt;.
Follow the instructions in the file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Once the function is complete, uncomment Line 53 to call the function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file with &lt;code&gt;spark-submit&lt;/code&gt; and observe the output. Can you guess how the indexes are assigned to the categories
of the feature &lt;code&gt;property_type&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is time to find out how one-hot encoding works.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 57, 58, 61, 64, 67. These lines instantiate an estimator that is called &lt;code&gt;OneHotEncoder&lt;/code&gt;.
This estimator uses the indexes in &lt;code&gt;property_type_index&lt;/code&gt; and creates a new column &lt;code&gt;property_type_ohe&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The estimator is trained on the training set and a new transformer is obtained (line 61).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The transformer is used on the training set to transform it into a new DataFrame, where a new column &lt;code&gt;property_type_ohe&lt;/code&gt; exists.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Line 67 prints the selected columns of this transformed training set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the file with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can you understand how one-hot encoding works?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now, you have all the ingredients to create a full pipeline to train and test a
linear regression model by using all features.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Create a new file &lt;code&gt;full_pipeline.py&lt;/code&gt; where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You select the categorical features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a &lt;code&gt;StringIndexer&lt;/code&gt; and a &lt;code&gt;OneHotEncoder&lt;/code&gt; to apply one-hot encoding to all categorical features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Obtain the numeric features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a &lt;code&gt;VectorAssembler&lt;/code&gt; to put into a single vector the one-hot encoded features and the numerical ones.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a linear regression model that takes in all features and the variable to predict (&lt;code&gt;price&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mix all these ingredients in a &lt;code&gt;Pipeline&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;Pipeline&lt;/code&gt; to train and test the model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can display the first five rows of the DataFrame to see the predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-a-model&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Evaluating a model&lt;/h1&gt;
&lt;p&gt;In the previous sections we looked at the predictions to get a vague idea of how our
estimator performs.
In order to quantify the quality of our estimator, we need some evaluation measures.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Use a &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-tuning.html#train-validation-split&#34; target=&#34;_blank&#34;&gt;TrainValidationSplit object&lt;/a&gt;
to do a model selection based on a grid search of parameters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning.&lt;/strong&gt; TrainValidationSplit expects the features to be in a column named “features” and the variable to predict in a column named “label”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark Structured API</title>
      <link>/courses/bigdata-fr/spark-sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-fr/spark-sql/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;nombre-de-partitions-dans-un-rdd&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Nombre de partitions dans un RDD&lt;/h1&gt;
&lt;p&gt;Nous considérons un ensemble de fichiers CSV contenant des mesures de température sur plusieurs années.
Chaque ligne a le contenu suivant : &lt;code&gt;année, mois, jour, heure, minute, seconde, température&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ces fichiers sont stockés à l’emplacement suivant : &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voici le détail de chaque fichier :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contient une mesure par jour entre 1980 et 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contient une mesure toutes les 2880 secondes entre 1980 et 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86.csv&lt;/code&gt; contient une mesure toutes les 86 secondes pour la seule année 1980.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; contient une mesure toutes les 10 secondes entre 1980 et 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Un squelette du code à écrire est disponible dans le fichier &lt;code&gt;avg_temperatures_rdd.py&lt;/code&gt;, que vous pourrez copier dans votre dossier
personnel à l’aide de la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Ce fichier contient une implémentation efficace de la fonction qui calcule la température moyenne annuelle, comme nous l’avons vu
dans un précédent tutoriel.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performances-et-nombre-de-partitions&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Performances et nombre de partitions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 70.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 80.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;. Remplacez YOUR_DIRECTORY par &lt;code&gt;asi1/asi1_XX/&lt;/code&gt; (où XX correspond au numéro de votre nom d’utilisateur).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observez les instructions aux &lt;strong&gt;lignes 98&lt;/strong&gt; et &lt;strong&gt;lignes 104&lt;/strong&gt;. Elles obtiennent et affichent
le nombre de partitions du RDD en argument &lt;code&gt;text_file&lt;/code&gt; et du RDD de sortie &lt;code&gt;temperatures&lt;/code&gt; respectivement.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Complétez le tableau 1. Exécutez le programme sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;. Notez le temps d’exécution et le nombre de partitions des deux RDDs
&lt;code&gt;text_file&lt;/code&gt; et &lt;code&gt;temperatures&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Rappel.&lt;/strong&gt; La commande pour exécuter le programme est la suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution &lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Nombre de partitions &lt;br&gt; (RDD &lt;code&gt;text_file&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Nombre de partitions &lt;br&gt; (RDD &lt;code&gt;temperatures&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
&lt;b&gt;Table 1. Temps d’exécution et nombre de partitions (RDD).&lt;/b&gt;
&lt;/caption&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-du-fonctionnement-de-spark&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Analyse du fonctionnement de Spark&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pourriez-vous comprendre comment Spark détermine le nombre de partitions du RDD &lt;code&gt;fichier_texte&lt;/code&gt; en regardant la taille des fichiers d’entrée ?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Coup de pouce.&lt;/strong&gt; Si vous divisez la taille du fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; par le nombre de partitions du RDD &lt;code&gt;fichier_texte&lt;/code&gt;, quelle valeur obtenez-vous ?
par le nombre de partitions du RDD &lt;code&gt;text_file&lt;/code&gt;, quelle valeur obtenez-vous ? Que représente cette valeur ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Affichez les fichiers stockés dans le dossier de sortie &lt;code&gt;temperatures_10.rdd.out&lt;/code&gt;.
Que constatez-vous ? Existe-t-il une relation avec le nombre de partitions ?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Afin d;afficher le contenu du dossier sur HDFS, vous pouvez utiliser la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/asi1/asi1_XX/temperatures_10.rdd.out&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-de-lapi-dataframe-pour-le-calcul-des-températures-moyennes&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Utilisation de l’API DataFrame pour le calcul des températures moyennes&lt;/h1&gt;
&lt;p&gt;Vous allez maintenant implémenter un programme Spark pour calculer des températures moyennes en utilisant l’API Spark DataFrame.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Suivez les étapes suivantes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le modèle de code &lt;code&gt;avg_temperatures_df.py&lt;/code&gt; dans votre dossier personnel en exécutant la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_df.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;!--* The instructions **at line 109 and 115** show the number of partitions of the RDDs
underlying the dataframes ``df`` (input) and ``df_avg`` (output) respectively.--&gt;
&lt;ul&gt;
&lt;li&gt;Le résultat du calcul sera stocké dans le dossier &lt;code&gt;temperatures_*.df.out&lt;/code&gt;
sous votre dossier HDFS &lt;code&gt;asi1/asi1_XX&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 78.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 89.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 106.&lt;/strong&gt; Complétez l’instruction pour lire un fichier CSV.
Veuillez noter que les fichiers CSV donnés &lt;strong&gt;n’ont pas d’en-têtes&lt;/strong&gt;. N’utilisez pas l’inférence de schéma,
spécifiez simplement votre schéma manuellement. Pour rappel, les colonnes sont : &lt;code&gt;année&lt;/code&gt;, &lt;code&gt;mois&lt;/code&gt;,
&lt;code&gt;jour&lt;/code&gt;, &lt;code&gt;heure&lt;/code&gt;, &lt;code&gt;minute&lt;/code&gt;, &lt;code&gt;seconde&lt;/code&gt;, &lt;code&gt;température&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 55.&lt;/strong&gt; Complétez la définition de la fonction &lt;code&gt;avg_temperature_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez votre code sur tous les fichiers CSV donnés et &lt;strong&gt;complétez le tableau 2&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 2. RDDs vs. DataFrames.
&lt;/caption&gt;
&lt;/table&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Comparez les temps d’exécution avec ceux obtenus en utilisant les RDD.
Qu’observez-vous ? Comment expliquez-vous ces différences ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- ## Effect of the number of partitions

You&#39;re going to study the effect of the number of partitions on the performances of the program.

* In file ``avg_temperatures_df.py`` add the following instruction right below the initialisation of the 
``SparkSession``. The command sets the number of *shuffle* partitions to a certain value `X`.

``
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, X)
``

* Execute the program with different values of `X` (say, 1, 2, 10, 50, 100, 500, 1000, 5000, 100000) **only on file**
``temperatures_86400.csv`` and write down the execution times.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;

* Plot a graph where the x-axis contains the values of `X` and the y-axis contains the execution time.
What do you observe?


* Execute the program on the other files by using small values of `X` (1 and 2). 
Comment on the evolution of the execution times.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div id=&#34;mise-en-cache-dun-dataframe&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Mise en cache d’un DataFrame&lt;/h2&gt;
&lt;p&gt;Vous allez maintenant découvrir les avantages de la mise en cache d’un DataFrame.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Décommentez les deux dernières lignes du fichier &lt;code&gt;avg_temperatures_df.py&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Supprimez le fichier &lt;code&gt;temperatures_10.df.out&lt;/code&gt; en saisissant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/asi1/asi1_XX/temperatures_10.df.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Exécutez le code sur le fichier &lt;code&gt;températures_10.csv&lt;/code&gt;.
Quel est le temps d’exécution de chaque action ?
Pouvez-vous expliquer en détail ce qui se passe ici ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Supprimez les fichiers &lt;code&gt;temperatures_10.df.out&lt;/code&gt; et &lt;code&gt;temperatures_10.df.out.bis&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Mettez en cache le DataFrame &lt;code&gt;df_avg&lt;/code&gt; et
exécutez à nouveau le code sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Où devez-vous ajouter l’instruction de mise en cache ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution de chaque action ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pouvez-vous expliquer en détail ce qui se passe ici ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calcul-des-moyennes-avec-sql&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Calcul des moyennes avec SQL&lt;/h1&gt;
&lt;p&gt;Vous allez maintenant implémenter le calcul des températures moyennes annuelles en utilisant SQL sur les DataFrames Spark.&lt;/p&gt;
&lt;div id=&#34;utilisation-dune-vue&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Utilisation d’une vue&lt;/h2&gt;
&lt;p&gt;Une première option pour interroger un DataFrame avec SQL est de créer une &lt;strong&gt;vue&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;avg_temperatures_sql_view.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez le code des lignes 90, 101 et 118.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;avg_temperature_sql&lt;/code&gt; (ligne 57).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers CSV et &lt;strong&gt;complétez le tableau 3&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que pouvez-vous dire sur les temps d’exécution ? Trouvez-vous des différences
significatives entre l’utilisation de SQL sur une vue et l’utilisation des fonctions de l’API DataFrame ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Vue SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 3. RDDs vs DataFrames vs Vues.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-dune-table&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Utilisation d’une table&lt;/h2&gt;
&lt;p&gt;Une deuxième option pour interroger un DataFrame avec SQL est de créer une &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;avg_temperatures_sql_table.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez les lignes 50, 112, 123 et 140.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;avg_temperature_sql&lt;/code&gt; (ligne 76).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers CSV.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau 4.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparez les temps d’exécution avec ceux que vous avez obtenus avant. Discutez des résultats.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--While reading the code in file ``avg_temperatures_sql_table.py``, you probably noticed that we have specified 
the path to a HDFS folder for the **Hive metastore warehouse**. This folder will contain the data of each table 
that we create.--&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-10&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-10) &lt;/strong&gt;&lt;/span&gt;

* Look at the files in your home folder (in the local file system, not HDFS) by using the command ``ls``. 

* Do you notice the presence of new files?

* Can you explain what these files are?

* Execute the code on all CSV files and **complete Table 4**. 

* Compare the execution times against the ones that you obtained in the previous exercises.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::--&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Vue SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Table SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 4. RDDs vs DataFrames vs Vues vs Tables.
&lt;/caption&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Supprimez le fichier &lt;code&gt;temperatures_10.sql.table.out&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez à nouveau le code sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Quel est le temps d’exécution que vous obtenez maintenant ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-de-lapi-dataframe-sur-des-fichiers-volumineux&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Utilisation de l’API DataFrame sur des fichiers volumineux&lt;/h1&gt;
&lt;p&gt;Nous allons maintenant examiner les fichiers stockés sous &lt;code&gt;hdfs://sar01:9000/data/sales/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ces fichiers contiennent des données tabulaires relatives à la vente de produits dans une chaîne de magasins.
Nous considérons deux tables : &lt;code&gt;store_sales&lt;/code&gt; et &lt;code&gt;customer&lt;/code&gt;.
Dans la première table, nous trouvons des informations sur chaque vente,
comme l’identifiant du produit vendu,
l’identifiant de l’acheteur,
la quantité de produit acheté et le prix payé par le client.
Pour cette table, nous disposons de 4 fichiers, qui ne diffèrent que par leur taille :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.100.dat&lt;/code&gt; : contient 9.5 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.200.dat&lt;/code&gt; : contient 19 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.400.dat&lt;/code&gt; : contient 38 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.800.dat&lt;/code&gt; : contient 77 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans la table &lt;code&gt;customer&lt;/code&gt; nous trouvons des données sur les clients, comme le prénom, le nom et la date de naissance.
Nous n’avons qu’un seul fichier pour cette table :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;customer_10000.dat&lt;/code&gt; : contient 8.3 GiB de données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nous voulons tester les performances de l’API &lt;strong&gt;DataFrame&lt;/strong&gt; sur les requêtes suivantes
(&lt;strong&gt;Attention. vous devez écrire un code qui utilise les fonctions DataFrame, pas du SQL!&lt;/strong&gt;) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Requête Q1&lt;/strong&gt; : renvoie le nombre de clients. Cela correspond à la requête SQL suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;SELECT count(*) 
FROM customer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q2&lt;/strong&gt; : renvoie le prix du produit le plus cher. Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT max(ss_list_price) 
FROM store_sales&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q3&lt;/strong&gt; : renvoie la somme d’argent dépensée par chaque client. Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q4&lt;/strong&gt; : Requête Q3 + trier le résultat de façon à ce que le client qui a dépensé le plus d’argent apparaisse en haut.
Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q5&lt;/strong&gt; : Requête Q4 + jointure avec la table &lt;code&gt;customer&lt;/code&gt; pour obtenir le nom et le prénom des clients.
Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk 
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;développement-du-code-utilisant-lapi-dataframe.&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Développement du code utilisant l’API DataFrame.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;dataframe_api_benchmark.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modifiez le code en suivant les instructions que vous trouvez dans le fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur le fichier &lt;code&gt;store_sales_1_4.100.dat&lt;/code&gt; (le plus petit) pour tester que votre code ne contient pas de bogues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une fois que vous êtes sûr que votre code est correct, &lt;strong&gt;décommentez les lignes 82 et 83&lt;/strong&gt;. Cela mettra en cache les deux DataFrames.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers &lt;code&gt;store-sales_*.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chaque requête est exécutée 5 fois pour avoir une estimation correcte du temps d’exécution.
Vous verrez que les temps d’exécution fluctuent lors des premières itérations et qu’ils se stabilisent
au cours des itérations suivantes.
Lorsque vous notez les temps d’exécution, ne prenez en compte que ceux obtenus
à la dernière itération.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau 5 et notez le temps d’exécution de chaque requête pour chaque fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pourquoi le temps d’exécution des requêtes Q1 et Q2 est-il élevé à l’itération 0 ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pensez-vous que la différence entre les temps d’exécution des requêtes est raisonnable ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pensez-vous que l’augmentation des temps d’exécution est raisonnable compte tenu de la taille des fichiers d’entrée ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier / Requête&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Lecture&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q1&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q2&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q3&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q4&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q5&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.100.dat
&lt;/td&gt;
&lt;td&gt;
17.24
&lt;/td&gt;
&lt;td&gt;
0.94
&lt;/td&gt;
&lt;td&gt;
1.05
&lt;/td&gt;
&lt;td&gt;
1.17
&lt;/td&gt;
&lt;td&gt;
2.16
&lt;/td&gt;
&lt;td&gt;
5.22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.200.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.400.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.800.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 5. Temps d’exécution des requêtes sur le dataset &lt;code&gt;sales&lt;/code&gt;.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark Structured API</title>
      <link>/courses/bigdata-mds/labs/spark-sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-mds/labs/spark-sql/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;nombre-de-partitions-dans-un-rdd&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Nombre de partitions dans un RDD&lt;/h1&gt;
&lt;p&gt;Nous considérons un ensemble de fichiers CSV contenant des mesures de température sur plusieurs années.
Chaque ligne a le contenu suivant : &lt;code&gt;année, mois, jour, heure, minute, seconde, température&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ces fichiers sont stockés à l’emplacement suivant : &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voici le détail de chaque fichier :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contient une mesure par jour entre 1980 et 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contient une mesure toutes les 2880 secondes entre 1980 et 2018.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_86.csv&lt;/code&gt; contient une mesure toutes les 86 secondes pour la seule année 1980.&lt;/li&gt;
&lt;li&gt;Le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; contient une mesure toutes les 10 secondes entre 1980 et 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Un squelette du code à écrire est disponible dans le fichier &lt;code&gt;avg_temperatures_rdd.py&lt;/code&gt;, que vous pourrez copier dans votre dossier
personnel à l’aide de la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Ce fichier contient une implémentation efficace de la fonction qui calcule la température moyenne annuelle, comme nous l’avons vu
dans un précédent tutoriel.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performances-et-nombre-de-partitions&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Performances et nombre de partitions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 70.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 80.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;. Remplacez YOUR_DIRECTORY par &lt;code&gt;itpspark24/itpspark24_XX/&lt;/code&gt; (où XX correspond au numéro de votre nom d’utilisateur).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observez les instructions aux &lt;strong&gt;lignes 98&lt;/strong&gt; et &lt;strong&gt;lignes 104&lt;/strong&gt;. Elles obtiennent et affichent
le nombre de partitions du RDD en argument &lt;code&gt;text_file&lt;/code&gt; et du RDD de sortie &lt;code&gt;temperatures&lt;/code&gt; respectivement.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Complétez le tableau 1. Exécutez le programme sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;. Notez le temps d’exécution et le nombre de partitions des deux RDDs
&lt;code&gt;text_file&lt;/code&gt; et &lt;code&gt;temperatures&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Rappel.&lt;/strong&gt; La commande pour exécuter le programme est la suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution &lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Nombre de partitions &lt;br&gt; (RDD &lt;code&gt;text_file&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Nombre de partitions &lt;br&gt; (RDD &lt;code&gt;temperatures&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
&lt;b&gt;Table 1. Temps d’exécution et nombre de partitions (RDD).&lt;/b&gt;
&lt;/caption&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyse-du-fonctionnement-de-spark&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Analyse du fonctionnement de Spark&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pourriez-vous comprendre comment Spark détermine le nombre de partitions du RDD &lt;code&gt;fichier_texte&lt;/code&gt; en regardant la taille des fichiers d’entrée ?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Coup de pouce.&lt;/strong&gt; Si vous divisez la taille du fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt; par le nombre de partitions du RDD &lt;code&gt;fichier_texte&lt;/code&gt;, quelle valeur obtenez-vous ?
par le nombre de partitions du RDD &lt;code&gt;text_file&lt;/code&gt;, quelle valeur obtenez-vous ? Que représente cette valeur ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Affichez les fichiers stockés dans le dossier de sortie &lt;code&gt;temperatures_10.rdd.out&lt;/code&gt;.
Que constatez-vous ? Existe-t-il une relation avec le nombre de partitions ?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Afin d;afficher le contenu du dossier sur HDFS, vous pouvez utiliser la commande suivante :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/itpspark24/itpspark24_XX/temperatures_10.rdd.out&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-de-lapi-dataframe-pour-le-calcul-des-températures-moyennes&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Utilisation de l’API DataFrame pour le calcul des températures moyennes&lt;/h1&gt;
&lt;p&gt;Vous allez maintenant implémenter un programme Spark pour calculer des températures moyennes en utilisant l’API Spark DataFrame.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Suivez les étapes suivantes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le modèle de code &lt;code&gt;avg_temperatures_df.py&lt;/code&gt; dans votre dossier personnel en exécutant la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_df.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;!--* The instructions **at line 109 and 115** show the number of partitions of the RDDs
underlying the dataframes ``df`` (input) and ``df_avg`` (output) respectively.--&gt;
&lt;ul&gt;
&lt;li&gt;Le résultat du calcul sera stocké dans le dossier &lt;code&gt;temperatures_*.df.out&lt;/code&gt;
sous votre dossier HDFS &lt;code&gt;itpspark24/itpspark24_XX&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 78.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 89.&lt;/strong&gt; Remplacez &lt;code&gt;sarXX&lt;/code&gt; par &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 106.&lt;/strong&gt; Complétez l’instruction pour lire un fichier CSV.
Veuillez noter que les fichiers CSV donnés &lt;strong&gt;n’ont pas d’en-têtes&lt;/strong&gt;. N’utilisez pas l’inférence de schéma,
spécifiez simplement votre schéma manuellement. Pour rappel, les colonnes sont : &lt;code&gt;année&lt;/code&gt;, &lt;code&gt;mois&lt;/code&gt;,
&lt;code&gt;jour&lt;/code&gt;, &lt;code&gt;heure&lt;/code&gt;, &lt;code&gt;minute&lt;/code&gt;, &lt;code&gt;seconde&lt;/code&gt;, &lt;code&gt;température&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ligne 55.&lt;/strong&gt; Complétez la définition de la fonction &lt;code&gt;avg_temperature_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez votre code sur tous les fichiers CSV donnés et &lt;strong&gt;complétez le tableau 2&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 2. RDDs vs. DataFrames.
&lt;/caption&gt;
&lt;/table&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Comparez les temps d’exécution avec ceux obtenus en utilisant les RDD.
Qu’observez-vous ? Comment expliquez-vous ces différences ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- ## Effect of the number of partitions

You&#39;re going to study the effect of the number of partitions on the performances of the program.

* In file ``avg_temperatures_df.py`` add the following instruction right below the initialisation of the 
``SparkSession``. The command sets the number of *shuffle* partitions to a certain value `X`.

``
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, X)
``

* Execute the program with different values of `X` (say, 1, 2, 10, 50, 100, 500, 1000, 5000, 100000) **only on file**
``temperatures_86400.csv`` and write down the execution times.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;

* Plot a graph where the x-axis contains the values of `X` and the y-axis contains the execution time.
What do you observe?


* Execute the program on the other files by using small values of `X` (1 and 2). 
Comment on the evolution of the execution times.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div id=&#34;mise-en-cache-dun-dataframe&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Mise en cache d’un DataFrame&lt;/h2&gt;
&lt;p&gt;Vous allez maintenant découvrir les avantages de la mise en cache d’un DataFrame.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Décommentez les deux dernières lignes du fichier &lt;code&gt;avg_temperatures_df.py&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Supprimez le fichier &lt;code&gt;temperatures_10.df.out&lt;/code&gt; en saisissant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/itpspark24/itpspark24_XX/temperatures_10.df.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Exécutez le code sur le fichier &lt;code&gt;températures_10.csv&lt;/code&gt;.
Quel est le temps d’exécution de chaque action ?
Pouvez-vous expliquer en détail ce qui se passe ici ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Supprimez les fichiers &lt;code&gt;temperatures_10.df.out&lt;/code&gt; et &lt;code&gt;temperatures_10.df.out.bis&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Mettez en cache le DataFrame &lt;code&gt;df_avg&lt;/code&gt; et
exécutez à nouveau le code sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Où devez-vous ajouter l’instruction de mise en cache ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le temps d’exécution de chaque action ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pouvez-vous expliquer en détail ce qui se passe ici ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calcul-des-moyennes-avec-sql&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Calcul des moyennes avec SQL&lt;/h1&gt;
&lt;p&gt;Vous allez maintenant implémenter le calcul des températures moyennes annuelles en utilisant SQL sur les DataFrames Spark.&lt;/p&gt;
&lt;div id=&#34;utilisation-dune-vue&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Utilisation d’une vue&lt;/h2&gt;
&lt;p&gt;Une première option pour interroger un DataFrame avec SQL est de créer une &lt;strong&gt;vue&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;avg_temperatures_sql_view.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez le code des lignes 90, 101 et 118.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;avg_temperature_sql&lt;/code&gt; (ligne 57).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers CSV et &lt;strong&gt;complétez le tableau 3&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que pouvez-vous dire sur les temps d’exécution ? Trouvez-vous des différences
significatives entre l’utilisation de SQL sur une vue et l’utilisation des fonctions de l’API DataFrame ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Vue SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 3. RDDs vs DataFrames vs Vues.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-dune-table&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Utilisation d’une table&lt;/h2&gt;
&lt;p&gt;Une deuxième option pour interroger un DataFrame avec SQL est de créer une &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;avg_temperatures_sql_table.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez les lignes 50, 112, 123 et 140.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implémentez la fonction &lt;code&gt;avg_temperature_sql&lt;/code&gt; (ligne 76).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers CSV.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau 4.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparez les temps d’exécution avec ceux que vous avez obtenus avant. Discutez des résultats.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--While reading the code in file ``avg_temperatures_sql_table.py``, you probably noticed that we have specified 
the path to a HDFS folder for the **Hive metastore warehouse**. This folder will contain the data of each table 
that we create.--&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-10&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-10) &lt;/strong&gt;&lt;/span&gt;

* Look at the files in your home folder (in the local file system, not HDFS) by using the command ``ls``. 

* Do you notice the presence of new files?

* Can you explain what these files are?

* Execute the code on all CSV files and **complete Table 4**. 

* Compare the execution times against the ones that you obtained in the previous exercises.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::--&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Vue SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Temps d’exécution - Table SQL&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercice 1.1
&lt;/td&gt;
&lt;td&gt;
Exercice 2.1
&lt;/td&gt;
&lt;td&gt;
Exercice 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 4. RDDs vs DataFrames vs Vues vs Tables.
&lt;/caption&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Supprimez le fichier &lt;code&gt;temperatures_10.sql.table.out&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez à nouveau le code sur le fichier &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Quel est le temps d’exécution que vous obtenez maintenant ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;utilisation-de-lapi-dataframe-sur-des-fichiers-volumineux&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Utilisation de l’API DataFrame sur des fichiers volumineux&lt;/h1&gt;
&lt;p&gt;Nous allons maintenant examiner les fichiers stockés sous &lt;code&gt;hdfs://sar01:9000/data/sales/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ces fichiers contiennent des données tabulaires relatives à la vente de produits dans une chaîne de magasins.
Nous considérons deux tables : &lt;code&gt;store_sales&lt;/code&gt; et &lt;code&gt;customer&lt;/code&gt;.
Dans la première table, nous trouvons des informations sur chaque vente,
comme l’identifiant du produit vendu,
l’identifiant de l’acheteur,
la quantité de produit acheté et le prix payé par le client.
Pour cette table, nous disposons de 4 fichiers, qui ne diffèrent que par leur taille :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.100.dat&lt;/code&gt; : contient 9.5 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.200.dat&lt;/code&gt; : contient 19 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.400.dat&lt;/code&gt; : contient 38 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_1_4.800.dat&lt;/code&gt; : contient 77 GiB de données.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans la table &lt;code&gt;customer&lt;/code&gt; nous trouvons des données sur les clients, comme le prénom, le nom et la date de naissance.
Nous n’avons qu’un seul fichier pour cette table :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;customer_10000.dat&lt;/code&gt; : contient 8.3 GiB de données.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nous voulons tester les performances de l’API &lt;strong&gt;DataFrame&lt;/strong&gt; sur les requêtes suivantes
(&lt;strong&gt;Attention. vous devez écrire un code qui utilise les fonctions DataFrame, pas du SQL!&lt;/strong&gt;) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Requête Q1&lt;/strong&gt; : renvoie le nombre de clients. Cela correspond à la requête SQL suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;SELECT count(*) 
FROM customer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q2&lt;/strong&gt; : renvoie le prix du produit le plus cher. Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT max(ss_list_price) 
FROM store_sales&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q3&lt;/strong&gt; : renvoie la somme d’argent dépensée par chaque client. Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q4&lt;/strong&gt; : Requête Q3 + trier le résultat de façon à ce que le client qui a dépensé le plus d’argent apparaisse en haut.
Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Requête Q5&lt;/strong&gt; : Requête Q4 + jointure avec la table &lt;code&gt;customer&lt;/code&gt; pour obtenir le nom et le prénom des clients.
Cela correspond à la requête SQL suivante :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk 
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;développement-du-code-utilisant-lapi-dataframe.&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Développement du code utilisant l’API DataFrame.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Copiez le fichier &lt;code&gt;dataframe_api_benchmark.py&lt;/code&gt; dans votre dossier personnel à l’aide de la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp ~cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modifiez le code en suivant les instructions que vous trouvez dans le fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur le fichier &lt;code&gt;store_sales_1_4.100.dat&lt;/code&gt; (le plus petit) pour tester que votre code ne contient pas de bogues.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une fois que vous êtes sûr que votre code est correct, &lt;strong&gt;décommentez les lignes 82 et 83&lt;/strong&gt;. Cela mettra en cache les deux DataFrames.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez le code sur tous les fichiers &lt;code&gt;store-sales_*.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chaque requête est exécutée 5 fois pour avoir une estimation correcte du temps d’exécution.
Vous verrez que les temps d’exécution fluctuent lors des premières itérations et qu’ils se stabilisent
au cours des itérations suivantes.
Lorsque vous notez les temps d’exécution, ne prenez en compte que ceux obtenus
à la dernière itération.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complétez le tableau 5 et notez le temps d’exécution de chaque requête pour chaque fichier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pourquoi le temps d’exécution des requêtes Q1 et Q2 est-il élevé à l’itération 0 ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pensez-vous que la différence entre les temps d’exécution des requêtes est raisonnable ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pensez-vous que l’augmentation des temps d’exécution est raisonnable compte tenu de la taille des fichiers d’entrée ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;Fichier / Requête&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Lecture&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q1&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q2&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q3&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q4&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Requête Q5&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.100.dat
&lt;/td&gt;
&lt;td&gt;
17.24
&lt;/td&gt;
&lt;td&gt;
0.94
&lt;/td&gt;
&lt;td&gt;
1.05
&lt;/td&gt;
&lt;td&gt;
1.17
&lt;/td&gt;
&lt;td&gt;
2.16
&lt;/td&gt;
&lt;td&gt;
5.22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.200.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.400.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.800.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 5. Temps d’exécution des requêtes sur le dataset &lt;code&gt;sales&lt;/code&gt;.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark Structured API</title>
      <link>/courses/bigdata/tutorials/spark-sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/spark-sql/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;number-of-partitions-in-a-rdd&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Number of partitions in a RDD&lt;/h1&gt;
&lt;p&gt;We consider a set of CSV files that contain temperature measurements over several years.
Each line has the following content: &lt;code&gt;year,month,day,hour,minute,second,temperature&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These files are stored at the following location: &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is the detail for each file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contains one measurement per day between 1980 and 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contains one measurement every 2880 seconds between 1980 and 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86.csv&lt;/code&gt; contains one measurement every 86 seconds for the year 1980 alone.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_10.csv&lt;/code&gt; contains one measurement every 10 seconds between 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Get the file &lt;code&gt;avg_temperatures_rdd.py&lt;/code&gt; by typing the following command:&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;left&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This file contains an efficient implementation of the function that computes the average yearly temperature, as we have seen
in a previous tutorial.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-number-of-partitions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; About the number of partitions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 70.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 80.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;. Replace YOUR_DIRECTORY with &lt;code&gt;hpdaspark24/hpdaspark24_XX/&lt;/code&gt; (where XX corresponds to your username number).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observe the instructions at &lt;strong&gt;line 98&lt;/strong&gt; and &lt;strong&gt;line 104&lt;/strong&gt;. They get and show
the number of partitions of the input RDD &lt;code&gt;text_file&lt;/code&gt; and the output RDD &lt;code&gt;temperatures&lt;/code&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Complete Table 1. Run the program on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;. Write down the
execution time and the number of partitions of both RDDs &lt;code&gt;text_file&lt;/code&gt; and &lt;code&gt;temperatures&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Reminder.&lt;/strong&gt; The command to run the program is as follows:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time &lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Number of partitions &lt;br&gt; (&lt;code&gt;text_file&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Number of partitions &lt;br&gt; (&lt;code&gt;temperatures&lt;/code&gt;)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
&lt;b&gt;Table 1. Execution time and partition numbers with RDD.&lt;/b&gt;
&lt;/caption&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-of-sparks-operation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Analysis of Spark’s operation&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Could you understand how Spark determines the number of partitions of the RDD &lt;code&gt;text_file&lt;/code&gt;
by looking at the size of the input files?
&lt;br&gt;
&lt;strong&gt;HINT.&lt;/strong&gt; If you divide the size of the file &lt;code&gt;temperatures_10.csv&lt;/code&gt;
by the number of partitions of the RDD &lt;code&gt;text_file&lt;/code&gt;, which value do you obtain? What does this value represent?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;List the files that are stored in the output folder &lt;code&gt;temperatures_10.rdd.out&lt;/code&gt; under your
HDFS folder. What do you notice? Is there any relation with respect to the number of partitions?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to list the content of the folder in HDFS, you can use the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/hpdaspark24/hpdaspark24_XX/temperatures_10.rdd.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-dataframe-api-to-compute-the-average-temperatures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Using the DataFrame API to compute the average temperatures&lt;/h1&gt;
&lt;p&gt;You’re now going to implement a Spark program to compute the average temperatures by using the Spark DataFrame API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Go through the following steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the code template &lt;code&gt;avg_temperatures_df.py&lt;/code&gt; to your home folder by executing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_df.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;!--* The instructions **at line 109 and 115** show the number of partitions of the RDDs
underlying the dataframes ``df`` (input) and ``df_avg`` (output) respectively.--&gt;
&lt;ul&gt;
&lt;li&gt;The result of the computation will be stored in the folder &lt;code&gt;temperatures_*.df.out&lt;/code&gt;
under your HDFS folder &lt;code&gt;hpdaspark24/hpdaspark24_XX&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 78.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 89.&lt;/strong&gt; Replace &lt;code&gt;sarXX&lt;/code&gt; with &lt;code&gt;sar01&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 106.&lt;/strong&gt; Complete the instruction to read from the input CSV file.
Please note that the input CSV files &lt;strong&gt;do not have headers&lt;/strong&gt;. Don’t use schema inference,
just specify your schema manually. As a reminder, the columns are: &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt;,
&lt;code&gt;day&lt;/code&gt;, &lt;code&gt;hour&lt;/code&gt;, &lt;code&gt;minute&lt;/code&gt;, &lt;code&gt;second&lt;/code&gt;, &lt;code&gt;temperature&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line 55.&lt;/strong&gt; Complete the definition of function &lt;code&gt;avg_temperature_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Execute your code on all the input CSV files and &lt;strong&gt;complete Table 2&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 2. RDDs vs. DataFrames.
&lt;/caption&gt;
&lt;/table&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Compare the execution times with the ones obtained with the implementation using the RDDs.
What do you observe? How do you explain the differences?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- ## Effect of the number of partitions

You&#39;re going to study the effect of the number of partitions on the performances of the program.

* In file ``avg_temperatures_df.py`` add the following instruction right below the initialisation of the 
``SparkSession``. The command sets the number of *shuffle* partitions to a certain value `X`.

``
spark.conf.set(&#34;spark.sql.shuffle.partitions&#34;, X)
``

* Execute the program with different values of `X` (say, 1, 2, 10, 50, 100, 500, 1000, 5000, 100000) **only on file**
``temperatures_86400.csv`` and write down the execution times.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;

* Plot a graph where the x-axis contains the values of `X` and the y-axis contains the execution time.
What do you observe?


* Execute the program on the other files by using small values of `X` (1 and 2). 
Comment on the evolution of the execution times.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div id=&#34;caching-a-dataframe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Caching a DataFrame&lt;/h2&gt;
&lt;p&gt;You’re now going to discover the advantages of caching a DataFrame.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment the last two lines in file &lt;code&gt;avg_temperatures_df.py&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove the file &lt;code&gt;temperatures_10.df.out&lt;/code&gt; by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/hpdaspark24/hpdaspark24_XX/temperatures_10.df.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Execute the code on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.
What is the execution time of each action?
Can you explain in detail what is going on here?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Remove files &lt;code&gt;temperatures_10.df.out&lt;/code&gt; and &lt;code&gt;temperatures_10.df.out.bis&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Cache the DataFrame &lt;code&gt;df_avg&lt;/code&gt; and
execute the code again on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where should you add the cache instruction?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time of each action?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can you explain in detail what is going on here?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-averages-with-sql&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Computing averages with SQL&lt;/h1&gt;
&lt;p&gt;You’re now going to implement the computation of the yearly average temperatures by using SQL on Spark DataFrames.&lt;/p&gt;
&lt;div id=&#34;using-a-view&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Using a view&lt;/h2&gt;
&lt;p&gt;A first option to query a DataFrame with SQL is to create a &lt;strong&gt;view&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;avg_temperatures_sql_view.py&lt;/code&gt; to your home folder by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complete the code in lines 90, 101, 118.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;avg_temperature_sql&lt;/code&gt; (line 57).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all CSV files and &lt;strong&gt;complete Table 3&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What can you tell about the the running times ? Do you find significant differences between using SQL on a view and
DataFrame functions?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL view&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 3. RDDs vs DataFrames with views.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Using a table&lt;/h2&gt;
&lt;p&gt;A second option to query a DataFrame with SQL is to create a &lt;strong&gt;table&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;avg_temperatures_sql_table.py&lt;/code&gt; to your home directory by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Complete lines 50, 112, 123 and 140.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;avg_temperature_sql&lt;/code&gt; (line 76).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete Table 4.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Compare the execution times with the ones that you obtained. Discuss the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--While reading the code in file ``avg_temperatures_sql_table.py``, you probably noticed that we have specified 
the path to a HDFS folder for the **Hive metastore warehouse**. This folder will contain the data of each table 
that we create.--&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-10&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-10) &lt;/strong&gt;&lt;/span&gt;

* Look at the files in your home folder (in the local file system, not HDFS) by using the command ``ls``. 

* Do you notice the presence of new files?

* Can you explain what these files are?

* Execute the code on all CSV files and **complete Table 4**. 

* Compare the execution times against the ones that you obtained in the previous exercises.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::--&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time RDD&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time DataFrame&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL view&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Execution time SQL table&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86400.csv
&lt;/td&gt;
&lt;td&gt;
3.12
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_2880.csv
&lt;/td&gt;
&lt;td&gt;
3.67
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_86.csv
&lt;/td&gt;
&lt;td&gt;
4.59
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
temperatures_10.csv
&lt;/td&gt;
&lt;td&gt;
Exercise 1.1
&lt;/td&gt;
&lt;td&gt;
Exercise 2.1
&lt;/td&gt;
&lt;td&gt;
Exercise 3.1
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 4. RDDs vs DataFrames with views and tables.
&lt;/caption&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the file &lt;code&gt;temperatures_10.sql.table.out&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code again on file &lt;code&gt;temperatures_10.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
What is the execution time that you obtain now?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-dataframe-api-on-large-files&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Using the DataFrame API on large files&lt;/h1&gt;
&lt;p&gt;We now consider the files stored under &lt;code&gt;hdfs://sar01:9000/data/sales/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These files contain tabular data related to the sale of products in a chain of stores.
We consider two tables: &lt;code&gt;store_sales&lt;/code&gt; and &lt;code&gt;customer&lt;/code&gt;.
In the first table we find information about each sale,
such as the identifier of the product sold,
the identifier of the buyer,
the quantity of purchased product and the price paid by the customer.
For this table, we have 4 files, which only differ in size:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_.100.dat&lt;/code&gt;: contains 9.5 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_.200.dat&lt;/code&gt;: contains 19 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_.400.dat&lt;/code&gt;: contains 38 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;store_sales_.800.dat&lt;/code&gt;: contains 77 GiB of data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In table &lt;code&gt;customer&lt;/code&gt; we find data about customers, such as first and last names and birth dates.
We only have one file for this table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;customer_10000.dat&lt;/code&gt;: contains 8.3 GiB of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want to test the performances of the &lt;strong&gt;DataFrame API&lt;/strong&gt; on the following queries
(&lt;strong&gt;WARNING. you must write a code that uses DataFrame functions, not SQL!&lt;/strong&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query Q1&lt;/strong&gt;: returns the number of clients. This corresponds to the following SQL query:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;SELECT count(*) 
FROM customer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q2&lt;/strong&gt;: returns the price of the most expensive product. This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT max(ss_list_price) 
FROM store_sales&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q3&lt;/strong&gt;: returns the amount of money spent by each client. This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q4&lt;/strong&gt;: Query Q3 + sort the result so that the client that spent the most money appears on the top.
This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Query Q5&lt;/strong&gt;: Query Q4 + join with the table &lt;code&gt;customer&lt;/code&gt; to get the first and last name of the customers.
This corresponds to the following SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent 
FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk 
GROUP BY ss_customer_sk
ORDER BY amountSpent DESC&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;development-of-the-code-using-the-dataframe-api.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Development of the code using the DataFrame API.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;dataframe_api_benchmark.py&lt;/code&gt; to your home directory by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;pre&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modify the code by following the instructions in the file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code on file &lt;code&gt;store_sales_100.dat&lt;/code&gt; (the smallest one) to test that your code is bug-free.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once you’re sure that your code is correct, &lt;strong&gt;uncomment lines 82 and 83&lt;/strong&gt;. This will cache the two DataFrames.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code on all files &lt;code&gt;store-sales_*.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each query is executed 5 times to have a correct estimate of the execution time.
You’ll see that the execution times fluctuate on the first iterations and they stabilize
in the later iterations.
When you write down the execution times, only consider the execution times obtained
at the last iteration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Table 5&lt;/strong&gt; and write down the execution time of each query for each file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why the execution time of the queries Q1 and Q2 is large at the iteration 0?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you think that the difference between the execution times of the queries is reasonable?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do you think that the augmentation of the execution times is reasonable given the size of the input files?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;bigdata&#34;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;b&gt;File / query&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Read&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q1&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q2&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q3&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q4&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;b&gt;Query Q5&lt;br&gt;(sec)&lt;/b&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.100.dat
&lt;/td&gt;
&lt;td&gt;
17.24
&lt;/td&gt;
&lt;td&gt;
0.94
&lt;/td&gt;
&lt;td&gt;
1.05
&lt;/td&gt;
&lt;td&gt;
1.17
&lt;/td&gt;
&lt;td&gt;
2.16
&lt;/td&gt;
&lt;td&gt;
5.22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.200.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.400.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
store_sales_1_4.800.dat
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;caption&gt;
Table 5. Execution times of the queries on the sales dataset.
&lt;/caption&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark structured streaming</title>
      <link>/courses/bdia/tutorials/spark-structured-streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/spark-structured-streaming/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;The goal of this tutorial is to learn how to analyze streams of data with the Spark Structured Streaming API.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to answer the questions and do the exercises, you might want to refer to the following
documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html&#34; target=&#34;_blank&#34;&gt;Spark SQL API reference&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warming-up&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Warming up&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;warmup.py&lt;/code&gt; to your home folder by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/warmup.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Read the code and answer the following questions.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where does this program get its input from?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What object type does the variable &lt;code&gt;lines&lt;/code&gt; contain (instruction at &lt;strong&gt;Line 28&lt;/strong&gt;)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Where does this program write its output?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the output of this program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the option &lt;code&gt;checkpointLocation&lt;/code&gt; intended for?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the instruction &lt;code&gt;streamingQuery.awaitTermination()&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You can now verify your answers to the previous questions by &lt;strong&gt;executing the program&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a checkpoint directory for the first exercise (e.g., &lt;code&gt;checkpoint_dir&lt;/code&gt;) under your home directory
&lt;code&gt;hdfs://sar01:9000/bdiaspark23/bdiaspark23_X&lt;/code&gt; &lt;strong&gt;in HDFS&lt;/strong&gt;. The command to do so is as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -mkdir hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete line 21&lt;/strong&gt; of file &lt;code&gt;warmup.py&lt;/code&gt;. Choose a port number in the interval [49152, 65535].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete line 25&lt;/strong&gt; of file &lt;code&gt;warmup.py&lt;/code&gt;. Write the path to the checkpoint location that you created on step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open a new terminal window and connect &lt;strong&gt;to the same kyle machine to which you’re connected
in the first terminal window&lt;/strong&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;ssh kyleXX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the new terminal window, start a &lt;strong&gt;netcat server&lt;/strong&gt;.
Use the following command. Replace &lt;code&gt;[port_number]&lt;/code&gt; with the port number that you chose on step 2. &lt;strong&gt;The command will hang waiting for some input. This is normal.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;nc -lk [port_number]&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the old terminal window, execute the Spark program with &lt;code&gt;spark-submit&lt;/code&gt;. &lt;strong&gt;Wait until Spark displays the content of the first micro-batch&lt;/strong&gt;.
&lt;strong&gt;In case the program stops for an error, remove all files generated in the checkpoint location with the following command and restart the Spark program.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the terminal window where the netcat server is running, write few lines of text.
Look at the terminal where the Spark program is running and observe the output.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Stop the program&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When you’re done with your experiments, you can stop the Spark program by simply
typing CTRL-C in the terminal where Spark is running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all files in the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Don’t stop the netcat server&lt;/strong&gt;, you’ll need it in the next exercise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;triggers&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Triggers&lt;/h1&gt;
&lt;p&gt;In a Structured Streaming program we can choose a &lt;strong&gt;trigger type&lt;/strong&gt;.
A trigger determines when the streaming source is checked for new data.&lt;/p&gt;
&lt;p&gt;By looking at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;,
answer the following questions.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the trigger type in the previous program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the code of the previous program in order to set the
&lt;code&gt;Fixed interval micro-batches&lt;/code&gt; trigger type. Set an interval of 10 seconds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program. Write many lines back to back on the netcat server. How is the behavior of this program different from before?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;checkpoint-location-and-output-mode&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Checkpoint location and output mode&lt;/h1&gt;
&lt;p&gt;We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Look again at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;,
and check the options for the output mode.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the output mode of the previous program?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You’re now going to code a program that counts the number of occurrences of each word
in a streaming data source. But first do the actions that you find in the following Activity.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a copy of the file &lt;code&gt;warmup.py&lt;/code&gt; and rename it to &lt;code&gt;wordcount.py&lt;/code&gt; by typing the
following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp warmup.py wordcount.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Remove all files in the checkpoint location by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Before you code the program, you should first answer the following questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Observe the output of a batch in the previous executions of the Spark programs. What is the type of the output?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Given the type of the output, which Spark API are you going to use to code the program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which operations do you need to execute to count the number of occurrences of each word across batches? For the moment, don’t worry about
the specific functions you need to use, just think in abstract terms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;.
Which output mode can’t you use? Why?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add to the file &lt;code&gt;wordcount.py&lt;/code&gt; the instructions to count the number of occurrences of each word in
the stream. Where are you going to add the new instructions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the answer to the previous exercises, set the appropriate output mode. You have two choices.
Try one of them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure that the netcat server is still running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program with &lt;code&gt;spark-submit&lt;/code&gt;. In the netcat terminal, write few lines and observe the output of
the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the Spark program and run it again with no modifications.
&lt;strong&gt;The first time you get a java.lang.IndexOutOfBoundsException. Don’t lose your hope and run it again.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write few lines in the netcat terminal and observe the output of the Spark program.
What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the program and remove the files in the checkpoint location. Run the program again and write few lines
on the netcat terminal. What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program again with a different output mode and observe the result.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;window-operations-on-event-time&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Window operations on event time&lt;/h1&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Netcat and checkpoint&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important actions to do!&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Stop the netcat server now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all files from the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We’re now going to find out how to perform aggregations over a sliding event-time window.&lt;/p&gt;
&lt;p&gt;A given &lt;strong&gt;data source&lt;/strong&gt; generates two words every second for a certain amount of time.
Each word is accompanied with a timestamp that indicates the exact moment when the word is
generated. This timestamp is the &lt;strong&gt;event time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After generating a word, the data source saves the word and its timestamp into
a CSV file in a directory on HDFS.
For convenience, we’ll refer to this directory as the &lt;strong&gt;source directory&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At any given moment, the source will contain zero to many CSV files,
where each file only contains exactly one line in the format
&lt;code&gt;word,timestamp&lt;/code&gt; (no whitespace before nor after the comma).&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create the source directory under your home directory &lt;strong&gt;in HDFS&lt;/strong&gt;, by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -mkdir hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy to your home directory the file &lt;code&gt;tempws_gen.py&lt;/code&gt; (the data source) by typing the following command:
the data generator that you find at the following path&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/tempws_gen.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy to your home directory the file &lt;code&gt;win_events.py&lt;/code&gt; by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/win_events.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You now need to complete the code in file &lt;code&gt;win_events.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;code&gt;win_events.py&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 19.&lt;/strong&gt; Specify the path to the source directory that you created in the previous activity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 31.&lt;/strong&gt; Write the query to count the number of occurrences of each word within a 10 second window that slides every 5 seconds.
Look at the &lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window&#34; target=&#34;_blank&#34;&gt;documentation of the window function&lt;/a&gt; to learn how
to do it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 34.&lt;/strong&gt; Specify the path to the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 40.&lt;/strong&gt; Write the code to write the output of the streaming query to console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use triggers of 5 seconds.&lt;/li&gt;
&lt;li&gt;Use the output mode &lt;code&gt;update&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Don’t forget to specify the location of the checkpoint directory.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute &lt;code&gt;win_events.py&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you get some errors, stop the program. Correct the errors in the code and then re-execute the program again.&lt;/li&gt;
&lt;li&gt;If no error arises, the program should hang waiting for some input. Let the program wait and go on to the next exercise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We now test the Spark program.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open a new terminal window and type the following command to connect &lt;strong&gt;to the same kyle machine where you’re connected
in the first terminal window&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;ssh kyleXX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Execute the data generator program by typing the following command (&lt;strong&gt;remember to replace X with your account number!&lt;/strong&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py -s hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;After launching the data generator, the generated words are shown in the terminalyou should
see some output in the terminal window where you launched the Spark program.
&lt;strong&gt;Wait for the script &lt;code&gt;tempws_gen.py&lt;/code&gt; to terminate the data generation&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to rerun the Spark program and the data generator, make sure you &lt;strong&gt;delete all the files in the
checkpoint location and the source directory&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We now want to analyze the output of the program.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create a new folder on your own computer (not the computers in the cluster) and open it in a new Visual Studio Code window.
For the sake of simplicity, let’s call this folder &lt;code&gt;window_analysis&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The script &lt;code&gt;tempws_gen.py&lt;/code&gt; has generated a file &lt;code&gt;gen_words.csv&lt;/code&gt; in your home directory in the cluster.
This file contains the list of all words generated with the relative timestamps.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open &lt;code&gt;gen_words.csv&lt;/code&gt; and copy the whole content.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new file &lt;code&gt;gen_words.csv&lt;/code&gt; under &lt;code&gt;window_analysis&lt;/code&gt; and paste the content that you
copied on the previous step.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the file &lt;code&gt;timeline_visualization.py&lt;/code&gt; into your home folder in the cluster by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/timeline_visualization.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;code&gt;timeline_visualization.py&lt;/code&gt; and copy all its content.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new file &lt;code&gt;timeline_visualization.py&lt;/code&gt; under &lt;code&gt;window_analysis&lt;/code&gt; and
paste the content that you copied on the previous step.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Visual Studio Code window where you opened &lt;code&gt;window_analysis&lt;/code&gt;, open a terminal and run the following command:
&lt;strong&gt;WARNING.&lt;/strong&gt; If you’re on macOS, you should type &lt;code&gt;python3&lt;/code&gt; instead of &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python timeline_visualization.py -i gen_words.csv -ft [first timestamp] -lt [last timestamp] -it 5 --slide 5&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;gen_words.csv&lt;/code&gt; is the file that you previously created.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replace &lt;code&gt;first timestamp&lt;/code&gt; with the timestamp of the left boundary of the first window (look at the output of your Spark program).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replace &lt;code&gt;last timestamp&lt;/code&gt; with the timestamp of the right boundary of the last window (look at the output of your Spark program).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The option &lt;code&gt;-it&lt;/code&gt; is used to specify the interval between two consecutive triggers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The option &lt;code&gt;--slide&lt;/code&gt; is used to specify the window slide frequency.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The plot will show a grid line at each trigger and a blue line at each window boundary.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Analyze the output of your Spark program and the timeline of the generated words.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Describe how the counts are updated by the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;late-data-and-watermarking&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Late data and watermarking&lt;/h1&gt;
&lt;p&gt;We’re now going to learn how Structured Streaming handles
late data in windowed aggregations.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remove generated files&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove all the files in the source directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all the files in the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;The data generator &lt;code&gt;tempws_gen.py&lt;/code&gt;
can generate a stream of words, some of which
might be written to the directory &lt;code&gt;tempws&lt;/code&gt; with some amount of
delay. In other words, there is a gap between the event time (when the word is generated)
and the processing time (when the word is written to the directory).&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To generate data with some delay you can use the following command (&lt;strong&gt;remember to replace the X with your account number!&lt;/strong&gt;):&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py -s hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir --delay 0.8&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the previous command, the 0.8 indicates that the probability that an event arrives with some delay is 80%.
The delay is between 5 and 15 seconds.
You can adjust these values by using the appropriate options. To learn more, type the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py --help&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Spark uses a mechanism called &lt;strong&gt;watermarking&lt;/strong&gt; to specify the maximum delay that an event can take to be
counted. If the difference between the arrival time and the event time
is higher than a certain threshold (the watermark), the delayed data will be discarded.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write a Spark program that does the same aggregation as in the previous exercise.
Additionally, the program must use watermarking to handle late data.
You can &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking&#34; target=&#34;_blank&#34;&gt;look at the programming guide to learn how to do it&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Start the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generate some data with delay with the program &lt;code&gt;tempws_gen.py&lt;/code&gt;.
Once the data generation stops, you can stop the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visualize the generated words with the visualization tool.
Late words have the delay indicated between parentheses.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observe the output of the Spark program and describe how
the watermarking mechanism works on this example.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark structured streaming</title>
      <link>/courses/bdia_old/tutorials/structured-streaming-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/structured-streaming-lab/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;!--


# Introduction 

The goal of this lab assignment is to learn how to analyze streams of data with the Spark Structured Streaming API.
[Refer to this documentation](/courses/plp/overview/cluster-connection){target=&#34;_blank&#34;} to learn how to connect and 
interact with the cluster.

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Assignment submission**

This lab assignment will be **evaluated**.

In order to finalize the submission of the assignment, 
complete [this form available on Edunao](https://centralesupelec.edunao.com/mod/quiz/view.php?id=48267){target=&#34;_blank&#34;}.

The submission deadline is **Tuesday, March 30, 2021 8:00 AM**.

:::


::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Documentation**

In order to answer the questions and do the exercises, you might want to refer to the following 
documentation:

*  The [Structured Streaming programing guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html){target=&#34;_blank&#34;}.

* The [Spark SQL API reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html){target=&#34;_blank&#34;}.

:::



# Warming up

Consider the following program.

```
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

port_number = COMPLETE HERE
checkpoint_location = COMPLETE HERE

spark = (SparkSession.builder.appName(&#34;Structured Streaming - exo1&#34;).getOrCreate())

lines = (spark\
        .readStream.format(&#34;socket&#34;)\
        .option(&#34;host&#34;, &#34;localhost&#34;)\
        .option(&#34;port&#34;, port_number)\
        .load())

streamingQuery = lines.writeStream\
                .option(&#34;checkpointLocation&#34;, checkpoint_location)\
                .format(&#34;console&#34;).start()

streamingQuery.awaitTermination()
```

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;

1. Where does this program get its input from?

2. What object type does the variable ``lines`` contain? 

3. Where does this program write its output?

4. What is the output of this program?

5. What is the option  ``checkpointLocation`` intended for?

6. What does the instruction ``streamingQuery.awaitTermination()``?

&lt;/div&gt;\EndKnitrBlock{exercise}

:::

You can now verify your answers to the previous questions by **executing the program**. 

::: {.infobox .activitybox data-latex=&#34;{exercisebox}&#34;}
**Activity**



1. Connect to the cluster, if you haven&#39;t done so yet. [Refer to this documentation](/courses/plp/overview/cluster-connection){target=&#34;_blank&#34;}.

2. After running the command ``srun ...``, you should be connected to a machine on the cluster Kyle. 
Note the name of this machine (you should see it at the terminal prompt).

3. Create a checkpoint directory for the first exercise (e.g., ``checkpoint_exo1``) under your home directory
``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 

4. Copy and paste the code into a Python file (e.g., ``exo1.py``) that you&#39;ll save into your home directory **in the local filesystem** 
of the cluster machine.
   * Change the value of the variable ``checkpoint_location`` so that it points to the directory that you created at point 3.
   * Change the value of the variable ``port_number`` to any value in the range [49152, 65535].

5. Open a new terminal window, connect to ``phome.metz.supelec.fr`` and then to the same machine that you noted at point 2.

6. In the new terminal, start a **netcat server** listening on the port number that you selected at point 4. Use the following command:

``
nc -lk port_number
``

7. Run the Python code with the command ``spark-submit``. Wait until Spark does not display any more messages on screen.
   * In case the program stops for an error, read the box &#34;What to do in case of errors&#34; below.

8. In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output.

:::

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**What to do in case of errors**

If any error arises, **before** running the ``spark-submit`` again it would be better to remove 
all files from the checkpoint directory.

:::


::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Stop the program**

* When you&#39;re done with your experiments, you can stop the Spark program by simply 
typing CTRL-C in the terminal where Spark is running. 

* Don&#39;t stop the netcat server, you&#39;ll need it in the next exercise.

:::


# Triggering policy

In a Structured Streaming program we can choose a **triggering policy**.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;

1. What is a triggering policy?

2. What is the triggering policy in the previous program?

3. Modify the code of the previous program in order to set  the 
``Fixed interval micro-batch`` triggering policy.

4. Run the program. How is the behaviour of this program different from before?

&lt;/div&gt;\EndKnitrBlock{exercise}

:::


# Checkpoint location and output mode

We&#39;re now going to see the impact of  the checkpoint location and the output modes on a streaming query.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;

1. What is an output mode and what are the available options?

2. What is the output mode of the previous program?
&lt;/div&gt;\EndKnitrBlock{exercise}

:::


We&#39;re now going to write a new streaming query.




::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;

1. Create  a new checkpoint location in HDFS. 
You may also keep the same directory as before; in this case, make sure you **remove all files** from that 
directory.

2. Write a new program that reads a streaming text from a TCP socket and 
counts the number of occurrences of each word.

3. Which output mode are you going to choose and why?

4. Run the program. Write few lines on the netcat server and observe the output.

5. Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output.
What can you say about the word counts?

6. Stop the program and remove the files in the checkpoint location. Run the program again and write few lines 
on the netcat terminal. What can you say about the word counts?

7. Play with the different output modes and observe how the output changes.

&lt;/div&gt;\EndKnitrBlock{exercise}

:::


# Window operations on event time



::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Netcat and checkpoint**

1. You can stop the netcat server now. 

2. Remember to create a new checkpoint location for this exercise. 
Alternatively, you can also use the same directory as in the previous exercises, but
you should remove all its files.


:::


We&#39;re now going to find out how to perform aggregations over  a sliding event-time window.

A given data source generates some words for a certain time interval.
Each word is accompanied with a timestamp that indicates the exact moment when the word is 
generated. This timestamp is the **event time**.

After generating a word, the data source saves the word and its timestamp into 
a CSV file in a directory on HDFS. 
For convenience, we&#39;ll refer to this directory as the **source directory**.

At any given moment, the source  will contain zero to many CSV files, 
where each file only contains exactly one line in the format 
``word,timestamp`` (no whitespace before nor after the comma).


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;

Write a Spark program that:

1. Reads the stream of data from the source directory.

2. Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes.

3. Print the output counts to the console. Use triggers of 5 seconds.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

We now test the new Spark program.

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Data source and timeline visualization**

We provide two Python programs for this exercise: a data generator and 
a tool for visualizing words in a timeline. Instructions to get and run
these two programs are given in the activity below.

The **data generator** is our data source. 
It generates two words every second for a certain 
amount of time. 
Each word is saved in a separate CSV file in source directory.
It also saves the list of all generated 
words to a summary CSV file. 

The **visualization tool** takes as its input the summary CSV file 
written by the data generator and visualizes the words 
on a timeline.

:::


::: {.infobox .activitybox data-latex=&#34;{exercisebox}&#34;}
**Activity**

1. Create the source directory under your home directory
``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 

2. Copy to your home directory 
in the local filesystem 
the data generator that you find at the following path

``/usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py``. 

3. Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly.

4. In another terminal, run the Python script ``tempws_gen.py``. Use the 
following command to learn how to run this program:

``python3 tempws_gen.py --help``

For this exercise, do not introduce any delay 
(keep the default values of the parameters ``--delay``, ``--mindelay``, ``--maxdelay``).

5. After launching the data generator, you should 
see some output in the terminal where you launched the Spark program. 
**Wait for the script ``tempws_gen.py`` to terminate the data generation**.
The output might be a bit overwhelming. Scroll up to identify the results 
on each micro-batch.

6. If you need to rerun the Spark program and the data generator, make sure you delete all the files in the 
checkpoint location and the source directory.


:::

We now want to analyze the output of the program.


* The script ``tempws_gen.py`` has generated 
a file ``gen_words.csv`` in your home directory. 
This file contains the list of all words generated with the relative timestamps. 
**Download the file to your computer**.

* Download the visualization tool ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py`` 
to your computer.


::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Visualization tool**

Use the following command to learn how to run the visualization tool:

``
python timeline_visualization.py --help
``
 
The visualization tool displays a vertical blue bar at each trigger.
To this purpose, you&#39;ll need to pass the tool the timestamps associated to the first 
and last trigger and the interval (in seconds) between two consecutive triggers.

You can get the timestamps associated to the first and last trigger by analyzing the output of Spark.
More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you&#39;ll need 
to look at the timestamp associated to the first and last micro-batch.



:::



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;

1. Analyze the output of your Spark program and the timeline of the generated words.

2. Describe how the counts are updated by the Spark program.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

# Late data and watermarking

We&#39;re now going to learn how Structured Streaming handles 
late data in windowed aggregations.


::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Remove generated files**

Remove all the files in the directory ``tempws``.

:::

The data generator ``tempws_gen.py``
can generate a stream of words, some of which 
might be written to the directory ``tempws`` with some amount of 
delay. In other words, there is a gap between the event time (when the word is generated)
and the arrival time (when the word is written to the directory).

Use the following command to learn how to do it:

``python3 tempws_gen.py --help``


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-7&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-7) &lt;/strong&gt;&lt;/span&gt;

1. Write a Spark program that does the same aggregation as in the previous exercise.
Additionally, the program must use watermarking to handle late data.

2. Start the Spark program.

3. Generate some data with delay with the program ``tempws_gen.py``. 
Once the data generation stops, you can stop the Spark program.

4. Visualize the generated words with the visualization tool. 
Late words have the delay indicated between parentheses.

5. Observe the output of the Spark program and describe how 
the watermarking mechanism works on this example.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::


--&gt;
</description>
    </item>
    
    <item>
      <title>Spark structured streaming</title>
      <link>/courses/bigdata/tutorials/structured-streaming-lab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/structured-streaming-lab/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;The goal of this tutorial is to learn how to analyze streams of data with the Spark Structured Streaming API.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to answer the questions and do the exercises, you might want to refer to the following
documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html&#34; target=&#34;_blank&#34;&gt;Spark SQL API reference&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warming-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Warming up&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;warmup.py&lt;/code&gt; to your home folder by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/warmup.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Read the code and answer the following questions.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where does this program get its input from?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What object type does the variable &lt;code&gt;lines&lt;/code&gt; contain (instruction at &lt;strong&gt;Line 28&lt;/strong&gt;)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Where does this program write its output?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the output of this program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the option &lt;code&gt;checkpointLocation&lt;/code&gt; intended for?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the instruction &lt;code&gt;streamingQuery.awaitTermination()&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The program gets its input from a TCP socket.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;lines&lt;/code&gt; is a DataFrame.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The program writes the output to the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Any input is copied to the output without any transformation. We didn’t call any function on the DataFrame &lt;code&gt;line&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The checkpoint location is a reference to a HDFS folder where the output of a streaming query on a micro-batch is saved.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instruction &lt;code&gt;awaitTermination()&lt;/code&gt; waits for the user to stop the Spark program to interrupt the streaming query.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can now verify your answers to the previous questions by &lt;strong&gt;executing the program&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a checkpoint directory for the first exercise (e.g., &lt;code&gt;checkpoint_dir&lt;/code&gt;) under your home directory
&lt;code&gt;hdfs://sar01:9000/itpspark23/itpspark23_X&lt;/code&gt; &lt;strong&gt;in HDFS&lt;/strong&gt;. The command to do so is as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -mkdir hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete line 21&lt;/strong&gt; of file &lt;code&gt;warmup.py&lt;/code&gt;. Choose a port number in the interval [49152, 65535].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete line 25&lt;/strong&gt; of file &lt;code&gt;warmup.py&lt;/code&gt;. Write the path to the checkpoint location that you created on step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open a new terminal window and connect &lt;strong&gt;to the same kyle machine to which you’re connected
in the first terminal window&lt;/strong&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;ssh kyleXX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the new terminal window, start a &lt;strong&gt;netcat server&lt;/strong&gt;.
Use the following command. Replace &lt;code&gt;[port_number]&lt;/code&gt; with the port number that you chose on step 2. &lt;strong&gt;The command will hang waiting for some input. This is normal.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;nc -lk [port_number]&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the old terminal window, execute the Spark program with &lt;code&gt;spark-submit&lt;/code&gt;. &lt;strong&gt;Wait until Spark displays the content of the first micro-batch&lt;/strong&gt;.
&lt;strong&gt;In case the program stops for an error, remove all files generated in the checkpoint location with the following command and restart the Spark program.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In the terminal window where the netcat server is running, write few lines of text.
Look at the terminal where the Spark program is running and observe the output.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The complete code is available in file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/warmup.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Stop the program&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When you’re done with your experiments, you can stop the Spark program by simply
typing CTRL-C in the terminal where Spark is running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all files in the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Don’t stop the netcat server&lt;/strong&gt;, you’ll need it in the next exercise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;triggers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Triggers&lt;/h1&gt;
&lt;p&gt;In a Structured Streaming program we can choose a &lt;strong&gt;trigger type&lt;/strong&gt;.
A trigger determines when the streaming source is checked for new data.&lt;/p&gt;
&lt;p&gt;By looking at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;,
answer the following questions.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the trigger type in the previous program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the code of the previous program in order to set the
&lt;code&gt;Fixed interval micro-batches&lt;/code&gt; trigger type. Set an interval of 10 seconds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program. Write many lines back to back on the netcat server. How is the behavior of this program different from before?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We didn’t specify any trigger type, therefore the default one is applied (micro-batches mode).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The complete code is available in file:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/warmup-fixed-intervals.py&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Everything that we write within 10 seconds is considered to be part of the same micro-batch, while before
each line was a single micro-batch.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;checkpoint-location-and-output-mode&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Checkpoint location and output mode&lt;/h1&gt;
&lt;p&gt;We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Look again at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;,
and check the options for the output mode.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What is the output mode of the previous program?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We didn’t specify any output mode. So the default option is considered.
The default output mode is &lt;code&gt;append&lt;/code&gt;.
Only the new output since the last trigger will be displayed.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You’re now going to code a program that counts the number of occurrences of each word
in a streaming data source. But first do the actions that you find in the following Activity.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a copy of the file &lt;code&gt;warmup.py&lt;/code&gt; and rename it to &lt;code&gt;wordcount.py&lt;/code&gt; by typing the
following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp warmup.py wordcount.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Remove all files in the checkpoint location by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Before you code the program, you should first answer the following questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Observe the output of a batch in the previous executions of the Spark programs. What is the type of the output?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Given the type of the output, which Spark API are you going to use to code the program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which operations do you need to execute to count the number of occurrences of each word across batches? For the moment, don’t worry about
the specific functions you need to use, just think in abstract terms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Look at the &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programming guide&lt;/a&gt;.
Which output mode can’t you use? Why?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The output of a batch is a DataFrame with one column named &lt;code&gt;value&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We need to use the Spark DataFrame API.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We need to split the string in the column &lt;code&gt;value&lt;/code&gt; into its constituent words. The package &lt;code&gt;pyspark.sql.functions&lt;/code&gt; contains a function
&lt;code&gt;split&lt;/code&gt; that allows us to do so. Then, we need to aggregate by word and count.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Append mode (the default one) cannot be used. In fact, we need to use an aggregating function (&lt;code&gt;count&lt;/code&gt;), that can update the count
of a word seen in a previous batch.
This is not supported in append mode.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add to the file &lt;code&gt;wordcount.py&lt;/code&gt; the instructions to count the number of occurrences of each word in
the stream. Where are you going to add the new instructions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the answer to the previous exercises, set the appropriate output mode. You have two choices.
Try one of them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure that the netcat server is still running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program with &lt;code&gt;spark-submit&lt;/code&gt;. In the netcat terminal, write few lines and observe the output of
the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the Spark program and run it again with no modifications.
&lt;strong&gt;The first time you get a java.lang.IndexOutOfBoundsException. Don’t lose your hope and run it again.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write few lines in the netcat terminal and observe the output of the Spark program.
What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the program and remove the files in the checkpoint location. Run the program again and write few lines
on the netcat terminal. What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program again with a different output mode and observe the result.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The instructions need to be inserted right below the &lt;code&gt;load&lt;/code&gt; instruction and before writing the result to the output stream.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code is available in the following file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/wordcount.py&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The output mode that is chosen in the file is &lt;code&gt;complete&lt;/code&gt;. The other possible is update.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OK, running&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each micro-batch, the set of all words written up to that point is shown withe corresponding counts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The word counts that we had in the previous execution are still available in the output. They are saved in the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now, the words counts that we had in the previous executions are not available anymore.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since we tried the &lt;code&gt;complete&lt;/code&gt; mode, we try the &lt;code&gt;update&lt;/code&gt; mode.
At each micro-batch, only the word counts that change are displayed.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;window-operations-on-event-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Window operations on event time&lt;/h1&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Netcat and checkpoint&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important actions to do!&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Stop the netcat server now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all files from the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We’re now going to find out how to perform aggregations over a sliding event-time window.&lt;/p&gt;
&lt;p&gt;A given &lt;strong&gt;data source&lt;/strong&gt; generates two words every second for a certain amount of time.
Each word is accompanied with a timestamp that indicates the exact moment when the word is
generated. This timestamp is the &lt;strong&gt;event time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After generating a word, the data source saves the word and its timestamp into
a CSV file in a directory on HDFS.
For convenience, we’ll refer to this directory as the &lt;strong&gt;source directory&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At any given moment, the source will contain zero to many CSV files,
where each file only contains exactly one line in the format
&lt;code&gt;word,timestamp&lt;/code&gt; (no whitespace before nor after the comma).&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create the source directory under your home directory &lt;strong&gt;in HDFS&lt;/strong&gt;, by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -mkdir hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy to your home directory the file &lt;code&gt;tempws_gen.py&lt;/code&gt; (the data source) by typing the following command:
the data generator that you find at the following path&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/tempws_gen.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy to your home directory the file &lt;code&gt;win_events.py&lt;/code&gt; by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/win_events.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You now need to complete the code in file &lt;code&gt;win_events.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;code&gt;win_events.py&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 19.&lt;/strong&gt; Specify the path to the source directory that you created in the previous activity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 31.&lt;/strong&gt; Write the query to count the number of occurrences of each word within a 10 second window that slides every 5 seconds.
Look at the &lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window&#34; target=&#34;_blank&#34;&gt;documentation of the window function&lt;/a&gt; to learn how
to do it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 34.&lt;/strong&gt; Specify the path to the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete Line 40.&lt;/strong&gt; Write the code to write the output of the streaming query to console.
&lt;ul&gt;
&lt;li&gt;Use triggers of 5 seconds.&lt;/li&gt;
&lt;li&gt;Use the output mode &lt;code&gt;update&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Don’t forget to specify the location of the checkpoint directory.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Execute &lt;code&gt;win_events.py&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;If you get some errors, stop the program. Correct the errors in the code and then re-execute the program again.&lt;/li&gt;
&lt;li&gt;If no error arises, the program should hang waiting for some input. Let the program wait and go on to the next exercise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The complete code is available in the following file in the cluster:
&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/win_events.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We now test the Spark program.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open a new terminal window and type the following command to connect &lt;strong&gt;to the same kyle machine where you’re connected
in the first terminal window&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;ssh kyleXX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Execute the data generator program by typing the following command (&lt;strong&gt;remember to replace X with your account number!&lt;/strong&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py -s hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;After launching the data generator, the generated words are shown in the terminalyou should
see some output in the terminal window where you launched the Spark program.
&lt;strong&gt;Wait for the script &lt;code&gt;tempws_gen.py&lt;/code&gt; to terminate the data generation&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to rerun the Spark program and the data generator, make sure you &lt;strong&gt;delete all the files in the
checkpoint location and the source directory&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We now want to analyze the output of the program.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create a new folder on your own computer (not the computers in the cluster) and open it in a new Visual Studio Code window.
For the sake of simplicity, let’s call this folder &lt;code&gt;window_analysis&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The script &lt;code&gt;tempws_gen.py&lt;/code&gt; has generated a file &lt;code&gt;gen_words.csv&lt;/code&gt; in your home directory in the cluster.
This file contains the list of all words generated with the relative timestamps.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open &lt;code&gt;gen_words.csv&lt;/code&gt; and copy the whole content.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new file &lt;code&gt;gen_words.csv&lt;/code&gt; under &lt;code&gt;window_analysis&lt;/code&gt; and paste the content that you
copied on the previous step.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the file &lt;code&gt;timeline_visualization.py&lt;/code&gt; into your home folder in the cluster by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/timeline_visualization.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;code&gt;timeline_visualization.py&lt;/code&gt; and copy all its content.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new file &lt;code&gt;timeline_visualization.py&lt;/code&gt; under &lt;code&gt;window_analysis&lt;/code&gt; and
paste the content that you copied on the previous step.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Visual Studio Code window where you opened &lt;code&gt;window_analysis&lt;/code&gt;, open a terminal and run the following command:
&lt;strong&gt;WARNING.&lt;/strong&gt; If you’re on macOS, you should type &lt;code&gt;python3&lt;/code&gt; instead of &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python timeline_visualization.py -i gen_words.csv -ft [first timestamp] -lt [last timestamp] -it 5 --slide 5&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;gen_words.csv&lt;/code&gt; is the file that you previously created.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replace &lt;code&gt;first timestamp&lt;/code&gt; with the timestamp of the left boundary of the first window (look at the output of your Spark program).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replace &lt;code&gt;last timestamp&lt;/code&gt; with the timestamp of the right boundary of the last window (look at the output of your Spark program).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The option &lt;code&gt;-it&lt;/code&gt; is used to specify the interval between two consecutive triggers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The option &lt;code&gt;--slide&lt;/code&gt; is used to specify the window slide frequency.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The plot will show a grid line at each trigger and a blue line at each window boundary.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Analyze the output of your Spark program and the timeline of the generated words.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Describe how the counts are updated by the Spark program.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;late-data-and-watermarking&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Late data and watermarking&lt;/h1&gt;
&lt;p&gt;We’re now going to learn how Structured Streaming handles
late data in windowed aggregations.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remove generated files&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove all the files in the source directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all the files in the checkpoint directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;The data generator &lt;code&gt;tempws_gen.py&lt;/code&gt;
can generate a stream of words, some of which
might be written to the directory &lt;code&gt;tempws&lt;/code&gt; with some amount of
delay. In other words, there is a gap between the event time (when the word is generated)
and the processing time (when the word is written to the directory).&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To generate data with some delay you can use the following command (&lt;strong&gt;remember to replace the X with your account number!&lt;/strong&gt;):&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py -s hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir --delay 0.8&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the previous command, the 0.8 indicates that the probability that an event arrives with some delay is 80%.
The delay is between 5 and 15 seconds.
You can adjust these values by using the appropriate options. To learn more, type the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;python3 tempws_gen.py --help&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Spark uses a mechanism called &lt;strong&gt;watermarking&lt;/strong&gt; to specify the maximum delay that an event can take to be
counted. If the difference between the arrival time and the event time
is higher than a certain threshold (the watermark), the delayed data will be discarded.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write a Spark program that does the same aggregation as in the previous exercise.
Additionally, the program must use watermarking to handle late data.
You can &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking&#34; target=&#34;_blank&#34;&gt;look at the programming guide to learn how to do it&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Start the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generate some data with delay with the program &lt;code&gt;tempws_gen.py&lt;/code&gt;.
Once the data generation stops, you can stop the Spark program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visualize the generated words with the visualization tool.
Late words have the delay indicated between parentheses.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Observe the output of the Spark program and describe how
the watermarking mechanism works on this example.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The complete code is available in the following file in the cluster:
&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/win_event_watermarking.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Apache Spark — Structured Streaming</title>
      <link>/courses/big-data-marseille/tutorials/spark-streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/tutorials/spark-streaming/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The goal of this lab assignment is to learn how to analyze streams of data with the Spark Structured Streaming API.
&lt;a href=&#34;/courses/plp/overview/cluster-connection&#34; target=&#34;_blank&#34;&gt;Refer to this documentation&lt;/a&gt; to learn how to connect and
interact with the cluster.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In order to answer the questions and do the exercises, you might want to refer to the following
documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34; target=&#34;_blank&#34;&gt;Structured Streaming programing guide&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html&#34; target=&#34;_blank&#34;&gt;Spark SQL API reference&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;/div&gt;
&lt;div id=&#34;warming-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Warming up&lt;/h1&gt;
&lt;p&gt;Consider the following program.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

port_number = COMPLETE HERE
checkpoint_location = COMPLETE HERE

spark = (SparkSession.builder.appName(&amp;quot;Structured Streaming - exo1&amp;quot;).getOrCreate())

lines = (spark\
        .readStream.format(&amp;quot;socket&amp;quot;)\
        .option(&amp;quot;host&amp;quot;, &amp;quot;localhost&amp;quot;)\
        .option(&amp;quot;port&amp;quot;, port_number)\
        .load())

streamingQuery = lines.writeStream\
                .option(&amp;quot;checkpointLocation&amp;quot;, checkpoint_location)\
                .format(&amp;quot;console&amp;quot;).start()

streamingQuery.awaitTermination()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where does this program get its input from?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What object type does the variable &lt;code&gt;lines&lt;/code&gt; contain?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Where does this program write its output?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the output of this program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the option &lt;code&gt;checkpointLocation&lt;/code&gt; intended for?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the instruction &lt;code&gt;streamingQuery.awaitTermination()&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The input is a socket, where a program generates some data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;lines is a &lt;strong&gt;dataframe&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Just copies the input to the output.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is where the Spark program writes the progress of the streaming query.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instructions means that the program will block on this instruction. The program won’t stop until there is
data to process.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can now verify your answers to the previous questions by &lt;strong&gt;executing the program&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Connect to the cluster, if you haven’t done so yet. &lt;a href=&#34;/courses/plp/overview/cluster-connection&#34; target=&#34;_blank&#34;&gt;Refer to this documentation&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After running the command &lt;code&gt;srun ...&lt;/code&gt;, you should be connected to a machine on the cluster Kyle.
Note the name of this machine (you should see it at the terminal prompt).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a checkpoint directory for the first exercise (e.g., &lt;code&gt;checkpoint_exo1&lt;/code&gt;) under your home directory
&lt;code&gt;hdfs://sar01:9000/cpuecm1/cpuecm1_X&lt;/code&gt; &lt;strong&gt;in HDFS&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Copy and paste the code into a Python file (e.g., &lt;code&gt;exo1.py&lt;/code&gt;) that you’ll save into your home directory &lt;strong&gt;in the local filesystem&lt;/strong&gt;
of the cluster machine.
&lt;ul&gt;
&lt;li&gt;Change the value of the variable &lt;code&gt;checkpoint_location&lt;/code&gt; so that it points to the directory that you created at point 3.&lt;/li&gt;
&lt;li&gt;Change the value of the variable &lt;code&gt;port_number&lt;/code&gt; to any value in the range [49152, 65535].&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open a new terminal window, connect to &lt;code&gt;phome.metz.supelec.fr&lt;/code&gt; and then to the same machine that you noted at point 2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the new terminal, start a &lt;strong&gt;netcat server&lt;/strong&gt; listening on the port number that you selected at point 4. Use the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;nc -lk port_number&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run the Python code with the command &lt;code&gt;spark-submit&lt;/code&gt;. Wait until Spark does not display any more messages on screen.
&lt;ul&gt;
&lt;li&gt;In case the program stops for an error, read the box “What to do in case of errors” below.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;What to do in case of errors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If any error arises, &lt;strong&gt;before&lt;/strong&gt; running the &lt;code&gt;spark-submit&lt;/code&gt; again it would be better to remove
all files from the checkpoint directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Stop the program&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When you’re done with your experiments, you can stop the Spark program by simply
typing CTRL-C in the terminal where Spark is running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t stop the netcat server, you’ll need it in the next exercise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Remove all files&lt;/strong&gt; from the checkpoint location.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;triggering-policy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Triggering policy&lt;/h1&gt;
&lt;p&gt;In a Structured Streaming program we can choose a &lt;strong&gt;triggering policy&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is a triggering policy?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the triggering policy in the previous program?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the code of the previous program in order to set the
&lt;code&gt;Fixed interval micro-batch&lt;/code&gt; triggering policy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program. How is the behaviour of this program different from before?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;It is a policy that dictates the timing of streaming data processing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No triggering policy is specified, so the default is chosen.
In practice, as soon as the previous micro-batch finishes processing, the next one is read in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here is the code. We need to specify a `&lt;code&gt;trigger&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;streamingQuery = lines.writeStream
            .trigger(processingTime = &amp;quot;15 seconds&amp;quot;)
            .format(&amp;quot;console&amp;quot;).start()&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The new data is checked at the specified interval.
It is possible that within the specified interval (15 seconds), we write
many lines, so the program will get multiple lines in a micro-batch (unlike before, when
the processing is triggered as soon as there is data available).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;checkpoint-location-and-output-mode&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Checkpoint location and output mode&lt;/h1&gt;
&lt;p&gt;We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is an output mode and what are the available options?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;What is the output mode of the previous program?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The output mode tells Spark how the output is presented.
There are several options: append (only the new rows added to the Result Table since the last trigger are visible in the output),
complete (the whole Result Table is visible after each trigger) and update (only the rows that were updated since the last trigger are visible in the
output).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;In the previous program we didn’t specify any output mode, so the default mode (append) is selected.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We’re now going to write a new streaming query.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a new checkpoint location in HDFS.
You may also keep the same directory as before; in this case, make sure you &lt;strong&gt;remove all files&lt;/strong&gt; from that
directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a new program that reads a streaming text from a TCP socket and
counts the number of occurrences of each word.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which output mode are you going to choose and why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the program. Write few lines on the netcat server and observe the output.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output.
What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop the program and remove the files in the checkpoint location. Run the program again and write few lines
on the netcat terminal. What can you say about the word counts?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Play with the different output modes and observe how the output changes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The new program is as follows:&lt;/p&gt;
&lt;pre class=&#34;{python}&#34;&gt;&lt;code&gt;lines = (spark\
        .readStream.format(&amp;quot;socket&amp;quot;)\
        .option(&amp;quot;host&amp;quot;, &amp;quot;localhost&amp;quot;)\
        .option(&amp;quot;port&amp;quot;, port_number)\
        .load())

lines = lines.select(F.explode(F.split(lines.value, &amp;quot; &amp;quot;))\
                     .alias(&amp;quot;word&amp;quot;))\
                     .groupBy(&amp;quot;word&amp;quot;).count()


streamingQuery = lines.writeStream\
            .trigger(processingTime = &amp;quot;15 seconds&amp;quot;)\
            .option(&amp;quot;checkpointLocation&amp;quot;, checkpoint_location)\
            .outputMode(&amp;quot;update&amp;quot;)\
            .format(&amp;quot;console&amp;quot;)\
            .start()

streamingQuery.awaitTermination()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The append mode doesn’t work, because aggregating function might modify previous lines of the ResultTable.
So, the only options left are update and complete. We choose update to just have the values that changed since the last trigger.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;window-operations-on-event-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Window operations on event time&lt;/h1&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Netcat and checkpoint&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You can stop the netcat server now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remember to create a new checkpoint location for this exercise.
Alternatively, you can also use the same directory as in the previous exercises, but
you should remove all its files.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We’re now going to find out how to perform aggregations over a sliding event-time window.&lt;/p&gt;
&lt;p&gt;A given data source generates some words for a certain time interval.
Each word is accompanied with a timestamp that indicates the exact moment when the word is
generated. This timestamp is the &lt;strong&gt;event time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After generating a word, the data source saves the word and its timestamp into
a CSV file in a directory on HDFS.
For convenience, we’ll refer to this directory as the &lt;strong&gt;source directory&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create the source directory under your home directory
&lt;code&gt;hdfs://sar01:9000/cpuecm1/cpuecm1_X&lt;/code&gt; &lt;strong&gt;in HDFS&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At any given moment, the source will contain zero to many CSV files,
where each file only contains exactly one line in the format
&lt;code&gt;word,timestamp&lt;/code&gt; (no whitespace before nor after the comma).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Write a Spark program that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reads the stream of data from the source directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Print the output counts to the console. Use triggers of 5 seconds.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The new program is as follows:&lt;/p&gt;
&lt;pre class=&#34;{python}&#34;&gt;&lt;code&gt;
source_directory = hdfs://....

words = (spark
        .readStream.format(&amp;quot;csv&amp;quot;)
        .schema(&amp;quot;word STRING, timestamp TIMESTAMP&amp;quot;)
        .load(source_directory))


windowedCount = words.groupBy(F.window(words.timestamp, 
                &amp;quot;10 seconds&amp;quot;, &amp;quot;5 seconds&amp;quot;, startTime=0), words.word).count()

windowedQuery = windowedCount.withColumn(&amp;quot;trigger_timestamp&amp;quot;, F.expr(&amp;quot;get_current_timestamp()&amp;quot;)).writeStream\
            .trigger(processingTime=&amp;quot;5 seconds&amp;quot;)\
            .outputMode(&amp;quot;update&amp;quot;)\
            .format(&amp;quot;console&amp;quot;)\
            .option(&amp;quot;truncate&amp;quot;, False)\
            .start()

streamingQuery.awaitTermination()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We now test the new Spark program.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Data source and timeline visualization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We provide two Python programs for this exercise: a data generator and
a tool for visualizing words in a timeline. Instructions to get and run
these two programs are given in the activity below.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;data generator&lt;/strong&gt; is our data source.
It generates two words every second for a certain
amount of time.
Each word is saved in a separate CSV file in source directory.
It also saves the list of all generated
words to a summary CSV file.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;visualization tool&lt;/strong&gt; takes as its input the summary CSV file
written by the data generator and visualizes the words
on a timeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy to your home directory
in the local filesystem
the data generator that you find at the following path&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In another terminal, run the Python script &lt;code&gt;tempws_gen.py&lt;/code&gt;. Use the
following command to learn how to run this program:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python3 tempws_gen.py --help&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this exercise, do not introduce any delay
(keep the default values of the parameters &lt;code&gt;--delay&lt;/code&gt;, &lt;code&gt;--mindelay&lt;/code&gt;, &lt;code&gt;--maxdelay&lt;/code&gt;).&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;After launching the data generator, you should
see some output in the terminal where you launched the Spark program.
&lt;strong&gt;Wait for the script &lt;code&gt;tempws_gen.py&lt;/code&gt; to terminate the data generation&lt;/strong&gt;.
The output might be a bit overwhelming. Scroll up to identify the results
on each micro-batch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you need to rerun the Spark program and the data generator, make sure you delete all the files in the
checkpoint location and the source directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;We now want to analyze the output of the program.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The script &lt;code&gt;tempws_gen.py&lt;/code&gt; has generated
a file &lt;code&gt;gen_words.csv&lt;/code&gt; in your home directory.
This file contains the list of all words generated with the relative timestamps.
&lt;strong&gt;Download the file to your computer&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download the visualization tool that you find at the following path:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to your computer.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Visualization tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the following command to learn how to run the visualization tool:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python timeline_visualization.py --help&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The visualization tool displays a vertical blue bar at each trigger.
To this purpose, you’ll need to pass the tool the timestamps associated to the first
and last trigger and the interval (in seconds) between two consecutive triggers.&lt;/p&gt;
&lt;p&gt;You can get the timestamps associated to the first and last trigger by analyzing the output of Spark.
More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you’ll need
to look at the timestamp associated to the first and last micro-batch.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Analyze the output of your Spark program and the timeline of the generated words.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Describe how the counts are updated by the Spark program.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Unlike the previous exercise, here the number of occurrences of each word is counted based on a
time window of 10 seconds that slides every 5 seconds.
Each word is associated with an &lt;strong&gt;event time&lt;/strong&gt; that is used to compute the number of occurrences.
The time window starts from the first trigger, say at 12:00.
If a word arrives at 12:07, the count associated to this word are updated in two
time windows, 12:00 - 12:10 and 12:05 - 12:15.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data modeling</title>
      <link>/courses/bdalbert/tutorials/data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdalbert/tutorials/data-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to &lt;strong&gt;create a conceptual schema&lt;/strong&gt; of a database.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;draw an entity-relationship&lt;/strong&gt; (ER) diagram.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;translate&lt;/strong&gt; a &lt;strong&gt;conceptual model&lt;/strong&gt; into a &lt;strong&gt;logical model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;database-of-a-social-network-platform&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Database of a social network platform&lt;/h1&gt;
&lt;p&gt;A social network platform wants to design a relational database
to store information on its users.
For each user, the platform keeps its nickname,
that uniquely identifies the user in the platform, first and family name,
geographic location (city and country) and email address;
the user can register as many email addresses as s/he wishes.
Any user can share content on the platform; each post is characterized by its content,
date, time and, when available, the geolocation (latitude, longitude).
Optionally, users can tag one or more friends in their posts.&lt;/p&gt;
&lt;p&gt;Two users are linked by a friendship relationship
if both agree on befriending each other;
a user can also follow another user without necessarily befriending her.
For any type of relationship (friendship or follower),
the platform registers the date when the relationship is established.&lt;/p&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Translate the conceptual schema into a logical schema.
For each table, underline the primary key and specify the foreign keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;database-of-a-banking-system&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Database of a banking system&lt;/h1&gt;
&lt;p&gt;The following figure shows the ER diagram with the
conceptual schema of a banking system database.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:banking-system-db&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-1/banking-system-er.png&#34; alt=&#34;The conceptual schema of the bank database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: The conceptual schema of the bank database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each bank is identified by a unique code and name,
and has one or several branches.
A branch is responsible for opening accounts and
granting loans to customers.
Each account is identified by
a number (&lt;em&gt;acct_nbr&lt;/em&gt;)
and is either a checking or savings account
(property &lt;em&gt;acct_type&lt;/em&gt;).
Each customer is identified by its social
security number (&lt;em&gt;ssn&lt;/em&gt;);
a customer can be granted several loans and open as many accounts as s/he wishes.&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;Which primary key would you choose for the entity Bank? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Would you consider {&lt;em&gt;code_bank&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;}
as a valid candidate key for the entity &lt;TT&gt;Bank&lt;/TT&gt;? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Complete the
diagram in the figure
by adding the cardinalities to the relations.
Justify your choices when any ambiguity arises.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;car-dealership-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Car dealership database&lt;/h1&gt;
&lt;p&gt;We want to design the database of a car dealership.
The dealership sells both new and used cars,
and it operates a service facility.
The database should keep data about the cars
(serial number, make, model, colour, whether it is new or used),
the salespeople (first and family name)
and the customers (first and family name, phone number, address).
Also, the following business rules hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A salesperson may sell many cars, but each car is sold by only one salesperson.&lt;/li&gt;
&lt;li&gt;A customer may buy many cars, but each car is bought by only one customer.&lt;/li&gt;
&lt;li&gt;A salesperson writes a single invoice for each car s/he sells.
The invoice is identified by a number and indicates the sale date and the price.&lt;/li&gt;
&lt;li&gt;A customer gets an invoice for each car s/he buys.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a customer takes one or more cars in for repair,
one service ticket is written for each car.
The ticket is identified by a number and
indicates the date on which the car is received from the customer,
as well as the date on which the car should be returned to the customer.
A car brought in for service can be worked on by many mechanics,
and each mechanic may work on many cars.&lt;/p&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data modeling</title>
      <link>/courses/databases/tutorials/data-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/data-modeling/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to &lt;strong&gt;create a conceptual schema&lt;/strong&gt; of a database.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;draw an entity-relationship&lt;/strong&gt; (ER) diagram.&lt;/li&gt;
&lt;li&gt;How to &lt;strong&gt;translate&lt;/strong&gt; a &lt;strong&gt;conceptual model&lt;/strong&gt; into a &lt;strong&gt;logical model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having attended &lt;a href=&#34;/courses/cloud-computing/lectures/lectures&#34;&gt;Lecture 1&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;database-of-a-social-network-platform&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Database of a social network platform&lt;/h1&gt;
&lt;p&gt;A social network platform wants to design a relational database
to store information on its users.
For each user, the platform keeps its nickname,
that uniquely identifies the user in the platform, first and family name,
geographic location (city and country) and email address;
the user can register as many email addresses as s/he wishes.
Any user can share content on the platform; each post is characterized by its content,
date, time and, when available, the geolocation (latitude, longitude).
Optionally, users can tag one or more friends in their posts.&lt;/p&gt;
&lt;p&gt;Two users are linked by a friendship relationship
if both agree on befriending each other;
a user can also follow another user without necessarily befriending her.
For any type of relationship (friendship or follower),
the platform registers the date when the relationship is established.&lt;/p&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Translate the conceptual schema into a logical schema.
For each table, underline the primary key and specify the foreign keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;database-of-a-banking-system&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Database of a banking system&lt;/h1&gt;
&lt;p&gt;The following figure shows the ER diagram with the
conceptual schema of a banking system database.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:banking-system-db&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-1/banking-system-er.png&#34; alt=&#34;The conceptual schema of the bank database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: The conceptual schema of the bank database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each bank is identified by a unique code and name,
and has one or several branches.
A branch is responsible for opening accounts and
granting loans to customers.
Each account is identified by
a number (&lt;em&gt;acct_nbr&lt;/em&gt;)
and is either a checking or savings account
(property &lt;em&gt;acct_type&lt;/em&gt;).
Each customer is identified by its social
security number (&lt;em&gt;ssn&lt;/em&gt;);
a customer can be granted several loans and open as many accounts as s/he wishes.&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;Which primary key would you choose for the entity Bank? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Would you consider {&lt;em&gt;code_bank&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;}
as a valid candidate key for the entity &lt;TT&gt;Bank&lt;/TT&gt;? Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Complete the
diagram in the figure
by adding the cardinalities to the relations.
Justify your choices when any ambiguity arises.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;car-dealership-database&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Car dealership database&lt;/h1&gt;
&lt;p&gt;We want to design the database of a car dealership.
The dealership sells both new and used cars,
and it operates a service facility.
The database should keep data about the cars
(serial number, make, model, colour, whether it is new or used),
the salespeople (first and family name)
and the customers (first and family name, phone number, address).
Also, the following business rules hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A salesperson may sell many cars, but each car is sold by only one salesperson.&lt;/li&gt;
&lt;li&gt;A customer may buy many cars, but each car is bought by only one customer.&lt;/li&gt;
&lt;li&gt;A salesperson writes a single invoice for each car s/he sells.
The invoice is identified by a number and indicates the sale date and the price.&lt;/li&gt;
&lt;li&gt;A customer gets an invoice for each car s/he buys.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a customer takes one or more cars in for repair,
one service ticket is written for each car.
The ticket is identified by a number and
indicates the date on which the car is received from the customer,
as well as the date on which the car should be returned to the customer.
A car brought in for service can be worked on by many mechanics,
and each mechanic may work on many cars.&lt;/p&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Exercises&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Give the &lt;strong&gt;conceptual schema&lt;/strong&gt; of the database with an ER diagram.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Translate the conceptual schema into a logical schema.
For each table, underline the primary keys and specify the foreign keys.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with Docker</title>
      <link>/courses/cloud-computing/tutorials/tutorial-docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-computing/tutorials/tutorial-docker/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to run &lt;strong&gt;containers&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and build &lt;strong&gt;images&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to create and use &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and use &lt;strong&gt;networks&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having installed Docker on your computer or having imported the Linux virtual machine either with VirtualBox or with Multipass.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Related course material:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Slides of the first lecture. You can find them &lt;a href=&#34;https://centralesupelec.edunao.com/pluginfile.php/426017/mod_resource/content/9/cloud-cs-lecture-1.pdf&#34; target=&#34;_blank&#34;&gt;on Edunao&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Handbook (poly) of the first lecture. You can find it &lt;a href=&#34;https://centralesupelec.edunao.com/pluginfile.php/435165/mod_resource/content/10/chapter-1.pdf&#34; target=&#34;_blank&#34;&gt;on Edunao&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Using a virtual machine with Multipass? Click here for more info
&lt;/summary&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Multipass commands&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To start the virtual machine, type &lt;code&gt;multipass start cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The folder &lt;code&gt;/home/ubuntu/labs&lt;/code&gt; in the virtual machine is mounted (i.e., linked) to the folder
on YOUR computer (let’s call it &lt;code&gt;LAB&lt;/code&gt;) where you’ll store all your lab material.
You specified this folder when you installed the virtual machine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you don’t remember the path to the folder linked to &lt;code&gt;/home/ubuntu/labs&lt;/code&gt;, type the
command &lt;code&gt;multipass info cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To open a terminal into the virtual machine, type &lt;code&gt;multipass shell cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the terminal opens, the current working directory is &lt;code&gt;/home/ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;cd labs&lt;/code&gt; to move to the folder where you’ll find your lab material.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll use the virtual machine terminal only to type the Docker commands.
You can use all the tools installed on your machine to create and manage the files
in the directory &lt;code&gt;LAB&lt;/code&gt; directly from your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once the lab is over, type &lt;code&gt;exit&lt;/code&gt; to leave the virtual machine terminal. This will lead you back
to the terminal of your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal of your computer, type &lt;code&gt;multipass stop cloudvm&lt;/code&gt; to stop the virtual machine.
This will NOT destroy your files! It just stops the virtual machine from needlessly using the resources of your computer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Terminology&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll use the &lt;strong&gt;terminal&lt;/strong&gt; to run Docker commands.
The terminal is the &lt;strong&gt;client&lt;/strong&gt; that communicates with
the Docker daemon.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker runs &lt;strong&gt;containers&lt;/strong&gt; on your computer.
We’ll refer to your computer as the &lt;strong&gt;host&lt;/strong&gt;,
the containers being the &lt;strong&gt;guests&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;containerized application&lt;/strong&gt;
is an application running in a container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Running containers&lt;/h1&gt;
&lt;!--The command used to run a container 
is ``docker run`` followed by four parameters:

```shell
docker run [options] image-name [command] [arg]
```

The four parameters are:

* *options*. List of options. 
* *image-name*. The fully qualified name of the image used to run the container. 
* *command*. The command to be executed in the container.
* *arg*. The arguments taken by the command executed in the container.

Only the parameter *image-name* is mandatory. 
The fully qualified name of an image is specified as a sequence of four fields, 
formatted as follows:

```shell
registry_url/user/name:tag
```

where:

* *registry_url* (optional). The URL of the registry that provides the image. 
If its value is not specified, the image 
will be looked up for in the 
[DockerHub registry](https://hub.docker.com){target=&#34;_blank&#34;}.
* *user* (optional).  The identifier of the user or organization that created the image. 
The default value is *library*.
* *name* (mandatory). The name of the image. 
* *tag* (optional). It specifies the image version.
If its value is not specified, 
the tag *latest* is used, pointing to the latest image version. --&gt;
&lt;!--
::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
For each of the following images, 
specify the registry name, the user, the name and the tag.

1. registry.redhat.io/rhel8/mysql-80

2. alpine:3.11

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}
:::



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;What&#39;s the difference between the following image names?

1. alpine:latest

2. registry.hub.docker.com/library/alpine

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::

--&gt;
&lt;p&gt;We now learn how to use the command &lt;code&gt;docker run&lt;/code&gt; and some of
its options.
In the following exercises, we’ll run containers from the image named
&lt;em&gt;alpine&lt;/em&gt; that is
&lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;available on the DockerHub registry&lt;/a&gt;.
This image provides a lightweight distribution
(i.e., it doesn’t contain many features) of Linux.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;You want to run the container from the latest version of
the image &lt;em&gt;alpine&lt;/em&gt;.
Which command would you write in the terminal?
Execute it.
&lt;/div&gt;
&lt;/div&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;Execute the command that you proposed in the previous exercise, 
observe the output in the terminal and explain the actions 
taken by Docker to run the container.&lt;/div&gt;\EndKnitrBlock{exercise}


:::

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;Is the container still running?
&lt;/div&gt;
&lt;/div&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;What information is displayed for each container?&lt;/div&gt;\EndKnitrBlock{exercise}

:::

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;By looking at the command executed within the container (&lt;code&gt;/bin/sh&lt;/code&gt;),
can you tell why the container stopped without giving any output?
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We’re now going to do something useful with the image &lt;em&gt;alpine&lt;/em&gt;.
Make sure you read the &lt;strong&gt;good practices&lt;/strong&gt; that you should
adopt while playing with images and containers.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good practices&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Name your containers.&lt;/strong&gt; Although Docker assigns a default name to a new container,
it’s usually a good practice to give a container a name of your
choice to make it easily distinguishable. You can do it by using the option
&lt;code&gt;--name&lt;/code&gt;. Try the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker run --name my-alpine alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As before, the container stops immediately.
If you list all your containers by typing again:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see a container named &lt;em&gt;my-alpine&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove automatically a container if you use it once.&lt;/strong&gt;
Unless you want to reuse your container later, you can ask Docker to automatically remove it
when it stops by using the option &lt;code&gt;--rm&lt;/code&gt;.
This will prevent unused containers from taking up too much disk space.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Try the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name container-to-remove alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you list all the containers you should see that there is no container
named &lt;em&gt;container-to-remove&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused containers.&lt;/strong&gt; Stopped containers that have been run without
using the option &lt;code&gt;--rm&lt;/code&gt; are still stored in the host.
If you want to remove a specific
container (e.g., &lt;em&gt;my-alpine&lt;/em&gt;), use the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker container rm my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you want to remove all stopped containers, use the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container prune&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused images.&lt;/strong&gt; Images can take up a lot of disk space.
As a result, you should remember to remove those that you don’t intend to use
any longer.
The commands to remove a specific image
and prune unused ones are &lt;code&gt;docker image rm&lt;/code&gt;
and &lt;code&gt;docker image prune -a&lt;/code&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;pass-a-command-to-the-containerized-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Pass a command to the containerized application&lt;/h2&gt;
&lt;p&gt;Remember that the template of &lt;code&gt;docker run&lt;/code&gt; is the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run [options] image-name [command] [arg]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The optional parameter &lt;em&gt;command&lt;/em&gt; refers to a command
that you can pass the containerized application, possibly with some arguments
(parameter &lt;em&gt;arg&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Let’s see an example.
As we saw before, when we run a container from the image &lt;em&gt;alpine&lt;/em&gt;,
a Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is launched.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is run within the
container.
Henceforth, we’ll use the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Host terminal.&lt;/strong&gt; The terminal that you use to
interact with the operating system of your computer.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guest terminal.&lt;/strong&gt; The terminal that is run within
the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;By using the optional parameter &lt;em&gt;command&lt;/em&gt;, we can run
a command in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Run a container from the image &lt;em&gt;alpine&lt;/em&gt; and execute the
Linux command &lt;code&gt;ls&lt;/code&gt; that lists the content of the current directory.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where are the listed files stored?
In the host or in the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:command-in-guest&#34;&gt;1.4&lt;/a&gt; the command
&lt;code&gt;ls&lt;/code&gt; is executed in the guest terminal, but its
output is redirected to the host terminal.&lt;/p&gt;
&lt;p&gt;In other words, when we run the container, we
don’t interact directly with the guest terminal;
we just send a command and the output is redirected
to the host terminal.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s see how to execute a command in the guest terminal
that also requires an argument.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;By using the Linux utility &lt;code&gt;ping&lt;/code&gt;, check
whether the Web site &lt;code&gt;www.centralesupelec.fr&lt;/code&gt; is reachable.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--An application running in a container might need to interact 
with the user. 
For instance, the Linux 
command ``rev`` reverses whatever 
the user types on the keyboard.
In order to interact with a container, you should use the option
``-it`` of ``docker run``.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-9&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-9) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *alpine* to execute the 
Linux command `rev` and interact with it.
You can stop interacting with ``rev`` by typing Ctrl+C at any time.&lt;/div&gt;\EndKnitrBlock{exercise}


:::
--&gt;
&lt;p&gt;Now run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run  --name my-alpine -it alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; we didn’t use the option &lt;code&gt;--rm&lt;/code&gt; (the container will not be removed
when we stop it, we’re going to use it again).
Moreover, we didn’t specify any command to run in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;What do you obtain?
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-and-stopping-containers.&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Starting and stopping containers.&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run&lt;/code&gt; is a shorthand for two Docker commands, namely
&lt;code&gt;docker create&lt;/code&gt;, that creates a container from an image,
and &lt;code&gt;docker start&lt;/code&gt;, that starts the container after its creation.&lt;/p&gt;
&lt;p&gt;Suppose now that you want to download a Web page
by using Linux Alpine.
You can use the Linux command &lt;code&gt;wget&lt;/code&gt; followed by the URL of the page
that you want to download.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:wget-exo&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;By using the guest terminal
in the container &lt;em&gt;my-alpine&lt;/em&gt;,
download
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;this Web page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where will the Web page be saved? The host computer or the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Want to copy the donwloaded file from the container to your computer?
&lt;/summary&gt;
&lt;p&gt;To copy the file &lt;code&gt;presentation&lt;/code&gt; to your working directory, type the following command in
the host terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  docker cp &amp;lt;containerId&amp;gt;:/presentation .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the previous command, replace &lt;code&gt;&amp;lt;containerId&amp;gt;&lt;/code&gt; with the identifier of your container. You can obtain
the identifier of the container from the output of the command &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Where are the containers and image files stored?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use MacOS or Windows
&lt;/summary&gt;
&lt;p&gt;The files managed by Docker are not stored directly in your computer, but
in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).&lt;/p&gt;
&lt;p&gt;Therefore, you need to open a terminal inside that Linux virtual machine by typing the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the terminal is opened, you can follow the instructions given below for Linux users.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you use Docker on Linux
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All files managed by Docker are stored under folder &lt;code&gt;/var/lib/docker&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To access that folder, you need to be root (i.e., administrator). Type the command &lt;code&gt;sudo su&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;ls /var/lib/docker&lt;/code&gt; you can look at the folders stored under this directory. You’ll see that there are
folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To locate the files of a specific container, you first need to get the &lt;strong&gt;container identifier&lt;/strong&gt; by typing &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the command &lt;code&gt;docker inspect &amp;lt;container-id&amp;gt;&lt;/code&gt; (replace &lt;code&gt;&amp;lt;container-id&amp;gt;&lt;/code&gt; with the identifier of the container that you intend to inspect).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the field &lt;code&gt;UpperDir&lt;/code&gt;. The value of this field is the path to the directory (let’s call it &lt;code&gt;CONTAINER_DIR&lt;/code&gt;) that contains the upper layer of the container (the writable layer).
It should be a path that ends by &lt;code&gt;/diff&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;cd CONTAINER_DIR&lt;/code&gt; (replace &lt;code&gt;CONTAINER_DIR&lt;/code&gt; with the value of the field &lt;code&gt;UpperDir&lt;/code&gt;) you can finally see the files stored in
your container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;em&gt;my-alpine&lt;/em&gt; guest terminal type &lt;code&gt;exit&lt;/code&gt;.
This closes the guest terminal and, as a result, stops the
container.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTICE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stopping the container will not erase any of the files
stored in the container. Removing the container will.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to start the container &lt;em&gt;my-alpine&lt;/em&gt; again, you can
use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container start -ai my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will open the guest terminal of the container again;
type &lt;code&gt;ls&lt;/code&gt; to verify that
the Web page that you downloaded before is still there.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Homework (optional)
&lt;/summary&gt;
&lt;p&gt;Suppose that you need to download all the figures of
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;this Web page&lt;/a&gt;.
The Linux utility &lt;code&gt;wget&lt;/code&gt; comes in handy.
However, you don’t have Linux and you’d like to
avoid the hassle of installing it on your computer, or in a virtual machine,
just for this task.&lt;/p&gt;
&lt;p&gt;A great alternative is to run Linux in a Docker container.
Unfortunately, the Alpine distribution that we’ve been playing
with doesn’t provide
an implementation of &lt;code&gt;wget&lt;/code&gt; with all the options that we need.&lt;/p&gt;
&lt;p&gt;We turn to another Linux distribution, &lt;strong&gt;Ubuntu&lt;/strong&gt;,
for which DockerHub has
&lt;a href=&#34;https://hub.docker.com/_/ubuntu/&#34; target=&#34;_blank&#34;&gt;several images&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;Run a container with Ubuntu (latest) and open a guest terminal.
Call the container &lt;em&gt;dl-figures&lt;/em&gt;, and avoid the option
&lt;code&gt;--rm&lt;/code&gt;, we’ll use this container later.
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;From now on, we’ll be interacting with the guest Ubuntu terminal.
If you type the command &lt;code&gt;wget&lt;/code&gt;,
you’ll get an error (&lt;code&gt;bash: wget: command not found&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The image &lt;em&gt;Ubuntu&lt;/em&gt; doesn’t include all the commands that you’d find
in a full-blown Ubuntu distribution;
the reason is to keep the size of the image small,
a necessary constraint given that
images are transferred over the Internet.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Luckily, there’s a way to install &lt;code&gt;wget&lt;/code&gt; in our Ubuntu distribution.
Ubuntu provides a powerful command-line package manager called
&lt;strong&gt;Advanced Package Tool&lt;/strong&gt; (APT).
First, you need to run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;apt update&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;which fetches the available packages from a list of sources
available in file &lt;code&gt;/etc/apt/sources.list&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then, you can install &lt;code&gt;wget&lt;/code&gt; by running the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;apt install -y wget&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to obtain all the figures from a
Web page, type the following command:&lt;/p&gt;
&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;wget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see in the current directory a new folder
named &lt;em&gt;my-figures&lt;/em&gt; containing the downloaded figures;
verify it by typing &lt;code&gt;ls my-figures&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before terminating, don’t forget to read your fortune cookie.
In the shell, run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;apt-get install -y fortune&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and then:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/usr/games/fortune -s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;When you’re done, you can simply type the command &lt;code&gt;exit&lt;/code&gt; to quit
the guest terminal and stop the container.&lt;/p&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-images&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Creating Images&lt;/h1&gt;
&lt;p&gt;A Docker image can be thought of as a template to create and run a container.
An image is a file that contains a &lt;strong&gt;layered filesystem&lt;/strong&gt; with each layer being &lt;strong&gt;immutable&lt;/strong&gt;;
this means that the files that belong to a layer cannot be
modified or deleted, nor can files be added to a layer.&lt;/p&gt;
&lt;p&gt;When a container is created from an image, it
will be composed of all the image read-only layers and, on top of
them, a writable layer (termed the &lt;strong&gt;container layer&lt;/strong&gt;),
where all the new files created in the container will be written.
For example, the Web page that you downloaded in Exercise &lt;a href=&#34;#exr:wget-exo&#34;&gt;1.7&lt;/a&gt;
were stored in the writable layer of that container.&lt;/p&gt;
&lt;!--## Interactive image creation

When we run the container *dl-figures*  in Section \@ref(simple-use-case), 
we modified the container to
install the command ``wget``. 
You can see the modifications by typing the 
following command:

``
docker diff dl-figures
``

The output consists of a list of files tagged with the letter A, C or D, indicating 
respectively that the file has been added (A), changed (C) or deleted (D). 
In this list you&#39;ll find the downloaded figures, as well as 
other files that have been added or modified or deleted 
when we installed ``wget``.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-12&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-12) &lt;/strong&gt;&lt;/span&gt;If layers, except the top one, are immutable, 
how can files that belong to the lower layers be modified or deleted?&lt;/div&gt;\EndKnitrBlock{exercise}
:::



We can create a new image from the container *dl-figures*, one that provides
a Ubuntu distribution with the command ``wget`` already installed, 
with the following command:


```shell
docker commit dl-figures ubuntu-with-wget
```

The command creates a new image called *ubuntu-with-wget*.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-14&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-14) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *ubuntu-with-wget* and verify that the command
*wget* is actually installed. &lt;/div&gt;\EndKnitrBlock{exercise}

:::

--&gt;
&lt;div id=&#34;dockerfiles&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Dockerfiles&lt;/h2&gt;
&lt;p&gt;The most common way to create an image is to use a &lt;strong&gt;Dockerfile&lt;/strong&gt;, a
text file that contains all the instructions necessary to
build the image.
The advantage of the Dockerfile is that it can be interpreted
by the Docker engine, which makes the creation of images an automated
and repeatable task.&lt;/p&gt;
&lt;p&gt;Suppose that we want to create a containerized
application to download figures from a Web page.
As a template for this application, we need to build a new
image, that we’ll call &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The Dockerfile containing the instructions to build the image
&lt;em&gt;fig-downloader&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update
RUN apt-get install -y wget
RUN mkdir -p /my-figures
WORKDIR /my-figures
ENTRYPOINT [&amp;quot;wget&amp;quot;, &amp;quot;-nd&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-A&amp;quot;, &amp;quot;jpg,jpeg,bmp,png,gif&amp;quot;]
CMD [&amp;quot;https://www.centralesupelec.fr/fr/presentation&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the explanation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We use the image &lt;em&gt;ubuntu&lt;/em&gt; as the &lt;strong&gt;base image&lt;/strong&gt;.
This corresponds to the instruction &lt;code&gt;FROM ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We install the utility &lt;code&gt;wget&lt;/code&gt; in the base image.
This corresponds to the
instructions &lt;code&gt;RUN apt-get update&lt;/code&gt; and &lt;code&gt;RUN apt-get install -y wget&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create a directory &lt;code&gt;my-figures&lt;/code&gt; under the root directory of the image.
This corresponds to the instruction &lt;code&gt;RUN mkdir -p /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We set the newly created directory &lt;em&gt;/my-figures&lt;/em&gt; as the
&lt;strong&gt;working directory&lt;/strong&gt; of the image. This corresponds to the
instruction &lt;code&gt;WORKDIR /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We specify the command to be executed when a container is run from this image.
This corresponds to the instruction&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;ENTRYPOINT [&#34;wget&#34;, &#34;-nd&#34;, &#34;-r&#34;, &#34;-A&#34;, &#34;jpg,jpeg,bmp,png,gif&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This instruction means: execute &lt;code&gt;wget&lt;/code&gt; with the options
&lt;code&gt;-nd&lt;/code&gt;, &lt;code&gt;-r&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt;;
the last option takes a list of file
extensions (&lt;code&gt;jpg,jpeg,bmp,png,gif&lt;/code&gt;) as its argument.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remember that the utility &lt;code&gt;wget&lt;/code&gt; takes the URL of the Web page as
an argument. The URL will be specified when we run the container from
the image &lt;em&gt;fig-downloader&lt;/em&gt;.
Optionally, we can specify a default argument by using the
keyword CMD. The meaning of the instruction:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;CMD [&#34;https://www.centralesupelec.fr/fr/presentation&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;is: if we don’t give any URL when we run the container, the figures will
be downloaded from
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;https://www.centralesupelec.fr/fr/presentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;What’s the relation between the Dockerfile lines and the image layers?
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:dockerfile-creation&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Could you identify a problem in this Dockerfile?
Modify the Dockerfile accordingly.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-an-image&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Building an image&lt;/h2&gt;
&lt;p&gt;We’re now going to build an image from a Dockerfile.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a directory named &lt;em&gt;fig-downloader&lt;/em&gt; in your computer with
a file named &lt;em&gt;Dockerfile&lt;/em&gt; inside.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Dockerfile&lt;/em&gt; write the set of instructions that
you proposed in Exercise &lt;a href=&#34;#exr:dockerfile-creation&#34;&gt;2.2&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal, set the working directory to &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build an image called &lt;em&gt;fig-downloader&lt;/em&gt; by executing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.&lt;/code&gt; at the end of the command means that the Docker engine will
look for a file named &lt;em&gt;Dockerfile&lt;/em&gt; in the working directory.&lt;/p&gt;
&lt;!--

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-16&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-16) &lt;/strong&gt;&lt;/span&gt;Once the image is built, type the command ``docker image ls -a``. 
What are the images with repository and tag ``&lt;none&gt;``?
Why are there three of such images?&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;/div&gt;

--&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you give the Dockerfile a different name (say, &lt;em&gt;Dockerfile-fig-downloader&lt;/em&gt;),
the command to build the image will be:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader -f Dockerfile-fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The option &lt;code&gt;-f&lt;/code&gt; is used to specify the name of the Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In order to verify that the new image has been created, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;
&lt;!-- Let&#39;s dive deeper into the anatomy of an image.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-17&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-17) &lt;/strong&gt;&lt;/span&gt;Run the following command:

``
docker history fig-downloader
`` 

and analyze the layers of the new image. 

* Why do some layers have an ID, while others 
are marked as &lt;i&gt;missing&lt;/i&gt;?

::::: {.last-child}
* Can you find the identifiers of the intermediate images?
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:dl-1-container&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --name dl-1 fig-downloader&lt;/code&gt;&lt;/p&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --name dl-2 fig-downloader https://www.centralesupelec.fr/&lt;/code&gt;&lt;/pre&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;containerized-python-application&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Containerized Python application&lt;/h2&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-docker/word-frequency.zip&#34;&gt;this archive file&lt;/a&gt;
and unzip it into your working directory.
In this archive you’ll find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Dockerfile.&lt;/li&gt;
&lt;li&gt;A Python script &lt;em&gt;main.py&lt;/em&gt; that asks the user to enter the URL and the language of a Web page,
and prints the 10 most frequent words occurring in that page.&lt;/li&gt;
&lt;li&gt;A file &lt;em&gt;requirements.txt&lt;/em&gt; with the list of the Python packages
needed to run the given script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The content of the Dockerfile is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./main.py ./requirements.txt /app/
RUN pip install -r requirements.txt
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;Describe what this Dockerfile does.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34; word-latex=&#34;{exercisebox}&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;Build an image called &lt;em&gt;wordfreq&lt;/em&gt; from this Dockerfile.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;Without changing the Dockerfile, rebuild the same image.
What do you notice?
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;What happens if you modify a line in the Python script and
you rebuild the image?
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.9  &lt;/strong&gt;&lt;/span&gt;Based on the previous considerations,
can you tell what’s wrong with this Dockerfile?
Modify the Dockerfile accordingly and rebuild the image.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.10  &lt;/strong&gt;&lt;/span&gt;Modify &lt;em&gt;main.py&lt;/em&gt; by adding a new line of code and rebuild the image.
What changed?
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Play with the application by running the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it  wordfreq&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The containerized application will prompt you to insert the URL of a webpage and
the language of the page (in English).
The output will be the 20 most used words in the webpage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-volumes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Volumes&lt;/h1&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:dl-1-container&#34;&gt;2.3&lt;/a&gt; you’ve been asked to run a container named
&lt;em&gt;dl-1&lt;/em&gt; to download some figures from a Web page.
The figures were downloaded into the
directory &lt;em&gt;/my-figures&lt;/em&gt; of the container.
But we left a question unanswered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we transfer those figures from the container to the host computer?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The solution is to use &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/p&gt;
&lt;!--One way to go about that is to run the following command in the host terminal:

``
docker cp dl-1:/my-figures .
``

This will copy the directory */my-figures* from the container *dl-1* to
the host computer working directory.
You can verify it by yourself.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-25&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-25) &lt;/strong&gt;&lt;/span&gt;Can you tell why this solution is less than ideal?&lt;/div&gt;\EndKnitrBlock{exercise}


:::


## Using a host volume

A better solution is to **mount** (i.e., attach) 
a directory of the host computer at the container&#39;s directory 
*/my-figures* when we run it.
Let&#39;s see how it works.

**Step 1.** Create a directory named *figs-volume* in your working directory.

**Step 2.** Type and execute the following command:

```shell
docker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader
```

This command runs a container from the image *fig-downloader*. 

* With the option ``-v`` we specify  that we want to mount the directory
*\$(pwd)/figs-volume* (*\$(pwd)* indicates the host working directory)
at the directory *figs-volume* in the container;

* The option ``--rm`` indicates that we want the container to be 
removed when its execution is over.

**Step 3.** Verify that the pictures are in the folder *figs-volume*.

In this example, we&#39;ve used the directory *figs-volume* as a 
**volume** (essentially, an external storage area) of the container;
when the container is destroyed, the volume remains with all its data.--&gt;
&lt;div id=&#34;docker-volumes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Docker volumes&lt;/h2&gt;
&lt;p&gt;A volume can be seen as a virtual storage device attached to a container.
All files there are written to a volume survive the containerized application that
created them. In other words, when a container is destroyed,
all the files created by the application in the container remain in the volume.&lt;/p&gt;
&lt;p&gt;Let’s create a new Docker volume called &lt;em&gt;data-volume&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume create data-volume&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know (advanced notion)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where the data will be actually stored?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can inspect the new volume by typing the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume inspect data-volume&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;mount point&lt;/em&gt; is indicated; that’s the folder where the data
will be actually stored.
If your computer runs Linux, that folder will be available
on the host; if your computer runs Windows or MacOS,
you’ll not find that folder on your computer.
Instead, it will be available in the virtual machine
that Docker use on MacOS and Windows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do you want to see the directory? (Instructions for Windows and MacOS)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One way to look into the hidden VM is to run
the following containerized application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This application will open a guest terminal into the VM.
You can then use the commands &lt;code&gt;cd&lt;/code&gt; and &lt;code&gt;ls&lt;/code&gt;
to browse to the directory indicated as the mount path
of the new volume.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sharing-data&#34; class=&#34;section level3&#34; number=&#34;3.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1.1&lt;/span&gt; Sharing data&lt;/h3&gt;
&lt;p&gt;A Docker volume can be used to share data between containers.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-26&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;code&gt;ubuntu&lt;/code&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; at the container’s directory &lt;em&gt;/data&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type the following command in the guest Linux terminal to create a file
&lt;em&gt;test-file.txt&lt;/em&gt; in the directory &lt;em&gt;/data&lt;/em&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;echo &#34;This is a new file&#34; &amp;gt; /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Print to the console the content of the file with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;cat /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type &lt;code&gt;exit&lt;/code&gt; to leave the guest terminal. Since we’ve specified the option &lt;code&gt;--rm&lt;/code&gt;, the container
is &lt;strong&gt;destroyed&lt;/strong&gt;. Now we’re going to verify that &lt;code&gt;test-file.txt&lt;/code&gt; is still accessible.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-27&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;em&gt;alpine:latest&lt;/em&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; to the directory &lt;em&gt;/my-data&lt;/em&gt;
of the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-28&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;Verify that you can read the file &lt;em&gt;test-file.txt&lt;/em&gt;.
Which folder would you look in?
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Type &lt;code&gt;exit&lt;/code&gt; to exit the guest terminal and terminate the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;single-host-networking&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Single-Host Networking&lt;/h1&gt;
&lt;p&gt;In order to let containers communicate and, therefore, co-operate,
Docker defines a simple networking model known as
the &lt;strong&gt;container network model&lt;/strong&gt; (figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In this section, you’ll need to open several terminals in the virtual machine.
You can do it easily by using &lt;code&gt;byobu&lt;/code&gt;, an advanced window manager already available in your virtual machine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just type &lt;code&gt;byobu&lt;/code&gt; to launch the window manager.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to open a new terminal, just press F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4
(to move to next terminal).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to close a terminal, just type &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you close all terminals, &lt;code&gt;byobu&lt;/code&gt; will stop executing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm-docker&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-docker/cnm.png&#34; alt=&#34;The Docker container network model&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: The Docker container network model
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-29&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;Describe the output of the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-30&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;The following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect bridge&lt;/code&gt;&lt;/p&gt;
outputs the configuration of the network &lt;strong&gt;bridge&lt;/strong&gt;.
By looking at this configuration, can you tell
what IP addresses will be given to the containers attached to this
network? What’s the IP address of the router of this network?
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-networks&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Creating networks&lt;/h2&gt;
&lt;p&gt;By default, any new container is attached to the network named &lt;em&gt;bridge&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-31&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Explain why it is not a good practice to
attach all our containers to the same network.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In order to create a new network, you can use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create network_name&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-32&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;Create two networks named &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; that
use the driver &lt;em&gt;bridge&lt;/em&gt; (see figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;).
By using the &lt;code&gt;docker network inspect&lt;/code&gt; command,
look at the IP addresses of the new networks and write them down.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-33&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;Create three containers &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;porthos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt; and attach them
to the two networks &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; as displayed
in figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;.
&lt;strong&gt;The three containers will open a Linux Alpine shell&lt;/strong&gt;.
You’ll need to launch the commands in three separate tabs of your terminal window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What will the IP addresses of the three containers be in the two networks?
Remember that &lt;em&gt;porthos&lt;/em&gt; is attached to two networks, therefore it’ll have two
network interfaces (endpoints) and, as a result, two IP addresses.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Verify your answers by inspecting the two networks (use the
command &lt;code&gt;docker network inspect&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;communication-between-containers&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Communication between containers&lt;/h2&gt;
&lt;p&gt;Let’s see if and when the three containers can communicate.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-34&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;Which containers are able to communicate?
Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-35&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its IP address.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Which IP address of &lt;em&gt;porthos&lt;/em&gt; would you use?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-36&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.8  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its name.
Do you succeed? Are you surprised?
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You can now exit the three containers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Docker</title>
      <link>/courses/cloud-en/docker-en/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-en/docker-en/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to run &lt;strong&gt;containers&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and build &lt;strong&gt;images&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to create and use &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and use &lt;strong&gt;networks&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having installed Docker on your computer or having imported the Linux virtual machine either with VirtualBox or with Multipass.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Using a virtual machine with Multipass? Click here for more info
&lt;/summary&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Multipass commands&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To start the virtual machine, type &lt;code&gt;multipass start cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The folder &lt;code&gt;/home/ubuntu/labs&lt;/code&gt; in the virtual machine is mounted (i.e., linked) to the folder
on YOUR computer (let’s call it &lt;code&gt;LAB&lt;/code&gt;) where you’ll store all your lab material.
You specified this folder when you installed the virtual machine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you don’t remember the path to the folder linked to &lt;code&gt;/home/ubuntu/labs&lt;/code&gt;, type the
command &lt;code&gt;multipass info cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To open a terminal into the virtual machine, type &lt;code&gt;multipass shell cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the terminal opens, the current working directory is &lt;code&gt;/home/ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;cd labs&lt;/code&gt; to move to the folder where you’ll find your lab material.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll use the virtual machine terminal only to type the Docker commands.
You can use all the tools installed on your machine to create and manage the files
in the directory &lt;code&gt;LAB&lt;/code&gt; directly from your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once the lab is over, type &lt;code&gt;exit&lt;/code&gt; to leave the virtual machine terminal. This will lead you back
to the terminal of your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal of your computer, type &lt;code&gt;multipass stop cloudvm&lt;/code&gt; to stop the virtual machine.
This will NOT destroy your files! It just stops the virtual machine from needlessly using the resources of your computer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Terminology&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll use the &lt;strong&gt;terminal&lt;/strong&gt; to run Docker commands.
The terminal is the &lt;strong&gt;client&lt;/strong&gt; that communicates with
the Docker daemon.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker runs &lt;strong&gt;containers&lt;/strong&gt; on your computer.
We’ll refer to your computer as the &lt;strong&gt;host&lt;/strong&gt;,
the containers being the &lt;strong&gt;guests&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;containerized application&lt;/strong&gt;
is an application running in a container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Running containers&lt;/h1&gt;
&lt;!--The command used to run a container 
is ``docker run`` followed by four parameters:

```shell
docker run [options] image-name [command] [arg]
```

The four parameters are:

* *options*. List of options. 
* *image-name*. The fully qualified name of the image used to run the container. 
* *command*. The command to be executed in the container.
* *arg*. The arguments taken by the command executed in the container.

Only the parameter *image-name* is mandatory. 
The fully qualified name of an image is specified as a sequence of four fields, 
formatted as follows:

```shell
registry_url/user/name:tag
```

where:

* *registry_url* (optional). The URL of the registry that provides the image. 
If its value is not specified, the image 
will be looked up for in the 
[DockerHub registry](https://hub.docker.com){target=&#34;_blank&#34;}.
* *user* (optional).  The identifier of the user or organization that created the image. 
The default value is *library*.
* *name* (mandatory). The name of the image. 
* *tag* (optional). It specifies the image version.
If its value is not specified, 
the tag *latest* is used, pointing to the latest image version. --&gt;
&lt;!--
::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
For each of the following images, 
specify the registry name, the user, the name and the tag.

1. registry.redhat.io/rhel8/mysql-80

2. alpine:3.11

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}
:::



&lt;details&gt;

&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

1. Registry: *registry.redhat.io*, user: *rhel8*, name: *mysql-80*,
tag: *latest*

2. Registry: *DockerHub*, user: *library*, name: *alpine*, tag: *3.11*

3. Registry: *DockerHub*, user: *library*, name: *alpine*, tag: *latest*
:::

&lt;/details&gt;



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;What&#39;s the difference between the following image names?

1. alpine:latest

2. registry.hub.docker.com/library/alpine

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

There&#39;s no difference. They all point to the same image, that is
the latest version of *alpine* in the DockerHub registry.

:::

&lt;/details&gt;
--&gt;
&lt;p&gt;We now learn how to use the command &lt;code&gt;docker run&lt;/code&gt; and some of
its options.
In the following exercises, we’ll run containers from the image named
&lt;em&gt;alpine&lt;/em&gt; that is
&lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;available on the DockerHub registry&lt;/a&gt;.
This image provides a lightweight distribution
(i.e., it doesn’t contain many features) of Linux.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;You want to run the container from the latest version of
the image &lt;em&gt;alpine&lt;/em&gt;.
Which command would you write in the terminal?
Execute it.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The goal of this exercise is to start playing with the
&lt;code&gt;docker run&lt;/code&gt; command.
Since the question doesn’t say anything about the
options, nor does it mention the command to run inside the container,
we’d type:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;Execute the command that you proposed in the previous exercise, 
observe the output in the terminal and explain the actions 
taken by Docker to run the container.&lt;/div&gt;\EndKnitrBlock{exercise}


:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The output obtained from executing the command should 
look like as follows:

```bash
Unable to find image &#39;alpine:latest&#39; locally
latest: Pulling from library/alpine
aad63a933944: Pull complete 
Digest: sha256:b276d875eeed9c7d3f1cfa7edb06b22ed22b14219a7d67c52c56612330348239
Status: Downloaded newer image for alpine:latest
```

Here&#39;s what happens under the hood:

1. Docker looks for an image named *alpine:latest* in the host 
computer and cannot find it.

2. Docker *pulls* (i.e., downloads) the image from the DockerHub registry.

:::

&lt;/details&gt;--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;Is the container still running?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;In order to list all containers still running on the host, type the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your container shouldn’t appear in the output,
because it’s not running.
In order to see all containers, including those that are not
running, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;What information is displayed for each container?&lt;/div&gt;\EndKnitrBlock{exercise}

:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

* The identifier of the container.

* The name of the image used to run the container (it should be *alpine* for your 
container).

* The command executed within the container (it should be ``/bin/sh`` for your container).

* When the container has been created.

* The container current status (it should be *exited (0) x seconds ago* for your container).

* The network ports used by the container (we&#39;ll study them later).

* The name of the container. If you don&#39;t specify any when you 
run the container (as is our case),
Docker generates a random name by concatenating an adjective and 
a famous scientist&#39;s name (e.g., *agitated_newton*).

:::

&lt;/details&gt;--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;By looking at the command executed within the container (&lt;code&gt;/bin/sh&lt;/code&gt;),
can you tell why the container stopped without giving any output?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The command is &lt;code&gt;/bin/sh&lt;/code&gt;;
the container runs a Linux terminal.
But since we didn’t specify what to do with that terminal
(we didn’t run any Linux command, nor we tried to access the terminal),
the container stopped.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We’re now going to do something useful with the image &lt;em&gt;alpine&lt;/em&gt;.
Make sure you read the &lt;strong&gt;good practices&lt;/strong&gt; that you should
adopt while playing with images and containers.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good practices&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Name your containers.&lt;/strong&gt; Although Docker assigns a default name to a new container,
it’s usually a good practice to give a container a name of your
choice to make it easily distinguishable. You can do it by using the option
&lt;code&gt;--name&lt;/code&gt;. Try the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker run --name my-alpine alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As before, the container stops immediately.
If you list all your containers by typing again:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see a container named &lt;em&gt;my-alpine&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove automatically a container if you use it once.&lt;/strong&gt;
Unless you want to reuse your container later, you can ask Docker to automatically remove it
when it stops by using the option &lt;code&gt;--rm&lt;/code&gt;.
This will prevent unused containers from taking up too much disk space.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Try the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name container-to-remove alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you list all the containers you should see that there is no container
named &lt;em&gt;container-to-remove&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused containers.&lt;/strong&gt; Stopped containers that have been run without
using the option &lt;code&gt;--rm&lt;/code&gt; are still stored in the host.
If you want to remove a specific
container (e.g., &lt;em&gt;my-alpine&lt;/em&gt;), use the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker container rm my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you want to remove all stopped containers, use the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container prune&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused images.&lt;/strong&gt; Images can take up a lot of disk space.
As a result, you should remember to remove those that you don’t intend to use
any longer.
The commands to remove a specific image
and prune unused ones are &lt;code&gt;docker image rm&lt;/code&gt;
and &lt;code&gt;docker image prune -a&lt;/code&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;pass-a-command-to-the-containerized-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Pass a command to the containerized application&lt;/h2&gt;
&lt;p&gt;Remember that the template of &lt;code&gt;docker run&lt;/code&gt; is the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run [options] image-name [command] [arg]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The optional parameter &lt;em&gt;command&lt;/em&gt; refers to a command
that you can pass the containerized application, possibly with some arguments
(parameter &lt;em&gt;arg&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Let’s see an example.
As we saw before, when we run a container from the image &lt;em&gt;alpine&lt;/em&gt;,
a Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is launched.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is run within the
container.
Henceforth, we’ll use the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Host terminal.&lt;/strong&gt; The terminal that you use to
interact with the operating system of your computer.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guest terminal.&lt;/strong&gt; The terminal that is run within
the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;By using the optional parameter &lt;em&gt;command&lt;/em&gt;, we can run
a command in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Run a container from the image &lt;em&gt;alpine&lt;/em&gt; and execute the
Linux command &lt;code&gt;ls&lt;/code&gt; that lists the content of the current directory.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where are the listed files stored?
In the host or in the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ls-test alpine ls&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The command &lt;code&gt;ls&lt;/code&gt; is run in the guest terminal, therefore
what we see in the output is a list of files stored in the
container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:command-in-guest&#34;&gt;1.4&lt;/a&gt; the command
&lt;code&gt;ls&lt;/code&gt; is executed in the guest terminal, but its
output is redirected to the host terminal.&lt;/p&gt;
&lt;p&gt;In other words, when we run the container, we
don’t interact directly with the guest terminal;
we just send a command and the output is redirected
to the host terminal.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s see how to execute a command in the guest terminal
that also requires an argument.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;By using the Linux utility &lt;code&gt;ping&lt;/code&gt;, check
whether the Web site &lt;code&gt;www.centralesupelec.fr&lt;/code&gt; is reachable.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ping-test alpine ping www.centralesupelec.fr&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to interrupt &lt;code&gt;ping&lt;/code&gt;
just type the key combination that you’s use to
interrupt any other command in your terminal
(typically Ctrl-C on Windows and Cmd-C in MacOs).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--An application running in a container might need to interact 
with the user. 
For instance, the Linux 
command ``rev`` reverses whatever 
the user types on the keyboard.
In order to interact with a container, you should use the option
``-it`` of ``docker run``.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-9&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-9) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *alpine* to execute the 
Linux command `rev` and interact with it.
You can stop interacting with ``rev`` by typing Ctrl+C at any time.&lt;/div&gt;\EndKnitrBlock{exercise}


:::
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

``
docker run --rm --name rev -it alpine rev
``

After typing the command, type a word on your keyboard
(e.g., *deeps*), press *Return* and 
you should see the same word reversed (e.g., *speed*). 

The option ``-t`` opens a guest terminal (so we can see its
output); the option ``-i`` allows you to write directly 
into the guest terminal.

In order to stop using the guest terminal, 
you&#39;ll need to press Ctrl+D (both in Windows and MacOs).

:::

&lt;/details&gt;

--&gt;
&lt;p&gt;Now run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run  --name my-alpine -it alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; we didn’t use the option &lt;code&gt;--rm&lt;/code&gt; (the container will not be removed
when we stop it, we’re going to use it again).
Moreover, we didn’t specify any command to run in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;What do you obtain?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;When we run a container from the image &lt;em&gt;alpine&lt;/em&gt;, the command
&lt;code&gt;/bin/sh&lt;/code&gt; is executed within the container.
Since we specified the option &lt;code&gt;-it&lt;/code&gt;, what we obtain is an access to the
Linux terminal running in the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-and-stopping-containers.&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Starting and stopping containers.&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run&lt;/code&gt; is a shorthand for two Docker commands, namely
&lt;code&gt;docker create&lt;/code&gt;, that creates a container from an image,
and &lt;code&gt;docker start&lt;/code&gt;, that starts the container after its creation.&lt;/p&gt;
&lt;p&gt;Suppose now that you want to download a Web page
by using Linux Alpine.
You can use the Linux command &lt;code&gt;wget&lt;/code&gt; followed by the URL of the page
that you want to download.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:wget-exo&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;By using the guest terminal
in the container &lt;em&gt;my-alpine&lt;/em&gt;,
download
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;this Web page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where will the Web page be saved? The host computer or the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just type in &lt;em&gt;my-alpine&lt;/em&gt; guest terminal the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wget https://www.centralesupelec.fr/fr/presentation&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Web page will be saved in the current directory
of the container. You can verify that the file is there
by typing &lt;code&gt;ls&lt;/code&gt; in the guest terminal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Want to copy the donwloaded file from the container to your computer?
&lt;/summary&gt;
&lt;p&gt;To copy the file &lt;code&gt;presentation&lt;/code&gt; to your working directory, type the following command in
the host terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  docker cp &amp;lt;containerId&amp;gt;:/presentation .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the previous command, replace &lt;code&gt;&amp;lt;containerId&amp;gt;&lt;/code&gt; with the identifier of your container. You can obtain
the identifier of the container from the output of the command &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Where are the containers files stored?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use MacOS or Windows
&lt;/summary&gt;
&lt;p&gt;The files managed by Docker are not stored directly in your computer, but
in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).&lt;/p&gt;
&lt;p&gt;Therefore, you need to open a terminal inside that Linux virtual machine by typing the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the terminal is opened, you can follow the instructions given below for Linux users.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you use Docker on Linux
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All files managed by Docker are stored under folder &lt;code&gt;/var/lib/docker&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To access that folder, you need to be root (i.e., administrator). Type the command &lt;code&gt;sudo su&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;ls /var/lib/docker&lt;/code&gt; you can look at the folders stored under this directory. You’ll see that there are
folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To locate the files of a specific container, you first need to get the &lt;strong&gt;container identifier&lt;/strong&gt; by typing &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the command &lt;code&gt;docker inspect &amp;lt;container-id&amp;gt;&lt;/code&gt; (replace &lt;code&gt;&amp;lt;container-id&amp;gt;&lt;/code&gt; with the identifier of the container that you intend to inspect).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the field &lt;code&gt;UpperDir&lt;/code&gt;. The value of this field is the path to the directory (let’s call it &lt;code&gt;CONTAINER_DIR&lt;/code&gt;) that contains the upper layer of the container (the writable layer).
It should be a path that ends by &lt;code&gt;/diff&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;cd CONTAINER_DIR&lt;/code&gt; (replace &lt;code&gt;CONTAINER_DIR&lt;/code&gt; with the value of the field &lt;code&gt;UpperDir&lt;/code&gt;) you can finally see the files stored in
your container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;em&gt;my-alpine&lt;/em&gt; guest terminal type &lt;code&gt;exit&lt;/code&gt;.
This closes the guest terminal and, as a result, stops the
container.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTICE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stopping the container will not erase any of the files
stored in the container. Removing the container will.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to start the container &lt;em&gt;my-alpine&lt;/em&gt; again, you can
use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container start -ai my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will open the guest terminal of the container again;
type &lt;code&gt;ls&lt;/code&gt; to verify that
the Web page that you downloaded before is still there.&lt;/p&gt;
&lt;!-- &lt;details&gt;
&lt;summary&gt;Homework (optional)&lt;/summary&gt;

Suppose that you need to download all the figures of 
[this Web page](https://www.centralesupelec.fr/fr/presentation){target=&#34;_blank&#34;}. 
The Linux utility ``wget`` comes in handy. 
However, you don&#39;t have Linux and you&#39;d like to 
avoid the hassle of installing it on your computer, or in a virtual machine,
just for this task.

A great alternative is to run Linux in a Docker container. 
Unfortunately, the Alpine distribution that we&#39;ve been playing 
with doesn&#39;t provide
an implementation of ``wget`` with all the options that we need.

We turn to another Linux distribution, **Ubuntu**,
for which DockerHub has 
[several images](https://hub.docker.com/_/ubuntu/){target=&#34;_blank&#34;}.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-11&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-11) &lt;/strong&gt;&lt;/span&gt;Run a container with Ubuntu (latest) and open a guest terminal. 
Call the container *dl-figures*, and avoid the option 
``--rm``, we&#39;ll use this container later.&lt;/div&gt;\EndKnitrBlock{exercise}

:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

``
docker run --name dl-figures -it ubuntu
``


:::

&lt;/details&gt;



From now on, we&#39;ll be interacting with the guest Ubuntu terminal.
If you type the command ``wget``, 
you&#39;ll get an error (``bash: wget: command not found``).

::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Notice**

The image *Ubuntu* doesn&#39;t include all the commands that you&#39;d find 
in a full-blown Ubuntu distribution;
the reason is to keep the size of the image small, 
a necessary constraint given that 
images are transferred over the Internet.  


:::

Luckily, there&#39;s a way to install `wget` in our Ubuntu distribution.
Ubuntu provides a powerful command-line package manager called 
**Advanced Package Tool** (APT).
First, you need to run the following command:

``
apt update
``

which fetches the available packages from a list of sources 
available in file `/etc/apt/sources.list`.

Then, you can install `wget` by running the following command:

``
apt install -y wget
``

In order to obtain all the figures from a 
Web page, type the following command:

```shell
wget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation
```

You should see in the current directory a new folder 
named *my-figures* containing the downloaded figures;
verify it by typing ``ls my-figures``.

Before terminating, don&#39;t forget to read your fortune cookie.
In the shell, run the following command:

``
apt-get install -y fortune
``

and then:

``
/usr/games/fortune -s
``

When you&#39;re done, you can simply type the command `exit` to quit 
the guest terminal and stop the container.

&lt;/details&gt; --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-images&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Creating Images&lt;/h1&gt;
&lt;p&gt;A Docker image can be thought of as a template to create and run a container.
An image is a file that contains a &lt;strong&gt;layered filesystem&lt;/strong&gt; with each layer being &lt;strong&gt;immutable&lt;/strong&gt;;
this means that the files that belong to a layer cannot be
modified or deleted, nor can files be added to a layer.&lt;/p&gt;
&lt;p&gt;When a container is created from an image, it
will be composed of all the image read-only layers and, on top of
them, a writable layer (termed the &lt;strong&gt;container layer&lt;/strong&gt;),
where all the new files created in the container will be written.
For example, the Web page that you downloaded in Exercise &lt;a href=&#34;#exr:wget-exo&#34;&gt;1.7&lt;/a&gt;
were stored in the writable layer of that container.&lt;/p&gt;
&lt;!--## Interactive image creation

When we run the container *dl-figures*  in Section \@ref(simple-use-case), 
we modified the container to
install the command ``wget``. 
You can see the modifications by typing the 
following command:

``
docker diff dl-figures
``

The output consists of a list of files tagged with the letter A, C or D, indicating 
respectively that the file has been added (A), changed (C) or deleted (D). 
In this list you&#39;ll find the downloaded figures, as well as 
other files that have been added or modified or deleted 
when we installed ``wget``.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-12&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-12) &lt;/strong&gt;&lt;/span&gt;If layers, except the top one, are immutable, 
how can files that belong to the lower layers be modified or deleted?&lt;/div&gt;\EndKnitrBlock{exercise}
:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

All files marked with A are new and therefore are 
added to the writable layer of the container.

As for the existing files, they live in the immutable layers 
of the image, and therefore cannot be touched directly. 
Instead, they are copied from the bottom layers to the writable layer where 
they are modified.
This strategy is called **copy-on-write**.

The structure of layers generates a **layered filesystem** in the image; 
if different copies of the same file exist in different layers, 
the copy in the uppermost layer 
overwrites the others.

:::

&lt;/details&gt;



We can create a new image from the container *dl-figures*, one that provides
a Ubuntu distribution with the command ``wget`` already installed, 
with the following command:


```shell
docker commit dl-figures ubuntu-with-wget
```

The command creates a new image called *ubuntu-with-wget*.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-14&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-14) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *ubuntu-with-wget* and verify that the command
*wget* is actually installed. &lt;/div&gt;\EndKnitrBlock{exercise}

:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

Just type the following command:

``
docker run --rm -it ubuntu-with-wget
``
In the guest terminal type ``wget``: you should see the 
following output:

``
wget: missing URL
Usage: wget [OPTION]... [URL]...

Try `wget --help` for more options.
``

:::

&lt;/details&gt;--&gt;
&lt;div id=&#34;dockerfiles&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Dockerfiles&lt;/h2&gt;
&lt;p&gt;The most common way to create an image is to use a &lt;strong&gt;Dockerfile&lt;/strong&gt;, a
text file that contains all the instructions necessary to
build the image.
The advantage of the Dockerfile is that it can be interpreted
by the Docker engine, which makes the creation of images an automated
and repeatable task.&lt;/p&gt;
&lt;p&gt;Suppose that we want to create a containerized
application to download figures from a Web page.
As a template for this application, we need to build a new
image, that we’ll call &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The Dockerfile containing the instructions to build the image
&lt;em&gt;fig-downloader&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update
RUN apt-get install -y wget
RUN mkdir -p /my-figures
WORKDIR /my-figures
ENTRYPOINT [&amp;quot;wget&amp;quot;, &amp;quot;-nd&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-A&amp;quot;, &amp;quot;jpg,jpeg,bmp,png,gif&amp;quot;]
CMD [&amp;quot;https://www.centralesupelec.fr/fr/presentation&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the explanation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We use the image &lt;em&gt;ubuntu&lt;/em&gt; as the &lt;strong&gt;base image&lt;/strong&gt;.
This corresponds to the instruction &lt;code&gt;FROM ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We install the utility &lt;code&gt;wget&lt;/code&gt; in the base image.
This corresponds to the
instructions &lt;code&gt;RUN apt-get update&lt;/code&gt; and &lt;code&gt;RUN apt-get install -y wget&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create a directory &lt;code&gt;my-figures&lt;/code&gt; under the root directory of the image.
This corresponds to the instruction &lt;code&gt;RUN mkdir -p /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We set the newly created directory &lt;em&gt;/my-figures&lt;/em&gt; as the
&lt;strong&gt;working directory&lt;/strong&gt; of the image. This corresponds to the
instruction &lt;code&gt;WORKDIR /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We specify the command to be executed when a container is run from this image.
This corresponds to the instruction&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;ENTRYPOINT [&#34;wget&#34;, &#34;-nd&#34;, &#34;-r&#34;, &#34;-A&#34;, &#34;jpg,jpeg,bmp,png,gif&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This instruction means: execute &lt;code&gt;wget&lt;/code&gt; with the options
&lt;code&gt;-nd&lt;/code&gt;, &lt;code&gt;-r&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt;;
the last option takes a list of file
extensions (&lt;code&gt;jpg,jpeg,bmp,png,gif&lt;/code&gt;) as its argument.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remember that the utility &lt;code&gt;wget&lt;/code&gt; takes the URL of the Web page as
an argument. The URL will be specified when we run the container from
the image &lt;em&gt;fig-downloader&lt;/em&gt;.
Optionally, we can specify a default argument by using the
keyword CMD. The meaning of the instruction:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;CMD [&#34;https://www.centralesupelec.fr/fr/presentation&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;is: if we don’t give any URL when we run the container, the figures will
be downloaded from
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;https://www.centralesupelec.fr/fr/presentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;What’s the relation between the Dockerfile lines and the image layers?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each line corresponds to a new layer.
The first line corresponds to the bottom layer;
the last line to the top layer.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:dockerfile-creation&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Could you identify a problem in this Dockerfile?
Modify the Dockerfile accordingly.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;When creating an image, we should keep the number of layers relatively
small; in fact, the more the layers, the bigger the image will be.
Here we create three separate layers with three RUN commands; we can
simply merge the three layers.
The resulting Dockerfile will be:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y wget &amp;amp;&amp;amp; \
    mkdir -p /my-figures
WORKDIR /my-figures
ENTRYPOINT [&amp;quot;wget&amp;quot;, &amp;quot;-nd&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-A&amp;quot;, &amp;quot;jpg,jpeg,bmp,png,gif&amp;quot;]
CMD [&amp;quot;https://www.centralesupelec.fr/fr/presentation&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;building-an-image&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Building an image&lt;/h2&gt;
&lt;p&gt;We’re now going to build an image from a Dockerfile.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a directory named &lt;em&gt;fig-downloader&lt;/em&gt; in your computer with
a file named &lt;em&gt;Dockerfile&lt;/em&gt; inside.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Dockerfile&lt;/em&gt; write the set of instructions that
you proposed in Exercise &lt;a href=&#34;#exr:dockerfile-creation&#34;&gt;2.2&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal, set the working directory to &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build an image called &lt;em&gt;fig-downloader&lt;/em&gt; by executing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.&lt;/code&gt; at the end of the command means that the Docker engine will
look for a file named &lt;em&gt;Dockerfile&lt;/em&gt; in the working directory.&lt;/p&gt;
&lt;!--

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-16&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-16) &lt;/strong&gt;&lt;/span&gt;Once the image is built, type the command ``docker image ls -a``. 
What are the images with repository and tag ``&lt;none&gt;``?
Why are there three of such images?&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;/div&gt;

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

These are the **intermediate images**. 
Once a layer is compiled, an intermediate image is created that 
contains that layer and all the layers underneath.
In other words, the intermediate image 
corresponding to the layer $i$ contains all files 
up to the layer $i$, including layers 1 through $i-1$.

The intermediate layers are used by the **build cache**, 
of which we&#39;ll see an example later.

Although there are five layers in the new image, there are only 
three intermediate images because:

* the base image is *ubuntu:eoan*.
* the image corresponding to the top layer is the final image *fig-downloader*.

:::

&lt;/details&gt;
--&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you give the Dockerfile a different name (say, &lt;em&gt;Dockerfile-fig-downloader&lt;/em&gt;),
the command to build the image will be:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader -f Dockerfile-fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The option &lt;code&gt;-f&lt;/code&gt; is used to specify the name of the Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In order to verify that the new image has been created, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;
&lt;!-- Let&#39;s dive deeper into the anatomy of an image.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-17&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-17) &lt;/strong&gt;&lt;/span&gt;Run the following command:

``
docker history fig-downloader
`` 

and analyze the layers of the new image. 

* Why do some layers have an ID, while others 
are marked as &lt;i&gt;missing&lt;/i&gt;?

::::: {.last-child}
* Can you find the identifiers of the intermediate images?
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The layers with an ID correspond to the layers of 
the new image, including the top layer and the base image.
The layers marked as *missing* are those that compose the 
base image. Those layers are not stored in  
your computer, 
simply because they belong to an image 
that hasn&#39;t been built on your computer 
and you downloaded from the DockerHub registry.

By looking at the output of ``docker image ls -a`` and the output of this command,
we see that the layers between the base image and the top layer have the 
same identifiers as the intermediate images.

:::

&lt;/details&gt;

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:dl-1-container&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --name dl-1 fig-downloader&lt;/code&gt;&lt;/p&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We downloaded the figures
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;of this page&lt;/a&gt;.
The downloaded pictures are in the folder &lt;em&gt;/my-figures&lt;/em&gt;
of the container &lt;em&gt;dl-1&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --name dl-2 fig-downloader https://www.centralesupelec.fr/&lt;/code&gt;&lt;/pre&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We downloaded the figures
&lt;a href=&#34;https://www.centralesupelec.fr&#34; target=&#34;_blank&#34;&gt;of this page&lt;/a&gt;.
We basically overwrote the URL specified by the CMD keyword with a new
one.
The downloaded pictures are in the folder &lt;em&gt;/my-figures&lt;/em&gt;
of the container &lt;em&gt;dl-2&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;containerized-python-application&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Containerized Python application&lt;/h2&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-docker/word-frequency.zip&#34;&gt;this archive file&lt;/a&gt;
and unzip it into your working directory.
In this archive you’ll find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Dockerfile.&lt;/li&gt;
&lt;li&gt;A Python script &lt;em&gt;main.py&lt;/em&gt; that asks the user to enter the URL and the language of a Web page,
and prints the 10 most frequent words occurring in that page.&lt;/li&gt;
&lt;li&gt;A file &lt;em&gt;requirements.txt&lt;/em&gt; with the list of the Python packages
needed to run the given script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The content of the Dockerfile is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./main.py ./requirements.txt /app/
RUN pip install -r requirements.txt
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;Describe what this Dockerfile does.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Takes &lt;em&gt;python:3.7-slim&lt;/em&gt; as the base image.&lt;/li&gt;
&lt;li&gt;Creates a new folder &lt;em&gt;app&lt;/em&gt; in the image under the root directory.&lt;/li&gt;
&lt;li&gt;Changes the working directory to &lt;em&gt;/app&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Copies the files &lt;em&gt;main.py&lt;/em&gt; and &lt;em&gt;requirements.txt&lt;/em&gt; from the local
computer to the directory &lt;em&gt;/app&lt;/em&gt; in the image.&lt;/li&gt;
&lt;li&gt;Runs the command &lt;code&gt;pip install&lt;/code&gt; to install the Python libraries
specified in the file &lt;em&gt;requirements.txt&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Executes the command &lt;code&gt;python main.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34; word-latex=&#34;{exercisebox}&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;Build an image called &lt;em&gt;wordfreq&lt;/em&gt; from this Dockerfile.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker build -t wordfreq .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;Without changing the Dockerfile, rebuild the same image.
What do you notice?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The build is very fast.
Since we didn’t change the Dockerfile, the image
is rebuilt by using the image layers created
previously.
This is clearly indicated by the word &lt;strong&gt;CACHED&lt;/strong&gt; written
at each layer.
Using the already stored layers is called &lt;strong&gt;build cache&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;What happens if you modify a line in the Python script and
you rebuild the image?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Add any instruction at the end of &lt;em&gt;main.py&lt;/em&gt;, such as:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Finish!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then rebuild the image.
The three bottom layers are not affected by the modification, therefore
they benefit from the build cache.
Layer 4 is the first affected by the modification.
This layer, and those above, need therefore to be
rebuilt.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.9  &lt;/strong&gt;&lt;/span&gt;Based on the previous considerations,
can you tell what’s wrong with this Dockerfile?
Modify the Dockerfile accordingly and rebuild the image.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each time we modify &lt;em&gt;main.py&lt;/em&gt; and we rebuild the image,
the layer 4 and 5 are recreated, meaning that all the Python packages
are downloaded and installed.
Depending on the size and number of the packages, this might
take some while.
A better way to structure the Dockerfile is to install the
packages before copying the Python script to the image.
Here is how we should modify the Dockerfile:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./requirements.txt /app/
RUN pip install -r requirements.txt
COPY ./main.py /app/
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.10  &lt;/strong&gt;&lt;/span&gt;Modify &lt;em&gt;main.py&lt;/em&gt; by adding a new line of code and rebuild the image.
What changed?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The Python packages are not reinstalled, as a result rebuilding the image&lt;br /&gt;
takes much less time than before.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Play with the application by running the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it  wordfreq&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The containerized application will prompt you to insert the URL of a webpage and
the language of the page (in English).
The output will be the 20 most used words in the webpage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-volumes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Volumes&lt;/h1&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:dl-1-container&#34;&gt;2.3&lt;/a&gt; you’ve been asked to run a container named
&lt;em&gt;dl-1&lt;/em&gt; to download some figures from a Web page.
The figures were downloaded into the
directory &lt;em&gt;/my-figures&lt;/em&gt; of the container.
But we left a question unanswered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we transfer those figures from the container to the host computer?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The solution is to use &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/p&gt;
&lt;!--One way to go about that is to run the following command in the host terminal:

``
docker cp dl-1:/my-figures .
``

This will copy the directory */my-figures* from the container *dl-1* to
the host computer working directory.
You can verify it by yourself.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-25&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-25) &lt;/strong&gt;&lt;/span&gt;Can you tell why this solution is less than ideal?&lt;/div&gt;\EndKnitrBlock{exercise}


:::
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

1. After running the container we need to do an additional action to copy
the figures from the container to the host computer.

2. The container is created and run only to download some figures.
We&#39;d like to remove it automatically (with the option ``--rm``) when its 
execution is over. However, if we do so, the pictures will be lost before 
we can copy them to the host computer.

:::

&lt;/details&gt;

## Using a host volume

A better solution is to **mount** (i.e., attach) 
a directory of the host computer at the container&#39;s directory 
*/my-figures* when we run it.
Let&#39;s see how it works.

**Step 1.** Create a directory named *figs-volume* in your working directory.

**Step 2.** Type and execute the following command:

```shell
docker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader
```

This command runs a container from the image *fig-downloader*. 

* With the option ``-v`` we specify  that we want to mount the directory
*\$(pwd)/figs-volume* (*\$(pwd)* indicates the host working directory)
at the directory *figs-volume* in the container;

* The option ``--rm`` indicates that we want the container to be 
removed when its execution is over.

**Step 3.** Verify that the pictures are in the folder *figs-volume*.

In this example, we&#39;ve used the directory *figs-volume* as a 
**volume** (essentially, an external storage area) of the container;
when the container is destroyed, the volume remains with all its data.--&gt;
&lt;div id=&#34;docker-volumes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Docker volumes&lt;/h2&gt;
&lt;p&gt;A volume can be seen as a virtual storage device attached to a container.
All files there are written to a volume survive the containerized application that
created them. In other words, when a container is destroyed,
all the files created by the application in the container remain in the volume.&lt;/p&gt;
&lt;p&gt;Let’s create a new Docker volume called &lt;em&gt;data-volume&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume create data-volume&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know (advanced notion)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where the data will be actually stored?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can inspect the new volume by typing the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume inspect data-volume&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;mount point&lt;/em&gt; is indicated; that’s the folder where the data
will be actually stored.
If your computer runs Linux, that folder will be available
on the host; if your computer runs Windows or MacOS,
you’ll not find that folder on your computer.
Instead, it will be available in the virtual machine
that Docker use on MacOS and Windows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do you want to see the directory? (Instructions for Windows and MacOS)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One way to look into the hidden VM is to run
the following containerized application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This application will open a guest terminal into the VM.
You can then use the commands &lt;code&gt;cd&lt;/code&gt; and &lt;code&gt;ls&lt;/code&gt;
to browse to the directory indicated as the mount path
of the new volume.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sharing-data&#34; class=&#34;section level3&#34; number=&#34;3.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1.1&lt;/span&gt; Sharing data&lt;/h3&gt;
&lt;p&gt;A Docker volume can be used to share data between containers.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-26&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;code&gt;ubuntu&lt;/code&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; at the container’s directory &lt;em&gt;/data&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it -v data-volume:/data ubuntu&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type the following command in the guest Linux terminal to create a file
&lt;em&gt;test-file.txt&lt;/em&gt; in the directory &lt;em&gt;/data&lt;/em&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;echo &#34;This is a new file&#34; &amp;gt; /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Print to the console the content of the file with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;cat /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type &lt;code&gt;exit&lt;/code&gt; to leave the guest terminal. Since we’ve specified the option &lt;code&gt;--rm&lt;/code&gt;, the container
is &lt;strong&gt;destroyed&lt;/strong&gt;. Now we’re going to verify that &lt;code&gt;test-file.txt&lt;/code&gt; is still accessible.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-27&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;em&gt;alpine:latest&lt;/em&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; to the directory &lt;em&gt;/my-data&lt;/em&gt;
of the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker container run --rm -it -v data-volume:/my-data alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-28&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;Verify that you can read the file &lt;em&gt;test-file.txt&lt;/em&gt;.
Which folder would you look in?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We need to look in the folder &lt;em&gt;/my-data&lt;/em&gt; because this is where
we mounted &lt;em&gt;data-volume&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cat /my-data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Type &lt;code&gt;exit&lt;/code&gt; to exit the guest terminal and terminate the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;networks&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Networks&lt;/h1&gt;
&lt;p&gt;In order to let containers communicate and, therefore, co-operate,
Docker defines a simple networking model known as
the &lt;strong&gt;container network model&lt;/strong&gt; (figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In this section, you’ll need to open several terminals in the virtual machine.
You can do it easily by using &lt;code&gt;byobu&lt;/code&gt;, an advanced window manager already available in your virtual machine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just type &lt;code&gt;byobu&lt;/code&gt; to launch the window manager.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to open a new terminal, just press F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4
(to move to next terminal).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to close a terminal, just type &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you close all terminals, &lt;code&gt;byobu&lt;/code&gt; will stop executing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm-docker&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-docker/cnm.png&#34; alt=&#34;The Docker container network model&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: The Docker container network model
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-29&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;Describe the output of the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The command lists all the networks created by Docker on
your computer.
For each network, the values of four attributes are shown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The identifier.&lt;/li&gt;
&lt;li&gt;The name.&lt;/li&gt;
&lt;li&gt;The driver used by the network.&lt;/li&gt;
&lt;li&gt;The scope of the network (local or global).
A local scope means that the network connects containers
running on the same host, as opposed to a global scope that
means that containers on different hosts can communicate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on the containers that you used
in the past, you might see different networks.
However, three networks are worth noting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network named &lt;strong&gt;bridge&lt;/strong&gt;, that uses the driver &lt;strong&gt;bridge&lt;/strong&gt; and a local scope.
By default, any new container is attached to this network.&lt;/li&gt;
&lt;li&gt;The network named &lt;strong&gt;host&lt;/strong&gt;, that uses the driver &lt;strong&gt;host&lt;/strong&gt; and a local scope.
It’s used when we want a container to directly use the network interface of the host.
It’s important to remember that this network should only be used when analyzing the
host’s network traffic. In the other cases, using this network exposes
the container to all sorts of security risks.&lt;/li&gt;
&lt;li&gt;The network named &lt;strong&gt;none&lt;/strong&gt;, that uses the driver &lt;strong&gt;null&lt;/strong&gt; and a local scope.
Attaching a container to this network means that the container
isn’t connected to any network, and therefore it’s completely isolated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-30&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;The following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect bridge&lt;/code&gt;&lt;/p&gt;
outputs the configuration of the network &lt;strong&gt;bridge&lt;/strong&gt;.
By looking at this configuration, can you tell
what IP addresses will be given to the containers attached to this
network? What’s the IP address of the router of this network?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The information is specified in the field named &lt;strong&gt;IPAM&lt;/strong&gt;, more specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Subnet&lt;/strong&gt; indicates the range of IP addresses used by the network.
The value of this field should be 172.17.0.0/16;
the addresses range from 172.17.0.1 to 172.17.255.255.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gateway&lt;/strong&gt; indicates the IP address of the router of the network.
The value should be 172.17.0.1&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;creating-networks&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Creating networks&lt;/h2&gt;
&lt;p&gt;By default, any new container is attached to the network named &lt;em&gt;bridge&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-31&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Explain why it is not a good practice to
attach all our containers to the same network.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;All new containers will be able to communicate over this network.
This is not a good idea.
If a hacker can compromise any of these containers, s/he might
be able to attack the other containers as well.
As a rule of thumb, we should attach two containers to the same network &lt;strong&gt;only&lt;/strong&gt; on a
need-to-communicate basis.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;In order to create a new network, you can use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create network_name&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-32&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;Create two networks named &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; that
use the driver &lt;em&gt;bridge&lt;/em&gt; (see figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;).
By using the &lt;code&gt;docker network inspect&lt;/code&gt; command,
look at the IP addresses of the new networks and write them down.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just run the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The IP addresses for the network &lt;em&gt;buckingham&lt;/em&gt; are
172.18.0.0/16 (addresses from 172.18.0.1 to 172.18.255.255);
The IP addresses for the network &lt;em&gt;rochefort&lt;/em&gt; are:
172.19.0.0/16 (assuming that you create &lt;em&gt;buckingham&lt;/em&gt;
before &lt;em&gt;rochefort&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The IP addresses may be different on your machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-33&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;Create three containers &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;porthos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt; and attach them
to the two networks &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; as displayed
in figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;.
&lt;strong&gt;The three containers will open a Linux Alpine shell&lt;/strong&gt;.
You’ll need to launch the commands in three separate tabs of your terminal window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What will the IP addresses of the three containers be in the two networks?
Remember that &lt;em&gt;porthos&lt;/em&gt; is attached to two networks, therefore it’ll have two
network interfaces (endpoints) and, as a result, two IP addresses.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Verify your answers by inspecting the two networks (use the
command &lt;code&gt;docker network inspect&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Here are the commands to run &lt;em&gt;athos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt; while connecting
them to &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; respectively.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name athos --network buckingham  alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name aramis --network rochefort   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here’s the command to run &lt;em&gt;porthos&lt;/em&gt; and attach it to
&lt;em&gt;buckingham&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name porthos --network buckingham   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The following command attaches &lt;em&gt;porthos&lt;/em&gt; to the second network &lt;em&gt;rochefort&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network connect rochefort porthos&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As for the IP addresses, each network has IP addresses
in the range 172.x.0.0/16, where x is 18 in the
network &lt;em&gt;buckingham&lt;/em&gt; and 19 in the network &lt;em&gt;rochefort&lt;/em&gt;.
The address 172.x.0.1 is reserved for the router.
Therefore, the containers will be assigned
IP addresses from 172.x.0.2.
In this solution, we created &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;aramis&lt;/em&gt; and &lt;em&gt;portos&lt;/em&gt;
in this order.
Therefore, the IP addresses will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In network &lt;em&gt;buckingham&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;athos&lt;/em&gt;: 172.18.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.18.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In network &lt;em&gt;rochefort&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;aramis&lt;/em&gt;: 172.19.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.19.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can actually verify this configuration by inspecting
the two networks with the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The IP addresses might be different on your machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;communication-between-containers&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Communication between containers&lt;/h2&gt;
&lt;p&gt;Let’s see if and when the three containers can communicate.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-34&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;Which containers are able to communicate?
Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The only containers that cannot communicate are &lt;em&gt;athos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt;,
because they’re not connected to the same network.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-35&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its IP address.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Which IP address of &lt;em&gt;porthos&lt;/em&gt; would you use?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We need to use the IP address assigned to the endpoint linking
&lt;em&gt;porthos&lt;/em&gt; to the network &lt;em&gt;buckingham&lt;/em&gt;, to which &lt;em&gt;athos&lt;/em&gt; is connected.
In our case, this is 172.18.0.3.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-36&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.8  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its name.
Do you succeed? Are you surprised?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We succeed. Indeed, the network &lt;em&gt;buckingham&lt;/em&gt; provides a DNS server, that
can translate names into IP addresses.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can now exit the three containers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Docker</title>
      <link>/courses/cloudalbert/tutorials/docker-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloudalbert/tutorials/docker-tutorial/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to run &lt;strong&gt;containers&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and build &lt;strong&gt;images&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to create and use &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;How to define and use &lt;strong&gt;networks&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Terminology&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You’ll use the &lt;strong&gt;terminal&lt;/strong&gt; to run Docker commands.
Referring to the Docker architecture,
the terminal is the &lt;strong&gt;client&lt;/strong&gt; that communicates with
the Docker daemon (the &lt;strong&gt;server&lt;/strong&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker runs &lt;strong&gt;containers&lt;/strong&gt; on your computer.
We’ll refer to your computer as the &lt;strong&gt;host&lt;/strong&gt;,
the containers being the &lt;strong&gt;guests&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;containerized application&lt;/strong&gt;
is an application running in a container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Running containers&lt;/h1&gt;
&lt;!--The command used to run a container 
is ``docker run`` followed by four parameters:

```shell
docker run [options] image-name [command] [arg]
```

The four parameters are:

* *options*. List of options. 
* *image-name*. The fully qualified name of the image used to run the container. 
* *command*. The command to be executed in the container.
* *arg*. The arguments taken by the command executed in the container.

Only the parameter *image-name* is mandatory. 
The fully qualified name of an image is specified as a sequence of four fields, 
formatted as follows:

```shell
registry_url/user/name:tag
```

where:

* *registry_url* (optional). The URL of the registry that provides the image. 
If its value is not specified, the image 
will be looked up for in the 
[DockerHub registry](https://hub.docker.com){target=&#34;_blank&#34;}.
* *user* (optional).  The identifier of the user or organization that created the image. 
The default value is *library*.
* *name* (mandatory). The name of the image. 
* *tag* (optional). It specifies the image version.
If its value is not specified, 
the tag *latest* is used, pointing to the latest image version. --&gt;
&lt;!--
::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
For each of the following images, 
specify the registry name, the user, the name and the tag.

1. registry.redhat.io/rhel8/mysql-80

2. alpine:3.11

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}
:::



&lt;details&gt;

&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

1. Registry: *registry.redhat.io*, user: *rhel8*, name: *mysql-80*,
tag: *latest*

2. Registry: *DockerHub*, user: *library*, name: *alpine*, tag: *3.11*

3. Registry: *DockerHub*, user: *library*, name: *alpine*, tag: *latest*
:::

&lt;/details&gt;



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;What&#39;s the difference between the following image names?

1. alpine:latest

2. registry.hub.docker.com/library/alpine

::::: {.last-child}
3. alpine
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

There&#39;s no difference. They all point to the same image, that is
the latest version of *alpine* in the DockerHub registry.

:::

&lt;/details&gt;
--&gt;
&lt;p&gt;We now learn how to use the command &lt;code&gt;docker run&lt;/code&gt; and some of
its options.
In the following exercises, we’ll run containers from the image named
&lt;em&gt;alpine&lt;/em&gt; that is
&lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;available on the DockerHub registry&lt;/a&gt;.
This image provides a lightweight distribution
(i.e., it doesn’t contain many features) of Linux.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;You want to run the container from the latest version of
the image &lt;em&gt;alpine&lt;/em&gt;.
Which command would you write in the terminal?
Execute it.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The goal of this exercise is to start playing with the
&lt;code&gt;docker run&lt;/code&gt; command.
Since the question doesn’t say anything about the
options, nor does it mention the command to run inside the container,
we’d type:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;Execute the command that you proposed in the previous exercise, 
observe the output in the terminal and explain the actions 
taken by Docker to run the container.&lt;/div&gt;\EndKnitrBlock{exercise}


:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The output obtained from executing the command should 
look like as follows:

```bash
Unable to find image &#39;alpine:latest&#39; locally
latest: Pulling from library/alpine
aad63a933944: Pull complete 
Digest: sha256:b276d875eeed9c7d3f1cfa7edb06b22ed22b14219a7d67c52c56612330348239
Status: Downloaded newer image for alpine:latest
```

Here&#39;s what happens under the hood:

1. Docker looks for an image named *alpine:latest* in the host 
computer and cannot find it.

2. Docker *pulls* (i.e., downloads) the image from the DockerHub registry.

:::

&lt;/details&gt;--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;Is the container still running?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;In order to list all containers still running on the host, type the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your container shouldn’t appear in the output,
because it’s not running.
In order to see all containers, including those that are not
running, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;What information is displayed for each container?&lt;/div&gt;\EndKnitrBlock{exercise}

:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

* The identifier of the container.

* The name of the image used to run the container (it should be *alpine* for your 
container).

* The command executed within the container (it should be ``/bin/sh`` for your container).

* When the container has been created.

* The container current status (it should be *exited (0) x seconds ago* for your container).

* The network ports used by the container (we&#39;ll study them later).

* The name of the container. If you don&#39;t specify any when you 
run the container (as is our case),
Docker generates a random name by concatenating an adjective and 
a famous scientist&#39;s name (e.g., *agitated_newton*).

:::

&lt;/details&gt;--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;By looking at the command executed within the container (&lt;code&gt;/bin/sh&lt;/code&gt;),
can you tell why the container stopped without giving any output?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The command is &lt;code&gt;/bin/sh&lt;/code&gt;;
the container runs a Linux terminal.
But since we didn’t specify what to do with that terminal
(we didn’t run any Linux command, nor we tried to access the terminal),
the container stopped.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;We’re now going to do something useful with the image &lt;em&gt;alpine&lt;/em&gt;.
Make sure you read the &lt;strong&gt;good practices&lt;/strong&gt; that you should
adopt while playing with images and containers.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good practices&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Name your containers.&lt;/strong&gt; Although Docker assigns a default name to a new container,
it’s usually a good practice to give a container a name of your
choice to make it easily distinguishable. You can do it by using the option
&lt;code&gt;--name&lt;/code&gt;. Try the following:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker run --name my-alpine alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As before, the container stops immediately.
If you list all your containers by typing again:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see a container named &lt;em&gt;my-alpine&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove automatically a container if you use it once.&lt;/strong&gt;
Unless you want to reuse your container later, you can ask Docker to automatically remove it
when it stops by using the option &lt;code&gt;--rm&lt;/code&gt;.
This will prevent unused containers from taking up too much disk space.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Try the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name container-to-remove alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you list all the containers you should see that there is no container
named &lt;em&gt;container-to-remove&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused containers.&lt;/strong&gt; Stopped containers that have been run without
using the option &lt;code&gt;--rm&lt;/code&gt; are still stored in the host.
If you want to remove a specific
container (e.g., &lt;em&gt;my-alpine&lt;/em&gt;), use the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker container rm my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you want to remove all stopped containers, use the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container prune&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Remove unused images.&lt;/strong&gt; Images can take up a lot of disk space.
As a result, you should remember to remove those that you don’t intend to use
any longer.
The commands to remove a specific image
and prune unused ones are &lt;code&gt;docker image rm&lt;/code&gt;
and &lt;code&gt;docker image prune -a&lt;/code&gt; respectively.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;pass-a-command-to-the-containerized-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Pass a command to the containerized application&lt;/h2&gt;
&lt;p&gt;Remember that the template of &lt;code&gt;docker run&lt;/code&gt; is the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run [options] image-name [command] [arg]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The optional parameter &lt;em&gt;command&lt;/em&gt; refers to a command
that you can pass the containerized application, possibly with some arguments
(parameter &lt;em&gt;arg&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Let’s see an example.
As we saw before, when we run a container from the image &lt;em&gt;alpine&lt;/em&gt;,
a Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is launched.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Linux terminal &lt;code&gt;/bin/sh&lt;/code&gt; is run within the
container.
Henceforth, we’ll use the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Host terminal.&lt;/strong&gt; The terminal that you use to
interact with the operating system of your computer.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guest terminal.&lt;/strong&gt; The terminal that is run within
the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;By using the optional parameter &lt;em&gt;command&lt;/em&gt;, we can run
a command in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Run a container from the image &lt;em&gt;alpine&lt;/em&gt; and execute the
Linux command &lt;code&gt;ls&lt;/code&gt; that lists the content of the current directory.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where are the listed files stored?
In the host or in the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ls-test alpine ls&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The command &lt;code&gt;ls&lt;/code&gt; is run in the guest terminal, therefore
what we see in the output is a list of files stored in the
container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:command-in-guest&#34;&gt;1.4&lt;/a&gt; the command
&lt;code&gt;ls&lt;/code&gt; is executed in the guest terminal, but its
output is redirected to the host terminal.&lt;/p&gt;
&lt;p&gt;In other words, when we run the container, we
don’t interact directly with the guest terminal;
we just send a command and the output is redirected
to the host terminal.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s see how to execute a command in the guest terminal
that also requires an argument.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;By using the Linux utility &lt;code&gt;ping&lt;/code&gt;, check
whether the Web site &lt;code&gt;www.albertschool.com&lt;/code&gt; is reachable.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ping-test alpine ping en.albertschool.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to interrupt &lt;code&gt;ping&lt;/code&gt;
just type the key combination that you’s use to
interrupt any other command in your terminal.
(typically Ctrl-C on Windows and Cmd-C in MacOs).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--An application running in a container might need to interact 
with the user. 
For instance, the Linux 
command ``rev`` reverses whatever 
the user types on the keyboard.
In order to interact with a container, you should use the option
``-it`` of ``docker run``.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-9&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-9) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *alpine* to execute the 
Linux command `rev` and interact with it.
You can stop interacting with ``rev`` by typing Ctrl+C at any time.&lt;/div&gt;\EndKnitrBlock{exercise}


:::
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

``
docker run --rm --name rev -it alpine rev
``

After typing the command, type a word on your keyboard
(e.g., *deeps*), press *Return* and 
you should see the same word reversed (e.g., *speed*). 

The option ``-t`` opens a guest terminal (so we can see its
output); the option ``-i`` allows you to write directly 
into the guest terminal.

In order to stop using the guest terminal, 
you&#39;ll need to press Ctrl+D (both in Windows and MacOs).

:::

&lt;/details&gt;

--&gt;
&lt;p&gt;Now run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run  --name my-alpine -it alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; we didn’t use the option &lt;code&gt;--rm&lt;/code&gt; (the container will not be removed
when we stop it, we’re going to use it again).
Moreover, we didn’t specify any command to run in the guest terminal.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;What do you obtain?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;When we run a container from the image &lt;em&gt;alpine&lt;/em&gt;, the command
&lt;code&gt;/bin/sh&lt;/code&gt; is executed within the container.
Since we specified the option &lt;code&gt;-it&lt;/code&gt;, what we obtain is an access to the
Linux terminal running in the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-and-stopping-containers.&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Starting and stopping containers.&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run&lt;/code&gt; is a shorthand for two Docker commands, namely
&lt;code&gt;docker create&lt;/code&gt;, that creates a container from an image,
and &lt;code&gt;docker start&lt;/code&gt;, that starts the container after its creation.&lt;/p&gt;
&lt;p&gt;Suppose now that you want to download a Web page
by using Linux Alpine.
You can use the Linux command &lt;code&gt;wget&lt;/code&gt; followed by the URL of the page
that you want to download.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:wget-exo&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;By using the guest terminal
in the container &lt;em&gt;my-alpine&lt;/em&gt;,
download
&lt;a href=&#34;https://www.albertschool.com&#34; target=&#34;_blank&#34;&gt;this Web page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Where will the Web page be saved? The host computer or the container?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just type in &lt;em&gt;my-alpine&lt;/em&gt; guest terminal the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wget https://en.albertschool.com/formulaire&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Web page will be saved in the current directory
of the container. You can verify that the file is there
by typing &lt;code&gt;ls&lt;/code&gt; in the guest terminal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Want to copy the donwloaded file from the container to your computer?
&lt;/summary&gt;
&lt;p&gt;To copy the file &lt;code&gt;formulaire&lt;/code&gt; to your working directory, type the following command in
the host terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  docker cp &amp;lt;containerId&amp;gt;:/formulaire .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the previous command, replace &lt;code&gt;&amp;lt;containerId&amp;gt;&lt;/code&gt; with the identifier of your container. You can obtain
the identifier of the container from the output of the command &lt;code&gt;docker container ls&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Where are the containers and image files stored?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use MacOS or Windows
&lt;/summary&gt;
&lt;p&gt;The files managed by Docker are not stored directly in your computer, but
in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).&lt;/p&gt;
&lt;p&gt;Therefore, you need to open a terminal inside that Linux virtual machine by typing the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the terminal is opened, you can follow the instructions given below for Linux users.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you use Docker on Linux
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All files managed by Docker are stored under folder &lt;code&gt;/var/lib/docker&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To access that folder, you need to be root (i.e., administrator). Type the command &lt;code&gt;sudo su&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;ls /var/lib/docker&lt;/code&gt; you can look at the folders stored under this directory. You’ll see that there are
folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To locate the files of a specific container, you first need to get the &lt;strong&gt;container identifier&lt;/strong&gt; by typing &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the command &lt;code&gt;docker inspect &amp;lt;container-id&amp;gt;&lt;/code&gt; (replace &lt;code&gt;&amp;lt;container-id&amp;gt;&lt;/code&gt; with the identifier of the container that you intend to inspect).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Locate the field &lt;code&gt;UpperDir&lt;/code&gt;. The value of this field is the path to the directory (let’s call it &lt;code&gt;CONTAINER_DIR&lt;/code&gt;) that contains the upper layer of the container (the writable layer).
It should be a path that ends by &lt;code&gt;/diff&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you type &lt;code&gt;cd CONTAINER_DIR&lt;/code&gt; (replace &lt;code&gt;CONTAINER_DIR&lt;/code&gt; with the value of the field &lt;code&gt;UpperDir&lt;/code&gt;) you can finally see the files stored in
your container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;em&gt;my-alpine&lt;/em&gt; guest terminal type &lt;code&gt;exit&lt;/code&gt;.
This closes the guest terminal and, as a result, stops the
container.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTICE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stopping the container will not erase any of the files
stored in the container. Removing the container will.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to start the container &lt;em&gt;my-alpine&lt;/em&gt; again, you can
use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container start -ai my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will open the guest terminal of the container again;
type &lt;code&gt;ls&lt;/code&gt; to verify that
the Web page that you downloaded before is still there.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-images&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Creating Images&lt;/h1&gt;
&lt;p&gt;A Docker image can be thought of as a template to create and run a container.
An image is a file that contains a &lt;strong&gt;layered filesystem&lt;/strong&gt; with each layer being &lt;strong&gt;immutable&lt;/strong&gt;;
this means that the files that belong to a layer cannot be
modified or deleted, nor can files be added to a layer.&lt;/p&gt;
&lt;p&gt;When a container is created from an image, it
will be composed of all the image read-only layers and, on top of
them, a writable layer (termed the &lt;strong&gt;container layer&lt;/strong&gt;),
where all the new files created in the container will be written.
For example, the Web page that you downloaded in Exercise &lt;a href=&#34;#exr:wget-exo&#34;&gt;1.7&lt;/a&gt;
were stored in the writable layer of that container.&lt;/p&gt;
&lt;!--## Interactive image creation

When we run the container *dl-figures*  in Section \@ref(simple-use-case), 
we modified the container to
install the command ``wget``. 
You can see the modifications by typing the 
following command:

``
docker diff dl-figures
``

The output consists of a list of files tagged with the letter A, C or D, indicating 
respectively that the file has been added (A), changed (C) or deleted (D). 
In this list you&#39;ll find the downloaded figures, as well as 
other files that have been added or modified or deleted 
when we installed ``wget``.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-11&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-11) &lt;/strong&gt;&lt;/span&gt;If layers, except the top one, are immutable, 
how can files that belong to the lower layers be modified or deleted?&lt;/div&gt;\EndKnitrBlock{exercise}
:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

All files marked with A are new and therefore are 
added to the writable layer of the container.

As for the existing files, they live in the immutable layers 
of the image, and therefore cannot be touched directly. 
Instead, they are copied from the bottom layers to the writable layer where 
they are modified.
This strategy is called **copy-on-write**.

The structure of layers generates a **layered filesystem** in the image; 
if different copies of the same file exist in different layers, 
the copy in the uppermost layer 
overwrites the others.

:::

&lt;/details&gt;



We can create a new image from the container *dl-figures*, one that provides
a Ubuntu distribution with the command ``wget`` already installed, 
with the following command:


```shell
docker commit dl-figures ubuntu-with-wget
```

The command creates a new image called *ubuntu-with-wget*.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-13&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-13) &lt;/strong&gt;&lt;/span&gt;Run a container from the image *ubuntu-with-wget* and verify that the command
*wget* is actually installed. &lt;/div&gt;\EndKnitrBlock{exercise}

:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

Just type the following command:

``
docker run --rm -it ubuntu-with-wget
``
In the guest terminal type ``wget``: you should see the 
following output:

``
wget: missing URL
Usage: wget [OPTION]... [URL]...

Try `wget --help` for more options.
``

:::

&lt;/details&gt;--&gt;
&lt;div id=&#34;dockerfiles&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Dockerfiles&lt;/h2&gt;
&lt;p&gt;The most common way to create an image is to use a &lt;strong&gt;Dockerfile&lt;/strong&gt;, a
text file that contains all the instructions necessary to
build the image.
The advantage of the Dockerfile is that it can be interpreted
by the Docker engine, which makes the creation of images an automated
and repeatable task.&lt;/p&gt;
&lt;p&gt;Suppose that you want a quick way to download all the images from a Web page.
You learn that a quick way is to use the almighty Linux commands and you find
on the Web a Bash script that does exactly what you want:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#!/bin/bash
wget $1 -O page.html
grep -E &amp;quot;(https?:)?//[^/\s]+/\S+\.(jpg|png|gif|svg)&amp;quot; page.html -o | sed &amp;quot;s/(^https?)?\/\//https\:\/\//g&amp;quot; -r &amp;gt; urls.txt
sed -E &amp;quot;s/\/thumb//g; s/\/[0-9]+px-.+\.(jpg|png)$//g&amp;quot; urls.txt | uniq &amp;gt; urls-new.txt
wget -i urls-new.txt -P downloads/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Don’t worry if you don’t understand these instructions. Suffices to say that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first instruction gets an URL as an argument (&lt;code&gt;$1&lt;/code&gt;) and downloads the page pointed by the URL into a file called &lt;code&gt;page.html&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second instruction extracts from &lt;code&gt;page.html&lt;/code&gt; all the URLs pointing to images. The URLs are saved to a file called &lt;code&gt;url.txt&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third instruction filters the URLs that point to full-size images. It saves the filtered URLs to a file called &lt;code&gt;urls-new.txt&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The fourth instruction downloads the full-size images to a folder named &lt;code&gt;downloads&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You now create a folder in your computer named &lt;code&gt;myfigures&lt;/code&gt;;
you create a new text file that you name &lt;code&gt;dlimages.sh&lt;/code&gt; and you save it under folder &lt;code&gt;myfigures&lt;/code&gt;;
you copy the above Bash script into the new file and you save it.&lt;/p&gt;
&lt;p&gt;But now you wonder: how can you execute this script if you’re not using Linux?&lt;/p&gt;
&lt;p&gt;The answer is: you can create a new Docker image (let’s call it &lt;code&gt;dlimages&lt;/code&gt;)
from the official Ubuntu image!&lt;/p&gt;
&lt;p&gt;The Dockerfile containing the instructions to build the image
&lt;em&gt;dlimages&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update
RUN apt-get install -y wget
RUN mkdir -p /scripts
COPY dlimages.sh /scripts/dlimages.sh
RUN chmod +x /scripts/dlimages.sh
ENTRYPOINT [&amp;quot;/scripts/dlimages.sh&amp;quot;]
CMD [&amp;quot;https://en.wikipedia.org/wiki/Cat&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the explanation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We use the image &lt;em&gt;ubuntu&lt;/em&gt; as the &lt;strong&gt;base image&lt;/strong&gt;.
This corresponds to the instruction &lt;code&gt;FROM ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We install the utility &lt;code&gt;wget&lt;/code&gt; in the base image.
This corresponds to the
instructions &lt;code&gt;RUN apt-get update&lt;/code&gt; and &lt;code&gt;RUN apt-get install -y wget&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We create a directory &lt;code&gt;scripts&lt;/code&gt; under the root directory of the image.
This corresponds to the instruction &lt;code&gt;RUN mkdir -p /scripts&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We copy the file &lt;code&gt;dlimages.sh&lt;/code&gt; to the folder &lt;code&gt;scripts&lt;/code&gt; in the image. This corresponds to the &lt;code&gt;COPY&lt;/code&gt; instruction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We specify that &lt;code&gt;dlimages.sh&lt;/code&gt; can be executed. This corresponds to the &lt;code&gt;RUN chmod -x&lt;/code&gt; instruction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We specify the command to be executed when a container is run from this image.
This corresponds to the instruction &lt;code&gt;ENTRYPOINT&lt;/code&gt;.
When we run a container from this image, we execute the script &lt;code&gt;dlimages.sh&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We specify a default argument to be passed to the script. This corresponds to the instruction &lt;code&gt;CMD&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;What’s the relation between the Dockerfile lines and the image layers?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each line corresponds to a new layer.
The first line corresponds to the bottom layer;
the last line to the top layer.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:dockerfile-creation&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Could you identify a problem in this Dockerfile?
Modify the Dockerfile accordingly.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;When creating an image, we should keep the number of layers relatively
small; in fact, the more the layers, the bigger the image will be.
Here we create three separate layers with three RUN commands; we can
simply merge the three layers.
The resulting Dockerfile will be:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y wget &amp;amp;&amp;amp; \
    mkdir -p /scripts
COPY dlimages.sh /scripts/dlimages.sh
RUN chmod +x /scripts/dlimages.sh
ENTRYPOINT [&amp;quot;/scripts/dlimages.sh&amp;quot;]
CMD [&amp;quot;https://en.wikipedia.org/wiki/Cat&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;building-an-image&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Building an image&lt;/h2&gt;
&lt;p&gt;We’re now going to build an image from a Dockerfile.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a new file named &lt;code&gt;Dockerfile&lt;/code&gt; under the folder &lt;code&gt;myfigures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Dockerfile&lt;/em&gt; write the set of instructions that
you proposed in Exercise &lt;a href=&#34;#exr:dockerfile-creation&#34;&gt;2.2&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the terminal, set the working directory to &lt;em&gt;myfigures&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build an image called &lt;em&gt;dlimages&lt;/em&gt; by executing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker build -t dlimages .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.&lt;/code&gt; at the end of the command means that the Docker engine will
look for a file named &lt;em&gt;Dockerfile&lt;/em&gt; in the working directory.&lt;/p&gt;
&lt;!--

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-15&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-15) &lt;/strong&gt;&lt;/span&gt;Once the image is built, type the command ``docker image ls -a``. 
What are the images with repository and tag ``&lt;none&gt;``?
Why are there three of such images?&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;/div&gt;

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

These are the **intermediate images**. 
Once a layer is compiled, an intermediate image is created that 
contains that layer and all the layers underneath.
In other words, the intermediate image 
corresponding to the layer $i$ contains all files 
up to the layer $i$, including layers 1 through $i-1$.

The intermediate layers are used by the **build cache**, 
of which we&#39;ll see an example later.

Although there are five layers in the new image, there are only 
three intermediate images because:

* the base image is *ubuntu:eoan*.
* the image corresponding to the top layer is the final image *fig-downloader*.

:::

&lt;/details&gt;
--&gt;
&lt;p&gt;In order to verify that the new image has been created, type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;
&lt;!-- Let&#39;s dive deeper into the anatomy of an image.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-16&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-16) &lt;/strong&gt;&lt;/span&gt;Run the following command:

``
docker history fig-downloader
`` 

and analyze the layers of the new image. 

* Why do some layers have an ID, while others 
are marked as &lt;i&gt;missing&lt;/i&gt;?

::::: {.last-child}
* Can you find the identifiers of the intermediate images?
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The layers with an ID correspond to the layers of 
the new image, including the top layer and the base image.
The layers marked as *missing* are those that compose the 
base image. Those layers are not stored in  
your computer, 
simply because they belong to an image 
that hasn&#39;t been built on your computer 
and you downloaded from the DockerHub registry.

By looking at the output of ``docker image ls -a`` and the output of this command,
we see that the layers between the base image and the top layer have the 
same identifiers as the intermediate images.

:::

&lt;/details&gt;

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:dl-1-container&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --name dl1 dlimages&lt;/code&gt;&lt;/p&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We downloaded the figures
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cat&#34; target=&#34;_blank&#34;&gt;of this page&lt;/a&gt;.
The downloaded pictures are in the folder &lt;em&gt;/downloads&lt;/em&gt;
of the container &lt;em&gt;dl1&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You can copy them to your computer with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker cp dl1:/downloads .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --name dl2 dlimages https://en.wikipedia.org/wiki/Dog&lt;/code&gt;&lt;/pre&gt;
What does it do? Where are the downloaded pictures?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We downloaded the figures
&lt;a href=&#34;https://en.wikipedia.org/wiki/Dog&#34; target=&#34;_blank&#34;&gt;of this page&lt;/a&gt;.
We basically overwrote the URL specified by the CMD keyword with a new
one.
The downloaded pictures are in the folder &lt;em&gt;/downloads&lt;/em&gt;
of the container &lt;em&gt;dl2&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;containerized-python-application&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Containerized Python application&lt;/h2&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-docker/word-frequency.zip&#34;&gt;this archive file&lt;/a&gt;
and unzip it into your working directory.
In this archive you’ll find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Dockerfile.&lt;/li&gt;
&lt;li&gt;A Python script &lt;em&gt;main.py&lt;/em&gt; that asks the user to enter the URL and the language of a Web page,
and prints the 10 most frequent words occurring in that page.&lt;/li&gt;
&lt;li&gt;A file &lt;em&gt;requirements.txt&lt;/em&gt; with the list of the Python packages
needed to run the given script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The content of the Dockerfile is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./main.py ./requirements.txt /app/
RUN pip install -r requirements.txt
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;Describe what this Dockerfile does.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Takes &lt;em&gt;python:3.7-slim&lt;/em&gt; as the base image.&lt;/li&gt;
&lt;li&gt;Creates a new folder &lt;em&gt;app&lt;/em&gt; in the image under the root directory.&lt;/li&gt;
&lt;li&gt;Changes the working directory to &lt;em&gt;/app&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Copies the files &lt;em&gt;main.py&lt;/em&gt; and &lt;em&gt;requirements.txt&lt;/em&gt; from the local
computer to the directory &lt;em&gt;/app&lt;/em&gt; in the image.&lt;/li&gt;
&lt;li&gt;Runs the command &lt;code&gt;pip install&lt;/code&gt; to install the Python libraries
specified in the file &lt;em&gt;requirements.txt&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Executes the command &lt;code&gt;python main.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34; word-latex=&#34;{exercisebox}&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;Build an image called &lt;em&gt;wordfreq&lt;/em&gt; from this Dockerfile.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker build -t wordfreq .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;Without changing the Dockerfile, rebuild the same image.
What do you notice?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The build is very fast.
Since we didn’t change the Dockerfile, the image
is rebuilt by using the image layers created
previously.
This is clearly indicated by the word &lt;strong&gt;CACHED&lt;/strong&gt; written
at each layer.
Using the already stored layers is called &lt;strong&gt;build cache&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;What happens if you modify a line in the Python script and
you rebuild the image?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Add any instruction at the end of &lt;em&gt;main.py&lt;/em&gt;, such as:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Finish!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then rebuild the image.
The three bottom layers are not affected by the modification, therefore
they benefit from the build cache.
Layer 4 is the first affected by the modification.
This layer, and those above, need therefore to be
rebuilt.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.9  &lt;/strong&gt;&lt;/span&gt;Based on the previous considerations,
can you tell what’s wrong with this Dockerfile?
Modify the Dockerfile accordingly and rebuild the image.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each time we modify &lt;em&gt;main.py&lt;/em&gt; and we rebuild the image,
the layer 4 and 5 are recreated, meaning that all the Python packages
are downloaded and installed.
Depending on the size and number of the packages, this might
take some while.
A better way to structure the Dockerfile is to install the
packages before copying the Python script to the image.
Here is how we should modify the Dockerfile:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./requirements.txt /app/
RUN pip install -r requirements.txt
COPY ./main.py /app/
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.10  &lt;/strong&gt;&lt;/span&gt;Modify &lt;em&gt;main.py&lt;/em&gt; by adding a new line of code and rebuild the image.
What changed?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The Python packages are not reinstalled, as a result rebuilding the image&lt;br /&gt;
takes much less time than before.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Play with the application by running the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it  wordfreq&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The containerized application will prompt you to insert the URL of a webpage and
the language of the page (in English).
The output will be the 20 most used words in the webpage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-volumes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Volumes&lt;/h1&gt;
&lt;p&gt;In Exercise &lt;a href=&#34;#exr:dl-1-container&#34;&gt;2.3&lt;/a&gt; you’ve been asked to run a container named
&lt;em&gt;dl1&lt;/em&gt; to download images from a Web page.
The figures were downloaded into the
directory &lt;em&gt;/downloads&lt;/em&gt; of the container.
But we left a question unanswered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we transfer those figures from the container to the host computer?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yes, we can use the command &lt;code&gt;docker cp&lt;/code&gt;, but there is still a problem left: if we remove the container &lt;code&gt;dl1&lt;/code&gt;, we also lose all
the images. In other words, any file created within the container has a lifetime that depends on the container itself.
This is not a good option in production.&lt;/p&gt;
&lt;p&gt;The solution is to use &lt;strong&gt;volumes&lt;/strong&gt; to decouple the application from the the data.&lt;/p&gt;
&lt;!--One way to go about that is to run the following command in the host terminal:

``
docker cp dl-1:/my-figures .
``

This will copy the directory */my-figures* from the container *dl-1* to
the host computer working directory.
You can verify it by yourself.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-24&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-24) &lt;/strong&gt;&lt;/span&gt;Can you tell why this solution is less than ideal?&lt;/div&gt;\EndKnitrBlock{exercise}


:::
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

1. After running the container we need to do an additional action to copy
the figures from the container to the host computer.

2. The container is created and run only to download some figures.
We&#39;d like to remove it automatically (with the option ``--rm``) when its 
execution is over. However, if we do so, the pictures will be lost before 
we can copy them to the host computer.

:::

&lt;/details&gt;

## Using a host volume

A better solution is to **mount** (i.e., attach) 
a directory of the host computer at the container&#39;s directory 
*/my-figures* when we run it.
Let&#39;s see how it works.

**Step 1.** Create a directory named *figs-volume* in your working directory.

**Step 2.** Type and execute the following command:

```shell
docker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader
```

This command runs a container from the image *fig-downloader*. 

* With the option ``-v`` we specify  that we want to mount the directory
*\$(pwd)/figs-volume* (*\$(pwd)* indicates the host working directory)
at the directory *figs-volume* in the container;

* The option ``--rm`` indicates that we want the container to be 
removed when its execution is over.

**Step 3.** Verify that the pictures are in the folder *figs-volume*.

In this example, we&#39;ve used the directory *figs-volume* as a 
**volume** (essentially, an external storage area) of the container;
when the container is destroyed, the volume remains with all its data.--&gt;
&lt;div id=&#34;docker-volumes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Docker volumes&lt;/h2&gt;
&lt;p&gt;A volume can be seen as a virtual storage device attached to a container.
All files there are written to a volume survive the containerized application that
created them. In other words, when a container is destroyed,
all the files created by the application in the container remain in the volume.&lt;/p&gt;
&lt;p&gt;Let’s create a new Docker volume called &lt;em&gt;data-volume&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume create data-volume&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know (advanced notion)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where the data will be actually stored?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can inspect the new volume by typing the
following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume inspect data-volume&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;mount point&lt;/em&gt; is indicated; that’s the folder where the data
will be actually stored.
If your computer runs Linux, that folder will be available
on the host; if your computer runs Windows or MacOS,
you’ll not find that folder on your computer.
Instead, it will be available in the virtual machine
that Docker use on MacOS and Windows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do you want to see the directory? (Instructions for Windows and MacOS)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One way to look into the hidden VM is to run
the following containerized application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This application will open a guest terminal into the VM.
You can then use the commands &lt;code&gt;cd&lt;/code&gt; and &lt;code&gt;ls&lt;/code&gt;
to browse to the directory indicated as the mount path
of the new volume.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sharing-data&#34; class=&#34;section level3&#34; number=&#34;3.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1.1&lt;/span&gt; Sharing data&lt;/h3&gt;
&lt;p&gt;A Docker volume can be used to share data between containers.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-25&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;code&gt;ubuntu&lt;/code&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; at the container’s directory &lt;em&gt;/data&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Feel free to use the Web to find the answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it -v data-volume:/data ubuntu&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type the following command in the guest Linux terminal to create a file
&lt;em&gt;test-file.txt&lt;/em&gt; in the directory &lt;em&gt;/data&lt;/em&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;echo &#34;This is a new file&#34; &amp;gt; /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Print to the console the content of the file with the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;cat /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Type &lt;code&gt;exit&lt;/code&gt; to leave the guest terminal. Since we’ve specified the option &lt;code&gt;--rm&lt;/code&gt;, the container
is &lt;strong&gt;destroyed&lt;/strong&gt;. Now we’re going to verify that &lt;code&gt;test-file.txt&lt;/code&gt; is still accessible.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-26&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Run a container from the image &lt;em&gt;alpine:latest&lt;/em&gt;,
specifying the options to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove the container once its execution is over.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interact with the guest Linux terminal in the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Mount the volume &lt;em&gt;data-volume&lt;/em&gt; to the directory &lt;em&gt;/my-data&lt;/em&gt;
of the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker container run --rm -it -v data-volume:/my-data alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-27&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;Verify that you can read the file &lt;em&gt;test-file.txt&lt;/em&gt;.
Which folder would you look in?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We need to look in the folder &lt;em&gt;/my-data&lt;/em&gt; because this is where
we mounted &lt;em&gt;data-volume&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cat /my-data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Type &lt;code&gt;exit&lt;/code&gt; to exit the guest terminal and terminate the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;single-host-networking&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Single-Host Networking&lt;/h1&gt;
&lt;p&gt;In order to let containers communicate and, therefore, co-operate,
Docker defines a simple networking model known as
the &lt;strong&gt;container network model&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:docker-cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/docker-primer/cnm.png&#34; alt=&#34;Container network model&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: Container network model
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-28&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;Describe the output of the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The command lists all the networks created by Docker on
your computer.
For each network, the values of four attributes are shown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The identifier.&lt;/li&gt;
&lt;li&gt;The name.&lt;/li&gt;
&lt;li&gt;The driver used by the network.&lt;/li&gt;
&lt;li&gt;The scope of the network (local or global).
A local scope means that the network connects containers
running on the same host, as opposed to a global scope that
means that containers on different hosts can communicate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on the containers that you used
in the past, you might see different networks.
However, three networks are worth noting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network named &lt;strong&gt;bridge&lt;/strong&gt;, that uses the driver &lt;strong&gt;bridge&lt;/strong&gt; and a local scope.
By default, any new container is attached to this network.&lt;/li&gt;
&lt;li&gt;The network named &lt;strong&gt;host&lt;/strong&gt;, that uses the driver &lt;strong&gt;host&lt;/strong&gt; and a local scope.
It’s used when we want a container to directly use the network interface of the host.
It’s important to remember that this network should only be used when analyzing the
host’s network traffic. In the other cases, using this network exposes
the container to all sorts of security risks.&lt;/li&gt;
&lt;li&gt;The network named &lt;strong&gt;none&lt;/strong&gt;, that uses the driver &lt;strong&gt;null&lt;/strong&gt; and a local scope.
Attaching a container to this network means that the container
isn’t connected to any network, and therefore it’s completely isolated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-29&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;The following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect bridge&lt;/code&gt;&lt;/p&gt;
outputs the configuration of the network &lt;strong&gt;bridge&lt;/strong&gt;.
By looking at this configuration, can you tell
what IP addresses will be given to the containers attached to this
network? What’s the IP address of the router of this network?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The information is specified in the field named &lt;strong&gt;IPAM&lt;/strong&gt;, more specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Subnet&lt;/strong&gt; indicates the range of IP addresses used by the network.
The value of this field should be 172.17.0.0/16;
the addresses range from 172.17.0.1 to 172.17.255.255.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gateway&lt;/strong&gt; indicates the IP address of the router of the network.
The value should be 172.17.0.1&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;creating-networks&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Creating networks&lt;/h2&gt;
&lt;p&gt;By default, any new container is attached to the network named &lt;em&gt;bridge&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-30&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Explain why it is not a good practice to
attach all our containers to the same network.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;All new containers will be able to communicate over this network.
This is not a good idea.
If a hacker can compromise any of these containers, s/he might
be able to attack the other containers as well.
As a rule of thumb, we should attach two containers to the same network &lt;strong&gt;only&lt;/strong&gt; on a
need-to-communicate basis.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;In order to create a new network, you can use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create network_name&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-31&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;Create two networks named &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; that
use the driver &lt;em&gt;bridge&lt;/em&gt;.
By using the &lt;code&gt;docker network inspect&lt;/code&gt; command,
look at the IP addresses of the new networks and write them down.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just run the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The IP addresses for the network &lt;em&gt;buckingham&lt;/em&gt; are
172.18.0.0/16 (addresses from 172.18.0.1 to 172.18.255.255);
The IP addresses for the network &lt;em&gt;rochefort&lt;/em&gt; are:
172.19.0.0/16 (assuming that you create &lt;em&gt;buckingham&lt;/em&gt;
before &lt;em&gt;rochefort&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The IP addresses may be different on your machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-32&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;Create three containers &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;porthos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt; and attach them
to the two networks &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; as displayed in figure.
&lt;strong&gt;The three containers will open a Linux Alpine shell&lt;/strong&gt;.
You’ll need to launch the commands in three separate tabs of your terminal window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What will the IP addresses of the three containers be in the two networks?
Remember that &lt;em&gt;porthos&lt;/em&gt; is attached to two networks, therefore it’ll have two
network interfaces (endpoints) and, as a result, two IP addresses.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Verify your answers by inspecting the two networks (use the
command &lt;code&gt;docker network inspect&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Here are the commands to run &lt;em&gt;athos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt; while connecting
them to &lt;em&gt;buckingham&lt;/em&gt; and &lt;em&gt;rochefort&lt;/em&gt; respectively.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name athos --network buckingham  alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name aramis --network rochefort   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here’s the command to run &lt;em&gt;porthos&lt;/em&gt; and attach it to
&lt;em&gt;buckingham&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name porthos --network buckingham   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The following command attaches &lt;em&gt;porthos&lt;/em&gt; to the second network &lt;em&gt;rochefort&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network connect rochefort porthos&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As for the IP addresses, each network has IP addresses
in the range 172.x.0.0/16, where x is 18 in the
network &lt;em&gt;buckingham&lt;/em&gt; and 19 in the network &lt;em&gt;rochefort&lt;/em&gt;.
The address 172.x.0.1 is reserved for the router.
Therefore, the containers will be assigned
IP addresses from 172.x.0.2.
In this solution, we created &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;aramis&lt;/em&gt; and &lt;em&gt;portos&lt;/em&gt;
in this order.
Therefore, the IP addresses will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In network &lt;em&gt;buckingham&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;athos&lt;/em&gt;: 172.18.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.18.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In network &lt;em&gt;rochefort&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;aramis&lt;/em&gt;: 172.19.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.19.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can actually verify this configuration by inspecting
the two networks with the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The IP addresses might be different on your machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;communication-between-containers&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Communication between containers&lt;/h2&gt;
&lt;p&gt;Let’s see if and when the three containers can communicate.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-33&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;Which containers are able to communicate?
Justify your answer.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The only containers that cannot communicate are &lt;em&gt;athos&lt;/em&gt; and &lt;em&gt;aramis&lt;/em&gt;,
because they’re not connected to the same network.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-34&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its IP address.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Which IP address of &lt;em&gt;porthos&lt;/em&gt; would you use?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We need to use the IP address assigned to the endpoint linking
&lt;em&gt;porthos&lt;/em&gt; to the network &lt;em&gt;buckingham&lt;/em&gt;, to which &lt;em&gt;athos&lt;/em&gt; is connected.
In our case, this is 172.18.0.3.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-35&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.8  &lt;/strong&gt;&lt;/span&gt;Try to ping &lt;em&gt;porthos&lt;/em&gt; from &lt;em&gt;athos&lt;/em&gt; by using its name.
Do you succeed? Are you surprised?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We succeed. Indeed, the network &lt;em&gt;buckingham&lt;/em&gt; provides a DNS server, that
can translate names into IP addresses.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can now exit the three containers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Atelier pratique sur Docker</title>
      <link>/courses/cloud-fr/docker-fr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-fr/docker-fr/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dans ce TD, vous apprendrez à :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exécuter des &lt;strong&gt;conteneurs&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;définir et construire des &lt;strong&gt;images&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;créer et utiliser des &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;définir et utiliser des &lt;strong&gt;réseaux&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prérequis :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoir installé Docker sur votre ordinateur ou avoir importé la machine virtuelle Linux soit avec VirtualBox, soit avec Multipass.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Vous utilisez une machine virtuelle avec Multipass ? Cliquez ici pour plus d’infos
&lt;/summary&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Commandes Multipass&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pour démarrer la machine virtuelle, tapez &lt;code&gt;multipass start cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le dossier &lt;code&gt;/home/ubuntu/labs&lt;/code&gt; dans la machine virtuelle est monté (c’est-à-dire lié) au dossier
sur VOTRE ordinateur (appelons-le &lt;code&gt;LAB&lt;/code&gt;) où vous stockerez tous les fichiers que vous créérez pendant les TD et les TP.
Vous avez spécifié ce dossier lors de l’installation de la machine virtuelle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous ne vous souvenez plus du chemin vers le dossier lié à &lt;code&gt;/home/ubuntu/labs&lt;/code&gt;, tapez la
commande &lt;code&gt;multipass info cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour ouvrir un terminal dans la machine virtuelle, tapez &lt;code&gt;multipass shell cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque le terminal s’ouvre, le dossier courant est &lt;code&gt;/home/ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tapez &lt;code&gt;cd labs&lt;/code&gt; pour vous déplacer vers le dossier où vous trouverez votre matériel de TP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vous utiliserez le terminal de la machine virtuelle uniquement pour taper les commandes Docker.
Vous pouvez utiliser tous les outils installés sur votre machine pour créer et gérer les fichiers
dans le dossier &lt;code&gt;LAB&lt;/code&gt; directement depuis votre ordinateur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une fois le TP terminé, tapez &lt;code&gt;exit&lt;/code&gt; pour quitter le terminal de la machine virtuelle. Vous reviendrez alors
au terminal de votre ordinateur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans le terminal de votre ordinateur, tapez &lt;code&gt;multipass stop cloudvm&lt;/code&gt; pour arrêter la machine virtuelle.
Cela ne détruira PAS vos fichiers ! Cela arrête simplement la machine virtuelle pour qu’elle n’utilise plus inutilement les ressources de votre ordinateur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Terminologie&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vous utiliserez le &lt;strong&gt;terminal&lt;/strong&gt; pour exécuter des commandes Docker.
Le terminal est le &lt;strong&gt;client&lt;/strong&gt; qui communique avec
le démon Docker.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker exécute des &lt;strong&gt;conteneurs&lt;/strong&gt; sur votre ordinateur.
Nous ferons référence à votre ordinateur comme étant l’&lt;strong&gt;hôte&lt;/strong&gt;,
les conteneurs étant les &lt;strong&gt;invités&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Une &lt;strong&gt;application conteneurisée&lt;/strong&gt;
est une application qui s’exécute dans un conteneur.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Exécuter des conteneurs&lt;/h1&gt;
&lt;p&gt;Nous allons maintenant apprendre à utiliser la commande &lt;code&gt;docker run&lt;/code&gt; ainsi que quelques-unes de
ses options.&lt;br /&gt;
Dans les exercices suivants, nous exécuterons des conteneurs à partir de l’image nommée
&lt;em&gt;alpine&lt;/em&gt; qui est
&lt;a href=&#34;https://hub.docker.com/_/alpine&#34; target=&#34;_blank&#34;&gt;disponible sur le registre DockerHub&lt;/a&gt;.&lt;br /&gt;
Cette image fournit une distribution légère
(c’est-à-dire qu’elle ne contient pas beaucoup de fonctionnalités) de Linux.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;Vous souhaitez exécuter un conteneur à partir de la dernière version de
l’image &lt;em&gt;alpine&lt;/em&gt;.&lt;br /&gt;
Quelle commande devez-vous taper dans le terminal ?&lt;br /&gt;
Exécutez-la.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le but de cet exercice est de commencer à jouer avec la commande &lt;code&gt;docker run&lt;/code&gt;.
Comme la question ne dit rien à propos des options, et ne mentionne pas non plus la commande à exécuter à l’intérieur du conteneur, on taperait :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;Le conteneur est-il toujours en cours d’exécution ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Afin de lister tous les conteneurs encore en cours d’exécution sur l’hôte, tapez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Votre conteneur ne devrait pas apparaître dans le résultat,
car il n’est pas en cours d’exécution.
Pour voir tous les conteneurs, y compris ceux qui ne sont pas
en cours d’exécution, tapez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;En regardant la commande exécutée dans le conteneur (&lt;code&gt;/bin/sh&lt;/code&gt;),
pouvez-vous dire pourquoi le conteneur s’est arrêté sans donner aucune sortie ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La commande est &lt;code&gt;/bin/sh&lt;/code&gt; ;
le conteneur exécute un terminal Linux.
Mais comme nous n’avons pas spécifié ce qu’il fallait faire avec ce terminal
(nous n’avons lancé aucune commande Linux, ni essayé d’accéder au terminal),
le conteneur s’est arrêté.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Nous allons maintenant faire quelque chose d’utile avec l’image &lt;em&gt;alpine&lt;/em&gt;.
Assurez-vous de lire les &lt;strong&gt;bonnes pratiques&lt;/strong&gt; que vous devriez
adopter lorsque vous utilisez des images et des conteneurs.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bonnes pratiques&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Nommez vos conteneurs.&lt;/strong&gt; Bien que Docker attribue un nom par défaut à un nouveau conteneur,
c’est généralement une bonne pratique de donner à un conteneur un nom de votre
choix pour le rendre facilement distinguable. Vous pouvez le faire en utilisant l’option
&lt;code&gt;--name&lt;/code&gt;. Essayez ce qui suit :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker run --name my-alpine alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Comme précédemment, le conteneur s’arrête immédiatement.
Si vous listez tous vos conteneurs en tapant à nouveau :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container ls -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vous devriez voir un conteneur nommé &lt;em&gt;my-alpine&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Supprimez automatiquement un conteneur si vous allez l’utiliser une seule fois.&lt;/strong&gt;
À moins que vous ne vouliez réutiliser votre conteneur plus tard, vous pouvez demander à Docker de le supprimer automatiquement
lorsqu’il s’arrête en utilisant l’option &lt;code&gt;--rm&lt;/code&gt;.
Cela empêchera les conteneurs inutilisés de prendre trop d’espace sur disque.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Essayez ce qui suit :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name container-to-remove alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Si vous listez tous les conteneurs, vous devriez voir qu’il n’y a pas de conteneur
nommé &lt;em&gt;container-to-remove&lt;/em&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Supprimez les conteneurs que vous n’utilisez plus.&lt;/strong&gt; Les conteneurs arrêtés qui ont été exécutés sans
l’option &lt;code&gt;--rm&lt;/code&gt; sont toujours stockés sur votre machine hôte.
Si vous voulez supprimer un conteneur
spécifique (par exemple, &lt;em&gt;my-alpine&lt;/em&gt;), utilisez la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker container rm my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Si vous souhaitez supprimer tous les conteneurs arrêtés, utilisez la commande
suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container prune&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Supprimez les images inutilisées.&lt;/strong&gt; Les images peuvent prendre beaucoup d’espace sur disque.
Par conséquent, vous devriez penser à supprimer celles que vous n’avez plus l’intention d’utiliser.
Les commandes pour supprimer une image spécifique
et élaguer les images inutilisées sont respectivement &lt;code&gt;docker image rm&lt;/code&gt;
et &lt;code&gt;docker image prune -a&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;passer-une-commande-à-une-application-conteneurisée&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Passer une commande à une application conteneurisée&lt;/h2&gt;
&lt;p&gt;Rappelez-vous que la commande &lt;code&gt;docker run&lt;/code&gt; suit le modèle suivant :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run [options] nom-image [commande] [args]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Le paramètre optionnel &lt;em&gt;commande&lt;/em&gt; fait référence à une commande
que vous pouvez passer à l’application conteneurisée, éventuellement avec quelques arguments
(paramètre &lt;em&gt;args&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Voyons un exemple.
Comme nous l’avons vu précédemment, lorsque nous lançons un conteneur à partir de l’image &lt;em&gt;alpine&lt;/em&gt;,
un terminal Linux &lt;code&gt;/bin/sh&lt;/code&gt; est lancé.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remarque&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Le terminal Linux &lt;code&gt;/bin/sh&lt;/code&gt; est exécuté dans le conteneur.
Par la suite, nous utiliserons les termes suivants :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Terminal hôte&lt;/strong&gt; Le terminal que vous utilisez pour
interagir avec le système d’exploitation de votre ordinateur.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Terminal invité.&lt;/strong&gt; Le terminal qui s’exécute dans le container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;En utilisant le paramètre optionnel &lt;em&gt;commande&lt;/em&gt;, nous pouvons exécuter
une commande dans le terminal de l’invité.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:command-in-guest&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Lancez un conteneur à partir de l’image &lt;em&gt;alpine&lt;/em&gt; et exécutez la commande Linux
&lt;code&gt;ls&lt;/code&gt; qui liste le contenu du dossier courant.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Où sont stockés les fichiers listés ?
Dans l’hôte ou dans le conteneur ?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ls-test alpine ls&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La commande &lt;code&gt;ls&lt;/code&gt; est exécutée dans le terminal de l’invité, par conséquent
ce que nous voyons dans la sortie est une liste de fichiers stockés dans le conteneur.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remarque&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dans l’exercice &lt;a href=&#34;#exr:command-in-guest&#34;&gt;1.4&lt;/a&gt; la commande
&lt;code&gt;ls&lt;/code&gt; est exécutée dans le terminal invité, mais sa sortie
est redirigée vers le terminal hôte.&lt;/p&gt;
&lt;p&gt;En d’autres termes, lorsque nous lançons le conteneur, nous
n’interagissons pas directement avec le terminal invité ;
nous envoyons juste une commande et la sortie est redirigée
vers le terminal hôte.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On va maintenant voir comment exécuter une commande avec arguments dans le terminal invité.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
En utilisant l’utilitaire Linux &lt;code&gt;ping&lt;/code&gt;, vérifiez
si le site Web &lt;code&gt;www.centralesupelec.fr&lt;/code&gt; est accessible.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm --name ping-test alpine ping www.centralesupelec.fr&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pour interrompre &lt;code&gt;ping&lt;/code&gt;
, il suffit de taper la combinaison de touches que vous utilisez pour
interrompre n’importe quelle autre commande dans votre terminal
(typiquement Ctrl-C sous Windows et Cmd-C sous MacOs).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Maintenant, exécutez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run  --name my-alpine -it alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarque :&lt;/strong&gt; nous n’avons pas utilisé l’option &lt;code&gt;--rm&lt;/code&gt; (le conteneur ne sera pas supprimé
lorsque nous l’arrêterons, nous allons le réutiliser).
De plus, nous n’avons pas spécifié de commande à exécuter dans le terminal invité.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;Qu’obtenez-vous ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Lorsque nous exécutons un conteneur à partir de l’image &lt;em&gt;alpine&lt;/em&gt;, la commande
&lt;code&gt;/bin/sh&lt;/code&gt; est exécutée dans le conteneur.
Comme nous avons spécifié l’option &lt;code&gt;-it&lt;/code&gt;, ce que nous obtenons est un accès au terminal Linux
fonctionnant dans le conteneur.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;démarrer-et-arrêter-les-conteneurs.&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Démarrer et arrêter les conteneurs.&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run&lt;/code&gt; est un raccourci pour deux commandes Docker, à savoir
&lt;code&gt;docker create&lt;/code&gt;, qui crée un conteneur à partir d’une image,
et &lt;code&gt;docker start&lt;/code&gt;, qui démarre le conteneur après sa création.&lt;/p&gt;
&lt;p&gt;Supposons maintenant que vous souhaitiez télécharger une page Web
en utilisant Linux Alpine.
Vous pouvez utiliser la commande Linux &lt;code&gt;wget&lt;/code&gt; suivie de l’URL de la page
que vous voulez télécharger.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:wget-exo&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;En utilisant le terminal invité
dans le conteneur &lt;em&gt;my-alpine&lt;/em&gt;,
téléchargez
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;cette page Web&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Où la page Web sera-t-elle sauvegardée ? Sur l’ordinateur hôte ou dans le conteneur ?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Tapez la commande suivante dans le terminal invité :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wget https://www.centralesupelec.fr/fr/presentation&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La page Web sera enregistrée dans le dossier courant
du conteneur. Vous pouvez vérifier que le fichier est bien là
en tapant &lt;code&gt;ls&lt;/code&gt; dans le terminal de l’invité.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Voulez-vous copier le fichier téléchargé vers votre ordinateur ?
&lt;/summary&gt;
&lt;p&gt;Pour copier le fichier &lt;code&gt;presentation&lt;/code&gt; vers votre dossier de travail, tapez la commande suivante dans
le terminal de l’hôte :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker cp &amp;lt;containerId&amp;gt;:/presentation .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dans la commande précédente, remplacez &lt;code&gt;&amp;lt;containerId&amp;gt;&lt;/code&gt; par l’identifiant de votre conteneur. Vous pouvez obtenir
l’identifiant du conteneur à partir de la sortie de la commande &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Où sont stockés les fichiers du conteneur ?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez MacOS ou Windows
&lt;/summary&gt;
&lt;p&gt;Les fichiers gérés par Docker ne sont pas stockés directement sur votre ordinateur, mais
dans la machine virtuelle Linux installée et exploitée par Docker Desktop (n’oubliez pas que Docker a toujours besoin de Linux pour être exécuté).&lt;/p&gt;
&lt;p&gt;Par conséquent, vous devez ouvrir un terminal à l’intérieur de cette machine virtuelle Linux en tapant la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Une fois le terminal ouvert, vous pouvez suivre les instructions données ci-dessous pour les utilisateurs de Linux.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez Docker sur Linux
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tous les fichiers gérés par Docker sont stockés dans le dossier &lt;code&gt;/var/lib/docker&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour accéder à ce dossier, vous devez être root (c’est-à-dire administrateur). Tapez la commande &lt;code&gt;sudo su&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous tapez &lt;code&gt;ls /var/lib/docker&lt;/code&gt; vous pouvez voir les dossiers stockés dans ce dossier. Vous verrez qu’il y a des
dossiers correspondant aux différents objets gérés par Docker (conteneurs, images, volumes et réseaux).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour localiser les fichiers d’un conteneur spécifique, vous devez d’abord obtenir l’&lt;strong&gt;identifiant du conteneur&lt;/strong&gt; en tapant &lt;code&gt;docker container ls -a&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tapez la commande &lt;code&gt;docker inspect &amp;lt;container-id&amp;gt;&lt;/code&gt; (remplacez &lt;code&gt;&amp;lt;container-id&amp;gt;&lt;/code&gt; par l’identifiant du conteneur que vous avez l’intention d’inspecter).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Localisez le champ &lt;code&gt;UpperDir&lt;/code&gt;. La valeur de ce champ est le chemin vers le dossier (appelons-le &lt;code&gt;CONTAINER_DIR&lt;/code&gt;) qui contient la couche conteneur (celle qui peut être modifiée).
Ce doit être un chemin qui se termine par &lt;code&gt;/diff&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous tapez &lt;code&gt;cd CONTAINER_DIR&lt;/code&gt; (remplacez &lt;code&gt;CONTAINER_DIR&lt;/code&gt; par la valeur du champ &lt;code&gt;UpperDir&lt;/code&gt;) vous pouvez enfin voir les fichiers stockés dans
votre conteneur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;p&gt;Dans le terminal invité &lt;em&gt;my-alpine&lt;/em&gt;, tapez &lt;code&gt;exit&lt;/code&gt;.
Cela ferme le terminal invité et, par conséquent, arrête le conteneur.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remarque&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;L’arrêt du conteneur n’efface aucun des fichiers
stockés dans le conteneur. La suppression du conteneur le fera.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Si vous souhaitez redémarrer le conteneur &lt;em&gt;my-alpine&lt;/em&gt;, vous pouvez
utiliser la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker container start -ai my-alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cela ouvrira à nouveau le terminal invité du conteneur ;
tapez &lt;code&gt;ls&lt;/code&gt; pour vérifier que
la page Web que vous avez téléchargée auparavant est toujours là.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;création-dimages&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Création d’images&lt;/h1&gt;
&lt;p&gt;Une image Docker peut être considérée comme un modèle pour créer et exécuter un conteneur.
Une image est un fichier qui contient un &lt;strong&gt;système de fichiers en couches&lt;/strong&gt;, chaque couche étant &lt;strong&gt;immuable&lt;/strong&gt; ;
cela signifie que les fichiers qui appartiennent à une couche ne peuvent pas être
modifiés ou supprimés, et que des fichiers ne peuvent pas être ajoutés à une couche.&lt;/p&gt;
&lt;p&gt;Lorsqu’un conteneur est créé à partir d’une image, il
sera composé de toutes les couches en lecture seule de l’image et, par-dessus
celles-ci, d’une couche en écriture (appelée &lt;strong&gt;couche conteneur&lt;/strong&gt;),
où tous les nouveaux fichiers créés dans le conteneur seront écrits.
Par exemple, la page Web que vous avez téléchargée dans l’exercice &lt;a href=&#34;#exr:wget-exo&#34;&gt;1.7&lt;/a&gt;
a été stockée dans la couche conteneur.&lt;/p&gt;
&lt;div id=&#34;dockerfiles&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Dockerfiles&lt;/h2&gt;
&lt;p&gt;La façon la plus utilisée de créer une image est d’utiliser un &lt;strong&gt;Dockerfile&lt;/strong&gt;, un fichier texte
qui contient toutes les instructions nécessaires pour
construire l’image.
L’avantage du Dockerfile est qu’il peut être interprété
par le moteur Docker, ce qui fait de la création d’images une tâche automatisée
et répétable.&lt;/p&gt;
&lt;p&gt;Supposons que nous voulions créer une application conteneurisée
pour télécharger des images à partir d’une page Web.
Comme modèle pour cette application, nous devons construire une nouvelle image
, que nous appellerons &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Le fichier Docker contenant les instructions pour construire l’image
&lt;em&gt;fig-downloader&lt;/em&gt; est le suivant :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update
RUN apt-get install -y wget
RUN mkdir -p /my-figures
WORKDIR /my-figures
ENTRYPOINT [&amp;quot;wget&amp;quot;, &amp;quot;-nd&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-A&amp;quot;, &amp;quot;jpg,jpeg,bmp,png,gif&amp;quot;]
CMD [&amp;quot;https://www.centralesupelec.fr/fr/presentation&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Voici l’explication :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Nous utilisons l’image &lt;em&gt;ubuntu&lt;/em&gt; comme &lt;strong&gt;image de base&lt;/strong&gt;.
Cela correspond à l’instruction &lt;code&gt;FROM ubuntu&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nous installons l’utilitaire &lt;code&gt;wget&lt;/code&gt; dans l’image de base.
Cela correspond aux instructions
&lt;code&gt;RUN apt-get update&lt;/code&gt; et &lt;code&gt;RUN apt-get install -y wget&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nous créons un dossier &lt;code&gt;my-figures&lt;/code&gt; sous le dossier racine de l’image.
Cela correspond à l’instruction &lt;code&gt;RUN mkdir -p /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nous définissons le dossier &lt;em&gt;/my-figures&lt;/em&gt; comme
&lt;strong&gt;dossier de travail&lt;/strong&gt; de l’image. Cela correspond à l’instruction
&lt;code&gt;WORKDIR /my-figures&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nous spécifions la commande à exécuter lorsqu’un conteneur est lancé à partir de cette image.
Cela correspond à l’instruction :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;ENTRYPOINT [&#34;wget&#34;, &#34;-nd&#34;, &#34;-r&#34;, &#34;-A&#34;, &#34;jpg,jpeg,bmp,png,gif&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cette instruction signifie : exécuter &lt;code&gt;wget&lt;/code&gt; avec les options
&lt;code&gt;-nd&lt;/code&gt;, &lt;code&gt;-r&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt; ;
la dernière option prend une liste d’extensions de fichiers
(&lt;code&gt;jpg,jpeg,bmp,png,gif&lt;/code&gt;) en arguments.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Rappelez-vous que l’utilitaire &lt;code&gt;wget&lt;/code&gt; prend l’URL de la page Web en argument. L’URL sera spécifiée lorsque nous lancerons le conteneur à partir de
l’image &lt;em&gt;fig-downloader&lt;/em&gt;.
En option, nous pouvons spécifier un argument par défaut en utilisant le mot-clé CMD.
La signification de l’instruction :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;CMD [&#34;https://www.centralesupelec.fr/fr/presentation&#34;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;est la suivante : si nous n’indiquons pas d’URL lorsque nous lançons le conteneur, les images seront
téléchargés à partir de
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;https://www.centralesupelec.fr/fr/presentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;Quelle est la relation entre les lignes du fichier Docker et les couches de l’image ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Chaque ligne correspond à une nouvelle couche.
La première ligne correspond à la couche inférieure ;
la dernière ligne correspond à la couche supérieure.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:dockerfile-creation&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;Pouvez-vous identifier un problème dans ce Dockerfile ?
Modifiez le fichier Docker en conséquence.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Lors de la création d’une image, il faut limiter le nombre de couche ; en fait, plus il y a de couches, plus l’image sera grande.
Ici, nous créons trois couches distinctes avec trois commandes RUN ; nous pouvons
simplement fusionner les trois couches.
Le fichier Docker sera ainsi :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM ubuntu
RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y wget &amp;amp;&amp;amp; \
    mkdir -p /my-figures
WORKDIR /my-figures
ENTRYPOINT [&amp;quot;wget&amp;quot;, &amp;quot;-nd&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-A&amp;quot;, &amp;quot;jpg,jpeg,bmp,png,gif&amp;quot;]
CMD [&amp;quot;https://www.centralesupelec.fr/fr/presentation&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;création-dune-image&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Création d’une image&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant créer (en anglais, &lt;em&gt;build&lt;/em&gt;) une image à partir d’un Dockerfile.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Créez un dossier nommé &lt;em&gt;fig-downloader&lt;/em&gt; sur votre ordinateur avec
un fichier nommé &lt;em&gt;Dockerfile&lt;/em&gt; à l’intérieur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans le &lt;em&gt;Dockerfile&lt;/em&gt;, écrivez l’ensemble des instructions que
vous avez proposées dans l’exercice &lt;a href=&#34;#exr:dockerfile-creation&#34;&gt;2.2&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;En utilisant le terminal, déplacez-vous dans le dossier de travail &lt;em&gt;fig-downloader&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Créez une image appelée &lt;em&gt;fig-downloader&lt;/em&gt; en exécutant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Le &lt;code&gt;.&lt;/code&gt; à la fin de la commande signifie que le moteur Docker
cherchera un fichier nommé &lt;em&gt;Dockerfile&lt;/em&gt; dans le dossier de travail.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous donnez au Dockerfile un nom différent (par exemple, &lt;em&gt;Dockerfile-fig-downloader&lt;/em&gt;),
la commande pour créer l’image sera :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker build -t fig-downloader -f Dockerfile-fig-downloader .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;L’option &lt;code&gt;-f&lt;/code&gt; est utilisée pour spécifier le nom du Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Afin de vérifier que la nouvelle image a été créée, tapez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;
&lt;!-- Let&#39;s dive deeper into the anatomy of an image.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercice**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-7&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-7) &lt;/strong&gt;&lt;/span&gt;Run the following command:

``
docker history fig-downloader
`` 

and analyze the layers of the new image. 

* Why do some layers have an ID, while others 
are marked as &lt;i&gt;missing&lt;/i&gt;?

::::: {.last-child}
* Can you find the identifiers of the intermediate images?
:::::
&lt;/div&gt;\EndKnitrBlock{exercise}


:::

&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The layers with an ID correspond to the layers of 
the new image, including the top layer and the base image.
The layers marked as *missing* are those that compose the 
base image. Those layers are not stored in  
your computer, 
simply because they belong to an image 
that hasn&#39;t been built on your computer 
and you downloaded from the DockerHub registry.

By looking at the output of ``docker image ls -a`` and the output of this command,
we see that the layers between the base image and the top layer have the 
same identifiers as the intermediate images.

:::

&lt;/details&gt;

--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:dl-1-container&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;Exécutez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --name dl-1 fig-downloader&lt;/code&gt;&lt;/p&gt;
Que fait-il ? Où se trouvent les images téléchargées ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous avons téléchargé les figures
&lt;a href=&#34;https://www.centralesupelec.fr/fr/presentation&#34; target=&#34;_blank&#34;&gt;de cette page&lt;/a&gt;.
Les images téléchargées se trouvent dans le dossier &lt;em&gt;/my-figures&lt;/em&gt;
du conteneur &lt;em&gt;dl-1&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;Exécutez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --name dl-2 fig-downloader https://www.centralesupelec.fr/&lt;/code&gt;&lt;/p&gt;
Qu’est-ce que cela fait ? Où se trouvent les images téléchargées ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous avons téléchargé les images
&lt;a href=&#34;https://www.centralesupelec.fr&#34; target=&#34;_blank&#34;&gt;de cette page&lt;/a&gt;.
En fait, nous avons remplacé l’URL spécifiée par le mot-clé CMD par une nouvelle URL.
Les images téléchargées se trouvent dans le dossier &lt;em&gt;/my-figures&lt;/em&gt;
du conteneur &lt;em&gt;dl-2&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;containerized-python-application&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Containerized Python application&lt;/h2&gt;
&lt;p&gt;Téléchargez &lt;a href=&#34;/courses/cloud-computing/tutorial-docker/word-frequency.zip&#34;&gt;cet archive&lt;/a&gt;
et décompressez-le dans votre dossier de travail.
Dans cette archive, vous trouverez :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un Dockerfile.&lt;/li&gt;
&lt;li&gt;Un script Python &lt;em&gt;main.py&lt;/em&gt; qui demande à l’utilisateur d’entrer l’URL et la langue d’une page Web,
et imprime les 10 mots les plus fréquents dans cette page.&lt;/li&gt;
&lt;li&gt;Un fichier &lt;em&gt;requirements.txt&lt;/em&gt; contenant la liste des paquets Python
nécessaires pour exécuter le script donné.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le contenu du Dockerfile est le suivant :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./main.py ./requirements.txt /app/
RUN pip install -r requirements.txt
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;Décrivez ce que fait ce Dockerfile.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Prend &lt;em&gt;python:3.7-slim&lt;/em&gt; comme image de base.&lt;/li&gt;
&lt;li&gt;Crée un nouveau dossier &lt;em&gt;app&lt;/em&gt; dans l’image sous le dossier racine.&lt;/li&gt;
&lt;li&gt;Modifie le dossier de travail en &lt;em&gt;/app&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Copie les fichiers &lt;em&gt;main.py&lt;/em&gt; et &lt;em&gt;requirements.txt&lt;/em&gt;
vers le dossier &lt;em&gt;/app&lt;/em&gt; stocké dans l’image.&lt;/li&gt;
&lt;li&gt;Exécute la commande &lt;code&gt;pip install&lt;/code&gt; pour installer les bibliothèques Python
spécifiées dans le fichier &lt;em&gt;requirements.txt&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Exécute la commande &lt;code&gt;python main.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34; word-latex=&#34;{exercisebox}&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;Créez une image &lt;em&gt;wordfreq&lt;/em&gt; en utilisant ce Dockerfile.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker build -t wordfreq .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;Sans modifier le Dockerfile, récréez la même image.
Que constatez-vous ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La création est très rapide.
Comme nous n’avons pas changé le Dockerfile, l’image
est récréée en utilisant les couches de l’image créée
précédemment.
Ceci est clairement indiqué par le mot &lt;strong&gt;CACHED&lt;/strong&gt; écrit
à côté de chaque couche.
L’utilisation des couches déjà stockées est appelée &lt;strong&gt;build cache&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;Que se passe-t-il si vous modifiez une ligne du script Python et que
vous récréez l’image ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Ajoutez une instruction à la fin de &lt;em&gt;main.py&lt;/em&gt;, comme par exemple :&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Finish!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ensuite récréez l’image.
Les trois couches inférieures ne sont pas affectées par la modification, donc
elles bénéficient du build cache.
La couche 4 est la première à être affectée par la modification.
Cette couche, ainsi que les couches supérieures, doivent donc être récrées.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.9  &lt;/strong&gt;&lt;/span&gt;Sur la base des considérations précédentes,
pouvez-vous dire ce qui ne va pas avec ce Dockerfile ?
Modifiez le Dockerfile en conséquence et reconstruisez l’image.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Chaque fois que nous modifions &lt;em&gt;main.py&lt;/em&gt; et que nous récréons l’image,
les couches 4 et 5 sont recréées, ce qui signifie que tous les paquets Python
sont téléchargés et installés.
En fonction de la taille et du nombre de paquets, cela peut
prendre un certain temps.
Une meilleure façon de structurer le Dockerfile est d’installer les paquets
avant de copier le script Python vers l’image.
Voici comment modifier le Dockerfile :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM python:3.7-slim
RUN mkdir -p /app
WORKDIR /app
COPY ./requirements.txt /app/
RUN pip install -r requirements.txt
COPY ./main.py /app/
ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.10  &lt;/strong&gt;&lt;/span&gt;Modifiez &lt;em&gt;main.py&lt;/em&gt; en ajoutant une nouvelle ligne de code et récréez l’image.
Qu’est-ce qui a changé ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Les paquets Python ne sont pas réinstallés ; la création de l’image
prend beaucoup moins de temps qu’auparavant.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Jouez avec l’application en exécutant la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it wordfreq&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;L’application conteneurisée vous demandera d’insérer l’URL d’une page web et
la langue de la page (en anglais).
Le résultat sera les 20 mots les plus utilisés dans la page web.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;volumes-de-données&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Volumes de données&lt;/h1&gt;
&lt;p&gt;Dans l’exercice &lt;a href=&#34;#exr:dl-1-container&#34;&gt;2.3&lt;/a&gt;, il vous a été demandé d’exécuter un conteneur nommé
&lt;em&gt;dl-1&lt;/em&gt; pour télécharger des chiffres à partir d’une page Web.
Les figures ont été téléchargées dans le dossier
&lt;em&gt;/my-figures&lt;/em&gt; du conteneur.
Mais nous avons laissé une question sans réponse.&lt;/p&gt;
&lt;p&gt;**Comment transférer ces chiffres du conteneur vers l’ordinateur hôte ?&lt;/p&gt;
&lt;p&gt;La solution consiste à utiliser des &lt;strong&gt;volumes&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;docker-volumes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Docker volumes&lt;/h2&gt;
&lt;p&gt;Un volume peut être considéré comme un périphérique de stockage virtuel attaché à un conteneur.
Tous les fichiers sont écrits sur un volume qui ne sera pas supprimé lorsque le conteneur est supprimé.&lt;/p&gt;
&lt;p&gt;Créons un nouveau volume Docker appelé &lt;em&gt;data-volume&lt;/em&gt; :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume create data-volume&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir (notion avancée)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Où les données seront-elles réellement stockées ? &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vous pouvez inspecter le nouveau volume en tapant la commande
suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume inspect data-volume&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Un &lt;em&gt;point de montage&lt;/em&gt; est indiqué ; il s’agit du dossier dans lequel les données
seront effectivement stockées.
Si vous êtes sou Linux, ce dossier sera disponible sur l’hôte ;
si vous êtes sous Windows ou MacOS,
vous ne trouverez pas ce dossier sur votre ordinateur.
Il sera disponible dans la machine virtuelle
que Docker utilise sur MacOS et Windows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voulez-vous voir le dossier ? (Instructions pour Windows et MacOS)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Une façon de voir la VM cachée est d’exécuter
l’application conteneurisée suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run -it --rm --privileged --pid=host justincormack/nsenter1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cette application ouvrira un terminal invité dans la VM.
Vous pouvez ensuite utiliser les commandes &lt;code&gt;cd&lt;/code&gt; et &lt;code&gt;ls&lt;/code&gt;
pour naviguer jusqu’au dossier indiqué comme chemin de montage
du nouveau volume.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partage-de-données&#34; class=&#34;section level3&#34; number=&#34;3.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1.1&lt;/span&gt; Partage de données&lt;/h3&gt;
&lt;p&gt;Un volume Docker peut être utilisée pour permettre à deux conteneurs Docker de partager des données.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;Exécutez un conteneur à partir de l’image &lt;code&gt;ubuntu&lt;/code&gt;,
en spécifiant les options pour :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;supprimer le conteneur une fois son exécution terminée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interagir avec le terminal Linux invité dans le conteneur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;monter le volume &lt;em&gt;data-volume&lt;/em&gt; dans le dossier &lt;em&gt;/data&lt;/em&gt; du conteneur.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it -v data-volume:/data ubuntu&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Tapez la commande suivante dans le terminal Linux invité pour créer un fichier
&lt;em&gt;test-file.txt&lt;/em&gt; dans le dossier &lt;em&gt;/data&lt;/em&gt; :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;echo &#34;This is a new file&#34; &amp;gt; /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Affichez sur le terminal le contenu du fichier avec la commande suivante :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;cat /data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Tapez &lt;code&gt;exit&lt;/code&gt; pour quitter le terminal invité. Puisque nous avons spécifié l’option &lt;code&gt;--rm&lt;/code&gt;, le conteneur
est &lt;strong&gt;supprimez&lt;/strong&gt;. Maintenant, nous allons vérifier que &lt;code&gt;test-file.txt&lt;/code&gt; est toujours accessible.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;Exécutez un conteneur à partir de l’image &lt;em&gt;alpine:latest&lt;/em&gt;,
en spécifiant les options pour :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;supprimer le conteneur une fois son exécution terminée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interagir avec le terminal Linux invité dans le conteneur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;monter le volume &lt;em&gt;data-volume&lt;/em&gt; dans le dossier &lt;em&gt;/my-data&lt;/em&gt;
du conteneur.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;docker container run --rm -it -v data-volume:/my-data alpine&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;Vérifiez que vous pouvez lire le fichier &lt;em&gt;test-file.txt&lt;/em&gt;.
Dans quel dossier allez-vous chercher ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous devons regarder dans le dossier &lt;em&gt;/my-data&lt;/em&gt; car c’est là que
nous avons monté &lt;em&gt;data-volume&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cat /my-data/test-file.txt&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Tapez &lt;code&gt;exit&lt;/code&gt; pour sortir du terminal invité et arrêter le conteneur.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;réseaux&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Réseaux&lt;/h1&gt;
&lt;p&gt;Afin de permettre aux conteneurs de communiquer et, par conséquent, de coopérer,
Docker définit un modèle de réseau simple connu sous le nom de
&lt;strong&gt;container network model&lt;/strong&gt; (figure (fig:cnm-docker)).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez une machine virtuelle Linux avec Multipass
&lt;/summary&gt;
&lt;p&gt;Dans cette section, vous devrez ouvrir plusieurs terminaux dans la machine virtuelle.
Vous pouvez le faire facilement en utilisant &lt;code&gt;byobu&lt;/code&gt;, un gestionnaire de fenêtres avancé déjà disponible dans votre machine virtuelle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tapez simplement &lt;code&gt;byobu&lt;/code&gt; pour lancer le gestionnaire de fenêtres.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez ouvrir un nouveau terminal, appuyez simplement sur F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez passer d’un terminal à un autre, appuyez simplement sur F3 (pour passer au terminal précédent) ou F4
(pour passer au terminal suivant).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez fermer un terminal, tapez simplement &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque vous fermez tous les terminaux, &lt;code&gt;byobu&lt;/code&gt; cesse de s’exécuter.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm-docker&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-docker/cnm.png&#34; alt=&#34;Le modèle de réseau de Docker&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: Le modèle de réseau de Docker
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;Décrivez la sortie de la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La commande répertorie tous les réseaux créés par Docker sur
votre ordinateur.
Pour chaque réseau, les valeurs de quatre attributs sont affichées :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L’identifiant.&lt;/li&gt;
&lt;li&gt;Le nom.&lt;/li&gt;
&lt;li&gt;Le pilote utilisé par le réseau.&lt;/li&gt;
&lt;li&gt;La portée du réseau (locale ou globale).
Une portée locale signifie que le réseau relie des conteneurs
fonctionnant sur le même hôte, alors qu’une portée globale&lt;br /&gt;
signifie que des conteneurs situés sur des hôtes différents peuvent communiquer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il est possible que vous voyez différents réseaux, si vous avez déjà utilisé Docker avant ce cours.
Cependant, trois réseaux méritent d’être signalés :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Le réseau nommé &lt;strong&gt;bridge&lt;/strong&gt;, qui utilise le pilote &lt;strong&gt;bridge&lt;/strong&gt; et a une portée locale.
Par défaut, tout nouveau conteneur est attaché à ce réseau.&lt;/li&gt;
&lt;li&gt;Le réseau nommé &lt;strong&gt;host&lt;/strong&gt;, qui utilise le pilote &lt;strong&gt;host&lt;/strong&gt; et a une portée locale.
Il est utilisé lorsque nous voulons qu’un conteneur utilise directement l’interface réseau de l’hôte.
Il est important de se rappeler que ce réseau ne doit être utilisé que pour analyser le trafic réseau de l’hôte. Dans les autres cas, l’utilisation de ce réseau expose
le conteneur à toutes sortes de risques de sécurité.&lt;/li&gt;
&lt;li&gt;Le réseau nommé &lt;strong&gt;none&lt;/strong&gt;, qui utilise le pilote &lt;strong&gt;null&lt;/strong&gt; et a une portée locale.
Attacher un conteneur à ce réseau signifie que le conteneur
n’est connecté à aucun réseau, et qu’il est donc complètement isolé.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;La commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect bridge&lt;/code&gt;&lt;/p&gt;
affiche la configuration du réseau &lt;strong&gt;bridge&lt;/strong&gt;.
En regardant cette configuration, pouvez-vous dire
quelles adresses IP seront données aux conteneurs attachés à ce réseau
? Quelle est l’adresse IP du routeur de ce réseau ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Les informations sont spécifiées dans le champ nommé &lt;strong&gt;IPAM&lt;/strong&gt;, plus précisément :&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sous-réseau&lt;/strong&gt; indique la plage d’adresses IP utilisée par le réseau.
La valeur de ce champ doit être 172.17.0.0/16 ;
les adresses vont de 172.17.0.1 à 172.17.255.255.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gateway&lt;/strong&gt; indique l’adresse IP du routeur du réseau.
La valeur doit être 172.17.0.1.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;création-des-réseaux&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Création des réseaux&lt;/h2&gt;
&lt;p&gt;Par défaut, tout nouveau conteneur est rattaché au réseau nommé &lt;em&gt;bridge&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Expliquez pourquoi ce n’est pas une bonne pratique d’attacher
tous nos conteneurs au même réseau.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Tous les nouveaux conteneurs pourront communiquer sur ce réseau.
Ce n’est pas une bonne idée.
Si un attaquant parvient à compromettre l’un de ces conteneurs, il pourrait
être en mesure d’attaquer également les autres conteneurs.
En règle générale, nous devrions attacher deux conteneurs au même réseau &lt;strong&gt;uniquement&lt;/strong&gt; s’il est nécessaire qu’ils communiquent.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Nous pouvons utiliser la commande suivante pour créer un réseau.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create network_name&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;Créez deux réseaux nommés &lt;em&gt;buckingham&lt;/em&gt; et &lt;em&gt;rochefort&lt;/em&gt; qui
utilisent le pilote &lt;em&gt;bridge&lt;/em&gt; (voir figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;).
En utilisant la commande &lt;code&gt;docker network inspect&lt;/code&gt;,
regardez les adresses IP des nouveaux réseaux et notez-les.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Exécutez les commandes suivantes :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network create rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Les adresses IP du réseau &lt;em&gt;buckingham&lt;/em&gt; sont
172.18.0.0/16 (adresses de 172.18.0.1 à 172.18.255.255) ;
Les adresses IP du réseau &lt;em&gt;rochefort&lt;/em&gt; sont :
172.19.0.0/16 (en supposant que vous créez &lt;em&gt;buckingham&lt;/em&gt;
avant &lt;em&gt;rochefort&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Les adresses IP peuvent être différentes sur vos machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;
Créez trois conteneurs &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;porthos&lt;/em&gt; et &lt;em&gt;aramis&lt;/em&gt; et attachez-les
aux deux réseaux &lt;em&gt;buckingham&lt;/em&gt; et &lt;em&gt;rochefort&lt;/em&gt; comme indiqué
dans la figure &lt;a href=&#34;#fig:cnm-docker&#34;&gt;4.1&lt;/a&gt;.
&lt;strong&gt;Les trois conteneurs ouvriront un shell Linux Alpine&lt;/strong&gt;.
Vous devrez lancer les commandes dans trois onglets distincts de votre terminal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quelles seront les adresses IP des trois conteneurs dans les deux réseaux ?
Rappelez-vous que &lt;em&gt;porthos&lt;/em&gt; est attaché à deux réseaux, il aura donc deux interfaces réseau
(endpoints) et, par conséquent, deux adresses IP.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez vos réponses en inspectant les deux réseaux (utilisez la commande
&lt;code&gt;docker network inspect&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Voici les commandes pour exécuter &lt;em&gt;athos&lt;/em&gt; et &lt;em&gt;aramis&lt;/em&gt; tout en les connectant
à &lt;em&gt;buckingham&lt;/em&gt; et &lt;em&gt;rochefort&lt;/em&gt; respectivement.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name athos --network buckingham  alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name aramis --network rochefort   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Voici la commande pour exécuter &lt;em&gt;porthos&lt;/em&gt; et l’attacher à
&lt;em&gt;buckingham&lt;/em&gt; :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -it --name porthos --network buckingham   alpine&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;La commande suivante attache &lt;em&gt;porthos&lt;/em&gt; au second réseau &lt;em&gt;rochefort&lt;/em&gt; :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network connect rochefort porthos&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;En ce qui concerne les adresses IP, chaque réseau a des adresses IP
dans l’intervalle 172.x.0.0/16, où x est 18 dans le réseau
&lt;em&gt;buckingham&lt;/em&gt; et 19 dans le réseau &lt;em&gt;rochefort&lt;/em&gt;.
L’adresse 172.x.0.1 est réservée au routeur.
Par conséquent, les conteneurs se verront attribuer des adresses IP
à partir de 172.x.0.2.
Dans cette solution, nous avons créé &lt;em&gt;athos&lt;/em&gt;, &lt;em&gt;aramis&lt;/em&gt; et &lt;em&gt;portos&lt;/em&gt;
dans cet ordre.
Par conséquent, les adresses IP seront les suivantes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Réseau &lt;em&gt;buckingham&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;athos&lt;/em&gt;: 172.18.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.18.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Réseau &lt;em&gt;rochefort&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;aramis&lt;/em&gt;: 172.19.0.2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;porthos&lt;/em&gt;: 172.19.0.3&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vous pouvez vérifier cette configuration en inspectant
les deux réseaux avec les commandes suivantes :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect buckingham&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network inspect rochefort&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Les adresses IP peuvent être différentes sur vos machines.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;communication-entre-les-conteneurs&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Communication entre les conteneurs&lt;/h2&gt;
&lt;p&gt;Maintenant on va vérifier quels conteneuers arrivent à communiquer entre eux.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;Quels conteneurs arrivent à communiquer ? Justifiez votre réponse.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Les seuls conteneurs qui ne peuvent pas communiquer sont &lt;em&gt;athos&lt;/em&gt; et &lt;em&gt;aramis&lt;/em&gt;,
parce qu’ils ne sont pas connectés au même réseau.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;
Essayez d’envoyer un ping à &lt;em&gt;porthos&lt;/em&gt; depuis &lt;em&gt;athos&lt;/em&gt; en utilisant son adresse IP.&lt;/p&gt;
&lt;div class=&#34;last-child&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Quelle adresse IP de &lt;em&gt;porthos&lt;/em&gt; utiliseriez-vous ?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous devons utiliser l’adresse IP attribuée à l’&lt;em&gt;endoint&lt;/em&gt; reliant
&lt;em&gt;porthos&lt;/em&gt; au réseau &lt;em&gt;buckingham&lt;/em&gt;, auquel &lt;em&gt;athos&lt;/em&gt; est connecté.
Dans notre cas, il s’agit de 172.18.0.3.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-25&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.8  &lt;/strong&gt;&lt;/span&gt;Essayez d’envoyer un ping à &lt;em&gt;porthos&lt;/em&gt; depuis &lt;em&gt;athos&lt;/em&gt; en utilisant son nom.
Réussissez-vous ? Êtes-vous surpris ?
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous y parvenons. En effet, le réseau &lt;em&gt;buckingham&lt;/em&gt; fournit un serveur DNS, qui
peut traduire les noms en adresses IP.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Vous pouvez maintenant arrêter les conteneurs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-service applications</title>
      <link>/courses/cloud-computing/tutorials/tutorial-kube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-computing/tutorials/tutorial-kube/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you use the Linux VM&lt;/strong&gt;, you need to change the hardware configuration of the VM before
booting it. You can also &lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v12619ffbdbb7mpc6xbp/&#34; target=&#34;_blank&#34;&gt;watch this video&lt;/a&gt;.
Here are the necessary modifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Increase the main memory to 4GB.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set the number of CPUs to 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without these modifications you’re not going to have a good experience with Kubernetes.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;build&lt;/strong&gt; and &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Docker Compose&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;push an image&lt;/strong&gt; to &lt;strong&gt;DockerHub&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Kubernetes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This tutorial is adapted from the examples presented in Chapters
14 and 20 of the book &lt;a href=&#34;https://www.packtpub.com/product/learn-docker-fundamentals-of-docker-19-x-second-edition/9781838827472&#34; target=&#34;_blank&#34;&gt;G. Schenker, &lt;em&gt;Learn Docker - Fundamentals of Docker 19.x&lt;/em&gt; (March 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;docker-compose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Docker Compose&lt;/h1&gt;
&lt;p&gt;In this section, you’re going to build and deploy a multi-service application
by using &lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-kube/pet-store.zip&#34;&gt;this archive file&lt;/a&gt; and unzip it into a folder on your
own computer.
The archive contains all the necessary files to build and run
a web application consisting of two services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;web&lt;/strong&gt;. This is the &lt;strong&gt;frontend&lt;/strong&gt; (the part the user interacts with)
of the application.
It consists of HTML and JavaScript code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt;. This is the &lt;strong&gt;backend&lt;/strong&gt; (the part hidden to the user).
It is a &lt;a href=&#34;https://www.postgresql.org/&#34; target=&#34;_blank&#34;&gt;PostgreSQL&lt;/a&gt; (relational) database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The structure of the application is shown in Figure &lt;a href=&#34;#fig:pet-store-struct&#34;&gt;1.1&lt;/a&gt;.
The root directory of the application contains two subdirectories, one for each service
(&lt;code&gt;database&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pet-store-struct&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-kube/pet-store-files.png&#34; alt=&#34;Structure of the application&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Structure of the application
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The files with extension &lt;code&gt;.conf&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contain
configuration parameters of the PostgreSQL database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The file &lt;code&gt;init-db.sql&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contains the SQL queries
to populate the database with some data (photos of cats).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The directory &lt;code&gt;web&lt;/code&gt; contains the HTML and JavaScript code of the web application.
The file &lt;code&gt;package.json&lt;/code&gt; contains the dependencies to install.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Both directories contain a &lt;strong&gt;Dockerfile&lt;/strong&gt;.
The one in directory &lt;code&gt;database&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM postgres
COPY init-db.sql /docker-entrypoint-initdb.d/
RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql
ENV POSTGRES_USER dockeruser
ENV POSTGRES_PASSWORD dockerpass
ENV POSTGRES_DB pets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dockerfile builds on an existing image called &lt;code&gt;postgres&lt;/code&gt;,
that is &lt;a href=&#34;https://hub.docker.com/_/postgres&#34; target=&#34;_blank&#34;&gt;documented here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Consider the following line in the Dockerfile:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;COPY init-db.sql /docker-entrypoint-initdb.d/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;By looking at the documentation of the image &lt;code&gt;postgres&lt;/code&gt;,
answer the two following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where is the directory &lt;code&gt;/docker-entrypoint-initdb.d/&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why do we copy the file &lt;code&gt;init-db.sql&lt;/code&gt; to this directory?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The last three lines of the Dockerfile contain a keyword (&lt;code&gt;ENV&lt;/code&gt;)
that we never came across before.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Look again at the documentation of the image &lt;code&gt;postgres&lt;/code&gt;
and try to explain the meaning of the last three lines of the Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The Dockerfile of the web application is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM node:9.6-alpine
RUN mkdir /app
WORKDIR /app
COPY package.json /app/
RUN npm install
COPY ./src /app/src
EXPOSE 3000
CMD node src/server.js&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dockerfile builds on the image &lt;code&gt;node:9.6-alpine&lt;/code&gt; that contains
a Node.js environment, a JavaScript-based platform for server-side
applications.
The instructions in the Dockerfile look like the ones of the examples
that we’ve seen in the first tutorial and in the lectures.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The command &lt;code&gt;npm install&lt;/code&gt; installs all the dependencies specified
in the file &lt;code&gt;package.json&lt;/code&gt;
from the &lt;a href=&#34;https://docs.npmjs.com/about-npm&#34; target=&#34;_blank&#34;&gt;software registry npm&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instruction &lt;code&gt;EXPOSE 3000&lt;/code&gt; informs that the container
listens on port 3000 when it is executed from the image.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#expose&#34; target=&#34;_blank&#34;&gt;Docker documentation&lt;/a&gt;
we learn
that the &lt;em&gt;“&lt;code&gt;EXPOSE&lt;/code&gt; instruction
does not actually publish the port.
It functions as a type of
documentation between
the person who builds the image
and the person who runs the container, about which ports are intended to be published”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In order to actually publish the port, we’ll use the file
&lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;describing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Describing the application&lt;/h2&gt;
&lt;p&gt;The application showcases a building block of a pet store.
In the current version, the application shows a few pictures
of cats.&lt;/p&gt;
&lt;p&gt;The root directory of the application contains a file named
&lt;code&gt;docker-compose.yml&lt;/code&gt; that contains the &lt;strong&gt;declarative configuration&lt;/strong&gt;
of the application.
The content of the file is as follows. It is a sequence of key-value pairs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: &amp;quot;3.6&amp;quot;
services:
  web:
    build: web
    image: pet-store-web
    networks:
      - backend
    ports:
      - 5000:3000
  db:
    build: database
    image: pet-store-db
    networks:
      - backend
    volumes:
      - pets-data:/var/lib/postgresql/data

networks:
  backend:

volumes:
  pets-data:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are three main sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;services&lt;/code&gt;. Defines the services of the application. Here two services
are defined: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;networks&lt;/code&gt;. Defines the networks used by the application.
Here one network is defined: &lt;code&gt;backend&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;volumes&lt;/code&gt;. Defines the volumes used by the application.
Here one volume is defined: &lt;code&gt;pets-data&lt;/code&gt;.
The volume is attached to the directory &lt;code&gt;/var/lib/postgresql/data&lt;/code&gt;
(that is in the container).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
What key informs &lt;code&gt;docker compose&lt;/code&gt; where to find the
Dockerfile of the two services?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;When we’ll build the application from the file &lt;code&gt;docker-compose.yml&lt;/code&gt;,
two images will be created, one for each service.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
What will the name of the two images be?
What key in the file &lt;code&gt;docker-compose.yml&lt;/code&gt; gives you this information?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-application&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Building the application&lt;/h2&gt;
&lt;p&gt;We now build the application.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open the command-line terminal and by using the command &lt;code&gt;cd&lt;/code&gt;
position yourself in the root directory
&lt;code&gt;pet-store&lt;/code&gt; of the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;During the build you might get a &lt;code&gt;npm&lt;/code&gt; warning. Just ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the build is complete, verify that the two images corresponding
to the two services have been created (which docker command do you need here?).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;executing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Executing the application&lt;/h2&gt;
&lt;p&gt;We now execute the application with the following command:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In this section, you’ll need to open several terminals in the virtual machine.
You can do it easily by using &lt;code&gt;byobu&lt;/code&gt;, an advanced window manager already available in your virtual machine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just type &lt;code&gt;byobu&lt;/code&gt; to launch the window manager.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to open a new terminal, just press F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4
(to move to next terminal).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to close a terminal, just type &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you close all terminals, &lt;code&gt;byobu&lt;/code&gt; will stop executing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;p&gt;&lt;code&gt;docker-compose up&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If your application fails to start because the port number that you chose is already in use,
try using another port.&lt;/p&gt;
&lt;p&gt;Don’t use the following ports as they are deemed unsafe by the browsers:&lt;/p&gt;
&lt;p&gt;1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The execution of the command will print a series of messages on screen.
The terminal should hang when the messages stop
(the last message should be &lt;code&gt;database system is ready to accept connections&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Take no action on the screen and open a new terminal or a new tab in the current terminal
window.&lt;/p&gt;
&lt;!--
:::{.infobox .curiosity data-latex=&#34;curiosity&#34;}
**Good to know**

The option ``-d`` in the previous command means that the application is 
executed **in the background** or, in other words, 
the containers are run as **daemons** (hence, the ``-d``).
This means that any output of the application is not
shown in the terminal.

:::
--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Verify that the network and the volumes associated with the
application have been correctly created.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
How many containers do you expect to be associated to
the application?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You can verify the answer to the previous question by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose ps&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command is exactly equivalent to &lt;code&gt;docker container ls&lt;/code&gt;, except that
it only shows the containers associated with the application
that we’ve just executed.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;
In the output of the command &lt;code&gt;docker-compose ps&lt;/code&gt;, can you explain
the meaning of the following?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.0.0.0:5000-&amp;gt;3000/tcp&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;
Can you
tell which URL you have to type in your web browser
to access the application?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In order to open a Web browser window in the VM, you need to open a new
terminal window on your computer and follow these instructions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;multipass ls&lt;/code&gt; to list all the virtual machines managed by &lt;code&gt;Multipass&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the first IP address associated to the virtual machine &lt;code&gt;cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;left&#34;&gt;
&lt;code&gt;xpra start ssh://ubuntu@ip/ --start=firefox&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;where you’ll replace &lt;code&gt;ip&lt;/code&gt; with the IP address that you copied above.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A Firefox window should appear.&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to see the cats, you need to append
&lt;code&gt;/pet&lt;/code&gt; to the URL that you found in the previous question.
This is determined in file &lt;code&gt;server.js&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;You can shut down the application by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose down&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.9  &lt;/strong&gt;&lt;/span&gt;
Does shutting down the application remove the networks created
for the application?
What about the volumes?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-a-service&#34; class=&#34;section level2&#34; number=&#34;1.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Scaling a service&lt;/h2&gt;
&lt;p&gt;When we launch the application, we can specify the
number of instances of each service.
This is useful when we expect our application to be solicited by many users;
the workload will be automatically balanced across all the instances of the service.&lt;/p&gt;
&lt;p&gt;Let’s launch our application with 3 instances of the service &lt;code&gt;web&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose up --scale web=3 -d&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.10  &lt;/strong&gt;&lt;/span&gt;
Running the previous command results in an error.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can you explain why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What fix would you propose in file &lt;code&gt;docker-compose.yml&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to shut down the application before relaunching it again.
In fact, even if we got an error, an instance of the &lt;code&gt;db&lt;/code&gt; service and
an instance of the service &lt;code&gt;web&lt;/code&gt; are still running.&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modify the file &lt;code&gt;docker-compose.yml&lt;/code&gt; and relaunch the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the following command and verify that you actually have three running instances of the
service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose ps&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try to connect to the application by using the three port numbers indicated in the output
of the previous command.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Don’t forget to shut down the application before you
go on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-an-application&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Pushing an application&lt;/h1&gt;
&lt;p&gt;Once we have built an application, we might want to
share it by pushing it to the DockerHub registry or any
other container registry, be it private or public.&lt;/p&gt;
&lt;div id=&#34;creating-an-account-on-dockerhub&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Creating an account on DockerHub&lt;/h2&gt;
&lt;p&gt;You need to create an account on the &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34;&gt;DockerHub website&lt;/a&gt;
in order to perform this activity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renaming-the-images&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Renaming the images&lt;/h2&gt;
&lt;p&gt;In order to push your images to the registry, you need to rename them, so as the new name
has the following structure:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yourusername/image-name:image-tag&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For instance, since my username is &lt;code&gt;quercinigia&lt;/code&gt;, I’ll rename
the two images &lt;code&gt;pet-store-web&lt;/code&gt; and &lt;code&gt;pet-store-db&lt;/code&gt; with the
&lt;code&gt;docker tag&lt;/code&gt; command as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-web quercinigia/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-db quercinigia/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I chosen 1.0 as a tag, but feel free to pick another one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-in&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Logging in&lt;/h2&gt;
&lt;p&gt;You need to log in to your DockerHub account.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;macOS users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-macos/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Windows users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-windows/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for the Linux VM
&lt;/summary&gt;
&lt;p&gt;Log in to DockerHub by typing the following command in the terminal
(replace YOUR-USERNAME with your actual username).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker login -u YOUR-USERNAME&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You might get a warning that
your password will be stored unencrypted.
There are methods to prevent this from happening.
These methods being out of the scope of this course,
the interested reader can
look them up &lt;a href=&#34;https://www.techrepublic.com/article/how-to-setup-secure-credential-storage-for-docker/&#34; target=&#34;_blank&#34;&gt;at this link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-the-images&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Pushing the images&lt;/h2&gt;
&lt;p&gt;Once you’re successfully logged in, you can type the following
commands in the terminal to push the two images of your application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Remember to replace YOUR-USERNAME with your actual username in the commands above.&lt;/p&gt;
&lt;p&gt;After the task is completed, verify that the images appear
in &lt;a href=&#34;https://hub.docker.com/repositories&#34; target=&#34;_blank&#34;&gt;your Docker registry&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here we manually tagged and uploaded the two images.
This method becomes quickly annoying when the application consists of
more than two images.
Another way to go about this task is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specify the image names with your username in file &lt;strong&gt;docker-compose.yml&lt;/strong&gt;.
For instance, in my &lt;strong&gt;docker-compose.yml&lt;/strong&gt; I would write:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;services:
  web:
    build: web
    image: quercinigia/pet-store-web:1.0
  ...
  db:
    build: database
    image: quercinigia/pet-store-db:1.0
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Build the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Push the images of the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This way, with just two commands we push to the registry all the images
of the application, no matter how many they are.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-kubernetes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Introduction to Kubernetes&lt;/h1&gt;
&lt;p&gt;Kubernetes is the most popular &lt;strong&gt;orchestrator&lt;/strong&gt; to date.
It is used to manage a multi-service application, usually deployed
across multiple &lt;strong&gt;hosts&lt;/strong&gt; in a &lt;strong&gt;cluster&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;While using Docker Compose, a &lt;strong&gt;service&lt;/strong&gt; corresponds
to a &lt;strong&gt;Docker image&lt;/strong&gt; and
a &lt;strong&gt;service instance&lt;/strong&gt; to a &lt;strong&gt;Docker container&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As opposed to that, in Kubernetes the computation unit is a &lt;strong&gt;pod&lt;/strong&gt;, that is
a &lt;strong&gt;collection of containers&lt;/strong&gt;.
In other words, we don’t reason in terms of containers anymore,
but in terms of pods. Of course, a pod can also consist of just one container.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In practice, we rarely have to manipulate pods directly in Kubernetes,
as there are higher-level objects that manage them.
We’ll use these objects in the next sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activate-kubernetes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Activate Kubernetes&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;p&gt;You need to follow all the instructions documented
&lt;a href=&#34;https://docs.docker.com/desktop/kubernetes/&#34; target=&#34;_blank&#34;&gt;on this page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Before moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see a node called &lt;code&gt;docker-desktop&lt;/code&gt; whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This solution might not be working for you.
In that case, disable Kubernetes in Docker Desktop and install
minikube, by following &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for the Linux VM
&lt;/summary&gt;
&lt;p&gt;In the Linux VM you’ll find &lt;strong&gt;Minikube&lt;/strong&gt;, a single-node Kubernetes cluster
in VirtualBox.
Please follow &lt;strong&gt;all the instructions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start the cluster, type the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube start&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that Kubernetes is correctly activated by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see a node called &lt;code&gt;minikube&lt;/code&gt; whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open a &lt;strong&gt;new terminal&lt;/strong&gt; and type the command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube tunnel&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;When prompted to enter a password, just type ENTER.
The command will start to produce some output.
&lt;strong&gt;Leave that terminal open&lt;/strong&gt; and go back to the previous terminal.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Tunnel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Minikube tunnel is used to create a route to services deployed
with type &lt;code&gt;LoadBalancer&lt;/code&gt;. If you don’t activate the tunnel,
you won’t be able to use these services in the exercises below.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;deploying-a-pod&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Deploying a pod&lt;/h2&gt;
&lt;p&gt;In order to deploy &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34; target=&#34;_blank&#34;&gt;a pod&lt;/a&gt;,
we first have to give its &lt;strong&gt;specification&lt;/strong&gt;, basically its name
and the containers that compose it with their settings.
Similarly to Docker Compose, a pod is specified in a &lt;strong&gt;declarative way&lt;/strong&gt; with a
YAML configuration file.&lt;/p&gt;
&lt;p&gt;Consider the following specification.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web-pod
spec:
  containers:
  - name: web
    image: nginx:alpine
    ports:
    - containerPort: 80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is an explanation of the properties in the specification.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;apiVersion&lt;/em&gt;. Defines the versioned schema of this representation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;kind&lt;/em&gt;. The type of the resource that we intend to create.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;metadata&lt;/em&gt;. The resource metadata. The list of
all metadata is specified &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;spec&lt;/em&gt;. The specification of the desired behaviour of the pod.
The list of the possible specifications can be found &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, the previous specification defines a pod with a container that is launched from the
image &lt;code&gt;nginx:alpine&lt;/code&gt; (a Web server) and listens to port 80.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy the previous specification to a file named &lt;code&gt;sample-pod.yaml&lt;/code&gt; (or any other name
of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-pod.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the pod by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the pod is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first time you run the last command, you might see that the pod is not
ready yet. You need to wait for the image &lt;code&gt;nginx:alpine&lt;/code&gt; to be pulled from DockerHub.
Wait few seconds and try the command again until the pod is marked as running.&lt;/p&gt;
&lt;p&gt;You can also get more information on the running pod (e.g., its assigned IP address)
by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod -o wide web-pod&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Open a web browser and type &lt;code&gt;http://localhost:80&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What do you get? What if you try to use the IP of the pod instead of &lt;code&gt;locahost&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What should we define in order to fix the problem?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The following configuration
defines a Service object of type &lt;strong&gt;LoadBalancer&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
      app: web-pod&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;LoadBalancer&lt;/strong&gt; service is a &lt;strong&gt;NodePort&lt;/strong&gt; service.
that offers load balancing capabilities.
It is intended to expose an IP address
that client applications (external to the Kubernetes cluster) can use to access
the service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
What is the field &lt;code&gt;selector&lt;/code&gt; in the definition of the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
In the specification of the service, what &lt;code&gt;port&lt;/code&gt; and &lt;code&gt;targetPort&lt;/code&gt; mean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the service specification into a file named &lt;code&gt;sample-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the service by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command returns all services running in Kubernetes.
For each service, you also get a name.
In order to only target the service that you’ve just created, simply type
the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your service’s name is &lt;code&gt;nginx-service&lt;/code&gt; (as specified in file &lt;code&gt;sample-service.yml&lt;/code&gt;);
&lt;code&gt;svc&lt;/code&gt; is only the namespace where all Kubernetes services are
put.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
By looking at the output of the command &lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;,
which URL do you need to type in the Web browser in order to access the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stop the pod and the service before moving on. Here are the commands to do so:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete po/web-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kubernetes-deploying-an-application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Kubernetes: deploying an application&lt;/h1&gt;
&lt;p&gt;In this section, we’re going to deploy our pet store in Kubernetes.
As a reminder, our application consists of two services: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to define an application in Kubernetes,
we need to use two types of objects &lt;strong&gt;for each service&lt;/strong&gt; of the application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;workload resource&lt;/strong&gt; that gives the specification of the service, such as
the pods that make up the service itself (images, network settings) and metadata, such as
the desired number of instances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;Service&lt;/strong&gt; object. As we have seen previously, a service object
exposes an IP address that allows client applications,
both inside and outside the Kubernetes cluster, to connect to the service.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we define an application in Kubernetes, we rarely,
if ever, need to play directly with pods.
Instead, we can resort to higher-level objects, called &lt;strong&gt;Controllers&lt;/strong&gt;, for an easier
definition of the &lt;strong&gt;desired state&lt;/strong&gt; of the application itself.
The type of the controller that we need to use depends on the nature of the service
itself: &lt;strong&gt;stateless&lt;/strong&gt; or &lt;strong&gt;stateful&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;web&lt;/code&gt; of our application stateless or stateful?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;db&lt;/code&gt; of our application stateless of stateful?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-web-service-deployment&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: deployment&lt;/h2&gt;
&lt;p&gt;For &lt;strong&gt;stateless application services&lt;/strong&gt;, we can use
a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt; for the deployment of a set of identical pods
that are launched across several nodes in the Kubernetes cluster.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Deployment and ReplicaSet&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deployments and ReplicaSets have been introduced
in Lecture 3.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We here define the specification of
a Deployment corresponding to the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the explanation of the specification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A Deployment named &lt;code&gt;web&lt;/code&gt; is created, as indicated by the field &lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment creates three replicated pods, as indicated by the field
&lt;code&gt;spec.replicas&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment considers that the pods with &lt;strong&gt;both&lt;/strong&gt; labels &lt;code&gt;app: pets&lt;/code&gt; and
&lt;code&gt;service: web&lt;/code&gt; are part of the deployment.
This is indicated by the field &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
This field is used to let the Deployment know how to find its pods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The configuration of the pods that are part of the Deployment is
given in the field &lt;code&gt;spec.template&lt;/code&gt; and its subfields.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each pod of the Deployment is given labels &lt;code&gt;app: pets&lt;/code&gt; and &lt;code&gt;service: web&lt;/code&gt;.
This is indicated by the field &lt;code&gt;spec.template.metadata.labels&lt;/code&gt;.
Note that here we specify exactly the same values as in &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
This field is used to give pods labels so that they can be identified and located in
Kubernetes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each pod has exactly one container, named &lt;code&gt;web&lt;/code&gt;, run from the
image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt; stored in the DockerHub. The container listens on port 3000
and uses the TCP protocol. This is specified in the field &lt;code&gt;spec.template.spec.containers&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-deployment.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-deployment.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this Deployment in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-deployment.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
Type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created following the creation of the
Deployment?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Get the name of one of the running pods and
kill it by using the following command&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete name-of-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If the command hangs in the terminal, feel free to type Ctrl-C to
get back control of the terminal.&lt;/p&gt;
&lt;p&gt;Type again following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How many pods do you see? Is it surprising?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-web-service-service&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: service&lt;/h2&gt;
&lt;p&gt;Now we need to define a &lt;strong&gt;Service&lt;/strong&gt; in order to expose the web service to the public.
Here is the definition:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Describe the specification of this service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-service.yaml&lt;/code&gt;
(or any name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create the service with the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Locate the external IP (let’s call it EXTERNAL-IP) of the web service and type
the URL &lt;code&gt;http://EXTERNAL-IP:8080&lt;/code&gt; in your Web browser. You should
see a Web page where the phrase &lt;em&gt;Pet store&lt;/em&gt; appears.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-statefulset&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: StatefulSet&lt;/h2&gt;
&lt;p&gt;Kubernetes has defined a special type of ReplicaSet for stateful services that
is called &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;StatefulSet&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s define a StatefulSet to give the specification of
the &lt;code&gt;db&lt;/code&gt; service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: pets
      service: db
  serviceName: db
  template:
    metadata:
      labels:
        app: pets
        service: db
    spec:
      containers:
      - image: quercinigia/pet-store-db:1.0
        name: db
        ports:
        - containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: pets-data
  volumeClaimTemplates:
  - metadata:
      name: pets-data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now you shouldn’t have any problem understanding the meaning of the fields
in this specification.
The fields are also
documented in the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec&#34; target=&#34;_blank&#34;&gt;official Kubernetes documentation&lt;/a&gt;.
Two points are worth a comment.
First, the specification of a &lt;code&gt;StatefulSet&lt;/code&gt; needs an attribute &lt;code&gt;serviceName&lt;/code&gt; that indicates the name
of the service that is responsible for the network identify of the pods in the set.
That attribute is mandatory.&lt;/p&gt;
&lt;p&gt;Another novelty is the field &lt;code&gt;volumeClaimTemplates&lt;/code&gt;.
It describes additional constraints on the volumes defined in the specification, in this case the volume
named &lt;code&gt;pets-data&lt;/code&gt; (where PostgreSQL keeps the data). In particular, two claims are given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The access mode &lt;code&gt;ReadWriteOnce&lt;/code&gt; means that the volume can be mounted
as read-write by a single node in the Kubernetes cluster.
Access modes are &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34; target=&#34;_blank&#34;&gt;documented here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We request at least 100MB of storage for the database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-db:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this StatefulSet in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-stateful-set.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;
Type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created when you deployed the StatefulSet?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;
Open the Web browser and type the following URL:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://localhost:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Minikube on the Linux VM
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://minikube-ip:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where you’ll replace &lt;code&gt;minikube-ip&lt;/code&gt; with the external IP address associated with the
&lt;code&gt;web&lt;/code&gt; service.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Right after, type the command &lt;code&gt;kubectl get all&lt;/code&gt;.
What do you observe? Can you explain the reason?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In the output of the command &lt;code&gt;get kubectl all&lt;/code&gt;, look at the names of the
pods that are instances of the &lt;code&gt;web&lt;/code&gt; service.
&lt;!-- in the TP you can even ask why all the three instances got an error--&gt;
Take the name of any these pods,
and put it in place of NAME-OF-POD in the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;
Does the output of the previous command confirm the explanation
given in the previous question?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-service-object&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: Service object&lt;/h1&gt;
&lt;p&gt;From the above observations, we understand that we need to define
a service to expose the database to the clients.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Should the &lt;code&gt;db&lt;/code&gt; service be accessible to client applications that are external to
the Kubernetes cluster?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Given the answer to the previous question, what should the type of this service be?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Write the specification of the &lt;code&gt;db&lt;/code&gt; service in a file
named &lt;code&gt;db-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caution.&lt;/strong&gt; In the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; the field &lt;code&gt;spec.serviceName&lt;/code&gt; indicates the name
that the service must have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy the service by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that you can reach the application at &lt;code&gt;http://localhost:8080/pet&lt;/code&gt; (Minikube users:
replace &lt;code&gt;localhost&lt;/code&gt; with the external IP address associated to the &lt;code&gt;web&lt;/code&gt; service!).
&lt;strong&gt;It might happen&lt;/strong&gt; that the database service is not ready yet, and so you’ll get
a connection error. Just wait and retry later.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application-1&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;After you’re done with the application, you can shut it down with the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete deploy/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete statefulset/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The order in which you type these commands doesn’t matter.&lt;/p&gt;
&lt;p&gt;If you type multiple times the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see that the resources progressively disappear.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Conclusion&lt;/h2&gt;
&lt;p&gt;In the previous exercises, we deployed an application with two services, for which we had to create
four files and type as many commands.
For larger applications this gets a bit annoying.
We can write all the definitions in a single file (e.g., &lt;code&gt;pets.yaml&lt;/code&gt;)
where each specification is terminated by - - -.&lt;/p&gt;
&lt;p&gt;Here is an example, where we write the specification of the
Service and Deployment associated with the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Docker Compose and Kubernetes</title>
      <link>/courses/cloud-en/kubernetes-en/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-en/kubernetes-en/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you use the Linux VM&lt;/strong&gt;, you need to change the hardware configuration of the VM before
booting it. You can also &lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v12619ffbdbb7mpc6xbp/&#34; target=&#34;_blank&#34;&gt;watch this video&lt;/a&gt;.
Here are the necessary modifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Increase the main memory to 4GB.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set the number of CPUs to 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without these modifications you’re not going to have a good experience with Kubernetes.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;build&lt;/strong&gt; and &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Docker Compose&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;push an image&lt;/strong&gt; to &lt;strong&gt;DockerHub&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Kubernetes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This tutorial is adapted from the examples presented in Chapters
14 and 20 of the book &lt;a href=&#34;https://www.packtpub.com/product/learn-docker-fundamentals-of-docker-19-x-second-edition/9781838827472&#34; target=&#34;_blank&#34;&gt;G. Schenker, &lt;em&gt;Learn Docker - Fundamentals of Docker 19.x&lt;/em&gt; (March 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;docker-compose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Docker Compose&lt;/h1&gt;
&lt;p&gt;In this section, you’re going to build and deploy a multi-service application
by using &lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-kube/pet-store.zip&#34;&gt;this archive file&lt;/a&gt; and unzip it into a folder on your
own computer.
The archive contains all the necessary files to build and run
a web application consisting of two services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;web&lt;/strong&gt;. This is the &lt;strong&gt;frontend&lt;/strong&gt; (the part the user interacts with)
of the application.
It consists of HTML and JavaScript code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt;. This is the &lt;strong&gt;backend&lt;/strong&gt; (the part hidden to the user).
It is a &lt;a href=&#34;https://www.postgresql.org/&#34; target=&#34;_blank&#34;&gt;PostgreSQL&lt;/a&gt; (relational) database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The structure of the application is shown in Figure &lt;a href=&#34;#fig:pet-store-struct&#34;&gt;1.1&lt;/a&gt;.
The root directory of the application contains two subdirectories, one for each service
(&lt;code&gt;database&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pet-store-struct&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-kube/pet-store-files.png&#34; alt=&#34;Structure of the application&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Structure of the application
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The files with extension &lt;code&gt;.conf&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contain
configuration parameters of the PostgreSQL database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The file &lt;code&gt;init-db.sql&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contains the SQL queries
to populate the database with some data (photos of cats).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The directory &lt;code&gt;web&lt;/code&gt; contains the HTML and JavaScript code of the web application.
The file &lt;code&gt;package.json&lt;/code&gt; contains the dependencies to install.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Both directories contain a &lt;strong&gt;Dockerfile&lt;/strong&gt;.
The one in directory &lt;code&gt;database&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM postgres
COPY init-db.sql /docker-entrypoint-initdb.d/
RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql
ENV POSTGRES_USER dockeruser
ENV POSTGRES_PASSWORD dockerpass
ENV POSTGRES_DB pets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dockerfile builds on an existing image called &lt;code&gt;postgres&lt;/code&gt;,
that is &lt;a href=&#34;https://hub.docker.com/_/postgres&#34; target=&#34;_blank&#34;&gt;documented here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Consider the following line in the Dockerfile:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;COPY init-db.sql /docker-entrypoint-initdb.d/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;By looking at the documentation of the image &lt;code&gt;postgres&lt;/code&gt;,
answer the two following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Where is the directory &lt;code&gt;/docker-entrypoint-initdb.d/&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why do we copy the file &lt;code&gt;init-db.sql&lt;/code&gt; to this directory?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The directory already exists in the base image.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By looking at the documentation, we learn that
we can put any initialization script in this directory.
In other words, at the startup the database will execute the
SQL queries in file &lt;code&gt;init-db.sql&lt;/code&gt; so that the database
is populated with new data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the documentation, we also learn that the script is not executed
if the data directory is not empty. This means that already existing databases
will not be touched.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;The last three lines of the Dockerfile contain a keyword (&lt;code&gt;ENV&lt;/code&gt;)
that we never came across before.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Look again at the documentation of the image &lt;code&gt;postgres&lt;/code&gt;
and try to explain the meaning of the last three lines of the Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;These lines set the value of three &lt;strong&gt;environment variables&lt;/strong&gt;, useful
to pass some parameters to the database.
From the documentation we learn that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_USER&lt;/code&gt; is the name of the database user. If not set,
the default is the user &lt;code&gt;postgres&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt; is the password associated with the
user. This is the only mandatory variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_DB&lt;/code&gt; is the name given to the database.
If not specified, the database name would be the same as the username.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;The Dockerfile of the web application is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM node:9.6-alpine
RUN mkdir /app
WORKDIR /app
COPY package.json /app/
RUN npm install
COPY ./src /app/src
EXPOSE 3000
CMD node src/server.js&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dockerfile builds on the image &lt;code&gt;node:9.6-alpine&lt;/code&gt; that contains
a Node.js environment, a JavaScript-based platform for server-side
applications.
The instructions in the Dockerfile look like the ones of the examples
that we’ve seen in the first tutorial and in the lectures.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The command &lt;code&gt;npm install&lt;/code&gt; installs all the dependencies specified
in the file &lt;code&gt;package.json&lt;/code&gt;
from the &lt;a href=&#34;https://docs.npmjs.com/about-npm&#34; target=&#34;_blank&#34;&gt;software registry npm&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instruction &lt;code&gt;EXPOSE 3000&lt;/code&gt; informs that the container
listens on port 3000 when it is executed from the image.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#expose&#34; target=&#34;_blank&#34;&gt;Docker documentation&lt;/a&gt;
we learn
that the &lt;em&gt;“&lt;code&gt;EXPOSE&lt;/code&gt; instruction
does not actually publish the port.
It functions as a type of
documentation between
the person who builds the image
and the person who runs the container, about which ports are intended to be published”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In order to actually publish the port, we’ll use the file
&lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;describing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Describing the application&lt;/h2&gt;
&lt;p&gt;The application showcases a building block of a pet store.
In the current version, the application shows a few pictures
of cats.&lt;/p&gt;
&lt;p&gt;The root directory of the application contains a file named
&lt;code&gt;docker-compose.yml&lt;/code&gt; that contains the &lt;strong&gt;declarative configuration&lt;/strong&gt;
of the application.
The content of the file is as follows. It is a sequence of key-value pairs.
Each key is also called a &lt;strong&gt;field&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: &amp;quot;3.6&amp;quot;
services:
  web:
    build: web
    image: pet-store-web
    networks:
      - backend
    ports:
      - 5000:3000
  db:
    build: database
    image: pet-store-db
    networks:
      - backend
    volumes:
      - pets-data:/var/lib/postgresql/data

networks:
  backend:

volumes:
  pets-data:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are three main sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;services&lt;/code&gt;. Defines the services of the application. Here two services
are defined: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;networks&lt;/code&gt;. Defines the networks used by the application.
Here one network is defined: &lt;code&gt;backend&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;volumes&lt;/code&gt;. Defines the volumes used by the application.
Here one volume is defined: &lt;code&gt;pets-data&lt;/code&gt;.
The volume is attached to the directory &lt;code&gt;/var/lib/postgresql/data&lt;/code&gt;
(that is in the container).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
What field informs &lt;code&gt;docker compose&lt;/code&gt; where to find the
Dockerfile of the two services?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The value of the field &lt;code&gt;build&lt;/code&gt; is the name of the directory that contains
the Dockefile. For the service &lt;code&gt;web&lt;/code&gt;, this is the directory
&lt;code&gt;web&lt;/code&gt;; for the service &lt;code&gt;db&lt;/code&gt;, it is the directory &lt;code&gt;database&lt;/code&gt;.
All paths are relative to the position of the file &lt;code&gt;docker-compose.yml&lt;/code&gt;
itself.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;When we’ll build the application from the file &lt;code&gt;docker-compose.yml&lt;/code&gt;,
two images will be created, one for each service.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
What will the name of the two images be?
What field in the file &lt;code&gt;docker-compose.yml&lt;/code&gt; gives you this information?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The name of the image corresponding to the service &lt;code&gt;web&lt;/code&gt;
will be &lt;code&gt;pet-store-web&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The name of the image corresponding to the service &lt;code&gt;db&lt;/code&gt;
will be &lt;code&gt;pet-store-db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This information is given by the field &lt;code&gt;image&lt;/code&gt;
in the file &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-application&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Building the application&lt;/h2&gt;
&lt;p&gt;We now build the application.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open the command-line terminal and by using the command &lt;code&gt;cd&lt;/code&gt;
position yourself in the root directory
&lt;code&gt;pet-store&lt;/code&gt; of the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;During the build you might get a &lt;code&gt;npm&lt;/code&gt; warning. Just ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the build is complete, verify that the two images corresponding
to the two services have been created (which docker command do you need here?).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;docker images&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;executing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Executing the application&lt;/h2&gt;
&lt;p&gt;We now execute the application with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose up&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In this section, you’ll need to open several terminals in the virtual machine.
You can do it easily by using &lt;code&gt;byobu&lt;/code&gt;, an advanced window manager already available in your virtual machine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just type &lt;code&gt;byobu&lt;/code&gt; to launch the window manager.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to open a new terminal, just press F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4
(to move to next terminal).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to close a terminal, just type &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you close all terminals, &lt;code&gt;byobu&lt;/code&gt; will stop executing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If your application fails to start because the port number that you chose is already in use,
try using another port.&lt;/p&gt;
&lt;p&gt;Don’t use the following ports as they are deemed unsafe by the browsers:&lt;/p&gt;
&lt;p&gt;1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The execution of the command will print a series of messages on screen.
The terminal should hang when the messages stop
(the last message should be &lt;code&gt;database system is ready to accept connections&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Take no action on the screen and open a new terminal or a new tab in the current terminal
window.&lt;/p&gt;
&lt;!--
:::{.infobox .curiosity data-latex=&#34;curiosity&#34;}
**Good to know**

The option ``-d`` in the previous command means that the application is 
executed **in the background** or, in other words, 
the containers are run as **daemons** (hence, the ``-d``).
This means that any output of the application is not
shown in the terminal.

:::
--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Verify that the network and the volumes associated with the
application have been correctly created.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Type the commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;to verify respectively that the volumes and the networks have been created&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
How many containers do you expect to be associated to
the application?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each &lt;strong&gt;image&lt;/strong&gt; corresponds to a &lt;strong&gt;service&lt;/strong&gt;.
Here we have two.
Each &lt;strong&gt;container&lt;/strong&gt; corresponds to a &lt;strong&gt;service instance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We didn’t specify any parameter when we launched the application,
so by default one instance of each service is executed.&lt;/p&gt;
&lt;p&gt;So, we expect to have &lt;strong&gt;two containers&lt;/strong&gt; associated to the application:
one container for the service &lt;code&gt;web&lt;/code&gt; and one for the
service &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can verify the answer to the previous question by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose ps&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command is exactly equivalent to &lt;code&gt;docker container ls&lt;/code&gt;, except that
it only shows the containers associated with the application
that we’ve just executed.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;
In the output of the command &lt;code&gt;docker compose ps&lt;/code&gt;, can you explain
the meaning of the following?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.0.0.0:5000-&amp;gt;3000/tcp&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The notation 5000-&amp;gt;3000 means that the port
5000 of the host computer is &lt;strong&gt;mapped&lt;/strong&gt;
to the port 3000 of the container.
More specifically,
when a client application connects to the port 5000 of the host computer,
the connection is redirected to the port 3000 of the container.&lt;/p&gt;
&lt;p&gt;The IP address 0.0.0.0 means that we can connect to the container by
using any IP address on the host computer.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;
Can you
tell which URL you have to type in your web browser
to access the application?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you use a Linux virtual machine with Multipass
&lt;/summary&gt;
&lt;p&gt;In order to open a Web browser window in the VM, you need to open a new
terminal window on your computer and follow these instructions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;multipass ls&lt;/code&gt; to list all the virtual machines managed by &lt;code&gt;Multipass&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the first IP address associated to the virtual machine &lt;code&gt;cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;left&#34;&gt;
&lt;code&gt;xpra start ssh://ubuntu@ip/ --start=firefox&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;where you’ll replace &lt;code&gt;ip&lt;/code&gt; with the IP address that you copied above.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A Firefox window should appear.&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;http://localhost:5000&lt;/code&gt; or &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the network settings of you computer, you can also
get the IP address associated by the DHCP server of your&lt;br /&gt;
local network and use that address to connect to the application
from another device &lt;strong&gt;connected to the same network&lt;/strong&gt;
(for instance, your mobile phone).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to see the cats, you need to append
&lt;code&gt;/pet&lt;/code&gt; to the URL that you found in the previous question.
This is determined in file &lt;code&gt;server.js&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;You can shut down the application by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose down&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.9  &lt;/strong&gt;&lt;/span&gt;
Does shutting down the application remove the networks created
for the application?
What about the volumes?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and you’ll see that the network &lt;code&gt;backend&lt;/code&gt; is not there anymore.&lt;/p&gt;
&lt;p&gt;Type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and you’ll see that the volume &lt;code&gt;pet-store_pets-data&lt;/code&gt; is still there.&lt;/p&gt;
&lt;p&gt;By default volumes are not removed, to avoid the risk of permanently deleting
data that might still be useful later.
If you wish to remove the volumes when shutting down the application,
you can type:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose down -v&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-a-service&#34; class=&#34;section level2&#34; number=&#34;1.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Scaling a service&lt;/h2&gt;
&lt;p&gt;When we launch the application, we can specify the
number of instances of each service.
This is useful when we expect our application to be solicited by many users;
the workload will be automatically balanced across all the instances of the service.&lt;/p&gt;
&lt;p&gt;Let’s launch our application with 3 instances of the service &lt;code&gt;web&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose up --scale web=3 -d&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.10  &lt;/strong&gt;&lt;/span&gt;
Running the previous command results in an error.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can you explain why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What fix would you propose in file &lt;code&gt;docker-compose.yml&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We’re trying to bind three instances to the port 5000 on the host computer. This is not
possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the section &lt;code&gt;ports&lt;/code&gt; of file
&lt;code&gt;docker-compose.yml&lt;/code&gt; we need to remove 5000 and we only leave 3000. This way, each time we run a container, a random port will be chosen on the host computer to bind it
with port 3000 of the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to shut down the application before relaunching it again.
In fact, even if we got an error, an instance of the &lt;code&gt;db&lt;/code&gt; service and
an instance of the service &lt;code&gt;web&lt;/code&gt; are still running.&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modify the file &lt;code&gt;docker-compose.yml&lt;/code&gt; and relaunch the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the following command and verify that you actually have three running instances of the
service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose ps&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try to connect to the application by using the three port numbers indicated in the output
of the previous command.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Don’t forget to shut down the application before you
go on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-an-application&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Pushing an application&lt;/h1&gt;
&lt;p&gt;Once we have built an application, we might want to
share it by pushing it to the DockerHub registry or any
other container registry, be it private or public.&lt;/p&gt;
&lt;div id=&#34;creating-an-account-on-dockerhub&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Creating an account on DockerHub&lt;/h2&gt;
&lt;p&gt;You need to create an account on the &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34;&gt;DockerHub website&lt;/a&gt;
in order to perform this activity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renaming-the-images&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Renaming the images&lt;/h2&gt;
&lt;p&gt;In order to push your images to the registry, you need to rename them, so as the new name
has the following structure:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yourusername/image-name:image-tag&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For instance, since my username is &lt;code&gt;quercinigia&lt;/code&gt;, I’ll rename
the two images &lt;code&gt;pet-store-web&lt;/code&gt; and &lt;code&gt;pet-store-db&lt;/code&gt; with the
&lt;code&gt;docker tag&lt;/code&gt; command as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-web quercinigia/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-db quercinigia/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I chosen 1.0 as a tag, but feel free to pick another one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-in&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Logging in&lt;/h2&gt;
&lt;p&gt;You need to log in to your DockerHub account.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;macOS users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-macos/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Windows users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-windows/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for the Linux VM
&lt;/summary&gt;
&lt;p&gt;Log in to DockerHub by typing the following command in the terminal
(replace YOUR-USERNAME with your actual username).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker login -u YOUR-USERNAME&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You might get a warning that
your password will be stored unencrypted.
There are methods to prevent this from happening.
These methods being out of the scope of this course,
the interested reader can
look them up &lt;a href=&#34;https://www.techrepublic.com/article/how-to-setup-secure-credential-storage-for-docker/&#34; target=&#34;_blank&#34;&gt;at this link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-the-images&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Pushing the images&lt;/h2&gt;
&lt;p&gt;Once you’re successfully logged in, you can type the following
commands in the terminal to push the two images of your application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Remember to replace YOUR-USERNAME with your actual username in the commands above.&lt;/p&gt;
&lt;p&gt;After the task is completed, verify that the images appear
in &lt;a href=&#34;https://hub.docker.com/repositories&#34; target=&#34;_blank&#34;&gt;your Docker registry&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here we manually tagged and uploaded the two images.
This method becomes quickly annoying when the application consists of
more than two images.
Another way to go about this task is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specify the image names with your username in file &lt;strong&gt;docker-compose.yml&lt;/strong&gt;.
For instance, in my &lt;strong&gt;docker-compose.yml&lt;/strong&gt; I would write:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;services:
  web:
    build: web
    image: quercinigia/pet-store-web:1.0
  ...
  db:
    build: database
    image: quercinigia/pet-store-db:1.0
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Build the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Push the images of the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This way, with just one command we push to the registry all the images
of the application, no matter how many they are.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-kubernetes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Introduction to Kubernetes&lt;/h1&gt;
&lt;p&gt;Kubernetes is the most popular &lt;strong&gt;orchestrator&lt;/strong&gt; to date.
It is used to manage a multi-service application, usually deployed
across multiple &lt;strong&gt;hosts&lt;/strong&gt; in a &lt;strong&gt;cluster&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;While using Docker Compose, a &lt;strong&gt;service&lt;/strong&gt; corresponds
to a &lt;strong&gt;Docker image&lt;/strong&gt; and
a &lt;strong&gt;service instance&lt;/strong&gt; to a &lt;strong&gt;Docker container&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As opposed to that, in Kubernetes the computation unit is a &lt;strong&gt;Pod&lt;/strong&gt;, that is
a &lt;strong&gt;collection of containers&lt;/strong&gt;.
In other words, we don’t reason in terms of containers anymore,
but in terms of Pods. Of course, a Pod can also consist of just one container.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In practice, we rarely have to manipulate Pods directly in Kubernetes,
as there are higher-level objects that manage them.
We’ll use these objects in the next sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activate-kubernetes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Activate Kubernetes&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;p&gt;You need to follow all the instructions documented
&lt;a href=&#34;https://docs.docker.com/desktop/kubernetes/&#34; target=&#34;_blank&#34;&gt;on this page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Before moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see a node called &lt;code&gt;docker-desktop&lt;/code&gt; whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This solution might not be working for you.
In that case, disable Kubernetes in Docker Desktop and install
&lt;strong&gt;minikube&lt;/strong&gt;, by following &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for the Linux VM
&lt;/summary&gt;
&lt;p&gt;In the Linux VM you’ll find &lt;strong&gt;Minikube&lt;/strong&gt;, a single-node Kubernetes cluster
in VirtualBox.
Please follow &lt;strong&gt;all the instructions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start the cluster, type the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube start&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that Kubernetes is correctly activated by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see a node called &lt;code&gt;minikube&lt;/code&gt; whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open a &lt;strong&gt;new terminal&lt;/strong&gt; and type the command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube tunnel&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;When prompted to enter a password, just type ENTER.
The command will start to produce some output.
&lt;strong&gt;Leave that terminal open&lt;/strong&gt; and go back to the previous terminal.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Tunnel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Minikube tunnel is used to create a route to services deployed
with type &lt;code&gt;LoadBalancer&lt;/code&gt;. If you don’t activate the tunnel,
you won’t be able to use these services in the exercises below.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;deploying-a-pod&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Deploying a Pod&lt;/h2&gt;
&lt;p&gt;In order to deploy &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34; target=&#34;_blank&#34;&gt;a Pod&lt;/a&gt;,
we first have to give its &lt;strong&gt;specification&lt;/strong&gt;, basically its name
and the containers that compose it with their settings.
Similarly to Docker Compose, a Pod is specified in a &lt;strong&gt;declarative way&lt;/strong&gt; with a
YAML configuration file.&lt;/p&gt;
&lt;p&gt;Consider the following specification.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web-pod
spec:
  containers:
  - name: web
    image: nginx:alpine
    ports:
    - containerPort: 80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is an explanation of the properties in the specification.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;apiVersion&lt;/em&gt;. Defines the versioned schema of this representation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;kind&lt;/em&gt;. The type of the resource that we intend to create.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;metadata&lt;/em&gt;. The resource metadata. The list of
all metadata is specified &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;spec&lt;/em&gt;. The specification of the desired behaviour of the Pod.
The list of the possible specifications can be found &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/Pod-v1/#PodSpec&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, the previous specification defines a Pod with a container that is launched from the
image &lt;code&gt;nginx:alpine&lt;/code&gt; (a Web server) and listens to port 80.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy the previous specification to a file named &lt;code&gt;sample-pod.yaml&lt;/code&gt; (or any other name
of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-pod.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the Pod by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the Pod is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first time you run the last command, you might see that the Pod is not
ready yet. You need to wait for the image &lt;code&gt;nginx:alpine&lt;/code&gt; to be pulled from DockerHub.
Wait few seconds and try the command again until the Pod is marked as running.&lt;/p&gt;
&lt;p&gt;You can also get more information on the running Pod (e.g., its assigned IP address)
by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod -o wide web-pod&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Open a web browser and type &lt;code&gt;http://localhost:80&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What do you get? What if you try to use the IP of the Pod instead of &lt;code&gt;locahost&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What should we define in order to fix the problem?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The connection is refused both when using &lt;code&gt;localhost&lt;/code&gt; and
the IP address of the Pod.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to connect to the service instance, we need to
define a &lt;strong&gt;Service&lt;/strong&gt; object in Kubernetes, that exposes an IP address
that client applications can use.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;The following configuration
defines a Service object of type &lt;strong&gt;LoadBalancer&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
      app: web-pod&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;LoadBalancer&lt;/strong&gt; service is a &lt;strong&gt;NodePort&lt;/strong&gt; service.
that offers load balancing capabilities.
It is intended to expose an IP address
that client applications (external to the Kubernetes cluster) can use to access
the service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
What is the field &lt;code&gt;selector&lt;/code&gt; in the definition of the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The selector is used internally by Kubernetes to identify the Pods that
are the instances of the service.
If you look at the specification of the Pod above, the field &lt;code&gt;metadata&lt;/code&gt;
contains a field &lt;code&gt;labels&lt;/code&gt;.
One of the labels is &lt;code&gt;app: web-pod&lt;/code&gt; ;
the selector matches this label.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
In the specification of the service, what do &lt;code&gt;port&lt;/code&gt; and &lt;code&gt;targetPort&lt;/code&gt; mean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;port&lt;/code&gt; specifies the port number of the Service (used by the external clients).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;targetPort&lt;/code&gt; specifies the port number opened at the Pod’s network interface.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the service specification into a file named &lt;code&gt;sample-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the service by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command returns all services running in Kubernetes.
For each service, you also get a name.
In order to only target the service that you’ve just created, simply type
the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your service’s name is &lt;code&gt;nginx-service&lt;/code&gt; (as specified in file &lt;code&gt;sample-service.yml&lt;/code&gt;);
&lt;code&gt;svc&lt;/code&gt; is only the namespace where all Kubernetes services are
put.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
By looking at the output of the command &lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;,
which URL do you need to type in the Web browser in order to access the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;CLUSTER-IP&lt;/code&gt; is an IP address that is only visible &lt;strong&gt;within the cluster&lt;/strong&gt;.
In other words, only the services running within the cluster can connect to our
&lt;code&gt;nginx-service&lt;/code&gt; by using this IP address.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What we need to use is the IP address specified under &lt;code&gt;EXTERNAL-IP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; is likely to be &lt;code&gt;localhost&lt;/code&gt;
or &lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Minikube in the Linux VM
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; will be whatever IP address assigned to the service.&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;As for the port number, under PORT(S) you should see something like 8080:30548
(the second number is likely different, but it should be something in the range [30000-32767]).
The port 30548 is opened on each node of the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, when we type &lt;code&gt;http://localhost:8080&lt;/code&gt;, the load balancer replaces it
with the IP address of a node in the cluster,
followed by port number 30548.
When the &lt;code&gt;kube-proxy&lt;/code&gt; on that node receives this request,
it forwards it to an instance of the service,
even if that instance runs in another node.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stop the Pod and the service before moving on. Here are the commands to do so:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete po/web-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kubernetes-deploying-an-application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Kubernetes: deploying an application&lt;/h1&gt;
&lt;p&gt;In this section, we’re going to deploy our pet store in Kubernetes.
As a reminder, our application consists of two services: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to define an application in Kubernetes,
we need to use two types of objects &lt;strong&gt;for each service&lt;/strong&gt; of the application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;workload resource&lt;/strong&gt; that gives the specification of the service, such as
the Pods that make up the service itself (images, network settings) and metadata, such as
the desired number of instances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;Service&lt;/strong&gt; object. As we have seen previously, a service object
exposes an IP address that allows client applications,
both inside and outside the Kubernetes cluster, to connect to the service.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we define an application in Kubernetes, we rarely,
if ever, need to play directly with Pods.
Instead, we can resort to higher-level objects, called &lt;strong&gt;Controllers&lt;/strong&gt;, for an easier
definition of the &lt;strong&gt;desired state&lt;/strong&gt; of the application itself.
The type of the controller that we need to use depends on the nature of the service
itself: &lt;strong&gt;stateless&lt;/strong&gt; or &lt;strong&gt;stateful&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;web&lt;/code&gt; of our application stateless or stateful?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;db&lt;/code&gt; of our application stateless of stateful?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The service &lt;code&gt;web&lt;/code&gt; is &lt;strong&gt;stateless&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The service &lt;code&gt;db&lt;/code&gt; is &lt;strong&gt;stateful&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;the-web-service-deployment&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: deployment&lt;/h2&gt;
&lt;p&gt;For &lt;strong&gt;stateless application services&lt;/strong&gt;, we can use
a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt; for the deployment of a set of identical Pods
that are launched across several nodes in the Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;We here define the specification of
a Deployment corresponding to the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the explanation of the specification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A Deployment named &lt;code&gt;web&lt;/code&gt; is created, as indicated by the field &lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment creates three replicated Pods, as indicated by the field
&lt;code&gt;spec.replicas&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment considers that the Pods with &lt;strong&gt;both&lt;/strong&gt; labels &lt;code&gt;app: pets&lt;/code&gt; and
&lt;code&gt;service: web&lt;/code&gt; are part of the deployment.
This is indicated by the field &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
This field is used to let the Deployment know how to find its Pods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The configuration of the Pods that are part of the Deployment is
given in the field &lt;code&gt;spec.template&lt;/code&gt; and its subfields.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each Pod of the Deployment is given labels &lt;code&gt;app: pets&lt;/code&gt; and &lt;code&gt;service: web&lt;/code&gt;.
This is indicated by the field &lt;code&gt;spec.template.metadata.labels&lt;/code&gt;.
Note that here we specify exactly the same values as in &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each Pod has exactly one container, named &lt;code&gt;web&lt;/code&gt;, run from the
image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt; stored in the DockerHub. The container listens on port 3000
and uses the TCP protocol. This is specified in the field &lt;code&gt;spec.template.spec.containers&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-deployment.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-deployment.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this Deployment in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-deployment.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
Type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created following the creation of the
Deployment?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We can clearly see three types of objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Three Pods, that correspond to the three replicas that we specified in the configuration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One deployment. Note that the 3/3 indicates that the deployment has
3 identical Pods, of which 3 are running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One ReplicaSet. The Deployment creates a ReplicaSet, that is the actual object that
controls the three Pods. The Deployment provides some additional services (i.e., updates)
on top of a ReplicaSet.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Get the name of one of the running Pods and
kill it by using the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete name-of-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If the command hangs in the terminal, feel free to type Ctrl-C to
get back control of the terminal.&lt;/p&gt;
&lt;p&gt;Type again following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How many Pods do you see? Is it surprising?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;If we type the command right after deleting the Pod, it is likely that we’ll see
four Pods, one that is &lt;em&gt;terminating&lt;/em&gt; and another one that is &lt;em&gt;running&lt;/em&gt;.
Try to type the command until you see three running Pods.&lt;/p&gt;
&lt;p&gt;This is not surprising.
If we kill the Pod, the orchestrator detects a
deviation from the &lt;strong&gt;desired state&lt;/strong&gt; (which is, we want three replicas) and
reschedules immediately another Pod.
This is what we mean by a &lt;strong&gt;self-healing system&lt;/strong&gt;. This is one of the major roles
of an orchestrator.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;the-web-service-service&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: Service&lt;/h2&gt;
&lt;p&gt;Now we need to define a &lt;strong&gt;Service&lt;/strong&gt; in order to expose the web service to the public.
Here is the definition:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Describe the specification of this service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We create a service named &lt;code&gt;web&lt;/code&gt;. This indicated in the field
&lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The service has type &lt;code&gt;LoadBalancer&lt;/code&gt;. This is indicated in the field
&lt;code&gt;spec.type&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The service listens on port 8080 and uses the TCP protocol.
Each service instance (i.e., Pod) listens on port 3000.
This is indicated in the field &lt;code&gt;spec.ports&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instances of the service (i.e., the Pods) are those with labels
&lt;code&gt;app: pets&lt;/code&gt; and &lt;code&gt;service: web&lt;/code&gt;.
This is indicated in the field &lt;code&gt;spec.selector&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-service.yaml&lt;/code&gt;
(or any name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create the service with the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Locate the external IP (let’s call it EXTERNAL-IP) of the web service and type
the URL &lt;code&gt;http://EXTERNAL-IP:8080&lt;/code&gt; in your Web browser. You should
see a Web page where the phrase &lt;em&gt;Pet store&lt;/em&gt; appears.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-statefulset&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: StatefulSet&lt;/h2&gt;
&lt;p&gt;Kubernetes has defined a special type of ReplicaSet for stateful services that
is called &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;StatefulSet&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s define a StatefulSet to give the specification of
the &lt;code&gt;db&lt;/code&gt; service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: pets
      service: db
  serviceName: db
  template:
    metadata:
      labels:
        app: pets
        service: db
    spec:
      containers:
      - image: quercinigia/pet-store-db:1.0
        name: db
        ports:
        - containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: pets-data
  volumeClaimTemplates:
  - metadata:
      name: pets-data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now you shouldn’t have any problem understanding the meaning of the fields
in this specification.
The fields are also
documented in the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec&#34; target=&#34;_blank&#34;&gt;official Kubernetes documentation&lt;/a&gt;.
Two points are worth a comment.
First, the specification of a &lt;code&gt;StatefulSet&lt;/code&gt; needs an attribute &lt;code&gt;serviceName&lt;/code&gt; that indicates the name
of the service that is responsible for the network identify of the Pods in the set.
That attribute is mandatory.&lt;/p&gt;
&lt;p&gt;Another novelty is the field &lt;code&gt;volumeClaimTemplates&lt;/code&gt;.
A volume claim template is a way to define the storage needs of a Pod.
The claim will result in the generation of a volume
&lt;code&gt;pets-data&lt;/code&gt; (where PostgreSQL keeps the data).
Also, the following parameters are given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The access mode &lt;code&gt;ReadWriteOnce&lt;/code&gt; means that the volume can be mounted
as read-write by a single node in the Kubernetes cluster.
Access modes are &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34; target=&#34;_blank&#34;&gt;documented here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We request at least 100MB of storage for the database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-db:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this StatefulSet in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-stateful-set.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;
Type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created when you deployed the StatefulSet?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A Pod (likely to be called &lt;code&gt;pod/db-0&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A new StatefulSet that governs the Pod.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the StatefulSet has only one Pod, which is normal,
since we didn’t specify a higher number of replicas.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;
Open the Web browser and type the following URL:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://localhost:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Minikube
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://minikube-ip:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where you’ll replace &lt;code&gt;minikube-ip&lt;/code&gt; with the external IP address associated with the
&lt;code&gt;web&lt;/code&gt; service.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Right after, type the command &lt;code&gt;kubectl get all&lt;/code&gt;.
What do you observe? Can you explain the reason?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We cannot access the application.
If you type the command &lt;code&gt;kubectl get all&lt;/code&gt; quickly, you should show that the
three Pods that are instances of the service &lt;code&gt;web&lt;/code&gt; are in an error state.
If you were too slow, you should see that the restart count of each Pod has changed.
This means that the Pods have been restarted because of an error.&lt;/p&gt;
&lt;p&gt;What happens here is that we launched the database,
but we didn’t create the Service that lets the clients
access the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;In the output of the command &lt;code&gt;get kubectl all&lt;/code&gt;, look at the names of the
Pods that are instances of the &lt;code&gt;web&lt;/code&gt; service.
&lt;!-- in the TP you can even ask why all the three instances got an error--&gt;
Take the name of any these Pods,
and put it in place of NAME-OF-POD in the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;
Does the output of the previous command confirm the explanation
given in the previous question?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Yes, we can clearly read ENOTFOUND db db:5432.
This means that Kubernetes cannot resolve the name &lt;code&gt;db&lt;/code&gt; to an IP address.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-service-object&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: Service object&lt;/h1&gt;
&lt;p&gt;From the above observations, we understand that we need to define
a service to expose the database to the clients.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Should the &lt;code&gt;db&lt;/code&gt; service be accessible to client applications that are external to
the Kubernetes cluster?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;No, this component is the backend of the application. The only client
that needs to access the database is the frontend, that is the
&lt;code&gt;web&lt;/code&gt; service, that runs inside the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Given the answer to the previous question, what should the type of this service be?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;strong&gt;ClusterIP&lt;/strong&gt;; this is the type of a service that does not need to
be exposed to the outside world.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Write the specification of the &lt;code&gt;db&lt;/code&gt; service in a file
named &lt;code&gt;db-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caution.&lt;/strong&gt; In the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; the field &lt;code&gt;spec.serviceName&lt;/code&gt; indicates the name
that the service must have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  type: ClusterIP
  ports:
  - port: 5432
    protocol: TCP
  selector:
    app: pets
    service: db&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy the service by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that you can reach the application at &lt;code&gt;http://localhost:8080/pet&lt;/code&gt; (Minikube users:
replace &lt;code&gt;localhost&lt;/code&gt; with the external IP address associated to the &lt;code&gt;web&lt;/code&gt; service!).
&lt;strong&gt;It might happen&lt;/strong&gt; that the database service is not ready yet, and so you’ll get
a connection error. Just wait a couple of seconds and retry.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application-1&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;After you’re done with the application, you can shut it down with the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete deploy/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete statefulset/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The order in which you type these commands doesn’t matter.&lt;/p&gt;
&lt;p&gt;If you type multiple times the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see that the resources progressively disappear.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Conclusion&lt;/h2&gt;
&lt;p&gt;In the previous exercises, we deployed an application with two services, for which we had to create
four files and type as many commands.
For larger applications this gets a bit annoying.
We can write all the definitions in a single file (e.g., &lt;code&gt;pets.yaml&lt;/code&gt;)
where each specification is terminated by - - -.&lt;/p&gt;
&lt;p&gt;Here is an example, where we write the specification of the
Service and Deployment associated with the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Kubernetes</title>
      <link>/courses/cloudalbert/tutorials/kube-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloudalbert/tutorials/kube-tutorial/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!--::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Warning**

 **If you use the Linux VM**, you need to change the hardware configuration of the VM before 
booting it. You can also [watch this video](https://webtv.centralesupelec.fr/permalink/v12619ffbdbb7mpc6xbp/){target=&#34;_blank&#34;}.
Here are the necessary modifications:

* Increase the main memory to 4GB.

* Set the number of CPUs to 2.

Without these modifications you&#39;re not going to have a good experience with Kubernetes.

::: --&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;build&lt;/strong&gt; and &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Docker Compose&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;push an image&lt;/strong&gt; to &lt;strong&gt;DockerHub&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to &lt;strong&gt;deploy&lt;/strong&gt; a &lt;strong&gt;multi-service application&lt;/strong&gt; with &lt;strong&gt;Kubernetes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This tutorial is adapted from the examples presented in Chapters
14 and 20 of the book &lt;a href=&#34;https://www.packtpub.com/product/learn-docker-fundamentals-of-docker-19-x-second-edition/9781838827472&#34; target=&#34;_blank&#34;&gt;G. Schenker, &lt;em&gt;Learn Docker - Fundamentals of Docker 19.x&lt;/em&gt; (March 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;docker-compose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Docker Compose&lt;/h1&gt;
&lt;p&gt;In this section, you’re going to build and deploy a multi-service application
by using &lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download &lt;a href=&#34;/courses/cloud-computing/tutorial-kube/pet-store.zip&#34;&gt;this archive file&lt;/a&gt; and unzip it into a folder on your
own computer.
The archive contains all the necessary files to build and run
a web application consisting of two services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;web&lt;/strong&gt;. This is the &lt;strong&gt;frontend&lt;/strong&gt; (the part the user interacts with)
of the application.
It consists of HTML and JavaScript code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt;. This is the &lt;strong&gt;backend&lt;/strong&gt; (the part hidden to the user).
It is a &lt;a href=&#34;https://www.postgresql.org/&#34; target=&#34;_blank&#34;&gt;PostgreSQL&lt;/a&gt; (relational) database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The structure of the application is shown in Figure &lt;a href=&#34;#fig:pet-store-struct&#34;&gt;1.1&lt;/a&gt;.
The root directory of the application contains two subdirectories, one for each service
(&lt;code&gt;database&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pet-store-struct&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-kube/pet-store-files.png&#34; alt=&#34;Structure of the application&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Structure of the application
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The files with extension &lt;code&gt;.conf&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contain
configuration parameters of the PostgreSQL database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The file &lt;code&gt;init-db.sql&lt;/code&gt; under the directory &lt;code&gt;database&lt;/code&gt; contains the SQL queries
to populate the database with some data (photos of cats).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The directory &lt;code&gt;web&lt;/code&gt; contains the HTML and JavaScript code of the web application.
The file &lt;code&gt;package.json&lt;/code&gt; contains the dependencies to install.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Both directories contain a &lt;strong&gt;Dockerfile&lt;/strong&gt;.
The one in directory &lt;code&gt;database&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM postgres
COPY init-db.sql /docker-entrypoint-initdb.d/
RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql
ENV POSTGRES_USER dockeruser
ENV POSTGRES_PASSWORD dockerpass
ENV POSTGRES_DB pets&lt;/code&gt;&lt;/pre&gt;
&lt;!-- The Dockerfile builds on an existing image called ``postgres:10-2-alpine``, 
that is [documented here](https://hub.docker.com/_/postgres){target=&#34;_blank&#34;}.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
Consider the following line in the Dockerfile:

``
COPY init-db.sql /docker-entrypoint-initdb.d/
``

By looking at the documentation of the image ``postgres:10-2-alpine``, 
answer the two following questions:

* Where is the directory ``/docker-entrypoint-initdb.d/``?

* Why do we copy the file ``init-db.sql`` to this directory?

&lt;/div&gt;\EndKnitrBlock{exercise}


:::



&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

* The directory already exists in the base image.

* By looking at the documentation, we learn that 
we can put any initialization script in this directory.
In other words, at the startup the database will execute the 
SQL queries in file ``init-db.sql`` so that the database
is populated with new data.

From the documentation, we also learn that the script is not executed 
if the data directory is not empty. This means that already existing databases
will not be touched. 

:::

&lt;/details&gt; --&gt;
&lt;!-- The last three lines of the Dockerfile contain a keyword (``ENV``)
that we never came across before.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;
Look again at the documentation of the image ``postgres:10-2-alpine`` 
and try to explain the meaning of the last three lines of the Dockerfile.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::


&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

These lines set the value of  three **environment variables**, useful 
to pass some parameters to the database.
From the documentation we learn that:

* ``POSTGRES_USER`` is the name of the database user. If not set, 
the default is the user ``postgres``.

* ``POSTGRES_PASSWORD`` is the password associated with the 
user. This is the only mandatory variable.

* ``POSTGRES_DB`` is the name given to the database. 
If not specified, the database name would be the same as the username.

:::

&lt;/details&gt; --&gt;
&lt;p&gt;The Dockerfile of the web application is as follows:&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM node:9.6-alpine
RUN mkdir /app
WORKDIR /app
COPY package.json /app/
RUN npm install
COPY ./src /app/src
EXPOSE 3000
CMD node src/server.js&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Dockerfile builds on the image &lt;code&gt;node:9.6-alpine&lt;/code&gt; that contains
a Node.js environment, a JavaScript-based platform for server-side
applications.
The instructions in the Dockerfile look like the ones of the examples
that we’ve seen in the first tutorial and in the lectures.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The command &lt;code&gt;npm install&lt;/code&gt; installs all the dependencies specified
in the file &lt;code&gt;package.json&lt;/code&gt;
from the &lt;a href=&#34;https://docs.npmjs.com/about-npm&#34; target=&#34;_blank&#34;&gt;software registry npm&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instruction &lt;code&gt;EXPOSE 3000&lt;/code&gt; informs that the container
listens on port 3000 when it is executed from the image.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#expose&#34; target=&#34;_blank&#34;&gt;Docker documentation&lt;/a&gt;
we learn
that the &lt;em&gt;“&lt;code&gt;EXPOSE&lt;/code&gt; instruction
does not actually publish the port.
It functions as a type of
documentation between
the person who builds the image
and the person who runs the container, about which ports are intended to be published”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In order to actually publish the port, we’ll use the file
&lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;describing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Describing the application&lt;/h2&gt;
&lt;p&gt;The application showcases a building block of a pet store.
In the current version, the application shows a few pictures
of cats.&lt;/p&gt;
&lt;p&gt;The root directory of the application contains a file named
&lt;code&gt;docker-compose.yml&lt;/code&gt; that contains the &lt;strong&gt;declarative configuration&lt;/strong&gt;
of the application.
The content of the file is as follows. It is a sequence of key-value pairs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: &amp;quot;3.6&amp;quot;
services:
  web:
    build: web
    image: pet-store-web
    networks:
      - backend
    ports:
      - 5000:3000
  db:
    build: database
    image: pet-store-db
    networks:
      - backend
    volumes:
      - pets-data:/var/lib/postgresql/data

networks:
  backend:

volumes:
  pets-data:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are three main sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;services&lt;/code&gt;. Defines the services of the application. Here two services
are defined: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;networks&lt;/code&gt;. Defines the networks used by the application.
Here one network is defined: &lt;code&gt;backend&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;volumes&lt;/code&gt;. Defines the volumes used by the application.
Here one volume is defined: &lt;code&gt;pets-data&lt;/code&gt;.
The volume is attached to the directory &lt;code&gt;/var/lib/postgresql/data&lt;/code&gt;
(that is in the container).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
What key informs &lt;code&gt;docker compose&lt;/code&gt; where to find the
Dockerfile of the two services?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The value of the key &lt;code&gt;build&lt;/code&gt; is the name of the directory that contains
the Dockefile. For the service &lt;code&gt;web&lt;/code&gt;, this is the directory
&lt;code&gt;web&lt;/code&gt;; for the service &lt;code&gt;db&lt;/code&gt;, it is the directory &lt;code&gt;database&lt;/code&gt;.
All paths are relative to the position of the file &lt;code&gt;docker-compose.yml&lt;/code&gt;
itself.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;When we’ll build the application from the file &lt;code&gt;docker-compose.yml&lt;/code&gt;,
two images will be created, one for each service.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What will the name of the two images be?
What key in the file &lt;code&gt;docker-compose.yml&lt;/code&gt; gives you this information?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The name of the image corresponding to the service &lt;code&gt;web&lt;/code&gt;
will be &lt;code&gt;pet-store-web&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The name of the image corresponding to the service &lt;code&gt;db&lt;/code&gt;
will be &lt;code&gt;pet-store-db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This information is given by the key &lt;code&gt;image&lt;/code&gt;
in the file &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-application&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Building the application&lt;/h2&gt;
&lt;p&gt;We now build the application.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open the command-line terminal and by using the command &lt;code&gt;cd&lt;/code&gt;
position yourself in the root directory
&lt;code&gt;pet-store&lt;/code&gt; of the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;During the build you might get a &lt;code&gt;npm&lt;/code&gt; warning. Just ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the build is complete, verify that the two images corresponding
to the two services have been created (which docker command do you need here?).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;docker images&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;executing-the-application&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Executing the application&lt;/h2&gt;
&lt;p&gt;We now execute the application with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose up&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If your application fails to start because the port number that you chose is already in use,
try using another port.&lt;/p&gt;
&lt;p&gt;Don’t use the following ports as they are deemed unsafe by the browsers:&lt;/p&gt;
&lt;p&gt;1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The execution of the command will print a series of messages on screen.
The terminal should hang when the messages stop
(the last message should be &lt;code&gt;database system is ready to accept connections&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Take no action on the screen and open a new terminal or a new tab in the current terminal
window.&lt;/p&gt;
&lt;!--
:::{.infobox .curiosity data-latex=&#34;curiosity&#34;}
**Good to know**

The option ``-d`` in the previous command means that the application is 
executed **in the background** or, in other words, 
the containers are run as **daemons** (hence, the ``-d``).
This means that any output of the application is not
shown in the terminal.

:::
--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Verify that the network and the volumes associated with the
application have been correctly created.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Type the commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;to verify respectively that the volumes and the networks have been created&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
How many containers do you expect to be associated to
the application?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Each &lt;strong&gt;image&lt;/strong&gt; corresponds to a &lt;strong&gt;service&lt;/strong&gt;.
Here we have two.
Each &lt;strong&gt;container&lt;/strong&gt; corresponds to a &lt;strong&gt;service instance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We didn’t specify any parameter when we launched the application,
so by default one instance of each service is executed.&lt;/p&gt;
&lt;p&gt;So, we expect to have &lt;strong&gt;two containers&lt;/strong&gt; associated to the application:
one container for the service &lt;code&gt;web&lt;/code&gt; and one for the
service &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;You can verify the answer to the previous question by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose ps&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command is exactly equivalent to &lt;code&gt;docker container ls&lt;/code&gt;, except that
it only shows the containers associated with the application
that we’ve just executed.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
In the output of the command &lt;code&gt;docker-compose ps&lt;/code&gt;, can you explain
the meaning of the following?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.0.0.0:5000-&amp;gt;3000/tcp&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The notation 5000-&amp;gt;3000 means that the port
5000 of the host computer is &lt;strong&gt;mapped&lt;/strong&gt;
to the port 3000 of the container.
More specifically,
when a client application connects to the port 5000 of the host computer,
the connection is redirected to the port 3000 of the container.&lt;/p&gt;
&lt;p&gt;The IP address 0.0.0.0 means that we can connect to the container by
using any IP address on the host computer.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
Can you
tell which URL you have to type in your web browser
to access the application?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;http://localhost:5000&lt;/code&gt; or &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the network settings of you computer, you can also
get the IP address associated by the DHCP server of your&lt;br /&gt;
local network and use that address to connect to the application
from another device &lt;strong&gt;connected to the same network&lt;/strong&gt;
(for instance, your mobile phone).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to see the cats, you need to append
&lt;code&gt;/pet&lt;/code&gt; to the URL that you found in the previous question.
This is determined in file &lt;code&gt;server.js&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;You can shut down the application by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose down&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;
Does shutting down the application remove the networks created
for the application?
What about the volumes?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Just type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and you’ll see that the network &lt;code&gt;backend&lt;/code&gt; is not there anymore.&lt;/p&gt;
&lt;p&gt;Type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and you’ll see that the volume &lt;code&gt;pet-store_pets-data&lt;/code&gt; is still there.&lt;/p&gt;
&lt;p&gt;By default volumes are not removed, to avoid the risk of permanently deleting
data that might still be useful later.
If you wish to remove the volumes when shutting down the application,
you can type:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose down -v&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-a-service&#34; class=&#34;section level2&#34; number=&#34;1.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Scaling a service&lt;/h2&gt;
&lt;p&gt;When we launch the application, we can specify the
number of instances of each service.
This is useful when we expect our application to be solicited by many users;
the workload will be automatically balanced across all the instances of the service.&lt;/p&gt;
&lt;p&gt;Let’s launch our application with 3 instances of the service &lt;code&gt;web&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker-compose up --scale web=3 -d&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;
Running the previous command results in an error.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can you explain why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What fix would you propose in file &lt;code&gt;docker-compose.yml&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We’re trying to bind three instances to the port 5000 on the host computer. This is not
possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the section &lt;code&gt;ports&lt;/code&gt; of file
&lt;code&gt;docker-compose.yml&lt;/code&gt; we need to remove 5000 and we only leave 3000. This way, each time we run a container, a random port will be chosen on the host computer to bind it
with port 3000 of the container.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to shut down the application before relaunching it again.
In fact, even if we got an error, an instance of the &lt;code&gt;db&lt;/code&gt; service and
an instance of the service &lt;code&gt;web&lt;/code&gt; are still running.&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modify the file &lt;code&gt;docker-compose.yml&lt;/code&gt; and relaunch the application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the following command and verify that you actually have three running instances of the
service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose ps&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try to connect to the application by using the three port numbers indicated in the output
of the previous command.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Don’t forget to shut down the application before you
go on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-an-application&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Pushing an application&lt;/h1&gt;
&lt;p&gt;Once we have built an application, we might want to
share it by pushing it to the DockerHub registry or any
other container registry, be it private or public.&lt;/p&gt;
&lt;div id=&#34;creating-an-account-on-dockerhub&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Creating an account on DockerHub&lt;/h2&gt;
&lt;p&gt;You need to create an account on the &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34;&gt;DockerHub website&lt;/a&gt;
in order to perform this activity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renaming-the-images&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Renaming the images&lt;/h2&gt;
&lt;p&gt;In order to push your images to the registry, you need to rename them, so as the new name
has the following structure:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yourusername/image-name:image-tag&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For instance, since my username is &lt;code&gt;quercinigia&lt;/code&gt;, I’ll rename
the two images &lt;code&gt;pet-store-web&lt;/code&gt; and &lt;code&gt;pet-store-db&lt;/code&gt; with the
&lt;code&gt;docker tag&lt;/code&gt; command as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-web quercinigia/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-db quercinigia/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I chosen 1.0 as a tag, but feel free to pick another one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-in&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Logging in&lt;/h2&gt;
&lt;p&gt;You need to log in to your DockerHub account.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;macOS users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-macos/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Windows users: to see how to log in &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-windows/&#34; target=&#34;_blank&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;pushing-the-images&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Pushing the images&lt;/h2&gt;
&lt;p&gt;Once you’re successfully logged in, you can type the following
commands in the terminal to push the two images of your application:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push YOUR-USERNAME/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Remember to replace YOUR-USERNAME with your actual username in the commands above.&lt;/p&gt;
&lt;p&gt;After the task is completed, verify that the images appear
in &lt;a href=&#34;https://hub.docker.com/repositories&#34; target=&#34;_blank&#34;&gt;your Docker registry&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here we manually tagged and uploaded the two images.
This method becomes quickly annoying when the application consists of
more than two images.
Another way to go about this task is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specify the image names with your username in file &lt;strong&gt;docker-compose.yml&lt;/strong&gt;.
For instance, in my &lt;strong&gt;docker-compose.yml&lt;/strong&gt; I would write:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;services:
  web:
    build: web
    image: quercinigia/pet-store-web:1.0
  ...
  db:
    build: database
    image: quercinigia/pet-store-db:1.0
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Build the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Push the images of the application with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker-compose push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This way, with just two commands we push to the registry all the images
of the application, no matter how many they are.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-kubernetes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Introduction to Kubernetes&lt;/h1&gt;
&lt;p&gt;Kubernetes is the most popular &lt;strong&gt;orchestrator&lt;/strong&gt; to date.
It is used to manage a multi-service application, usually deployed
across multiple &lt;strong&gt;hosts&lt;/strong&gt; in a &lt;strong&gt;cluster&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;While using Docker Compose, a &lt;strong&gt;service&lt;/strong&gt; corresponds
to a &lt;strong&gt;Docker image&lt;/strong&gt; and
a &lt;strong&gt;service instance&lt;/strong&gt; to a &lt;strong&gt;Docker container&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As opposed to that, in Kubernetes the computation unit is a &lt;strong&gt;pod&lt;/strong&gt;, that is
a &lt;strong&gt;collection of containers&lt;/strong&gt;.
In other words, we don’t reason in terms of containers anymore,
but in terms of pods. Of course, a pod can also consist of just one container.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In practice, we rarely have to manipulate pods directly in Kubernetes,
as there are higher-level objects that manage them.
We’ll use these objects in the next sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activate-kubernetes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Activate Kubernetes&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions for Docker Desktop (macOS and Windows)
&lt;/summary&gt;
&lt;p&gt;You need to follow all the instructions documented
&lt;a href=&#34;https://docs.docker.com/desktop/kubernetes/&#34; target=&#34;_blank&#34;&gt;on this page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Before moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see a node called &lt;code&gt;docker-desktop&lt;/code&gt; whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This solution might not be working for you.
In that case, disable Kubernetes in Docker Desktop and install
minikube, by following &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!-- &lt;details&gt;
&lt;summary&gt;Instructions for the Linux VM&lt;/summary&gt;

In the Linux VM you&#39;ll find **Minikube**, a single-node Kubernetes cluster
in VirtualBox. 
Please follow **all the instructions**:

* Start the cluster, type the following command:

``
minikube start
``

* Verify that Kubernetes is correctly activated by typing the following command:

``
kubectl get nodes
``

You should see a node called ``minikube`` whose status is set to READY.
If the status is NOT READY, just wait few seconds before typing the command again.


* Open a **new terminal** and type the command:

``
minikube tunnel
``

When prompted to enter a password, just type ENTER.
The command will start to produce some output. 
**Leave that terminal open** and go back to the previous terminal.

:::{.infobox .curiosity data-latex=&#34;curiosity&#34;}
**Tunnel**

The Minikube tunnel is used to create a route to services deployed 
with type ``LoadBalancer``. If you don&#39;t activate the tunnel, 
you won&#39;t be able to use these services in the exercises below.

:::


&lt;/details&gt; --&gt;
&lt;/div&gt;
&lt;div id=&#34;deploying-a-pod&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Deploying a pod&lt;/h2&gt;
&lt;p&gt;In order to deploy &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34; target=&#34;_blank&#34;&gt;a pod&lt;/a&gt;,
we first have to give its &lt;strong&gt;specification&lt;/strong&gt;, basically its name
and the containers that compose it with their settings.
Similarly to Docker Compose, a pod is specified in a &lt;strong&gt;declarative way&lt;/strong&gt; with a
YAML configuration file.&lt;/p&gt;
&lt;p&gt;Consider the following specification.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web-pod
spec:
  containers:
  - name: web
    image: nginx:alpine
    ports:
    - containerPort: 80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is an explanation of the properties in the specification.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;apiVersion&lt;/em&gt;. Defines the versioned schema of this representation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;kind&lt;/em&gt;. The type of the resource that we intend to create.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;metadata&lt;/em&gt;. The resource metadata.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;spec&lt;/em&gt;. The specification of the desired behaviour of the pod.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these fields are described in the &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/#pod-v1-core&#34; target=&#34;_blank&#34;&gt;Kubernetes API reference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Essentially, the previous specification defines a pod with a container that is launched from the
image &lt;code&gt;nginx:alpine&lt;/code&gt; (a Web server) and listens to ports 80 and 443.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy the previous specification to a file named &lt;code&gt;sample-pod.yaml&lt;/code&gt; (or any other name
of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-pod.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the pod by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the pod is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first time you run the last command, you might see that the pod is not
ready yet. You need to wait for the image &lt;code&gt;nginx:alpine&lt;/code&gt; to be pulled from DockerHub.
Wait few seconds and try the command again until the pod is marked as running.&lt;/p&gt;
&lt;p&gt;You can also get more information on the running pod (e.g., its assigned IP address)
by typing the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod -o wide web-pod&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Open a web browser and type &lt;code&gt;http://localhost:80&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What do you get? What if you try to use the IP of the pod instead of &lt;code&gt;locahost&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What should we define in order to fix the problem?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The connection is refused both when using &lt;code&gt;localhost&lt;/code&gt; and
the IP address of the pod.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to connect to the service instance, we need to
define a &lt;strong&gt;Service&lt;/strong&gt; object in Kubernetes, that exposes an IP address
that client applications can use.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;The following configuration
defines a Service object of type &lt;strong&gt;LoadBalancer&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
      app: web-pod&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;LoadBalancer&lt;/strong&gt; service is a &lt;strong&gt;NodePort&lt;/strong&gt; service
that offers load balancing capabilities.
It is intended to expose an IP address
that client applications (external to the Kubernetes cluster) can use to access
the service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
What is the field &lt;code&gt;selector&lt;/code&gt; in the definition of the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The selector is used internally by Kubernetes to identify the pods that
are the instances of the service.
If you look at the specification of the pod above, the field &lt;code&gt;metadata&lt;/code&gt;
contains a field &lt;code&gt;labels&lt;/code&gt;.
One of the labels is &lt;code&gt;app: web-pod&lt;/code&gt; ;
the selector matches this label.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
In the specification of the service, what &lt;code&gt;port&lt;/code&gt; and &lt;code&gt;targetPort&lt;/code&gt; mean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;port&lt;/code&gt; specifies the port number where the service will be available.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;targetPort&lt;/code&gt; specifies the port number opened at the pods that are instances of the service.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the service specification into a file named &lt;code&gt;sample-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using the command &lt;code&gt;cd&lt;/code&gt; in the terminal, place yourself in the directory
where the file &lt;code&gt;sample-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy the service by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service is deployed by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command returns all services running in Kubernetes.
For each service, you also get a name.
In order to only target the service that you’ve just created, simply type
the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Your service’s name is &lt;code&gt;nginx-service&lt;/code&gt; (as specified in file &lt;code&gt;sample-service.yml&lt;/code&gt;);
&lt;code&gt;svc&lt;/code&gt; is only the namespace where all Kubernetes services are
put.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
By looking at the output of the command &lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;,
which URL do you need to type in the Web browser in order to access the service?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;CLUSTER-IP&lt;/code&gt; is an IP address that is only visible &lt;strong&gt;within the cluster&lt;/strong&gt;.
In other words, only the services running within the cluster can connect to our
&lt;code&gt;nginx-service&lt;/code&gt; by using this IP address.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What we need to use is the IP address specified under &lt;code&gt;EXTERNAL-IP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; is likely to be &lt;code&gt;localhost&lt;/code&gt;
or &lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Minikube
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; will be whatever IP address assigned to the service.&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;As for the port number, under PORT(S) you should see something like 8080:30548
(the second number is likely different, but it should be something in the range [30000-32767]).
The port 30548 is opened on each node of the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, when we type &lt;code&gt;http://localhost:8080&lt;/code&gt;, the load balancer replaces it
with the IP address of a node in the cluster,
followed by port number 30548.
When the &lt;code&gt;kube-proxy&lt;/code&gt; on that node receives this request,
it forwards it to an instance of the service,
even if that instance runs in another node.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Stop the pod and the service before moving on. Here are the commands to do so:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete po/web-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kubernetes-deploying-an-application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Kubernetes: deploying an application&lt;/h1&gt;
&lt;p&gt;In this section, we’re going to deploy our pet store in Kubernetes.
As a reminder, our application consists of two services: &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to define an application in Kubernetes,
we need to use two types of objects &lt;strong&gt;for each service&lt;/strong&gt; of the application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;workload resource&lt;/strong&gt; that gives the specification of the service, such as
the pods that make up the service itself (images, network settings) and metadata, such as
the desired number of instances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;Service&lt;/strong&gt; object. As we have seen previously, a service object
exposes an IP address that allows client applications,
both inside and outside the Kubernetes cluster, to connect to the service.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we define an application in Kubernetes, we rarely,
if ever, need to play directly with pods.
Instead, we can resort to higher-level objects, called &lt;strong&gt;Controllers&lt;/strong&gt;, for an easier
definition of the &lt;strong&gt;desired state&lt;/strong&gt; of the application itself.
The type of the controller that we need to use depends on the nature of the service
itself: &lt;strong&gt;stateless&lt;/strong&gt; or &lt;strong&gt;stateful&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;web&lt;/code&gt; of our application stateless or stateful?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the service &lt;code&gt;db&lt;/code&gt; of our application stateless of stateful?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stateful&lt;/strong&gt; service: one that manages permanent data and is bound to that data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stateless&lt;/strong&gt; service: one that is not bound to permanent data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The service &lt;code&gt;web&lt;/code&gt; is &lt;strong&gt;stateless&lt;/strong&gt; because it does not have to be tightly bound to the data.
In other words, the service &lt;code&gt;web&lt;/code&gt; uses the data that is stored in the backend database, but
it can be scheduled on any node of the cluster independently of that data.&lt;/p&gt;
&lt;p&gt;The service &lt;code&gt;db&lt;/code&gt; is &lt;strong&gt;stateful&lt;/strong&gt; because it directly manages permanent data.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;the-web-service-deployment&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: deployment&lt;/h2&gt;
&lt;p&gt;For &lt;strong&gt;stateless application services&lt;/strong&gt;, we can use
a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt; for the deployment of a set of identical pods
that are launched across several nodes in the Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;We here define the specification of
a Deployment corresponding to the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the explanation of the specification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A Deployment named &lt;code&gt;web&lt;/code&gt; is created, as indicated by the field &lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment creates three replicated pods, as indicated by the field
&lt;code&gt;spec.replicas&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Deployment considers that the pods with &lt;strong&gt;both&lt;/strong&gt; labels &lt;code&gt;app: pets&lt;/code&gt; and
&lt;code&gt;service: web&lt;/code&gt; are part of the deployment.
This is indicated by the field &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
This field is used to let the Deployment know how to find its pods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The configuration of the pods that are part of the Deployment is
given in the field &lt;code&gt;spec.template&lt;/code&gt; and its subfields.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each pod of the Deployment is given labels &lt;code&gt;app: pets&lt;/code&gt; and &lt;code&gt;service: web&lt;/code&gt;.
This is indicated by the field &lt;code&gt;spec.template.metadata.labels&lt;/code&gt;.
Note that here we specify exactly the same values as in &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
This field is used to give pods labels so that they can be identified and located in
Kubernetes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each pod has exactly one container, named &lt;code&gt;web&lt;/code&gt;, run from the
image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt; stored in the DockerHub. The container listens on port 3000
and uses the TCP protocol. This is specified in the field &lt;code&gt;spec.template.spec.containers&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-deployment.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-deployment.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this Deployment in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-deployment.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
Type the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created following the creation of the
Deployment?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We can clearly see three types of objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Three pods, that correspond to the three replicas that we specified in the configuration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One deployment. Note that the 3/3 indicates that the deployment has
3 identical pods, of which 3 are running.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One ReplicaSet. The Deployment creates a ReplicaSet, that is the actual object that
controls the three pods. The Deployment provides some additional services (i.e., updates)
on top of a ReplicaSet.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Get the name of one of the running pods and
kill it by using the following command&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete name-of-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If the command hangs in the terminal, feel free to type Ctrl-C to
get back control of the terminal.&lt;/p&gt;
&lt;p&gt;Type again following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;How many pods do you see? Is it surprising?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;If we type the command right after deleting the pod, it is likely that we’ll see
four pods, one that is &lt;em&gt;terminating&lt;/em&gt; and another one that is &lt;em&gt;running&lt;/em&gt;.
Try to type the command until you see three running pods.&lt;/p&gt;
&lt;p&gt;This is not surprising.
If we kill the pod, the orchestrator detects a
deviation from the &lt;strong&gt;desired state&lt;/strong&gt; (which is, we want three replicas) and
reschedules immediately another pod.
This is what we mean by a &lt;strong&gt;self-healing system&lt;/strong&gt;. This is one of the major roles
of an orchestrator.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;the-web-service-service&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; The &lt;code&gt;web&lt;/code&gt; service: service&lt;/h2&gt;
&lt;p&gt;Now we need to define a &lt;strong&gt;Service&lt;/strong&gt; in order to expose the web service to the public.
Here is the definition:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Describe the specification of this service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We create a service named &lt;code&gt;web&lt;/code&gt;. This indicated in the field
&lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The service has type &lt;code&gt;LoadBalancer&lt;/code&gt;. This is indicated in the field
&lt;code&gt;spec.type&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The service listens on port 8080 and uses the TCP protocol.
Each service instance (i.e., pod) listens on port 3000.
This is indicated in the field &lt;code&gt;spec.ports&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The instances of the service (i.e., the pods) are those with labels
&lt;code&gt;app: pets&lt;/code&gt; and &lt;code&gt;service: web&lt;/code&gt;.
This is indicated in the field &lt;code&gt;spec.selector&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;web-service.yaml&lt;/code&gt;
(or any name of your liking).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;web-service.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create the service with the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Locate the external IP (let’s call it EXTERNAL-IP) of the web service and type
the URL &lt;code&gt;http://EXTERNAL-IP:8080&lt;/code&gt; in your Web browser. You should
see a Web page where the phrase &lt;em&gt;Pet store&lt;/em&gt; appears.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-statefulset&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: StatefulSet&lt;/h2&gt;
&lt;p&gt;Kubernetes has defined a special type of ReplicaSet for stateful services that
is called &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;StatefulSet&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s define a StatefulSet to give the specification of
the &lt;code&gt;db&lt;/code&gt; service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: pets
      service: db
  serviceName: db
  template:
    metadata:
      labels:
        app: pets
        service: db
    spec:
      containers:
      - image: quercinigia/pet-store-db:1.0
        name: db
        ports:
        - containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: pets-data
  volumeClaimTemplates:
  - metadata:
      name: pets-data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now you shouldn’t have any problem understanding the meaning of the fields
in this specification.
The field &lt;code&gt;volumeClaimTemplates&lt;/code&gt; describes additional constraints on the volumes defined in the specification, in this case the volume
named &lt;code&gt;pets-data&lt;/code&gt; (where PostgreSQL keeps the data). In particular, two claims are given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The access mode &lt;code&gt;ReadWriteOnce&lt;/code&gt; means that the volume can be mounted
as read-write by a single node in the Kubernetes cluster.
Access modes are &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34; target=&#34;_blank&#34;&gt;documented here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We request at least 100MB of storage for the database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copy and paste the previous specification to a new file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt;
(or any name of your liking).
Make sure that you &lt;strong&gt;replace&lt;/strong&gt; the image &lt;code&gt;quercinigia/pet-store-db:1.0&lt;/code&gt;
with the one that you pushed to your DockerHub registry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the command &lt;code&gt;cd&lt;/code&gt; in the terminal to position yourself in the directory
where the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; is stored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deploy this StatefulSet in Kubernetes by typing the following command:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-stateful-set.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;
Type the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which objects have been created when you deployed the StatefulSet?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A pod (likely to be called &lt;code&gt;pod/db-0&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A new StatefulSet that governs the pod.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the StatefulSet has only one pod, which is normal,
since we didn’t specify a higher number of replicas.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;
Open the Web browser and type the following URL:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://localhost:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
If you’re using Minikube
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://minikube-ip:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where you’ll replace &lt;code&gt;minikube-ip&lt;/code&gt; with the external IP address associated with the
&lt;code&gt;web&lt;/code&gt; service.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Right after, type the command &lt;code&gt;kubectl get all&lt;/code&gt;.
What do you observe? Can you explain the reason?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;We cannot access the application.
If you type the command &lt;code&gt;kubectl get all&lt;/code&gt; quickly, you should show that the
three pods that are instances of the service &lt;code&gt;web&lt;/code&gt; are in an error state.
If you were too slow, you should see that the restart count of each pod has changed.
This means that the pods have been restarted because of an error.&lt;/p&gt;
&lt;p&gt;What happens here is that we launched the database,
but we didn’t create the Service that lets the clients
access the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;In the output of the command &lt;code&gt;get kubectl all&lt;/code&gt;, look at the names of the
pods that are instances of the &lt;code&gt;web&lt;/code&gt; service.
&lt;!-- in the TP you can even ask why all the three instances got an error--&gt;
Take the name of any these pods,
and put it in place of NAME-OF-POD in the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;
Does the output of the previous command confirm the explanation
given in the previous question?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Yes, we can clearly read ENOTFOUND db db:5432.
This means that Kubernetes cannot resolve the name &lt;code&gt;db&lt;/code&gt; to an IP address.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-db-service-service-object&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The &lt;code&gt;db&lt;/code&gt; service: Service object&lt;/h1&gt;
&lt;p&gt;From the above observations, we understand that we need to define
a service to expose the database to the clients.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Should the &lt;code&gt;db&lt;/code&gt; service be accessible to client applications that are external to
the Kubernetes cluster?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;No, this component is the backend of the application. The only client
that needs to access the database is the frontend, that is the
&lt;code&gt;web&lt;/code&gt; service, that runs inside the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Given the answer to the previous question, what should the type of this service be?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;strong&gt;ClusterIP&lt;/strong&gt;; this is the type of a service that does not need to
be exposed to the outside world.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Write the specification of the &lt;code&gt;db&lt;/code&gt; service in a file
named &lt;code&gt;db-service.yaml&lt;/code&gt; (or any other name of your liking).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caution.&lt;/strong&gt; In the file &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; the field &lt;code&gt;spec.serviceName&lt;/code&gt; indicates the name
that the service must have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  type: ClusterIP
  ports:
  - port: 5432
    protocol: TCP
  selector:
    app: pets
    service: db&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy the service by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the service has been created with the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that you can reach the application at &lt;code&gt;http://localhost:8080/pet&lt;/code&gt; (Minikube users:
replace &lt;code&gt;localhost&lt;/code&gt; with the external IP address associated to the &lt;code&gt;web&lt;/code&gt; service!).
&lt;strong&gt;It might happen&lt;/strong&gt; that the database service is not ready yet, and so you’ll get
a connection error. Just wait and retry later.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;shutting-down-the-application-1&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Shutting down the application&lt;/h2&gt;
&lt;p&gt;After you’re done with the application, you can shut it down with the following commands:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete deploy/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete statefulset/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The order in which you type these commands doesn’t matter.&lt;/p&gt;
&lt;p&gt;If you type multiple times the command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you should see that the resources progressively disappear.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Conclusion&lt;/h2&gt;
&lt;p&gt;In the previous exercises, we deployed an application with two services, for which we had to create
four files and type as many commands.
For larger applications this gets a bit annoying.
We can write all the definitions in a single file (e.g., &lt;code&gt;pets.yaml&lt;/code&gt;)
where each specification is terminated by - - -.&lt;/p&gt;
&lt;p&gt;Here is an example, where we write the specification of the
Service and Deployment associated with the service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Docker Compose et Kubernetes</title>
      <link>/courses/cloud-fr/kubernetes-fr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/cloud-fr/kubernetes-fr/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Si vous utilisez la VM Linux&lt;/strong&gt;, vous devez modifier la configuration matérielle de la VM avant de la démarrer.
Vous pouvez également &lt;a href=&#34;https://webtv.centralesupelec.fr/permalink/v12619ffbdbb7mpc6xbp/&#34; target=&#34;_blank&#34;&gt;regarder cette vidéo&lt;/a&gt;.
Voici les modifications nécessaires :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Augmenter la mémoire principale à 4 Go.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fixer le nombre de CPU à 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sans ces modifications, Kubernetes pourrait ne pas fonctionner correctement.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Dans ce TD, vous apprendrez à :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Créer&lt;/strong&gt; et &lt;strong&gt;déployer&lt;/strong&gt; une &lt;strong&gt;application multiservice&lt;/strong&gt; avec &lt;strong&gt;Docker Compose&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pousser une image&lt;/strong&gt; vers &lt;strong&gt;DockerHub&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Déployer&lt;/strong&gt; une &lt;strong&gt;application multiservice&lt;/strong&gt; avec &lt;strong&gt;Kubernetes&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ce TD est adapté des exemples présentés dans les chapitres
14 et 20 du livre
&lt;a href=&#34;https://www.packtpub.com/product/learn-docker-fundamentals-of-docker-19-x-second-edition/9781838827472&#34; target=&#34;_blank&#34;&gt;G. Schenker, &lt;em&gt;Learn Docker - Fundamentals of Docker 19.x&lt;/em&gt; (March 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;docker-compose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Docker Compose&lt;/h1&gt;
&lt;p&gt;Dans cette section, vous allez créer et déployer une application multiservice en utilisant &lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Téléchargez &lt;a href=&#34;/courses/cloud-computing/tutorial-kube/pet-store.zip&#34;&gt;cet archive&lt;/a&gt; et décompressez-le dans un dossier de votre ordinateur.
L’archive contient tous les fichiers nécessaires pour créer et exécuter
une application web composée de deux services :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;web&lt;/strong&gt;. C’est le &lt;strong&gt;frontend&lt;/strong&gt; (la partie avec laquelle l’utilisateur interagit)
de l’application.
Il se compose de code HTML et JavaScript.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt;. C’est le &lt;strong&gt;backend&lt;/strong&gt; (la partie cachée à l’utilisateur).
Il s’agit d’une base de données (relationnelle) &lt;a href=&#34;https://www.postgresql.org/&#34; target=&#34;_blank&#34;&gt;PostgreSQL&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La structure de l’application est présentée dans la figure &lt;a href=&#34;#fig:pet-store-struct&#34;&gt;1.1&lt;/a&gt;.
Le dossier racine de l’application contient deux sous-dossiers, un pour chaque service
(&lt;code&gt;database&lt;/code&gt; et &lt;code&gt;web&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pet-store-struct&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/cloud-computing/tutorial-kube/pet-store-files.png&#34; alt=&#34;Structure de l&#39;application.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Structure de l’application.
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Les fichiers avec l’extension &lt;code&gt;.conf&lt;/code&gt; dans le dossier &lt;code&gt;database&lt;/code&gt; contiennent les paramètres de configuration de la base de données PostgreSQL.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le fichier &lt;code&gt;init-db.sql&lt;/code&gt; dans le dossier &lt;code&gt;database&lt;/code&gt; contient les requêtes SQL pour alimenter la base de données avec des données (photos de chats).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le dossier &lt;code&gt;web&lt;/code&gt; contient le code HTML et JavaScript de l’application web.
Le fichier &lt;code&gt;package.json&lt;/code&gt; contient les dépendances à installer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Les deux dossiers contiennet un &lt;strong&gt;Dockerfile&lt;/strong&gt; chacun.
Le Dockerfile qui se trouve dans le dossier &lt;code&gt;database&lt;/code&gt; est le suivant :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM postgres
COPY init-db.sql /docker-entrypoint-initdb.d/
RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql
ENV POSTGRES_USER dockeruser
ENV POSTGRES_PASSWORD dockerpass
ENV POSTGRES_DB pets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ce Dockerfile utilise comme base une image existante appelée &lt;code&gt;postgres&lt;/code&gt;,
qui est &lt;a href=&#34;https://hub.docker.com/_/postgres&#34; target=&#34;_blank&#34;&gt;documentée ici&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Considérez l’instruction suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;COPY init-db.sql /docker-entrypoint-initdb.d/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;En lisant la documentation de l’image &lt;code&gt;postgres&lt;/code&gt;,
répondez aux deux questions suivantes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Où se trouve le dossier &lt;code&gt;/docker-entrypoint-initdb.d/&lt;/code&gt; ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pourquoi copions-nous le fichier &lt;code&gt;init-db.sql&lt;/code&gt; vers ce dossier ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Le dossier existe déjà dans l’image de base.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;En regardant la documentation, nous apprenons que
nous pouvons placer n’importe quel script d’initialisation dans ce dossier.
En d’autres termes, au démarrage, la base de données exécutera les requêtes SQL contenues dans le fichier &lt;code&gt;init-db.sql&lt;/code&gt;.
afin que la base soit alimentée en nouvelles données.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La documentation nous apprend également que le script n’est pas exécuté
si le dossier de données n’est pas vide. Cela signifie que les bases de données existantes
ne seront pas touchées.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Les trois dernières lignes du fichier Docker contiennent un mot-clé (&lt;code&gt;ENV&lt;/code&gt;)
que nous n’avons jamais rencontré auparavant.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Lisez à nouveau la documentation de l’image &lt;code&gt;postgres&lt;/code&gt;
et essayez d’expliquer la signification des trois dernières lignes du Dockerfile.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Ces lignes définissent la valeur de trois &lt;strong&gt;variables d’environnement&lt;/strong&gt;, utiles
pour passer certains paramètres au gestionnaire de base de données (SGBD).
La documentation nous apprend que :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_USER&lt;/code&gt; est le nom d’utilisateur du SGBD. S’il n’est pas défini,
l’utilisateur par défaut est &lt;code&gt;postgres&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_PASSWORD&lt;/code&gt; est le mot de passe associé à l’utilisateur.
C’est la seule variable obligatoire.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;POSTGRES_DB&lt;/code&gt; est le nom donné à la base de données.
S’il n’est pas spécifié, le nom de la base de données sera le même que le nom d’utilisateur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Le Dockerfile de l’application Web est le suivant :&lt;/p&gt;
&lt;pre class=&#34;dockerfile&#34;&gt;&lt;code&gt;FROM node:9.6-alpine
RUN mkdir /app
WORKDIR /app
COPY package.json /app/
RUN npm install
COPY ./src /app/src
EXPOSE 3000
CMD node src/server.js&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Le Dockerfile utilise l’image &lt;code&gt;node:9.6-alpine&lt;/code&gt; comme base ; Cette image contient
un environnement Node.js, permettant l’exécution de JavaScript côté serveur.
Les instructions du Dockerfile ressemblent à celles des exemples que nous avons vus dans le premier TD.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;La commande &lt;code&gt;npm install&lt;/code&gt; installe toutes les dépendances spécifiées dans le fichier &lt;code&gt;package.json&lt;/code&gt;.
Les bibliothèques sont téléchargées depuis le &lt;a href=&#34;https://docs.npmjs.com/about-npm&#34; target=&#34;blank&#34;&gt;registre du gestionnaire de paquet npm&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;L’instruction &lt;code&gt;EXPOSE 3000&lt;/code&gt; informe que le conteneur
sera disponible au port 3000 lorsqu’il sera exécuté à partir de l’image.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dans la &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#expose&#34; target=&#34;blank&#34;&gt;documentation de Docker&lt;/a&gt;
nous apprenons
que l’instruction &lt;code&gt;EXPOSE&lt;/code&gt; ne publie pas le port.
Elle a vocation à être une sorte de documentation
à destination des utilisateurs de l’image.&lt;/p&gt;
&lt;p&gt;Afin de publier réellement le port, nous utiliserons le fichier
&lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;description-de-lapplication&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Description de l’application&lt;/h2&gt;
&lt;p&gt;L’application que nous considérons présente les éléments constitutifs d’une animalerie en ligne.
Dans la version actuelle, l’application se limite à montrer quelques photos de chats.&lt;/p&gt;
&lt;p&gt;Le dossier racine de l’application contient un fichier nommé
&lt;code&gt;docker-compose.yml&lt;/code&gt; qui contient la &lt;strong&gt;configuration&lt;/strong&gt; de l’application.
Le contenu du fichier est le suivant. Il s’agit d’une séquence de paires clé-valeur.
Chaque clé est aussi appelé un &lt;strong&gt;champ&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: &amp;quot;3.6&amp;quot;
services:
  web:
    build: web
    image: pet-store-web
    networks:
      - backend
    ports:
      - 5000:3000
  db:
    build: database
    image: pet-store-db
    networks:
      - backend
    volumes:
      - pets-data:/var/lib/postgresql/data

networks:
  backend:

volumes:
  pets-data:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Il y a trois sections principales :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;services&lt;/code&gt;. Définit les services de l’application. Ici, deux services
sont définis : &lt;code&gt;web&lt;/code&gt; et &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;networks&lt;/code&gt;. Définit les réseaux utilisés par l’application.
Ici, un réseau est défini : &lt;code&gt;backend&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;volumes&lt;/code&gt;. Définit les volumes utilisés par l’application.
Ici, un volume est défini : &lt;code&gt;pets-data&lt;/code&gt;.
Le volume est attaché au dossier &lt;code&gt;/var/lib/postgresql/data&lt;/code&gt;
(qui se trouve dans le conteneur).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Quel champ indique à Docker Compose où trouver le fichier
Dockerfile des deux services ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La valeur du champ &lt;code&gt;build&lt;/code&gt; est le nom du dossier qui contient le Dockefile.
Pour le service &lt;code&gt;web&lt;/code&gt;, il s’agit du dossier
&lt;code&gt;web&lt;/code&gt; ; pour le service &lt;code&gt;db&lt;/code&gt;, il s’agit du dossier &lt;code&gt;database&lt;/code&gt;.
Tous les chemins sont relatifs à la position du fichier &lt;code&gt;docker-compose.yml&lt;/code&gt;
lui-même.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Quand on créera l’application en utilisant la configuration spécifiée dans le fichier &lt;code&gt;docker-compose.yml&lt;/code&gt;,
on créera deux images, une pour chaque service.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Quel sera le nom des deux images ?
Quel champ du fichier &lt;code&gt;docker-compose.yml&lt;/code&gt; vous donne cette information ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le nom de l’image correspondant au service &lt;code&gt;web&lt;/code&gt;
sera &lt;code&gt;pet-store-web&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Le nom de l’image correspondant au service &lt;code&gt;db&lt;/code&gt;
sera &lt;code&gt;pet-store-db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Cette information est donnée par le champ &lt;code&gt;image&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;création-de-lapplication&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Création de l’application&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant créer l’application.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ouvrez un terminal et, en utilisant la commande &lt;code&gt;cd&lt;/code&gt;, positionnez-vous dans le dossier racine &lt;code&gt;pet-store&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pendant la création, vous pourriez lire un avertissement de &lt;code&gt;npm&lt;/code&gt;. Ignorez-le.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque la création est terminée, vérifiez que les deux images correspondant aux deux services ont bien été créées (quelle commande docker vous faut-il ici ?).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;docker images&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;exécutiuon-de-lapplication&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Exécutiuon de l’application&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant exécuter l’application via la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose up&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez une machine virtuelle Linux avec Multipass
&lt;/summary&gt;
&lt;p&gt;Dans cette section, vous aurez besoin d’ouvrir plusieurs terminaux dans la machine virtuelle.
Vous pouvez le faire facilement en utilisant &lt;code&gt;byobu&lt;/code&gt;, un gestionnaire de fenêtres avancé déjà disponible dans votre machine virtuelle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tapez simplement &lt;code&gt;byobu&lt;/code&gt; pour lancer le gestionnaire de fenêtres.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez ouvrir un nouveau terminal, appuyez simplement sur F2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez passer d’un terminal à un autre, appuyez simplement sur F3 (pour aller au terminal précédent) ou F4 (pour aller au terminal suivant).
(pour passer au terminal suivant).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous voulez fermer un terminal, tapez simplement &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque vous fermez tous les terminaux, &lt;code&gt;byobu&lt;/code&gt; cesse de s’exécuter.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si votre application ne démarre pas parce que le numéro de port que vous avez choisi est déjà utilisé,
essayez d’utiliser un autre port.&lt;/p&gt;
&lt;p&gt;N’utilisez pas les ports suivants, car ils sont considérés comme dangereux par les navigateurs :&lt;/p&gt;
&lt;p&gt;1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;L’exécution de la commande affichera une série de messages à l’écran.
Lorsque les messages s’arrêtent (le dernier message devrait être « database system is ready to accept connections »),
ouvrez un nouveau terminal ou un nouvel onglet dans la fenêtre du terminal actuel.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.5  &lt;/strong&gt;&lt;/span&gt;
Vérifiez que le réseau et les volumes associés à l’application ont bien été créés.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Tapez les commandes :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;et&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.6  &lt;/strong&gt;&lt;/span&gt;
Combien de conteneurs seront associés à l’application ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Chaque &lt;strong&gt;image&lt;/strong&gt; correspond à un &lt;strong&gt;service&lt;/strong&gt;.
Ici, nous en avons deux.
Chaque &lt;strong&gt;conteneur&lt;/strong&gt; correspond à une &lt;strong&gt;instance de service&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nous n’avons spécifié aucun paramètre lors du lancement de l’application,
donc par défaut une instance de chaque service est exécutée.&lt;/p&gt;
&lt;p&gt;Nous nous attendons donc à avoir &lt;strong&gt;deux conteneurs&lt;/strong&gt; associés à l’application :
un conteneur pour le service &lt;code&gt;web&lt;/code&gt; et un pour le service
service &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Vous pouvez vérifier la réponse à la question précédente en tapant la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose ps&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cette commande est exactement équivalente à &lt;code&gt;docker container ls&lt;/code&gt;, sauf qu’elle n’affiche que les conteneurs associés à l’application
que nous venons d’exécuter.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.7  &lt;/strong&gt;&lt;/span&gt;
Dans la sortie de la commande &lt;code&gt;docker compose ps&lt;/code&gt;, pouvez-vous expliquer la signification de ce qui suit ?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.0.0.0:5000-&amp;gt;3000/tcp&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La notation 5000-&amp;gt;3000 signifie que le port
5000 de l’ordinateur hôte est &lt;strong&gt;publiée et liée&lt;/strong&gt;
au port 3000 du conteneur.
Plus précisément,
lorsqu’une application se connecte au port 5000 de l’ordinateur hôte, la connexion est redirigée vers le port 3000 du conteneur,&lt;/p&gt;
&lt;p&gt;L’adresse IP 0.0.0.0 signifie que nous pouvons nous connecter au conteneur en
utilisant n’importe quelle adresse IP associée à l’ordinateur hôte.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.8  &lt;/strong&gt;&lt;/span&gt;
Pouvez-vous dire quelle URL vous devez taper dans votre navigateur web
pour accéder à l’application ?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez une machine virtuelle Linux avec Multipass
&lt;/summary&gt;
&lt;p&gt;Afin d’ouvrir une fenêtre de navigateur Web dans la VM, vous devez ouvrir une nouvelle fenêtre de terminal sur votre ordinateur et suivre ces instructions :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Tapez &lt;code&gt;multipass ls&lt;/code&gt; pour énumerer toutes les machines virtuelles gérées par &lt;code&gt;Multipass&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copiez la première adresse IP associée à la machine virtuelle &lt;code&gt;cloudvm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tapez la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;left&#34;&gt;
&lt;code&gt;xpra start ssh://ubuntu@ip/ --start=firefox&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;où vous remplacerez &lt;code&gt;ip&lt;/code&gt; par l’adresse IP que vous avez copiée ci-dessus.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Une fenêtre Firefox devrait apparaître.&lt;/li&gt;
&lt;/ol&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;code&gt;http://localhost:5000&lt;/code&gt; ou &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Dans les paramètres réseau de votre ordinateur, vous pouvez également
obtenir l’adresse IP associée par le serveur DHCP de votre réseau local et utiliser cette
adresse pour vous connecter à l’application
à partir d’un autre appareil &lt;strong&gt;connecté au même réseau&lt;/strong&gt;
(par exemple, votre téléphone portable).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous voulez voir les photos des chats, vous devez ajouter
&lt;code&gt;/pet&lt;/code&gt; à l’URL que vous avez trouvée à la question précédente.
Ceci est déterminé dans le fichier &lt;code&gt;server.js&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;arrêt-de-lapplication&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Arrêt de l’application&lt;/h2&gt;
&lt;p&gt;Vous pouvez arrêter l’application via la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose down&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.9  &lt;/strong&gt;&lt;/span&gt;
L’arrêt de l’application supprime-t-il les réseaux créés pour l’application ?
Qu’en est-il des volumes ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Tapez la commande :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker network ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;et vous verrez que le réseau &lt;code&gt;backend&lt;/code&gt; n’existe plus.&lt;/p&gt;
&lt;p&gt;Tapez la commande :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker volume ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;et vous verrez que le volume &lt;code&gt;pet-store_pets-data&lt;/code&gt; est encore là.&lt;/p&gt;
&lt;p&gt;Par défaut, les volumes ne sont pas supprimés, afin d’éviter le risque d’effacer définitivement des données qui pourraient encore être utiles plus tard.
Si vous souhaitez supprimer les volumes lors de l’arrêt de l’application,
vous pouvez saisir la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose down -v&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;mise-à-léchelle-dun-service&#34; class=&#34;section level2&#34; number=&#34;1.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Mise à l’échelle d’un service&lt;/h2&gt;
&lt;p&gt;Lorsque nous exécutons l’application, nous pouvons spécifier le nombre d’instances de chaque service.
Ceci est utile lorsque nous nous attendons à ce que notre application soit sollicitée par de nombreux utilisateurs ;
la charge de travail sera automatiquement équilibrée entre toutes les instances du service.&lt;/p&gt;
&lt;p&gt;Nous pouvons exécuter 3 instances du service &lt;code&gt;web&lt;/code&gt; via la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker compose up --scale web=3 -d&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.10  &lt;/strong&gt;&lt;/span&gt;
L’exécution de la commande précédente entraîne une erreur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pouvez-vous expliquer pourquoi ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle correction au fichier &lt;code&gt;docker-compose.yml&lt;/code&gt; proposeriez-vous ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nous essayons de lier trois instances au port 5000 de l’ordinateur hôte. Ce n’est pas
possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans la section &lt;code&gt;ports&lt;/code&gt; du fichier
&lt;code&gt;docker-compose.yml&lt;/code&gt; nous devons supprimer 5000 et nous ne laissons que 3000. De cette façon, à chaque fois que nous lançons un conteneur, un port aléatoire sera choisi sur l’ordinateur hôte pour le lier au port 3000 du conteneur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vous devez arrêter l’application avant de la relancer.
En fait, même si nous avons eu une erreur, une instance du service &lt;code&gt;db&lt;/code&gt; et une instance du service &lt;code&gt;web&lt;/code&gt; seront toujours en cours d’exécution.&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modifiez le fichier &lt;code&gt;docker-compose.yml&lt;/code&gt; et relancez l’application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exécutez la commande suivante et vérifiez que vous avez bien trois instances du service &lt;code&gt;web&lt;/code&gt; en cours d’exécution.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose ps&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Essayez de vous connecter à l’application en utilisant les trois numéros de port indiqués dans la sortie de la commande précédente.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;N’oubliez pas d’arrêter l’application avant de continuer le TD.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pousser-une-application&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Pousser une application&lt;/h1&gt;
&lt;p&gt;Une fois que nous avons créé une application, nous pourrions vouloir la
partager en la poussant vers le registre DockerHub ou tout autre registre de conteneurs, qu’il soit privé ou public.&lt;/p&gt;
&lt;div id=&#34;créer-un-compte-sur-dockerhub&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Créer un compte sur DockerHub&lt;/h2&gt;
&lt;p&gt;Vous devez créer un compte sur le &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34;&gt;site web de DockerHub&lt;/a&gt;
afin d’effectuer cette activité.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renommer-les-images&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Renommer les images&lt;/h2&gt;
&lt;p&gt;Afin de pousser vos images vers le registre, vous devez les renommer, de sorte que le nouveau nom
ait la structure suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;votre-nom-d&#39;utilisateur/nom-de-l&#39;image:image-tag&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Par exemple, puisque mon nom d’utilisateur est &lt;code&gt;quercinigia&lt;/code&gt;, je vais renommer
les deux images &lt;code&gt;pet-store-web&lt;/code&gt; et &lt;code&gt;pet-store-db&lt;/code&gt; via la commande &lt;code&gt;docker tag&lt;/code&gt; comme suit :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-web quercinigia/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker tag pet-store-db quercinigia/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;J’ai choisi 1.0 comme tag, mais n’hésitez pas à en choisir un autre.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;se-connecter&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Se connecter&lt;/h2&gt;
&lt;p&gt;Vous devez vous connecter à votre compte DockerHub.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions pour Docker Desktop (macOS et Windows)
&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Utilisateurs mcOS: &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-macos/&#34; target=&#34;_blank&#34;&gt;Regardez cette vidéo&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Utilisateurs Windows: &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/log-in-docker-windows/&#34; target=&#34;_blank&#34;&gt;Regardez cette vidéo&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions pour la machine virtuelle Linux
&lt;/summary&gt;
&lt;p&gt;Connectez-vous au DockerHub via la commande suivante (remplacez YOUR-USERNAME avec votre nom d’utilisateur):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker login -u YOUR-USERNAME&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Il se peut que vous receviez un avertissement indiquant que
votre mot de passe sera stocké en clair.
Il existe des méthodes pour éviter cela.
Ces méthodes sortent du cadre de ce cours,
vous pouvez lire davantage &lt;a href=&#34;https://www.techrepublic.com/article/how-to-setup-secure-credential-storage-for-docker/&#34; target=&#34;_blank&#34;&gt;ici&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;pousser-les-images&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Pousser les images&lt;/h2&gt;
&lt;p&gt;Une fois que vous avez réussi à vous connecter, vous pouvez taper les commandes suivantes dans le terminal
pour pousser les deux images de votre application :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push VOTRE-USERNAME/pet-store-web:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker push VOTRE-USERNAME/pet-store-db:1.0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;N’oubliez pas de remplacer VOTRE-USERNAME par votre nom d’utilisateur.&lt;/p&gt;
&lt;p&gt;Une fois la tâche terminée, vérifiez que les images apparaissent
dans &lt;a href=&#34;https://hub.docker.com/repositories&#34; target=&#34;_blank&#34;&gt;le registre Docker&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ici, nous avons étiqueté et téléchargé manuellement les deux images.
Cette méthode devient rapidement ennuyeuse lorsque l’application consiste en
plus de deux images.
Une autre façon de procéder est la suivante :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spécifiez les noms des images avec votre nom d’utilisateur dans le fichier &lt;strong&gt;docker-compose.yml&lt;/strong&gt;.
Par exemple, dans mon &lt;strong&gt;docker-compose.yml&lt;/strong&gt; j’écrirais :&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;services:
  web:
    build: web
    image: quercinigia/pet-store-web:1.0
  ...
  db:
    build: database
    image: quercinigia/pet-store-db:1.0
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Créez l’application via la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose build&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poussez les images de l’application via la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker compose push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;De cette manière, nous pouvons pousser autant d’images que nous souhaitons via une seule commande.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-à-kubernetes&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Introduction à Kubernetes&lt;/h1&gt;
&lt;p&gt;Kubernetes est l’&lt;strong&gt;orchestrateur&lt;/strong&gt; le plus populaire à ce jour.
Il est utilisé pour gérer une application multiservice, généralement déployée sur plusieurs &lt;strong&gt;hôtes&lt;/strong&gt; composant un &lt;strong&gt;cluster&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En utilisant Docker Compose, un &lt;strong&gt;service&lt;/strong&gt; correspond à une &lt;strong&gt;image Docker&lt;/strong&gt;.
et
une &lt;strong&gt;instance de service&lt;/strong&gt; à un &lt;strong&gt;conteneur Docker&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En revanche, dans Kubernetes, l’unité de calcul est un &lt;strong&gt;Pod&lt;/strong&gt;, c’est-à-dire une &lt;strong&gt;collection de conteneurs&lt;/strong&gt;.
En d’autres termes, nous ne raisonnons plus en termes de conteneurs,
mais en termes de Pods. Bien entendu, un Pod peut également être constitué d’un seul conteneur.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;En pratique, nous avons rarement à manipuler des Pods directement dans Kubernetes,
car il existe des objets plus avancés qui les gèrent.
Nous utiliserons ces objets dans les sections suivantes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activation-de-kubernetes&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Activation de Kubernetes&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions pour Docker Desktop (macOS etWindows)
&lt;/summary&gt;
&lt;p&gt;Suivez les instructions documentées
&lt;a href=&#34;https://docs.docker.com/desktop/kubernetes/&#34; target=&#34;_blank&#34;&gt;dans cette page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Avant de continer, n’oubliez pas de vérifier que Kubernetes est actif via la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Vous devriez voir un noeud appelé &lt;code&gt;docker-desktop&lt;/code&gt; dont le statut est READY.
Si le statut n’est pas READY, attendez quelques secondes avant de retaper la commande.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous n’arrivez pas à activer Kubernetes sous Docker Desktop,
installez &lt;strong&gt;minikube&lt;/strong&gt;, en suivant &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34;&gt;ces instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Instructions pour la machine virtuelle Linux
&lt;/summary&gt;
&lt;p&gt;Dans la VM Linux, vous trouverez &lt;strong&gt;Minikube&lt;/strong&gt;, un cluster Kubernetes avec un seul nœud.&lt;/p&gt;
&lt;p&gt;Veuillez suivre &lt;strong&gt;toutes les instructions&lt;/strong&gt; suivantes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Démarrez le cluster via la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube start&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que Kubernetes est correctement activé via la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Vous devriez voir un noeud appelé &lt;code&gt;minikube&lt;/code&gt; dont le statut est READY.
Si le statut n’est pas READY, attendez quelques secondes avant de retaper la commande.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ouvrez un &lt;strong&gt;nouveau terminal&lt;/strong&gt; et tapez la commande :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;minikube tunnel&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Lorsque l’on vous demandera d’entrer un mot de passe, tapez simplement la touché entrée.
La commande commencera à produire des résultats.
&lt;strong&gt;Laissez ce terminal ouvert&lt;/strong&gt; et retournez au terminal précédent.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Tunnel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Le tunnel Minikube est utilisé pour créer une route vers les services de type &lt;code&gt;LoadBalancer&lt;/code&gt;.
Si vous n’activez pas le tunnel, vous ne pourrez pas utiliser ces services dans les exercices ci-dessous.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;déploiement-dun-pod&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Déploiement d’un Pod&lt;/h2&gt;
&lt;p&gt;Pour déployer &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/&#34; target=&#34;_blank&#34;&gt;un Pod&lt;/a&gt;,
il faut d’abord lui donner sa &lt;strong&gt;spécification&lt;/strong&gt;, c’est-à-dire son nom
et les conteneurs qui le composent avec leurs paramètres.
De manière similaire à Docker Compose, un Pod est spécifié de manière &lt;strong&gt;déclarative&lt;/strong&gt; avec un fichier de configuration YAML.&lt;/p&gt;
&lt;p&gt;Considérons la spécification suivante.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web-pod
spec:
  containers:
  - name: web
    image: nginx:alpine
    ports:
    - containerPort: 80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Voici une explication des champs de la spécification.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;apiVersion&lt;/em&gt;. Définit la version du schéma de cette spécification.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;kind&lt;/em&gt;. Le type de ressource que nous avons l’intention de créer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;metadata&lt;/em&gt;. Les métadonnées de la ressource. La liste de
toutes les métadonnées est spécifiée &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta&#34; target=&#34;blank&#34;&gt;ici&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;spec&lt;/em&gt;. La spécification du comportement souhaité du Pod.
La liste des spécifications possibles se trouve &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/Pod-v1/#PodSpec&#34; target=&#34;blank&#34;&gt;ici&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentiellement, la spécification précédente définit un Pod avec un conteneur qui est lancé à partir de l’image
&lt;code&gt;nginx:alpine&lt;/code&gt; (un serveur Web) et qui écoute le port 80.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copiez la spécification ci-dessus dans un fichier nommé &lt;code&gt;sample-pod.yaml&lt;/code&gt; (ou tout autre nom de votre choix).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;En utilisant la commande &lt;code&gt;cd&lt;/code&gt; dans le terminal, placez-vous dans le répertoire où se trouve le fichier &lt;code&gt;sample-pod.yaml&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Déployez le Pod en tapant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que le Pod est déployé en tapant la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;La première fois que vous exécutez la dernière commande, vous pouvez voir que le pod n’est pas encore prêt.
Vous devez attendre que l’image &lt;code&gt;nginx:alpine&lt;/code&gt; soit téléchargée de DockerHub.
Attendez quelques secondes et réessayez la commande jusqu’à ce que le Pod soit marqué comme étant en cours d’exécution.&lt;/p&gt;
&lt;p&gt;Vous pouvez également obtenir plus d’informations sur le Pod en cours d’exécution (par exemple, l’adresse IP qui lui a été attribuée)
en tapant la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod -o wide web-pod&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Ouvrez un navigateur web et tapez &lt;code&gt;http://localhost:80&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Qu’obtenez-vous ? Que se passe-t-il si vous essayez d’utiliser l’IP du Pod au lieu de &lt;code&gt;locahost&lt;/code&gt; ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que devrions-nous définir pour résoudre le problème ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;La connexion est refusée à la fois lorsque l’on utilise &lt;code&gt;localhost&lt;/code&gt; et l’adresse IP du Pod.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Afin de se connecter au Pod, nous devons
définir un objet &lt;strong&gt;Service&lt;/strong&gt; dans Kubernetes, auquel Kubernetes associe une adresse IP
que les applications clientes peuvent utiliser.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;La configuration suivante définit un service de type &lt;strong&gt;LoadBalancer&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
      app: web-pod&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Un service &lt;strong&gt;LoadBalancer&lt;/strong&gt; est un service &lt;strong&gt;NodePort&lt;/strong&gt;.
qui offre une fonctionnalité d’équilibrage de charge.
Il est associé à une adresse IP
que les applications clientes (externes au cluster Kubernetes) peuvent utiliser pour accéder au service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Qu’est-ce le champ &lt;code&gt;selector&lt;/code&gt; dans la définition du Service ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le &lt;code&gt;selector&lt;/code&gt; est utilisé en interne par Kubernetes pour identifier les Pods qui
sont les instances du service.
Si vous regardez la spécification du Pod ci-dessus, le champ &lt;code&gt;metadata&lt;/code&gt;
contient un champ &lt;code&gt;labels&lt;/code&gt;.
L’une des étiquettes est &lt;code&gt;app : web-pod&lt;/code&gt; ;
le &lt;code&gt;selector&lt;/code&gt; correspond à cette étiquette.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Quelle est la signification de &lt;code&gt;port&lt;/code&gt; et &lt;code&gt;targetPort&lt;/code&gt; ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;port&lt;/code&gt; spécifie le numéro de port où le Service sera disponible (celui utilisé par les clients externes).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;targetPort&lt;/code&gt; spécifie le numéro de port ouvert sur l’interface réseau du Pod.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copiez et collez la spécification du service dans un fichier nommé &lt;code&gt;sample-service.yaml&lt;/code&gt; (ou tout autre nom de votre choix).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;En utilisant la commande &lt;code&gt;cd&lt;/code&gt; dans le terminal, placez-vous dans le répertoire où se trouve le fichier &lt;code&gt;sample-service.yaml&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Déployez le service via la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f sample-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que le service est déployé via la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cette commande renvoie tous les services en cours d’exécution dans Kubernetes.
Pour chaque service, vous obtenez également un nom.
Pour cibler uniquement le service que vous venez de créer, tapez simplement la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Le nom de votre service est &lt;code&gt;nginx-service&lt;/code&gt; (comme spécifié dans le fichier &lt;code&gt;sample-service.yml&lt;/code&gt;) ;
`svc`` est seulement l’espace de noms où tous les services Kubernetes sont placés.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;En regardant la sortie de la commande &lt;code&gt;kubectl get svc/nginx-service&lt;/code&gt;,
quelle URL devez-vous saisir dans le navigateur Web pour accéder au service ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Le &lt;code&gt;CLUSTER-IP&lt;/code&gt; est une adresse IP qui n’est visible que &lt;strong&gt;dans le cluster&lt;/strong&gt;.
En d’autres termes, seuls les services fonctionnant au sein du cluster peuvent se connecter au service
&lt;code&gt;nginx-service&lt;/code&gt; en utilisant cette adresse IP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ce que nous devons utiliser est l’adresse IP spécifiée dans &lt;code&gt;EXTERNAL-IP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; est probablement &lt;code&gt;localhost&lt;/code&gt;
ou &lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez Minikube dans la VM Linux
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;EXTERNAL-IP&lt;/code&gt; sera l’adresse IP assignée au service.&lt;/p&gt;
&lt;/details&gt;
&lt;ul&gt;
&lt;li&gt;Quant au numéro de port, sous PORT(S) vous devriez voir quelque chose comme 8080:30548
(le deuxième numéro est probablement différent, mais il devrait se situer dans la plage [30000-32767]).
Le port 30548 est ouvert sur chaque nœud du cluster Kubernetes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En pratique, lorsque nous tapons &lt;code&gt;http://localhost:8080&lt;/code&gt;, l’équilibreur de charge le remplace par l’adresse IP d’un nœud du cluster Kubernetes, suivie du numéro de port 30548.
Lorsque le &lt;code&gt;kube-proxy&lt;/code&gt; de ce nœud reçoit cette demande,
il la transmet à une instance du service,
même si cette instance s’exécute dans un autre nœud.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Arrêtez le Pod et le service avant de continuer. Voici les commandes nécessaires :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete po/web-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/nginx-service&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kubernetes-déploiement-dune-application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Kubernetes : déploiement d’une application&lt;/h1&gt;
&lt;p&gt;Dans cette section, nous allons déployer notre animalerie en ligne sur Kubernetes.
Pour rappel, notre application se compose de deux services : &lt;code&gt;web&lt;/code&gt; et &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pour définir une application dans Kubernetes,
nous devons utiliser deux types d’objets &lt;strong&gt;pour chaque service&lt;/strong&gt; de l’application :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Une &lt;strong&gt;ressource de charge de travail&lt;/strong&gt; qui donne la spécification du service, telle que
les Pods qui composent le service lui-même (images, paramètres réseau) et les métadonnées, telles que
le nombre d’instances souhaité.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Un objet &lt;strong&gt;Service&lt;/strong&gt;. Comme nous l’avons vu précédemment, un objet Service
est associée à une adresse IP qui permet aux applications clientes,
à l’intérieur et à l’extérieur du cluster Kubernetes, de se connecter au service.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lorsque nous définissons une application dans Kubernetes, nous avons rarement, voire jamais, besoin de manipuler directement des Pods.
Nous pouvons faire recours à des objets plus avancés, appelés &lt;strong&gt;Contrôleurs&lt;/strong&gt;, pour une
définition de l’état &lt;strong&gt;souhaité&lt;/strong&gt; de l’application elle-même.
Le type de contrôleur que nous devons utiliser dépend de la nature du service
lui-même : &lt;strong&gt;sans état&lt;/strong&gt; ou &lt;strong&gt;avec état&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Le service &lt;code&gt;web&lt;/code&gt; de notre application est-il avec ou sans état ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le service &lt;code&gt;db&lt;/code&gt; de notre application est-il sans état ou avec état ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le service &lt;code&gt;web&lt;/code&gt; est &lt;strong&gt;sans état&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Le service &lt;code&gt;db&lt;/code&gt; est &lt;strong&gt;avec état&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div id=&#34;le-service-web-deployment&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Le service &lt;code&gt;web&lt;/code&gt; : Deployment&lt;/h2&gt;
&lt;p&gt;Pour les &lt;strong&gt;services sans état&lt;/strong&gt;, nous pouvons utiliser
un objet &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt;
pour le déploiement d’un groupe de Pods identiques qui seront lancés sur plusieurs nœuds du cluster Kubernetes.&lt;/p&gt;
&lt;p&gt;Nous définissons ici la spécification d’un Deployment correspondant au service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Voici l’explication de la spécification :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Un Deployment nommé &lt;code&gt;web&lt;/code&gt; est créé, comme indiqué dans le champ &lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le Deployment crée trois Pods identiques, comme indiqué dans le champ &lt;code&gt;spec.replicas&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le Deployment gère tous les Pods ayant &lt;strong&gt;les deux labels&lt;/strong&gt; &lt;code&gt;app : pets&lt;/code&gt; et
&lt;code&gt;service : web&lt;/code&gt;. Ceci est indiqué dans le champ &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.
Ce champ est utilisé pour indiquer au Deployment comment trouver ses Pods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;La configuration des Pods qui font partie de ce Deployment est donnée dans le champ &lt;code&gt;spec.template&lt;/code&gt; et ses sous-champs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chaque Pod du Deployment a les labels &lt;code&gt;app : pets&lt;/code&gt; et &lt;code&gt;service : web&lt;/code&gt;.
Ceci est indiqué dans le champ &lt;code&gt;spec.template.metadata.labels&lt;/code&gt;.
Notez que nous spécifions ici exactement les mêmes valeurs que nous avons indiquées dans le champ &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chaque Pod a exactement un conteneur, nommé &lt;code&gt;web&lt;/code&gt;, exécuté à partir de l’image
&lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt; stockée sur le DockerHub. Le conteneur écoutera le port 3000
et utilisera le protocole TCP. Ceci est spécifié dans le champ &lt;code&gt;spec.template.spec.containers&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copiez et collez la spécification précédente dans un nouveau fichier &lt;code&gt;web-deployment.yaml&lt;/code&gt; (ou tout autre nom de votre choix).
Assurez-vous de &lt;strong&gt;remplacer&lt;/strong&gt; l’image &lt;code&gt;quercinigia/pet-store-web:1.0&lt;/code&gt;
avec celle que vous avez poussée vers votre registre DockerHub.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Utilisez la commande &lt;code&gt;cd&lt;/code&gt; dans le terminal pour vous positionner dans le répertoire
où le fichier &lt;code&gt;web-deployment.yaml&lt;/code&gt; est stocké.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Déployez ce Deployment dans Kubernetes en tapant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-deployment.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
Saisissez la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Quels objets ont été créés par cette commande ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous pouvons clairement voir trois types d’objets :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Trois Pods, qui correspondent aux trois répliques que nous avons spécifiées dans la configuration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Un déploiement. Notez que le 3/3 indique que le déploiement a
3 Pods identiques, dont 3 sont en cours d’exécution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Un ReplicaSet. Le déploiement crée un ReplicaSet, qui est l’objet réel qui contrôle les trois Pods.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Obtenez le nom de l’un des Pods en cours d’exécution et
supprimez-le en utilisant la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete nom-du-pod&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Si la commande bloque le terminal, n’hésitez pas à taper Ctrl-C pour
reprendre le contrôle du terminal.&lt;/p&gt;
&lt;p&gt;Tapez à nouveau la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Combien de Pods voyez-vous ? Est-ce surprenant ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Si nous tapons la commande juste après avoir supprimé le Pod,
il est probable que nous verrons quatre Pods, un qui est &lt;em&gt;terminé&lt;/em&gt; et un autre qui est &lt;em&gt;en cours d’exécution&lt;/em&gt;.
Essayez de taper la commande jusqu’à ce que vous voyiez trois Pods en cours d’exécution.&lt;/p&gt;
&lt;p&gt;Ce n’est pas surprenant.
Si nous supprimons le Pod, l’orchestrateur détecte une
déviation par rapport à l’état &lt;strong&gt;souhaité&lt;/strong&gt; (c’est-à-dire que nous voulons trois répliques) et
replanifie immédiatement un autre Pod.
C’est ce que nous entendons par &lt;strong&gt;système auto-réparateur&lt;/strong&gt;. C’est l’un des rôles majeurs
d’un orchestrateur.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;le-service-web-service&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Le service &lt;code&gt;web&lt;/code&gt; : Service&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant définir un &lt;strong&gt;Service&lt;/strong&gt; pour que les utilisateurs puissent accèder au service Web.
Voici la définition :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-18&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Décrivez la spécification de ce service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nous créons un service nommé &lt;code&gt;web&lt;/code&gt;. Ce service est indiqué dans le champ
&lt;code&gt;metadata.name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le service est de type &lt;code&gt;LoadBalancer&lt;/code&gt;. Ceci est indiqué dans le champ
&lt;code&gt;spec.type&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le service écoute le port 8080 et utilise le protocole TCP.
Chaque instance de service (c.-à-d. Pod) est écoute le port 3000.
Ceci est indiqué dans le champ &lt;code&gt;spec.ports&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Les instances du service (c’est-à-dire les Pods) sont celles qui portent des étiquettes
&lt;code&gt;app : pets&lt;/code&gt; et &lt;code&gt;service : web&lt;/code&gt;.
Cela est indiqué dans le champ &lt;code&gt;spec.selector&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copiez et collez la spécification précédente dans un nouveau fichier &lt;code&gt;web-service.yaml&lt;/code&gt; (ou tout autre nom de votre choix).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Utilisez la commande &lt;code&gt;cd&lt;/code&gt; dans le terminal pour vous positionner dans le répertoire où le fichier &lt;code&gt;web-service.yaml&lt;/code&gt; est stocké.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Créez le service avec la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f web-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que le service a été créé avec la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get services&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Localisez l’IP externe (appelons-le EXTERNAL-IP) du service web et tapez l’URL &lt;code&gt;http://EXTERNAL-IP:8080&lt;/code&gt; dans votre navigateur Web.&lt;br /&gt;
Vous devriez voir une page Web où la phrase &lt;em&gt;Pet store&lt;/em&gt; apparaît.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;le-service-db-statefulset&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Le service &lt;code&gt;db&lt;/code&gt; : StatefulSet&lt;/h2&gt;
&lt;p&gt;Kubernetes a défini un type spécial de ReplicaSet pour les services avec état,
appelé &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;StatefulSet&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Mous allons maintenant définir un StatefulSet pour donner la spécification du service &lt;code&gt;db&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  selector:
    matchLabels:
      app: pets
      service: db
  serviceName: db
  template:
    metadata:
      labels:
        app: pets
        service: db
    spec:
      containers:
      - image: quercinigia/pet-store-db:1.0
        name: db
        ports:
        - containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: pets-data
  volumeClaimTemplates:
  - metadata:
      name: pets-data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vous ne devriez pas avoir de mal à comprendre la signification des champs de cette spécification.
dans cette spécification.
Les champs sont également
documentés dans la &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec&#34; target=&#34;_blank&#34;&gt;documentation officielle de Kubernetes&lt;/a&gt;.
Deux points méritent d’être expliqués.
Premièrement, la spécification d’un &lt;code&gt;StatefulSet&lt;/code&gt; a besoin d’un attribut &lt;code&gt;serviceName&lt;/code&gt; qui indique le nom du service responsable de l’identification réseau
des Pods gérés par le StatefulSet.
Cet attribut est obligatoire.&lt;/p&gt;
&lt;p&gt;Une autre nouveauté est le champ &lt;code&gt;volumeClaimTemplates&lt;/code&gt;.
Un &lt;code&gt;volume claim template&lt;/code&gt; permet de définir les besoins de stockage d’un Pod.
La demande aboutira à la génération d’un volume
&lt;code&gt;pets-data&lt;/code&gt; (où PostgreSQL conserve les données).
De plus, les paramètres suivants sont donnés :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Le mode d’accès &lt;code&gt;ReadWriteOnce&lt;/code&gt; signifie que le volume peut être monté en lecture-écriture par un seul nœud.
en lecture-écriture par un seul nœud du cluster Kubernetes.
Les modes d’accès sont &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34; target=&#34;_blank&#34;&gt;documentés ici&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nous demandons au moins 100Mo de stockage pour la base de données.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Copiez et collez la spécification précédente dans un nouveau fichier &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; (ou un nom de votre choix).
Assurez-vous de &lt;strong&gt;remplacer&lt;/strong&gt; l’image &lt;code&gt;quercinigia/pet-store-db:1.0&lt;/code&gt;
avec celle que vous avez poussée vers votre registre DockerHub.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Utilisez la commande &lt;code&gt;cd&lt;/code&gt; dans le terminal pour vous positionner dans le répertoire où le fichier &lt;code&gt;db-stateful-set.yaml&lt;/code&gt; est stocké.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Déployez ce StatefulSet dans Kubernetes en tapant la commande suivante :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-stateful-set.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-19&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.5  &lt;/strong&gt;&lt;/span&gt;
Saisissez la commande :
&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Quels objets ont été créés lorsque vous avez déployé le StatefulSet ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Un Pod (qui s’appellera &lt;code&gt;pod/db-0&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Un nouveau StatefulSet qui gère le Pod.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notez que le StatefulSet n’a qu’un seul Pod, ce qui est normal,
puisque nous n’avons pas spécifié un nombre plus élevé de répliques.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-20&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.6  &lt;/strong&gt;&lt;/span&gt;
Ouvrez une fenêtre dans votre navigateur et saisissez l’URL suivante :&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez Docker Desktop
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://localhost:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
Si vous utilisez Minikube
&lt;/summary&gt;
&lt;p&gt;&lt;code&gt;http://minikube-ip:8080/pet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;où vous remplacerez &lt;code&gt;minikube-ip&lt;/code&gt; par l’adresse IP externe associée au
service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Juste après, tapez la commande &lt;code&gt;kubectl get all&lt;/code&gt;.
Qu’observez-vous ? Pouvez-vous en expliquer la raison ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Nous ne pouvons pas accéder à l’application.
Si vous tapez rapidement la commande &lt;code&gt;kubectl get all&lt;/code&gt;, vous devriez voir que les trois Pods correspondant aux instances du service &lt;code&gt;web&lt;/code&gt; sont
dans un état d’erreur.
Si vous avez été trop lent, vous devriez voir que le nombre de redémarrages de chaque Pod a changé.
Cela signifie que les Pods ont été redémarrés à cause d’une erreur.&lt;/p&gt;
&lt;p&gt;Ce qui se passe ici, c’est que nous avons lancé la base de données,
mais que nous n’avons pas créé le service qui permet aux clients d’accéder à la base de données.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Dans la sortie de la commande &lt;code&gt;get kubectl all&lt;/code&gt;, regardez les noms des
Pods correspondant au service &lt;code&gt;web&lt;/code&gt;.
Prenez le nom de n’importe lequel de ces Pods,
et mettez-le à la place de NAME-OF-POD dans la commande suivante :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl logs NAME-OF-POD --previous&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-21&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.7  &lt;/strong&gt;&lt;/span&gt;
La sortie de la commande précédente confirme-t-elle l’explication donnée dans la question précédente ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Oui, nous pouvons clairement lire ENOTFOUND db db:5432.
Cela signifie que Kubernetes n’arrive pas à traduire le nom &lt;code&gt;db&lt;/code&gt; en adresse IP.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;le-service-db-service&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Le service &lt;code&gt;db&lt;/code&gt; : Service&lt;/h1&gt;
&lt;p&gt;D’après les observations ci-dessus, nous comprenons qu’il nous faut définir
un service pour permettre les connexions à la base de données.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-22&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Le service &lt;code&gt;db&lt;/code&gt; doit-il être accessible aux applications clientes externes au cluster Kubernetes ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Non, ce composant est le backend de l’application. Le seul client
qui doit pouvoir accéder à la base de données est le frontend, c’est-à-dire le service
&lt;code&gt;web&lt;/code&gt;, qui s’exécute dans le cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-23&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Compte tenu de la réponse à la question précédente, quel devrait être le type de ce service ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;&lt;strong&gt;ClusterIP&lt;/strong&gt; ; c’est le type d’un service qui n’est pas accessible des applications externes.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-24&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.3  &lt;/strong&gt;&lt;/span&gt;
Ecrivez la spécification du service &lt;code&gt;db&lt;/code&gt; dans un fichier nommé &lt;code&gt;db-service.yaml&lt;/code&gt; (ou tout autre nom de votre choix).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention.&lt;/strong&gt; Dans le fichier &lt;code&gt;db-stateful-set.yaml&lt;/code&gt;, le champ &lt;code&gt;spec.serviceName&lt;/code&gt; indique le nom que le service devra avoir.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  type: ClusterIP
  ports:
  - port: 5432
    protocol: TCP
  selector:
    app: pets
    service: db&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Déployer le service en tapant la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl create -f db-service.yaml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que le service a bien été créé avec la commande suivante : &lt;code&gt;kubectl create -f db-service.yaml&lt;/code&gt; :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vérifiez que vous pouvez atteindre l’application en saisissant l’URL `&lt;code&gt;http://localhost:8080/pet&lt;/code&gt; (utilisateurs de Minikube :
remplacez &lt;code&gt;localhost&lt;/code&gt; par l’adresse IP externe associée au service &lt;code&gt;web&lt;/code&gt; !).
&lt;strong&gt;Il peut arriver&lt;/strong&gt; que le service de base de données ne soit pas encore prêt, et vous obtiendrez donc une erreur de connexion.
Attendez quelques secondes et réessayez.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;arrêt-de-lapplication-1&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Arrêt de l’application&lt;/h2&gt;
&lt;p&gt;Une fois que vous avez terminé avec l’application, vous pouvez l’arrêter avec les commandes suivantes :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete svc/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete deploy/web&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete statefulset/db&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;L’ordre dans lequel vous tapez ces commandes n’a pas d’importance.&lt;/p&gt;
&lt;p&gt;Si vous tapez plusieurs fois la commande :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl get all&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vous devriez voir que les ressources disparaissent progressivement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Conclusion&lt;/h2&gt;
&lt;p&gt;Dans les exercices précédents, nous avons déployé une application avec deux services, pour laquelle nous avons dû créer
quatre fichiers et taper autant de commandes.
Pour des applications plus importantes, cela devient un peu ennuyeux.
Nous pouvons écrire toutes les définitions dans un seul fichier (par exemple, &lt;code&gt;pets.yaml&lt;/code&gt;)
où chaque spécification est terminée par —.&lt;/p&gt;
&lt;p&gt;Voici un exemple, où nous écrivons la spécification du
Service et Deployment associés au service &lt;code&gt;web&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 3000
    protocol: TCP
  selector:
    app: pets
    service: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pets
      service: web
  template:
    metadata:
      labels:
        app: pets
        service: web
    spec:
      containers:
      - image: quercinigia/pet-store-web:1.0
        name: web
        ports:
        - containerPort: 3000
          protocol: TCP&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>/courses/bdia/tutorials/map-reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/map-reduce/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We are given a dataset that
contains the average monthly
temperature measurements
over the course of some years.
More precisely, the dataset is stored in a CSV file,
where each row corresponds to a monthly
measurement and the columns contain the following values:
year, month, average temperature in the month.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1980,3,10
1980,4,14
1980,5,17
....
1981,1,2
1981,2,1
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We intend to get the average monthly temperature for each year.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Write a MapReduce algorithm that generates key-value pairs
&lt;span class=&#34;math inline&#34;&gt;\((year, average\_temperature)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, temperatures_in_year):
    return (year, sum(temperatures_in_year) / len(temperatures_in_year))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Suppose now that we have a large CSV file
stored in a distributed file system (e.g., HDFS),
containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We can have up to one measurement per second in some years.
Like before, we’d like to compute key-value pairs (year, average_temperature) by using
a MapReduce algorithm.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What is the maximum number of measurements in a year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since we can have up to one measurement per second, the maximum number of
measurements &lt;span class=&#34;math inline&#34;&gt;\(M_{max}\)&lt;/span&gt; for a certain year is given by the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{max} = 365 \times 24 \times 60 \times 60 \approx 31.5 \times 10^6
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Considering the answer to the previous question, discuss the efficiency
of the first implementation of the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since there might be up to 31 million values
associated with a key, the bottleneck of the computation would be
the shuffle operation, since we need to copy a high number of
(key,value) pairs from the mappers to the reducers.&lt;/p&gt;
&lt;p&gt;Also, a reducer might have to
loop over a huge list of values in order to compute their average.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answer to the previous question,
propose a better implementation to handle the CSV file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Function &lt;code&gt;map()&lt;/code&gt; is the same as in the previous exercise.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1])) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we code the function &lt;code&gt;combine()&lt;/code&gt;
which will apply some computation on key-value pairs returned by the Map task.
More precisely, after a Map task applies the function &lt;code&gt;map()&lt;/code&gt; on all
lines of a block, a local shuffle operation is executed to group all values associated with the same key.
The function &lt;code&gt;combine()&lt;/code&gt; takes in a couple, of which the key (a year)
is associated with the list of all its values (the year’s temperatures).
The function returns a couple, where the key still represents a year and the value is itself a couple;
the first element of this couple is the sum all of year’s temperatures,
the second element is the number of temperatures in the year.
Please note that all these considerations are applied to data within a single bloc.
Therefore, the shuffle and the combine did not move data across blocks.
The result is that from each block we send out to the reducers only a couple per year,
which significantly reduces the amount of data sent over the network.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def combine(year, temperatures_in_year):
    return (year, (sum(temperatures_in_year), len(temperatures_in_year)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;reduce()&lt;/code&gt; takes in a a couple,
where the key is a year and the value is a list of couples, as output by
the function &lt;code&gt;combine()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, sum_len_tuples):
    sum_temps = sum(s for (s, _) in sum_len_tuples)
    nb_temps = sum(l for (_, l) in sum_len_tuples)
        
    return (year, sum_temps/nb_temps)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,B,C,D
B,A,D
C,A,D
D,A,B,C
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to obtain the list of the common friends for each pair of individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to find the common friends in a
social network satisfying the given constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The result of our algorithm is a collection of couples,
where the first element represents a couple of individuals,
and the second element is the common friends of the two individuals.&lt;/p&gt;
&lt;p&gt;We infer that a couple of individuals is a proper choice for a key.&lt;/p&gt;
&lt;p&gt;Now, the function &lt;code&gt;map()&lt;/code&gt; only takes in a single line.
The question we need to ask ourselves is: can we obtain a solution to our problem on
that single line? Otherwise stated, can we still get common friends between couple of individuals from
one single line?&lt;/p&gt;
&lt;p&gt;Take the example of line: A,D,B,C.
From this line, we know that A is a common friend of his friends : (B, D), (C, D) and (B, C).&lt;/p&gt;
&lt;p&gt;This gives us an idea as to the implementation of the function &lt;code&gt;map()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return [((x, y), values[0]) for x in values[1:] for y in values[1:] if x &amp;lt; y] &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the condition &lt;code&gt;x&amp;lt;y&lt;/code&gt; guarantees that we never output summetrical couples.&lt;/p&gt;
&lt;p&gt;The shuffle step will then associate the list of common friends to each couple of individuals.
This is exactly what we want, therefore there is no need for a
&lt;code&gt;reduce()&lt;/code&gt; function (identity function).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--
# Creating an inverted index

We have  a collection of $n$ documents in a directory and we want to create 
an **inverted index**,  one that associates each word 
to the list of the files the word occurs in. 
More precisely, for each word, the inverted index will 
have a list of the names of the documents that contain the word. 

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to create an inverted index over a 
collection of documents.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The input to the map will be a key-value pair, where 
the key is the name of a file $f$ and the value is the 
content $C$ of the file.

map: $(f, C) \rightarrow [(w, f)\ \forall w \in C]$

reduce: $(w, L) \rightarrow (w, L)$

where $L$ is the list of the files containing the word $w$.

We note that the reduce function is the identity.

Note also that in the map function we can add instructions 
to preprocess the text. For example, we can eliminate some words
that are not useful in the index (e.g., the stopwords) or remove 
special symbols.

:::

&lt;/details&gt;
--&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-average-and-standard-deviation&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Computing average and standard deviation&lt;/h1&gt;
&lt;p&gt;We consider again the large CSV file
with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).&lt;/p&gt;
&lt;p&gt;We can express the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1 \leq i \leq n\)&lt;/span&gt;) with two different
equations.&lt;/p&gt;
&lt;p&gt;The first equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Which equation of the standard deviation
is more appropriate in a Map-Reduce algorithm?
Why?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The second equation is more appropriate because it allows the computation
of the sum of the elements and of the square of the elements step by step
by using map and combine together.&lt;/p&gt;
&lt;p&gt;Instead, if we use the first equation, we need first to compute the average and then use it
to compute the variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to compute the average and the standard
deviation of the temperatures for each year.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1])) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def combine(year, temperatures_in_year):
    return (year, (sum(temperatures_in_year), sum(x**2 for x in temperatures_in_year), len(temperatures_in_year)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, sum_len_tuples):
    sum_temps = sum(s for (s, _, _) in sum_len_tuples)
    sum_sq_temps = sum(sq for (_, sq, _) in sum_len_tuples)
    nb_temps = sum(l for (_, _, l) in sum_len_tuples)
        
    return (year, (sum_temps/nb_temps, math.sqrt(sum_sq_temps/nb_temps - (sum_temps/nb_temps)**2)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>/courses/bdia_old/tutorials/map-reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/map-reduce/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We are given a dataset that
contains the average monthly
temperature measurements
over the course of some years.
More precisely, the dataset is stored in a CSV file,
where each row corresponds to a monthly
measurement and the columns contain the following values:
year, month, average temperature in the month.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1980,3,10
1980,4,14
1980,5,17
....
1981,1,2
1981,2,1
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We intend to get the average monthly temperature for each year.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Write a MapReduce algorithm that generates key-value pairs
&lt;span class=&#34;math inline&#34;&gt;\((year, average\_temperature)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, month, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, temps) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, sum(temps)/len(temps))\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(temps)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(temps)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Suppose now that we have a large CSV file
stored in a distributed file system (e.g., HDFS),
containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We can have up to one measurement per second in some years.
Like before, we’d like to compute key-value pairs (year, average_temperature) by using
a MapReduce algorithm.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What is the maximum number of measurements in a year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since we can have up to one measurement per second, the maximum number of
measurements &lt;span class=&#34;math inline&#34;&gt;\(M_{max}\)&lt;/span&gt; for a certain year is given by the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{max} = 365 \times 24 \times 60 \times 60 \approx 31.5 \times 10^6 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Considering the answer to the previous question, discuss the efficiency
of the first implementation of the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since there might be up to 31 million values
associated with a key, the bottleneck of the computation would be
the shuffle operation, since we need to copy a high number of
(key,value) pairs from the mappers to the reducers.&lt;/p&gt;
&lt;p&gt;Also, a reducer might have to
loop over a huge list of values in order to compute their average.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answer to the previous question,
propose a better implementation to handle the CSV file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, mo, d, mi, sec, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;combine: &lt;span class=&#34;math inline&#34;&gt;\((year, temps) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (sum(temps), len(temps)))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, [(s_i, l_i),\ i=1\dots n]) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, \frac{\sum_{i=1}^n s_i}{\sum_{i=1}^n l_i})\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(temps)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(temps)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,B,C,D
B,A,D
C,A,D
D,A,B,C
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to obtain the list of the common friends for each pair of individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to find the common friends in a
social network satisifying the given constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((x, F) \rightarrow [((u, v), x)\ \forall (u, v) \in F\ |\ u &amp;lt; v ]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\([(u, v), LCF] \rightarrow [(u, v), LCF]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the first item in a line.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is the list containing the items in a line except the first one (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s friends).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(LCF\)&lt;/span&gt; is the list of all individuals that are friends with both &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We note that the reduce function is the identity.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-average-and-standard-deviation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Computing average and standard deviation&lt;/h1&gt;
&lt;p&gt;We consider again the large CSV file
with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).&lt;/p&gt;
&lt;p&gt;We can express the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1 \leq i \leq n\)&lt;/span&gt;) with two different
equations.&lt;/p&gt;
&lt;p&gt;The first equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Which equation of the standard deviation
is more appropriate in a Map-Reduce algorithm?
Why?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The second equation is more appropriate because it allows the computation
of the sum of the elements and of the square of the elements step by step
by using map and combine together.&lt;/p&gt;
&lt;p&gt;Instead, if we use the first equation, we need first to compute the average and then use it
to compute the variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to compute the average and the standard
deviation of the temperatures for each year.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, mo, d, mi, sec, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;combine: &lt;span class=&#34;math inline&#34;&gt;\((year, T) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (sum(T), sum(T^2), len(T)))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, [(s_{i}, sq_{i}, l_{i}),\ i=1\dots n]) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (\mu, \sigma))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(T)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T^2 = [x^2 | x\in T]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(T)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \sum_{i=1}^n s_{i}/ \sum_{i=1}^n l_{i}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma = \sqrt{ (\sum_{i=1}^n sq_{i}/ \sum_{i=1}^n l_{i}) - \mu^2 }\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--
# Creating an inverted index

We have  a collection of $n$ documents in a directory and we want to create 
an **inverted index**,  one that associates each word 
to the list of the files the word occurs in. 
More precisely, for each word, the inverted index will 
have a list of the names of the documents that contain the word. 

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to create an inverted index over a 
collection of documents.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The input to the map will be a key-value pair, where 
the key is the name of a file $f$ and the value is the 
content $C$ of the file.

map: $(f, C) \rightarrow [(w, f)\ \forall w \in C]$

reduce: $(w, L) \rightarrow (w, L)$

where $L$ is the list of the files containing the word $w$.

We note that the reduce function is the identity.

Note also that in the map function we can add instructions 
to preprocess the text. For example, we can eliminate some words
that are not useful in the index (e.g., the stopwords) or remove 
special symbols.

:::

&lt;/details&gt;


--&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>/courses/big-data-marseille/tutorials/map-reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/big-data-marseille/tutorials/map-reduce/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We are given a dataset that
contains the average monthly
temperature measurements
over the course of some years.
More precisely, the dataset is stored in a CSV file,
where each row corresponds to a monthly
measurement and the columns contain the following values:
year, month, average temperature in the month.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1980,3,10
1980,4,14
1980,5,17
....
1981,1,2
1981,2,1
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We intend to get the average monthly temperature for each year.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Write a MapReduce algorithm that generates key-value pairs
&lt;span class=&#34;math inline&#34;&gt;\((year, average\_temperature)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, month, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, temps) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, sum(temps)/len(temps))\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(temps)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(temps)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Suppose now that we have a large CSV file
stored in a distributed file system (e.g., HDFS),
containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We can have up to one measurement per second in some years.
Like before, we’d like to compute key-value pairs (year, average_temperature) by using
a MapReduce algorithm.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What is the maximum number of measurements in a year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since we can have up to one measurement per second, the maximum number of
measurements &lt;span class=&#34;math inline&#34;&gt;\(M_{max}\)&lt;/span&gt; for a certain year is given by the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{max} = 365 \times 24 \times 60 \times 60 \approx 31.5 \times 10^6 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Considering the answer to the previous question, discuss the efficiency
of the first implementation of the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Since there might be up to 31 million values
associated with a key, the bottleneck of the computation would be
the shuffle operation, since we need to copy a high number of
(key,value) pairs from the mappers to the reducers.&lt;/p&gt;
&lt;p&gt;Also, a reducer might have to
loop over a huge list of values in order to compute their average.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answer to the previous question,
propose a better implementation to handle the CSV file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, mo, d, mi, sec, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;combine: &lt;span class=&#34;math inline&#34;&gt;\((year, temps) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (sum(temps), len(temps)))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, [(s_i, l_i),\ i=1\dots n]) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, \frac{\sum_{i=1}^n s_i}{\sum_{i=1}^n l_i})\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(temps)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(temps)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(temps\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-average-and-standard-deviation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Computing average and standard deviation&lt;/h1&gt;
&lt;p&gt;We consider again the large CSV file
with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).&lt;/p&gt;
&lt;p&gt;We can express the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1 \leq i \leq n\)&lt;/span&gt;) with two different
equations.&lt;/p&gt;
&lt;p&gt;The first equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Which equation of the standard deviation
is more appropriate in a Map-Reduce algorithm?
Why?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The second equation is more appropriate because it allows the computation
of the sum of the elements and of the square of the elements step by step
by using map and combine together.&lt;/p&gt;
&lt;p&gt;Instead, if we use the first equation, we need first to compute the average and then use it
to compute the variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to compute the average and the standard
deviation of the temperatures for each year.
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((year, mo, d, mi, sec, temperature) \rightarrow (year, temperature)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;combine: &lt;span class=&#34;math inline&#34;&gt;\((year, T) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (sum(T), sum(T^2), len(T)))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((year, [(s_{i}, sq_{i}, l_{i}),\ i=1\dots n]) \rightarrow\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((year, (\mu, \sigma))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the list of all temperatures in the same &lt;span class=&#34;math inline&#34;&gt;\(year\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(sum(T)\)&lt;/span&gt; sums all the elements in the list &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T^2 = [x^2 | x\in T]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(len(T)\)&lt;/span&gt; gives the length of the list &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \sum_{i=1}^n s_{i}/ \sum_{i=1}^n l_{i}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma = \sqrt{ (\sum_{i=1}^n sq_{i}/ \sum_{i=1}^n l_{i}) - \mu^2 }\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,B,C,D
B,A,D
C,A,D
D,A,B,C
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to obtain the list of the common friends for each pair of individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to find the common friends in a
social network satisifying the given constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((x, F) \rightarrow [((u, v), x)\ \forall (u, v) \in F\ |\ u &amp;lt; v ]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\([(u, v), LCF] \rightarrow [(u, v), LCF]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the first item in a line.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is the list containing the items in a line except the first one (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s friends).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(LCF\)&lt;/span&gt; is the list of all individuals that are friends with both &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We note that the reduce function is the identity.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-inverted-index&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Creating an inverted index&lt;/h1&gt;
&lt;p&gt;We have a collection of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; documents in a directory and we want to create
an &lt;strong&gt;inverted index&lt;/strong&gt;, one that associates each word
to the list of the files the word occurs in.
More precisely, for each word, the inverted index will
have a list of the names of the documents that contain the word.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to create an inverted index over a
collection of documents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The input to the map will be a key-value pair, where
the key is the name of a file &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and the value is the
content &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; of the file.&lt;/p&gt;
&lt;p&gt;map: &lt;span class=&#34;math inline&#34;&gt;\((f, C) \rightarrow [(w, f)\ \forall w \in C]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;reduce: &lt;span class=&#34;math inline&#34;&gt;\((w, L) \rightarrow (w, L)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is the list of the files containing the word &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We note that the reduce function is the identity.&lt;/p&gt;
&lt;p&gt;Note also that in the map function we can add instructions
to preprocess the text. For example, we can eliminate some words
that are not useful in the index (e.g., the stopwords) or remove
special symbols.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>/courses/bigdata-mds/labs/map-reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata-mds/labs/map-reduce/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;calcul-des-moyennes&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Calcul des moyennes&lt;/h1&gt;
&lt;p&gt;On nous donne un ensemble de données qui
contient les mesures moyennes des températures mensuelles
sur plusieurs années. Plus précisément, l’ensemble de données est stocké dans un fichier CSV,
où chaque ligne correspond à une mesure mensuelle
et les colonnes contiennent les valeurs suivantes :
année, mois, température moyenne du mois.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1981,1,2
1981,2,1
....
1980,3,10
1980,4,14
1980,5,17
....
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nous avons l’intention d’obtenir la température moyenne pour chaque année.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Écrire un algorithme MapReduce qui génère des paires clé-valeur
&lt;span class=&#34;math inline&#34;&gt;\((annee, temperature\_moyenne)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, temperatures_in_year):
    return (year, sum(temperatures_in_year) / len(temperatures_in_year))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Supposons maintenant que nous ayons un grand fichier CSV
stocké dans un système de fichiers distribué (par exemple, HDFS),
contenant une série de mesures au format « Année,Mois,Jour,Heure,Minute,Seconde,Température ».
Pour certaines années, nous pouvons avoir jusqu’à une mesure par seconde.
Comme précédemment, nous souhaitons calculer les paires clé-valeur (année, température moyenne) à l’aide d’un algorithme MapReduce.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Quel est le nombre maximum de mesures par an ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Comme nous pouvons avoir jusqu’à une mesure par seconde, le nombre maximum de mesures &lt;span class=&#34;math inline&#34;&gt;\(M_{max}\)&lt;/span&gt; pour une année est donné par la formule suivante :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M_{max} = 365 \times 24 \times 60 \times 60 \approx 31.5 \times 10^6
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Compte tenu de la réponse à la question précédente, discutez de l’efficacité de la première implémentation de l’algorithme.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Étant donné qu’il peut y avoir jusqu’à 31 millions de valeurs
associées à une clé, le goulot d’étranglement du calcul serait l’opération de shuffle, puisque nous devons copier un grand nombre de paires
(clé, valeur) des mappers vers les reducers.&lt;/p&gt;
&lt;p&gt;De plus, un reducer peut avoir à
boucler sur une énorme liste de valeurs afin de calculer leur moyenne.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
En fonction de la réponse à la question précédente,
proposez une meilleure implémentation pour traiter le fichier CSV.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La fonction &lt;code&gt;map()&lt;/code&gt; ne change pas par rapport à l’exercice précédent.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1])) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maintenant on code la fonction &lt;code&gt;combine()&lt;/code&gt; qui agira sur les couples clé-valeur renvoyées par
chaque tâche Map.
Concrètement, lorsqu’une tâche Map termine d’appliquer la fonction &lt;code&gt;map()&lt;/code&gt; sur toutes les lignes
de son bloc, une opération de shuffle est réalisée pour rassembler toutes les valeurs à la clé correspondante.
Ainsi, la fonction &lt;code&gt;combine()&lt;/code&gt; prend en argument un couple, dont la clé (une année) est associée à la liste de toutes ses valeurs (les températures de l’année).
La fonction renvoie un couple dont la clé représente toujours une année, mais la valeur est, elle-même, un couple, dont le premier élément est la somme des
températures sur l’année et le deuxième élément est la longueur de la liste des températures.&lt;/p&gt;
&lt;p&gt;Ce faisant, on enverra aux reducers seulement un couple pour chaque année, réduisant ainsi le nombre de données qui seront transmises via le réseau.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def combine(year, temperatures_in_year):
    return (year, (sum(temperatures_in_year), len(temperatures_in_year)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;La fonction &lt;code&gt;reduce()&lt;/code&gt; prend en arguments un couple, dont la clé représente une année et la valeur est une liste de couples reçues après l’étape de
&lt;code&gt;combine()&lt;/code&gt;. Il faudra ainsi sommer toutes les températures et toutes les longueurs, et ensuite calculer la moyenne.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, sum_len_tuples):
    sum_temps = sum(s for (s, _) in sum_len_tuples)
    nb_temps = sum(l for (_, l) in sum_len_tuples)
        
    return (year, sum_temps/nb_temps)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Amis communs dans un réseau social&lt;/h1&gt;
&lt;p&gt;Considérons un réseau social décrit par un graphe encodé dans un fichier texte.
Chaque ligne du fichier est une liste d’identifiants séparés par des virgules.
Par exemple, la ligne &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; signifie que &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; est ami avec &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
Un extrait du fichier ressemble à ce qui suit :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,A,D
A,D,B,C
D,C,A,B
C,D,A
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nous supposons que la relation d’amitié est symétrique : &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implique
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nous voulons obtenir la liste des amis communs pour chaque paire d’individus :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comme contrainte supplémentaire, nous voulons représenter un couple une seule fois et éviter de représenter un couple symétrique.
En d’autres termes, si nous produisons &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, nous ne voulons pas produire &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Proposez une implémentation MapReduce pour trouver les amis communs dans un réseau social satisfaisant les contraintes données.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;Le résultat de notre algorithme doit être une une collection de couples,
dont le premier élément représente un couple d’individus et le deuxième élément représente la liste des amis communs à ces individus.&lt;/p&gt;
&lt;p&gt;On en déduit qu’il faudrait choisir un couple d’individus en tant que clé.&lt;/p&gt;
&lt;p&gt;Or, la fonction &lt;code&gt;map()&lt;/code&gt; agit ligne par ligne. La question que nous devons nous poser est la suivante :
pouvons-nous déduire une information à propos d’amis communs entre deux individus?&lt;/p&gt;
&lt;p&gt;Prenons l’example de la ligne : A,D,B,C.
À partir de cette ligne, nous savons que A est l’ami commun à tous les couples de ses amis : (B, D), (C, D) et (B, C).&lt;/p&gt;
&lt;p&gt;Cela nous indique comment implémenter la fonction &lt;code&gt;map()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return [((x, y), values[0]) for x in values[1:] for y in values[1:] if x &amp;lt; y] &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;À noter que la condition &lt;code&gt;x&amp;lt;y&lt;/code&gt; nous garantit que nous ne produirons jamais de couples symétriques.&lt;/p&gt;
&lt;p&gt;La phase de shuffle associera ensuite à chaque couple d’individus tous leurs amis communs.
Ceci étant déjà le résultat voulu, nous n’avons pas besoin de coder une fonction &lt;code&gt;reduce()&lt;/code&gt; (fonction identité).&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;calcul-de-la-moyenne-et-de-lécart-type&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Calcul de la moyenne et de l’écart-type&lt;/h1&gt;
&lt;p&gt;Nous considérons à nouveau le grand fichier CSV
contenant une série de mesures au format &lt;em&gt;Année, Mois, Jour, Minute, Seconde, Température&lt;/em&gt;.
Nous souhaitons maintenant générer une série de paires clé-valeur &lt;em&gt;(année, (temperature_moy, ecart_type))&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Nous pouvons exprimer l’écart-type de &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; valeurs &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1 \leq i \leq n\)&lt;/span&gt;) avec deux équations différentes.&lt;/p&gt;
&lt;p&gt;La première équation est la suivante :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;La deuxième équation est la suivante :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Quelle équation de l’écart-type
est plus appropriée dans un algorithme MapReduce ?
Pourquoi ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;La deuxième équation est plus appropriée car elle permet de calculer la somme des éléments et le carré des éléments pas à pas
en utilisant conjointement &lt;code&gt;map()&lt;/code&gt; et &lt;code&gt;combine()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Au contraire, si nous utilisons la première équation, nous devons d’abord calculer la moyenne et ensuite l’utiliser pour calculer la variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercice&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Proposez une implémentation MapReduce pour calculer la moyenne et l’écart-type des températures pour chaque année.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;summary&gt;
Solution
&lt;/summary&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def map(line):
    values = line.split(&amp;quot;,&amp;quot;)
    return (values[0], float(values[-1])) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def combine(year, temperatures_in_year):
    return (year, (sum(temperatures_in_year), sum(x**2 for x in temperatures_in_year), len(temperatures_in_year)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def reduce(year, sum_len_tuples):
    sum_temps = sum(s for (s, _, _) in sum_len_tuples)
    sum_sq_temps = sum(sq for (_, sq, _) in sum_len_tuples)
    nb_temps = sum(l for (_, _, l) in sum_len_tuples)
        
    return (year, (sum_temps/nb_temps, math.sqrt(sum_sq_temps/nb_temps - (sum_temps/nb_temps)**2)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--
# Creating an inverted index

We have  a collection of $n$ documents in a directory and we want to create 
an **inverted index**,  one that associates each word 
to the list of the files the word occurs in. 
More precisely, for each word, the inverted index will 
have a list of the names of the documents that contain the word. 

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercice**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to create an inverted index over a 
collection of documents.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;

::: {.infobox .exosolution data-latex=&#34;{exercisebox}&#34;}

The input to the map will be a key-value pair, where 
the key is the name of a file $f$ and the value is the 
content $C$ of the file.

map: $(f, C) \rightarrow [(w, f)\ \forall w \in C]$

reduce: $(w, L) \rightarrow (w, L)$

where $L$ is the list of the files containing the word $w$.

We note that the reduce function is the identity.

Note also that in the map function we can add instructions 
to preprocess the text. For example, we can eliminate some words
that are not useful in the index (e.g., the stopwords) or remove 
special symbols.

:::

&lt;/details&gt;


--&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>/courses/bigdata/tutorials/map-reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bigdata/tutorials/map-reduce/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We are given a dataset that
contains the average monthly
temperature measurements
over the course of some years.
More precisely, the dataset is stored in a CSV file,
where each row corresponds to a monthly
measurement and the columns contain the following values:
year, month, average temperature in the month.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1980,3,10
1980,4,14
1980,5,17
....
1981,1,2
1981,2,1
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We intend to get the average monthly temperature for each year.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Write a MapReduce algorithm that generates key-value pairs
&lt;span class=&#34;math inline&#34;&gt;\((year, average\_temperature)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Suppose now that we have a large CSV file
stored in a distributed file system (e.g., HDFS),
containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We can have up to one measurement per second in some years.
Like before, we’d like to compute key-value pairs (year, average_temperature) by using
a MapReduce algorithm.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What is the maximum number of measurements in a year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Considering the answer to the previous question, discuss the efficiency
of the first implementation of the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answer to the previous question,
propose a better implementation to handle the CSV file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,A,D
A,D,B,C
D,C,A,B
C,D,A
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to obtain the list of the common friends for each pair of individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to find the common friends in a
social network satisifying the given constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-average-and-standard-deviation&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Computing average and standard deviation&lt;/h1&gt;
&lt;p&gt;We consider again the large CSV file
with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).&lt;/p&gt;
&lt;p&gt;We can express the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1 \leq i \leq n\)&lt;/span&gt;) with two different
equations.&lt;/p&gt;
&lt;p&gt;The first equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Which equation of the standard deviation
is more appropriate in a Map-Reduce algorithm?
Why?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to compute the average and the standard
deviation of the temperatures for each year.
&lt;/div&gt;
&lt;/div&gt;
&lt;!--
# Creating an inverted index

We have  a collection of $n$ documents in a directory and we want to create 
an **inverted index**,  one that associates each word 
to the list of the files the word occurs in. 
More precisely, for each word, the inverted index will 
have a list of the names of the documents that contain the word. 

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to create an inverted index over a 
collection of documents.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::




--&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Programming with MapReduce</title>
      <link>/courses/bdalbert/tutorials/mapreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdalbert/tutorials/mapreduce/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We are given a dataset that
contains the average monthly
temperature measurements
over the course of some years.
More precisely, the dataset is stored in a CSV file,
where each row corresponds to a monthly
measurement and the columns contain the following values:
year, month, average temperature in the month.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1980,1,5
1980,2,2
1980,3,10
1980,4,14
1980,5,17
....
1981,1,2
1981,2,1
1981,3,3
1981,4,10
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We intend to get the average monthly temperature for each year.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;
Write a MapReduce algorithm that generates key-value pairs
&lt;span class=&#34;math inline&#34;&gt;\((year, average\_temperature)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Suppose now that we have a large CSV file
stored in a distributed file system (e.g., HDFS),
containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”.
We can have up to one measurement per second in some years.
Like before, we’d like to compute key-value pairs (year, average_temperature) by using
a MapReduce algorithm.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
What is the maximum number of measurements in a year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;
Considering the answer to the previous question, discuss the efficiency
of the first implementation of the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answer to the previous question,
propose a better implementation to handle the CSV file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A,B,C,D
B,A,D
C,A,D
D,A,B,C
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to obtain the list of the common friends for each pair of individuals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Propose a MapReduce implementation to find the common friends in a
social network satisifying the given constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spark MLlib</title>
      <link>/courses/bdia/tutorials/mllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/mllib/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The link to the Edunao page where the students submit their work --&gt;
&lt;!-- The submission deadline --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;p&gt;In this tutorial, you’ll learn how to use the Spark MLlib library to train, test and evaluate machine learning models.
We’ll be using a dataset of AirBnb accommodation in the San Francisco area.
The dataset is available in HDFS at the following path:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/prof/cpu_quercini/ml/sf-airbnb-clean.parquet&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Dataset source&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The dataset has been obtained &lt;a href=&#34;https://github.com/databricks/LearningSparkV2&#34; target=&#34;_blank&#34;&gt;from this GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The goal of the exercise is to predict the price per night of an apartment given all the features
in the dataset.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Changing Python interpreter of the executors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The default Python interpreter used by the executors in the cluster does not have the package &lt;code&gt;numpy&lt;/code&gt; installed.
In order to use one that has &lt;code&gt;numpy&lt;/code&gt;, you need to run the following command in the terminal:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;export PYSPARK_PYTHON=/usr/bin/python3&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-a-training-and-test-set&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Obtaining a training and test set&lt;/h1&gt;
&lt;p&gt;In order to build and evaluate a machine learning model, we need to split our dataset into
a &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;test set&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Copy the file &lt;code&gt;train_test.py&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/train_test.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Complete Line 9&lt;/strong&gt;, by specifying your directory in HDFS, which is as follows (&lt;strong&gt;replace X with your account number&lt;/strong&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete Line 23.&lt;/strong&gt; Write the code to read the input file, which is stored in HDFS as a Parquet file, into the DataFrame &lt;code&gt;airbnb_df&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;From Line 26&lt;/strong&gt;, add the instructions necessary to print the schema of &lt;code&gt;airbnb_df&lt;/code&gt; and the first 5 rows. Which option should you use
to nicely see the values in the columns?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code with &lt;code&gt;spark-submit&lt;/code&gt; and verify that the data is correctly loaded.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We now split &lt;code&gt;airbnb_df&lt;/code&gt; into two separate DataFrames, &lt;code&gt;train_df&lt;/code&gt; and &lt;code&gt;test_df&lt;/code&gt;, containing the training and
test instances respectively.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Uncomment Line 32.&lt;/strong&gt; Write the code to split the dataset into a training and test set. 80% of the instances
must be taken as training instances, while the remaining 20% will be used a test instances.
&lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html&#34; target=&#34;_blank&#34;&gt;Looking at the DataFrame API documentation&lt;/a&gt;, which
function are you going to use?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;From Line 36&lt;/strong&gt;, write the code to print the number of instances in the training and test set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the code with &lt;code&gt;spark-submit&lt;/code&gt;. You should have 5780 training instances and 1366 test instances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is time to save the training and test sets to a HDFS file.
This way, we can load them whenever we need them to build a machine learning model
for this problem.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From Line 47&lt;/strong&gt;. Write the code to write the training and test sets to two Parquet files in HDFS.
The paths to the two files are already available in variables &lt;code&gt;training_set_file&lt;/code&gt; and &lt;code&gt;test_set_file&lt;/code&gt;
defined at Lines 40 and 43 respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-features&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Preparing the features&lt;/h1&gt;
&lt;p&gt;In both training and test sets, the features correspond to DataFrame columns.
Most of the machine learning algorithms in Spark need to have all features in one single vector.
We need to use a &lt;strong&gt;transformer&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Good to know&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt; take in a DataFrame and return a new DataFrame that has the same columns as the
input DataFrame and additional columns that are specified as an argument to the transformer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copy the file &lt;code&gt;train_test_model&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;cp /usr/users/prof/cpu_quercini/mllib/train_test_model.py .&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;read_training_set&lt;/code&gt; at Line 22.
The file reads into a Spark DataFrame the training set Parquet file that you generated in the previous section.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;read_test_function&lt;/code&gt; at Line 39. The file reads into a Spark DataFrame the test set Parquet file that you generated in the previous section.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the variables at Line 105 and 106 so that they point to the two HDFS Parquet files that contain the training and test sets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 109 and 110, so as to print the number of training and test instances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;. Check that the number of training and test instances correspond to what you
got in the first section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is now time to learn how to use a transformer to put all the necessary features into a vector.
For the time being, we’ll only be using the feature &lt;code&gt;bedrooms&lt;/code&gt; to predict the price.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;get_vector&lt;/code&gt; at Line 55. The comments in the file describe what the function is supposed to do.
You’ll need to use a &lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html#pyspark.ml.feature.VectorAssembler&#34; target=&#34;_blank&#34;&gt;VectorAssembler object&lt;/a&gt; to implement the function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 117 and 118 to call the function &lt;code&gt;get_vector&lt;/code&gt; and display the result.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;. Observe the content of the the DataFrame &lt;code&gt;train_vect&lt;/code&gt;. It should have a column named
&lt;code&gt;features&lt;/code&gt; containing a list with one value (the value of feature &lt;code&gt;bedrooms&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is now time to train a linear regression model on the given training set and the selected features.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement the function &lt;code&gt;train_linear_regression_model&lt;/code&gt; at Line 79. The comments in the file describe what the function is supposed to do.
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html&#34; target=&#34;_blank&#34;&gt;Looking at the documentation&lt;/a&gt;, try to identify the object that you need to use
to create a linear regressor and the function that you need to invoke to train the linear regressor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 121, 124, 125, 126 to call the function &lt;code&gt;train_linear_regression_model&lt;/code&gt; and display the coefficients learned by the linear regression model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can now use the trained model to make some predictions.
The model returned by the function that you implemented in the previous exercise
is an object of type &lt;code&gt;LinearRegressionModel&lt;/code&gt;.
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html#pyspark.ml.regression.LinearRegressionModel&#34; target=&#34;_true&#34;&gt;Looking at the documentation&lt;/a&gt;, we learn that there is a function &lt;code&gt;predict&lt;/code&gt; that allows us to
make a prediction given a single instance.
If we want to make a prediction on the whole test set, we should use the function &lt;code&gt;transform&lt;/code&gt;.
The function takes in a DataFrame with the test set and returns the same DataFrame with an additional column &lt;code&gt;prediction&lt;/code&gt; that contains the predicted value.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Look at Lines 130 and 131. Which DataFrame do we need to pass the function &lt;code&gt;transform&lt;/code&gt;? Is &lt;code&gt;test_df&lt;/code&gt; a good choice? Why?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete line 130 and uncomment lines 130, 131, 132.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file &lt;code&gt;train_test_model&lt;/code&gt; with &lt;code&gt;spark-submit&lt;/code&gt; and observe the result. Is the predicted value the one that you expect given that you
know the formula of the regression line?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pipelines&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Pipelines&lt;/h1&gt;
&lt;p&gt;In the previous section we learned that a training and test set need to go through the same transformation steps in
order to be fed to a machine learning model.
When we have few transformations, it is easy to remember the ones that we applied to a training set, so as to apply them to the
test set too.
However, when we need to apply a series of transformations, the order of which is important, it is easy to make mistakes (applying different sets of transformations
to the training and test instances, which leads to meaningless predictions).&lt;/p&gt;
&lt;p&gt;A good practice is to use the &lt;strong&gt;Pipeline API&lt;/strong&gt;.
A &lt;strong&gt;pipeline&lt;/strong&gt; is composed of &lt;strong&gt;stages&lt;/strong&gt;. Each stage may be a &lt;strong&gt;transformer&lt;/strong&gt; or an &lt;strong&gt;estimator&lt;/strong&gt;.
A &lt;strong&gt;transformer&lt;/strong&gt; is an object on which we call the function &lt;code&gt;transform&lt;/code&gt; to obtain a new DataFrame from an input DataFrame.
An &lt;strong&gt;estimator&lt;/strong&gt; is an object on which we call the function &lt;code&gt;fit&lt;/code&gt; to learn a model on a given DataFrame. The learned model is itself
a transformer.&lt;/p&gt;
&lt;p&gt;When the function &lt;code&gt;fit&lt;/code&gt; is called on a Pipeline, the training set goes through all the transformers and the estimators in the order in which they are declared in the pipeline;
at last, the estimator specified in the last stage is trained on the training set.
The model returned by applying the function &lt;code&gt;fit&lt;/code&gt; on the pipeline is itself a transformer.
If we invoke the function &lt;code&gt;transform&lt;/code&gt; on that model on the test set, we obtain a DataFrame that contains a column named &lt;code&gt;predictions&lt;/code&gt;.
Implicitly, all the transformations in the pipeline will be applied to the test set too, before making the predictions.&lt;/p&gt;
&lt;p&gt;Copy the file &lt;code&gt;pipeline_example.py&lt;/code&gt; to your home folder in the cluster by typing the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/pipeline_example.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-pipeline.html&#34; target=&#34;_blank&#34;&gt;Look at the documentation&lt;/a&gt; and
write a code from Line 34 to create a pipeline that does the same operations as in file &lt;code&gt;train_test_model.py&lt;/code&gt;
to train and test a linear regression model on the given training and test sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don’t forget to specify the paths to the training and test set files on HDFS at Lines 27 and 28.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;from-categorical-to-numerical-features&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; From categorical to numerical features&lt;/h2&gt;
&lt;p&gt;Many machine learning models do not handle categorical values: they need all features to be numerical.
Linear regression is such an example.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy the file &lt;code&gt;onehot_playground.py&lt;/code&gt; to your home directory in the cluster by typing the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp /usr/users/prof/cpu_quercini/mllib/onehot_playground.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Change lines 27 and 28 and write the paths to the files with the training and test sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;First, let’s find out which features are categorical in our dataset.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Complete the function &lt;code&gt;get_categorical_columns&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In order to get an idea as to how to get the categorical columns, execute the code
with &lt;code&gt;spark_submit&lt;/code&gt;. This execute the instruction at Line 86.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once you’re done with the implementation, execute the file with &lt;code&gt;spark-submit&lt;/code&gt; and observe the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;One example of categorical feature in our dataset is &lt;code&gt;property_type&lt;/code&gt;, which takes values such as &lt;em&gt;Apartment&lt;/em&gt;, &lt;em&gt;House&lt;/em&gt;, &lt;em&gt;Condominium&lt;/em&gt;…
Each value of a categorical feature is also referred to as a category.&lt;/p&gt;
&lt;p&gt;One way to turn this feature into a numerical one would be to assign a number to each category (e.g., &lt;em&gt;Apartment&lt;/em&gt; corresponds to 0, &lt;em&gt;House&lt;/em&gt; to 1….).
However, this implicitly introduces an order among the categories: the category &lt;em&gt;House&lt;/em&gt; would be worth twice as much as the category &lt;em&gt;Apartment&lt;/em&gt;;
this would inevitably bias the trained model.&lt;/p&gt;
&lt;p&gt;A commonly used method is &lt;strong&gt;one-hot encoding&lt;/strong&gt;. Let’s find out how it works.
Let’s focus only on the feature &lt;code&gt;property_type&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 40, 41 and 42. These lines instantiate an estimator that is called &lt;code&gt;StringIndexer&lt;/code&gt;.
This estimator associates numeric indexes to the values of &lt;code&gt;property_type&lt;/code&gt;. The indexes will be stored in another column
named &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Line 46. The &lt;code&gt;StringIndexer&lt;/code&gt; is applied to the training set to learn how to associate indexes to the values of
&lt;code&gt;property_type&lt;/code&gt;. The result of the instruction is a new transformer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Uncomment Line 49. The transformer learned on Line 46 is used to transform the training set into a new DataFrame.
This DataFrame contains an additional column called &lt;code&gt;property_type_index&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Complete the function &lt;code&gt;count_property_types&lt;/code&gt;.
Follow the instructions in the file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Once the function is complete, uncomment Line 53 to call the function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the file with &lt;code&gt;spark-submit&lt;/code&gt; and observe the output. Can you guess how the indexes are assigned to the categories
of the feature &lt;code&gt;property_type&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It is time to find out how one-hot encoding works.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uncomment Lines 57, 58, 61, 64, 67. These lines instantiate an estimator that is called &lt;code&gt;OneHotEncoder&lt;/code&gt;.
This estimator uses the indexes in &lt;code&gt;property_type_index&lt;/code&gt; and creates a new column &lt;code&gt;property_type_ohe&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The estimator is trained on the training set and a new transformer is obtained (line 61).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The transformer is used on the training set to transform it into a new DataFrame, where a new column &lt;code&gt;property_type_ohe&lt;/code&gt; exists.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Line 67 prints selected columns of this transformed training set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Execute the file with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can you understand how one-hot encoding works?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now, you have all the ingredients to create a full pipeline to train and test a
linear regression model by using all features.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Create a new file &lt;code&gt;full_pipeline.py&lt;/code&gt; where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You select the categorical features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a &lt;code&gt;StringIndexer&lt;/code&gt; and a &lt;code&gt;OneHotEncoder&lt;/code&gt; to apply one-hot encoding to all categorical features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Obtain the numeric features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a &lt;code&gt;VectorAssembler&lt;/code&gt; to put into a single vector the one-hot encoded features and the numerical ones.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set up a linear regression model that takes in all features and the variable to predict (&lt;code&gt;price&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mix all these ingredients in a &lt;code&gt;Pipeline&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;Pipeline&lt;/code&gt; to train and test the model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-a-model&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Evaluating a model&lt;/h1&gt;
&lt;p&gt;In the previous sections we visually looked at the predictions to get an vague idea of how our
estimator performs.
In order to quantify the quality of our estimator, we need some evaluation measures.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
&lt;a href=&#34;https://spark.apache.org/docs/latest/ml-tuning.html#train-validation-split&#34; target=&#34;_blank&#34;&gt;Look at the documentation&lt;/a&gt;
and play with the code to do model selection based on a grid search of parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB - Data modeling</title>
      <link>/courses/bdia/tutorials/mongodb-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/mongodb-modeling/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;use-case-scenario&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Use case scenario&lt;/h1&gt;
&lt;p&gt;We consider a relational database that holds the data of a chain of DVD stores; the &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-preface.html&#34; target=&#34;_blank&#34;&gt;database name is
Sakila&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Sakila database is serving an increasing number of queries from staff and customers
around the world.
A single monolithic database is not sufficient anymore to serve all
the requests and the company is thinking of distributing the database across
several servers (&lt;strong&gt;horizontal scalability&lt;/strong&gt;).
However, a relational database does not handle horizontal scalability very well, due
to the fact that the data is scattered across numerous tables, as the result of the normalization
process.
Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to
MongoDB.&lt;/p&gt;
&lt;p&gt;For the migration to happen, it is necessary to conceive a suitable data model.
From the first discussions with the Sakila management,
you quickly understand that one of the main use of the database is to
manage (add, update and read) rental information.&lt;/p&gt;
&lt;div id=&#34;description-of-the-relational-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Description of the relational model&lt;/h2&gt;
&lt;p&gt;The existing data model is recalled in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:sakila-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The logical schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: The logical schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here is the &lt;strong&gt;description of the tables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;actor&lt;/TT&gt; lists information for all actors. Columns: &lt;em&gt;actor_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;address&lt;/TT&gt; contains address information for customers, staff and stores. Columns: &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;address&lt;/em&gt;, &lt;em&gt;address2&lt;/em&gt;, &lt;em&gt;district&lt;/em&gt;, &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;postal_code&lt;/em&gt;, &lt;em&gt;phone&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;category&lt;/TT&gt; lists the categories that can be assigned to a film. Columns: &lt;em&gt;category_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;city&lt;/TT&gt; contains a list of cities. Columns: &lt;em&gt;city_id&lt;/em&gt;, &lt;em&gt;city&lt;/em&gt;, &lt;em&gt;country_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;country&lt;/TT&gt; contains a list of countries. Columns: &lt;em&gt;country_id&lt;/em&gt;, &lt;em&gt;country&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; contains a list of all customers. Columns: &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;create_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film&lt;/TT&gt; is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table.
Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;title&lt;/em&gt;, &lt;em&gt;description&lt;/em&gt;, &lt;em&gt;release_year&lt;/em&gt;, &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;original_language_id&lt;/em&gt;, &lt;em&gt;rental_duration&lt;/em&gt;, &lt;em&gt;rental_rate&lt;/em&gt;, &lt;em&gt;length&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_actor&lt;/TT&gt; is used to support a many-to-many relationship between films and actors. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;actor_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;film_category&lt;/TT&gt; is used to support a many-to-many relationship between films and categories. Columns: &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;category_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;inventory&lt;/TT&gt; contains one row for each copy of a given film in a given store. Columns: &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;film_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;language&lt;/TT&gt; contains possible languages that films can have for their language and original language values. Columns: &lt;em&gt;language_id&lt;/em&gt;, &lt;em&gt;name&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;payment&lt;/TT&gt; records each payment made by a customer, with information such as the amount and the rental being paid for.
Columns: &lt;em&gt;payment_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;amount&lt;/em&gt;, &lt;em&gt;payment_date&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned.
Columns: &lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;staff&lt;/TT&gt; lists all staff members. Columns: &lt;em&gt;staff_id&lt;/em&gt;, &lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;, &lt;em&gt;picture&lt;/em&gt;, &lt;em&gt;email&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;active&lt;/em&gt;, &lt;em&gt;username&lt;/em&gt;, &lt;em&gt;password&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The table &lt;TT&gt;store&lt;/TT&gt; lists all stores in the system. Columns: &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;manager_staff_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types-in-mongdb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data types in MongDB&lt;/h1&gt;
&lt;p&gt;A MongoDB document is represented as a JSON record.
However, internally MongoDB serializes the JSON record into a BSON record.
In practice, a BSON record is a binary representation of a JSON record.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Looking at
the &lt;a href=&#34;https://bsonspec.org/spec.html&#34; target=&#34;_blank&#34;&gt;specification of BSON&lt;/a&gt;, can you tell how many &lt;strong&gt;bytes&lt;/strong&gt; do you need to represent:
an integer, a date, a string and a boolean?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
The size of a document in MongoDB is limited to 16 MiB.
Can you tell why there is such a limit?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The table &lt;TT&gt;rental&lt;/TT&gt; has four integer columns (&lt;em&gt;rental_id&lt;/em&gt;, &lt;em&gt;inventory_id&lt;/em&gt;, &lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;staff_id&lt;/em&gt;) and
2 dates (&lt;em&gt;rental_date&lt;/em&gt;, &lt;em&gt;return_date&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The table &lt;TT&gt;customer&lt;/TT&gt; has three integer columns (&lt;em&gt;customer_id&lt;/em&gt;, &lt;em&gt;store_id&lt;/em&gt;, &lt;em&gt;address_id&lt;/em&gt;), three strings (&lt;em&gt;first_name&lt;/em&gt;, &lt;em&gt;last_name&lt;/em&gt; and &lt;em&gt;email&lt;/em&gt;),
one boolean value (&lt;em&gt;active&lt;/em&gt;) and one date (&lt;em&gt;create_date&lt;/em&gt;).&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimate the size of a document in both collections.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We make the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On average, each character needs 1.5 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An email address is 20 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A last name is 8 characters long on average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;A first name is 6 characters long on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;considerations-for-the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Considerations for the new model&lt;/h1&gt;
&lt;p&gt;Denormalization in MongoDB is &lt;a href=&#34;https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1&#34; target=&#34;_blank&#34;&gt;strongly encouraged&lt;/a&gt;
to read and write a record relative to an entity in one single operation.&lt;/p&gt;
&lt;p&gt;In the following exercises, we explore different options and analyze advantages and disadvantages.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Suppose that we create a collection &lt;TT&gt;Customer&lt;/TT&gt;,
where each document includes information about a customer.&lt;/p&gt;
&lt;p&gt;Suppose that we embed in each document the list of rentals
for a customer.&lt;/p&gt;
&lt;p&gt;How many rentals can we store for a given customer, knowing
that the size of a document in MongoDB cannot exceed 16 MiB?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Consider the two following options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;customer&lt;/TT&gt;, where each document contains the
information about a customer and an embedded list with the information
on all the rentals made by the customer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A collection &lt;TT&gt;rental&lt;/TT&gt;, where each document contains the information
about a rental and an embedded document with the information on the customer
that made the rental.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of a document in the two collections.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Suppose that we have in our database&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;512 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;16384 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, each customer has around 32 rentals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compute the size in byte&lt;/strong&gt; of the collections &lt;TT&gt;customer&lt;/TT&gt; and
&lt;TT&gt;rental&lt;/TT&gt; described in the previous question.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
Based on the answers in the previous questions, discuss advantages and disadvantages of the two options:
having a collection &lt;TT&gt;customer&lt;/TT&gt; (solution 1) or a collection &lt;TT&gt;rental&lt;/TT&gt; (solution 2).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;
Look again at the model in Figure &lt;a href=&#34;#fig:sakila-model&#34;&gt;1.1&lt;/a&gt;.
A rental document doesn’t only need to include
information on the customer who made the rental, but also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The staff member who took care of the rental.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The inventory item being rented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The payment information.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Discuss the different ways we can include this information in the collections
&lt;TT&gt;customer&lt;/TT&gt; and &lt;TT&gt;rental&lt;/TT&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the discussion, which solution would you retain?
A collection &lt;TT&gt;customer&lt;/TT&gt; or a collection &lt;TT&gt;rental&lt;/TT&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data-model-in-mongodb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; The data model in MongoDB&lt;/h1&gt;
&lt;p&gt;In the last question, we chose the collection that we intend to create.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Give the complete schema (name and type of the properties) of a document in the collection
that you chose in the previous question.&lt;/p&gt;
&lt;p&gt;If the value of any property is an embedded document:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specify the schema of that document too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If any property of an embedded document is an identifier referencing another entity,
use that identifier (don’t try, for now, to further denormalize the schema).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s take a closer look at the storage requirements of the adopted solution.
Consider that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information of an inventory item is 12 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size in bytes of a document storing the information about a payment is 20 bytes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;
If we denote by &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; the number of rentals, what is the size in bytes of the
database for the adopted solution?
What do you get if we set &lt;span class=&#34;math inline&#34;&gt;\(N_{rental}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Although the size that we determined in the previous exercise, may not sound
impressive, we still have to store other information
(films, actors….).
If we could save a bit of space, we would be happy.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.3  &lt;/strong&gt;&lt;/span&gt;
Discuss how you could save some space in the adopted solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HINT.&lt;/strong&gt; Do you really need to denormalize all data?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.4  &lt;/strong&gt;&lt;/span&gt;
Propose a solution for all the entities involved and
estimate the savings in terms of storage requirements.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-new-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; The new model&lt;/h1&gt;
&lt;p&gt;In this section we intend to obtain a complete model of the Sakila database.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Consider the model that we obtained at the end of the previous section.
Which data can you further denormalize?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;
Complete the diagram obtained in the previous exercise so as to obtain
a full data model for the Sakila database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB - Queries</title>
      <link>/courses/bdia/tutorials/mongodb-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/mongodb-queries/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;We learn how to write queries in MongoDB on the Sakila database.&lt;/p&gt;
&lt;div id=&#34;initialization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Initialization&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Launch a MongoDB server on your computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open MongoDB Compass.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connect to the MongoDB server with the following URI: &lt;code&gt;mongodb://localhost:27017&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-6/sakila-mongodb.zip&#34;&gt;&lt;strong&gt;this archive file&lt;/strong&gt;&lt;/a&gt; and extract it.
Each file corresponds to a collection.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In MongoDB Compass create a new database named &lt;code&gt;sakila&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create the collections &lt;code&gt;customer&lt;/code&gt;, &lt;code&gt;film&lt;/code&gt;, &lt;code&gt;rental&lt;/code&gt;, &lt;code&gt;staff&lt;/code&gt; and &lt;code&gt;store&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Import the data from the downloaded JSON files.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;p&gt;We might want to see a list of common operators &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query/#std-label-query-selectors&#34; target=&#34;_blank&#34;&gt;in this page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of the customers of Canadian stores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all rentals made by customers from Iran, where
the amount paid is strictly greater than 10 dollars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last names of the actors who played a role in film 213.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;operations-on-arrays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Operations on arrays&lt;/h1&gt;
&lt;p&gt;Useful array operators are &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/query-array/&#34; target=&#34;_blank&#34;&gt;listed here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have “Behind the Scenes” as special features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all the films where BURT POSEY played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the film that has exactly 15 actors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Aggregation framework&lt;/h1&gt;
&lt;p&gt;A useful reference for the aggregation pipeline &lt;a href=&#34;https://www.mongodb.com/docs/manual/meta/aggregation-quick-reference/&#34; target=&#34;_blank&#34;&gt;can be found here here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the total amount of payments across all rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors of each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sort the films by the number of actors (decreasing order).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the average number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the country where the customers have rented the most of the films in the category “Music”.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;join-operations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Join Operations&lt;/h1&gt;
&lt;p&gt;The join operation is &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/&#34; target=&#34;_blank&#34;&gt;explained here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the language of the film with title “ACE GOLDFINGER”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all the information about the staff member who took care of rental 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB queries</title>
      <link>/courses/bdalbert/tutorials/mongodb-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdalbert/tutorials/mongodb-queries/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn to write basic and advanced queries in MongoDB.&lt;/p&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Get the data&lt;/h1&gt;
&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-6/sakila-mongodb.zip&#34;&gt;&lt;strong&gt;this archive file&lt;/strong&gt;&lt;/a&gt; and extract it.
You’ll find a file for each collection to import into the database.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/h1&gt;
&lt;p&gt;Download &lt;a href=&#34;https://www.mongodb.com/try/download/compass&#34; target=&#34;blank&#34;&gt;MongoDB Compass&lt;/a&gt; and
follow the &lt;a href=&#34;https://webtv.centralesupelec.fr/videos/mongodb-compass-and-atlas/&#34; target=&#34;blank&#34;&gt;instructions that you find in this video to&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;install MongoDB Compass;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;start a MongoDB cluster on Atlas.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;connect to the MongoDB cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;create a database and import the data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the email of the customers of Canadian stores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all rentals made by customers from Iran, where
the amount paid is strictly greater than 10 dollars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last names of the actors who played a role in film 213.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find()&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find({}, {email:1})&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.customer.find({&#34;store.country&#34;: &#34;Canada&#34;}, {email:1});&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.rental.find({&#34;customer.country&#34;: &#34;Iran&#34;, amount: {$gt: 10}}, {rental_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({film_id: 213}, {&#34;actors.first_name&#34;:1, &#34;actors.last_name&#34;: 1}).sort({&#34;actors.last_name&#34;: -1});&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;operations-on-arrays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Operations on arrays&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have “Behind the Scenes” as special features.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of all the films where BURT POSEY played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier of the film that has exactly 15 actors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({special_features : {$elemMatch: {$eq: &#34;Behind the Scenes&#34;}}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({special_features : {$all: [&#34;Commentaries&#34;, &#34;Deleted Scenes&#34;]}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({&#34;actors.first_name&#34;: &#34;BURT&#34;, &#34;actors.last_name&#34;: &#34;POSEY&#34;}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;db.film.find({actors: {$size : 15}}, {film_id: 1, _id:0});&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregation-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Aggregation framework&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count the total amount of payments across all rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors of each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sort the films by the number of actors (decreasing order).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the average number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the customer who made the most of rentals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the country where the customers have rented the most of the films in the category “Music”.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {&#34;customer.first_name&#34;: &#34;TOMMY&#34;, &#34;customer.last_name&#34;: &#34;COLLAZO&#34;}}, 
                    {$project: {&#34;inventory.film.title&#34;: 1, _id:0}})
&lt;/pre&gt;
One can also express this query with the function find()
&lt;pre&gt;
db.rental.find({&#34;customer.first_name&#34;: &#34;TOMMY&#34;, &#34;customer.last_name&#34;: &#34;COLLAZO&#34;}, {&#34;inventory.film.title&#34;: 1, _id:0});
&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$group: {&#34;_id&#34;: null, total_amount: {$sum: &#34;$amount&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.film.aggregate({$project: {nb_actors: {$size: &#34;$actors&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.films.aggregate({$match: {actors: {$ne: null}}}, 
                            {$project: {title: 1, nb_actors: {$size: &#34;$actors&#34;}}}, 
                            {$sort: {nb_actors:-1}})
&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, 
                {$project: {film_id: 1, &#34;nb_actors&#34;: {$size: &#34;$actors&#34;}}}, 
                {$group: {_id: null, avg_actors: {$avg: &#34;$nb_actors&#34;}}})
&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rentals.aggregate(
{$project: {&#34;customer.first_name&#34;: 1, &#34;customer.last_name&#34;: 1}}, 
{$group: {_id: &#34;$customer&#34;, nb_rentals:{$sum: 1}}}, 
{$sort: {nb_rentals:-1}}, 
{$limit: 1})
&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
db.rental.aggregate(
{$match: {&#34;inventory.film.categories.category&#34;: &#34;Music&#34;}}, 
{$group: {_id: &#34;$customer.country&#34;, count: {$sum: 1}}}, 
{$sort:{count: -1}}, 
{$limit: 1})
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;join-operations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Join Operations&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;
Write the following queries in MongoDB using the aggregation framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the language of the film with title “ACE GOLDFINGER”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return all the information about the staff member who took care of rental 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {&#34;inventory.film.title&#34;: &#34;ACE GOLDFINGER&#34;}}, 
            {$lookup: {from: &#34;film&#34;, localField: &#34;inventory.film.film_id&#34;, foreignField:&#34;film_id&#34;, as:&#34;film&#34;}}, 
            {$project: {&#34;film.language&#34;: 1}}, {$limit : 1})
&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
db.rental.aggregate({$match: {rental_id: 2}}, 
                    {$lookup: {from: &#34;staff&#34;, localField: &#34;staff.staff_id&#34;, 
                    foreignField:&#34;staff_id&#34;, as:&#34;staff_member&#34;}}, {$project: {&#34;staff_member&#34;: 1}})
&lt;/pre&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Neo4j</title>
      <link>/courses/bdia_old/tutorials/neo4j-tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/neo4j-tutorial/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!--

# Setting up the work environment.

* Download [Neo4j Desktop](https://neo4j.com/download/){target=&#34;_blank&#34;} 
and install it on your computer.

* Create a new project by clicking on the button &#34;New&#34; 
that you&#39;ll find on the top left side of the window.

* Click on &#34;Add Database&#34;, then &#34;Create a Local Database&#34;.

* Give the database a name (e.g., *MovieLens*) and set 
a password that you can easily remember; then click on &#34;Create&#34;.
Choose version 4.1.1 for the database.



* Click on &#34;Start&#34; and wait for the database to become active.

* Click on the button &#34;Open&#34;. The *Neo4j Browser* will pop up.

In the next section, you&#39;ll have to type a sequence of commands to 
import the data. 
You&#39;ll write the commands in the text field on top of the 
*Neo4j Browser* (where you find the prompt *neo4j\$*).

## Import the data.

The dataset consists of 
data obtained from *MovieLens*, 
a recommender system 
whose users give movies a rate 
between 1 and 5, 
based on whether they dislike or love them. 
MovieLens uses the rates to recommend 
movies that its users might like. 
The dataset is modeled as a **directed graph** and 
consists of 100,004 rates on 9,125 movies 
across 671 users between January 9th, 1995 and October 16, 2016. 
The dataset also contains the names of 
the directors and the actors of each movie.

1. Import the **nodes** corresponding to the 
**movies** (label **Movie**) by 
using the following command (it took 31 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv&#39; as row
MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)})
RETURN count(m)
&lt;/pre&gt;

2. Create an **index** on the property *movie_id* of 
the nodes with label **Movie** with the following command:

&lt;pre&gt;
create index movie_idx for (m:Movie) on (m.movie_id)
&lt;/pre&gt;

3. Import the **nodes** corresponding to the 
**actors** (label **Actor**) by 
using the following command (it took 62 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv&#39; as row
MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name})
RETURN count(a)
&lt;/pre&gt;

4. Create an **index** on the property *actor_id* of 
the nodes with label **Actor** with the following command:

&lt;pre&gt;
create index actor_idx for (a:Actor) on (a.actor_id)
&lt;/pre&gt;

5. Import the **nodes** corresponding to the 
**directors** (label **Director**) by 
using the following command (it took 4 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv&#39; as row
MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name})
RETURN count(d)
&lt;/pre&gt;

6. Create an **index** on the property *director_id* of 
the nodes with label **Director** with the following command:

&lt;pre&gt;
create index director_idx for (d:Director) on (d.director_id)
&lt;/pre&gt;


7. Import the **nodes** corresponding to the 
**genres** (label **Genre**) by 
using the following command (it took 197 ms on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv&#39; as row
MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name})
RETURN count(g)
&lt;/pre&gt;

8. Create an **index** on the property *genre_id* of 
the nodes with label **Genre** with the following command:

&lt;pre&gt;
create index genre_idx for (g:Genre) on (g.genre_id)
&lt;/pre&gt;


9. Import the **nodes** corresponding to the 
**users** (label **User**) by 
using the following command (it took 347 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv&#39; as row
MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname})
RETURN count(u)
&lt;/pre&gt;

10. Create an **index** on the property *user_id* of 
the nodes with label **User** with the following command:

&lt;pre&gt;
create index user_idx for (u:User) on (u.user_id)
&lt;/pre&gt;

11. Import the links of type **ACTED_IN** 
between actors and movies with the following 
command (it took 2.5 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (a:Actor {actor_id: toInteger(row.actor_id)})
MERGE (a)-[r:ACTED_IN]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;

12. Import the links of type **DIRECTED** 
between directors and movies with the following 
command (it took 688 ms on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (d:Director {director_id: toInteger(row.director_id)})
MERGE (d)-[r:DIRECTED]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;

13. Import the links of type **HAS_GENRE** 
between movies and genres with the following 
command (it took 1 second on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (g:Genre {genre_id: toInteger(row.genre_id)})
MERGE (m)-[r:HAS_GENRE]-&gt;(g)
RETURN count(r)
&lt;/pre&gt;

14. Import the links of type **RATED** 
between users and movies with the following 
command (it took 5.9 seconds on my computer):

&lt;pre&gt;
:auto USING PERIODIC COMMIT 1000
LOAD CSV WITH HEADERS FROM &#39;https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv&#39; as row
MATCH (m:Movie {movie_id: toInteger(row.movie_id)})
MATCH (u:User {user_id: toInteger(row.user_id)})
MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-&gt;(m)
RETURN count(r)
&lt;/pre&gt;

# Exploratory queries

If you looked at the commands used to import the data, 
you might already have an idea as to the structure of 
the graph. 
You can get a glimpse on the node labels, 
the relationship types and the property keys by clicking on the 
button circled in the following figure:

&lt;img src=&#34;/courses/plp/tutorials/neo4j/neo4j-db-button.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
Write and execute the following query:

&lt;pre&gt;
MATCH (m:Movie {title_en:&#34;Toy Story&#34;}) 
RETURN m;
&lt;/pre&gt;

What do you obtain? What are the properties associated 
to a node with label *Movie*? 
Click once on the node to display its properties.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;
Double-click on the node displayed as the result of the previous query.
Analyze the neighbouring nodes (their labels and properties) 
and  the incident links (direction, type and properties).
You can move around the node by dragging it in the window.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Queries

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;
Write and execute the following queries:

Q1. The genres of the movies in the database.

Q2. The number of movies in the database.

Q3. The title of the movies released in 2015.

Q4. The number of directors by movie. Sort in decreasing order.

Q5. The names of the directors and the title of the movies that they 
directed and in which they also played.

Q6. The genres of the movies in which Tom Hanks played.

Q7. The title and the rate of all the movies that 
the user with identifier 3 rated.
Sort by rate in decreasing order.&lt;/div&gt;\EndKnitrBlock{exercise}

:::



## Query chaining

Cypher allows the specification of 
complex queries composed of 
several queries that are concatenated 
with the clause **WITH**. 
We are now going to see an example to 
obtain the titles of the movies 
that have been rated by at least 100 users.

At a first glance, the following query looks like a good solution:

&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) WHERE count(u) &gt;= 100
RETURN n.title_en
LIMIT 5;
&lt;/pre&gt;

However, executing this query returns the following error:

&lt;pre&gt;
Invalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41))
&#34;MATCH (n:Movie)&lt;-[:RATED]-(u:User) WHERE count(u) &gt;= 100&#34;
&lt;/pre&gt;

Similarly to SQL, we cannot use aggregating functions in the clause WHERE.

A correct formulation of the query requires the use of the clause WITH
to concatenate two queries: the first will count the number of rates
for each movie:

&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) 
RETURN n, count(u) as nb_rates
&lt;/pre&gt;

The second will take in the output of the first and will
filter all the movies where nb_rates &lt; 100. 
In order to chain the two queries, we&#39;ll replace the 
RETURN clause in the first query with a WITH clause, as follows:

&lt;pre&gt;
MATCH (n:Movie)&lt;-[:RATED]-(u:User) 
WITH n, count(u) as nb_rates
WHERE nb_rates &gt;= 100
RETURN n.title_en
&lt;/pre&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;
Write and execute a query to obtain  the five movies 
that obtained the best average rate among the movies 
that have been rated by at least 100 users.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Movie recommendation

We are now going to see how Neo4j 
can be effectively used in a real application 
by implementing queries that form the basis of a 
simple **movie recommendation system**. 
This system is based on the notion of **collaborative filtering**. 

This consists in recommending a user $u$ some films 
that s/he hasn’t rated yet and other 
users with similar preferences have loved. 
In our context, we say that a user loves 
a movie if s/he rated the movie at least 3.

This concept is explained in the following figure.

&lt;img src=&#34;/courses/plp/tutorials/neo4j/collaborative-filtering.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;

The user $u$ loves 6 movies, 
3 of which are also loved by the user $v$ (the black nodes); 
it is reasonable to think that $u$ 
may also love the two movies that $v$ loved and $u$ hasn’t rated yet.

The principle of collaborative filtering is 
based on the computation of a **similarity score** 
between two users. 
Several similarity scores are possible in this context; 
here, we are going to use the **Jaccard coefficient**. 
Let $L(u)$ and $L(v)$ be the 
sets of movies that $u$ and $v$ love
respectively; 
the similarity score $J(u,v)$ between $u$ and $v$ is given by:

$$
J(u, v) = \frac{|L(u) \cap L(v)|}{|L(u) \cup L(v)|}
$$

In order to recommend movies to 
a target user $v$, 
the recommender system computes the 
similarity score between $v$ and all 
the other users of the system and 
proposes to $v$ the movies 
that s/he hasn’t rated yet and that the $k$ most similar users loved.

We are now going to incrementally write a query to recommend some movies to the target user 3.
The first step consists in determining the value $|L(v)|$.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-5&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-5) &lt;/strong&gt;&lt;/span&gt;
Write and execute the query to obtain 
the number of movies that the user 3 loved. 
This query must return the target user 
and the number of movies that s/he loves.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::




Next, we are going to determine the value $|L(u)|$, 
for all users $u$ except $v$.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-6&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-6) &lt;/strong&gt;&lt;/span&gt;
Write and execute the query 
to obtain the number of movies that each user $u$ loves, 
except the target user 3.
This query must return each user $u$ and the number 
of movies that s/he loves.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



We put the two queries together with the clause WITH.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-7&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-7) &lt;/strong&gt;&lt;/span&gt;
Compose the two previous queries with the clause WITH.
This query must return 
the target user 3, 
the number of movies that s/he loves, 
the other users $u$ and the number of movies 
that they love.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



Now, we need to 
determine the value $L(u)\cup L(v)$, 
for each user $u$, and compute the similarity score with the 
Jaccard coefficient. 

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-8&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-8) &lt;/strong&gt;&lt;/span&gt;
Append (by using WITH) 
to the query written in the previous exercise 
a query that obtains 
the number of movies 
that any user $u$ loved 
and that the target user 3 loved too, 
and computes the similarity score
between the target user 3 and $u$. 
This query must return the five most similar 
users to the target user and the similarity scores.

&lt;details&gt;
&lt;summary&gt;Hint&lt;/summary&gt;
Multiply the numerator of the equation 
by 1.0, otherwise Cypher will compute an integer division.
&lt;/details&gt;

&lt;/div&gt;\EndKnitrBlock{exercise}

:::



The last step consists in recommending some movies to the target user.
From the previous query, 
take the identifier of the user $w$ with 
the highest similarity to the target user. 
You are going to use this identifier directly in the new query.

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**
\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-9&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-9) &lt;/strong&gt;&lt;/span&gt;
Write and execute 
the query to obtain the list of 
the movies that the user $w$ loved 
and that the target user hasn&#39;t rated yet. Sort this list by decreasing rate.

&lt;details&gt;
&lt;summary&gt;Hint&lt;/summary&gt;

*  First, write a query 
to obtain the list of the movies 
that the target user rated. 
In the MATCH clause, 
use the variable $m$ to indicate a movie that the target user rated. 
Conclude the query with:

&lt;pre&gt;
RETURN collect(m.title_en) AS movies
&lt;/pre&gt;

The function *collect* creates a list called *movies*.

* Replace RETURN with WITH in the previous query and add a 
second query to select the titles of the movies 
$m$ that the user $w$ loved and the target user 
did not rate. 
In order to exclude the films that the target user 
did not rate, use the following predicate:

&lt;pre&gt;
none(x in movies where x=m.title_en)
&lt;/pre&gt;

in the WHERE clause.

&lt;/details&gt;
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



--&gt;
</description>
    </item>
    
    <item>
      <title>Simulation d&#39;un réseau informatique</title>
      <link>/courses/network/labs/netbasics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/network/labs/netbasics/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ce module a pour objectif de vous initier à la manipulation et à l’administration d’équipements
réseau courants et d’illustrer par la pratique certains concepts abordés en cours concernant les
protocoles de la pile TCP/IP.&lt;/p&gt;
&lt;!-- ::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Informatios importantes**

Ce TP sera **évalué**. 

**Remise du devoir**  : il faut remettre **un seul fichier .zip** comprenant :

* Un rapport en PDF, complet d&#39;une page de garde avec vos noms et prénoms et la date. 
Ecrivez une réponse pour chaque question. Ne vous limitez pas à des phrases courtes et ambiguës. 
La qualité et l&#39;exhaustivité de vos réponses seront prises en compte dans la note finale.

* Un fichier ``basic-config.pkt`` contenant le réseau créé dans Cisco Packet Tracer pour répondre aux questions 2.1 et 2.2.

* Un fichier ``fullnet.pkt`` contenant le réseau créé dans Cisco Packet Tracer pour répondre à toutes les autres questions.


:::  --&gt;
&lt;div id=&#34;environnement-de-travail&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Environnement de travail&lt;/h1&gt;
&lt;p&gt;Pour cette activité nous allons utiliser Cisco Packet Tracer.
Le logiciel permet de configurer et simuler un réseau.
Il utilise des composants de la marque Cisco,
un important fabricant sur le marché des équipements réseau professionnels.
D’autre importants fabriquants sont : Huawei, HPE, Nokia, Netgear, Dell, Juniper, Aerohive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;première-configuration&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Première configuration&lt;/h1&gt;
&lt;p&gt;Dans cette section, nous allons configurer un simple réseau point à point entre deux PC
à l’aide d’un câble Ethernet.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Démarrez avec un document Packet Tracer vierge ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez deux PCs ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reliez les deux PC par un câble ;
vous pouvez utiliser le bouton vous permettant de choisir automatiquement le cable
(bouton avec un éclair orange come icône).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Quelles sont les informations nécessaires pour configurer correctement un PC sur ce réseau?&lt;/p&gt;
&lt;p&gt;Précisez les valeurs numériques de ces informations que vous aurez choisies pour vos deux PCs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Une fois que vous aurez configurez vos deux PCs, il faudra penser à vérifier que ces derniers peuvent communiquer correctement.
Nous allons utiliser la commande &lt;code&gt;ping&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur l’un des PC, sélectionnez l’onglet Bureau et ouvrez une invite de commande.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tapez la commande ping avec le paramètre approprié pour envoyer un message à l’autre PC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assurez-vous d’obtenir une réponse de l’autre PC.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Cisco Packet Tracer fournit aussi un outil de simulation.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode simulation (bouton en bas à droite de l’interface de Packet Tracer) ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Depuis l’invite de commande d’un PC, faites un ping vers l’autre PC ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans le panneau de simulation à droite,
cliquez sur le bouton d’avancement pour dérouler l’animation.
Attendez quelques secondes (jusqu’à ce que vous ne voyiez plus de paquets de type ICMP apparaître),
puis cliquez de nouveau sur le bouton pour arrêter la simulation.
Vous pouvez maintenant examiner chacun des évènements de la liste en cliquant dessus.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Quel est le protocole utilisé par Ping ? Quelle est la signification de son acronyme ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sélectionnez l’un des messages échangés par les deux PC dans la fenêtre de simulation.&lt;br /&gt;
D’après les informations qui s’affichent, pouvez-vous dire à quelle couche du modèle TCP/IP appartient ce protocole ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pouvez-vous expliquer les informations contenues dans un message de ce protocole ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;N’oubliez pas de sauvegarder l’espace de travail dans un fichier nommé &lt;code&gt;basic-config.pkt&lt;/code&gt; avant de continuer.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration-dun-réseau-local&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Configuration d’un réseau local&lt;/h1&gt;
&lt;p&gt;Le réseau point à point que nous avons configuré dans la section précédente n’est pas couramment utilisé.&lt;/p&gt;
&lt;p&gt;Dans cette section, nous allons configurer un exemple réaliste de &lt;strong&gt;réseau local&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Démarrez avec un document Packet Tracer vierge ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez un commutateur, par exemple un 2960 (nous le nommerons C1).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez quatre PC dans votre réseau (nous les nommerons PC1, PC2, PC3, PC4) ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reliez ces PC au commutateur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quelles sont les informations nécessaires pour configurer correctement un PC sur ce réseau?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour chaque PC, donnez les valeurs numériques de ces informations.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant que vous avez configuré le réseau, vous devez tester que les PCs peuvent communiquer.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode simulation (bouton en bas à droite de l’interface de Packet Tracer) ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans le panneau de simulation à droite,
cliquez sur le bouton d’avancement pour dérouler l’animation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Depuis l’invite de commande d’un PC, faites un ping vers un autre PC ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Déroulez la simulation jusqu’à ce que vous verrez sur l’invite de commande &lt;em&gt;Reply from…&lt;/em&gt;.
Seulement à ce moment, mettez en pause la simulation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quels sont les protocoles qui interviennent dans ces échanges ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;En vous aidant de ressources en ligne, pourriez-vous décrire en quelque mots chaque protocole ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pourriez-vous expliquer les interactions ARP ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interconnection-de-deux-réseaux&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Interconnection de deux réseaux&lt;/h1&gt;
&lt;p&gt;Nous allons maintenant créer un &lt;strong&gt;deuxième réseau&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Choisissez une plage d’adresses pour un deuxième réseau IPv4 distinct du premier ;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Créez un deuxième réseau comprenant au moins deux ordinateurs. N’essayez pas pour le moment de connecter les deux réseaux.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que les ordinateurs du deuxième réseau puissent communiquer entre eux.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant il faut connecter les deux réseaux à l’aide d’un &lt;strong&gt;routeur&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ajoutez un routeur 1941 (que nous nommerons R1).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le routeur a deux interfaces réseaux, nommées &lt;em&gt;GigabitEthernet0/0&lt;/em&gt; et &lt;em&gt;GigabitEthernet0/1&lt;/em&gt;. Le routeur permet ainsi l’interconnexion entre deux réseaux.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez les cables nécessaires pour connecter le premier réseau au deuxième à l’aide du routeur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Si vous avez bien fait le cablage, vous devriez remarquer des triangles rouges sur les cables reliant le routeur aux deux réseaux.
Cela indique un problème de connectivité que nous allons résoudre dans l’activité suivante.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si vous passez la souris sur un cable, une info-bulle s’affiche indiquant les noms des interfaces réseaux reliées par le cable.
Cela va vous être utile dans les activités suivantes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Il faudra attribuer une adresse IPv4 à chacune des interfaces du routeur. &lt;strong&gt;Attention&lt;/strong&gt;, l’adresse IP d’une interface doit être cohérente avec les adresses du réseau auquel l’interface est connectée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour ce faire, cliquez sur le routeur et sélectionnez &lt;em&gt;Config&lt;/em&gt;. Vous verrez un onglet pour chaque interface dans le menu à gauche.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Attribuez une adresse IPv4 à chaque interface.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;N’oubliez pas de cocher la case &lt;em&gt;On&lt;/em&gt; à côté de &lt;em&gt;Port status&lt;/em&gt;. Cela sert à activer l’interface.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Si votre configuration est correcte, les triangles rouges devraient changer de couleur et s’afficher en vert.
La transition entre les deux couleurs pourrait prendre un certain délai, pendant lequel un cercle orange s’affiche. Cela est normal, il faut juste patienter quelques secondes.&lt;/p&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pensez à sauvegarder votre travail !&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;realtime&lt;/em&gt; (bouton en bas à droite dans l’interface de Packet Tracer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez l’invite de commande sur un PC du deuxième réseau et essayez de faire un ping vers l’un des ordinateurs du premier réseau.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que se passe-t-il ? Pourriez-vous expliquer ce qui manque dans la configuration des PC pour permettre une communication entre les PC des deux réseaux ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez les informations de configuration nécessaires aux PC.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant, on procède à une vérification finale.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que vous pouvez toujours faire un &lt;em&gt;ping&lt;/em&gt; entre deux PC d’un même réseau (par exemple PC1 et PC2).
Si ce n’est pas le cas, cela signifie que votre configuration a regressé par rapport aux exercices précédents.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que vous pouvez faire un &lt;em&gt;ping&lt;/em&gt; entre tout PC d’un réseau et tout PC du deuxième réseau.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pensez également à faire un &lt;em&gt;ping&lt;/em&gt; d’un PC vers les deux interfaces du routeur.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Si à l’issu de l’activité précédente il n’y a pas eu de problèmes de communications, cela veut dire que vous avez correctement configuré votre réseau local !&lt;/p&gt;
&lt;p&gt;Il ne nous reste que comprendre un peu mieux ce qui se passe lorsqu’un PC d’un réseau fait un &lt;em&gt;ping&lt;/em&gt; vers un PC de l’autre réseau, ce qui est l’objectif
de l’activité suivante.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;simulation&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Faites un &lt;em&gt;ping&lt;/em&gt; d’un PC vers un PC de l’autre réseau.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Laissez dérouler la simulation jusqu’à ce que le &lt;em&gt;ping reply&lt;/em&gt; ne revienne à la source. A ce moment, mettez la simulation en pause.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant, répondez aux questions suivantes :&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dans le panneau de simulation, considérez les messages ICMP (relatifs au &lt;em&gt;ping&lt;/em&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quel est le chemin suivi par ces messages ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour chacun de ces messages: quelle est l’adresse MAC source ?
À quel dispositif correspond cette adresse ?&lt;br /&gt;
Quelle est l’adresse MAC destination ? À quel dispositif correspond cette adresse ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que pouvez-vous dire à propos des adresses IP source et destination de chacun de ces messages ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sur la base des observations faites aux questions précédentes, est-ce qu’un commutateur change les adresses IP/MAC source/destination des messages ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sur la base des observations faites aux questions précédentes, est-ce qu’un routeur change les adresses IP/MAC source/destination des messages ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration-dun-serveur-dhcp&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Configuration d’un serveur DHCP&lt;/h1&gt;
&lt;p&gt;Dans les activités précédentes, nous avons manuellement configuré les adresses IP des PC.
Dans un réseau domestique ou d’entreprise, cela obligerait toute personne souhaitant se connecter au réseau de rentrer cette configuration à la main.&lt;/p&gt;
&lt;p&gt;Dans la pratique, un serveur DHCP (Dynamic Host Configuration Protocol) nous permet de configurer automatiquement les PC.&lt;/p&gt;
&lt;p&gt;Nous allons dans un premier temps ajouter un serveur DHCP au premier réseau.&lt;/p&gt;
&lt;p&gt;Un serveur DHCP doit être configuré en lui donnant :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Une adresse IP statique.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;L’adresse IP de la passerelle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une plage d’adresses IPv4 que le serveur DHCP utilisera pour attribuer automatiquement une adresse IP à tout dispositif se connectant à son réseau.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Etant donné que le serveur DHCP sera connecté au premier réseau, répondez aux questions suivantes :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quelle adresse IP allez-vous attribuer au serveur DHCP ? Donnez la valeur numérique de cette adresse.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse IP de la passerelle par défaut du serveur ? Donnez la valeur numérique de cette adresse.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Donnez les valeurs numérique de la plage d’adresses IPv4 que le serveur utilisera pour attribuer des adresses IP à des PC.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant que la configuration du serveur est faite su papier, nous allons l’implémenter dans l’outil de simulation.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ajoutez au premier réseau un terminal de type &lt;em&gt;Server&lt;/em&gt;, que nous nommerons S.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur le serveur et ouvrez l’onglet &lt;em&gt;Config&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Changez le nom du serveur en S.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ajoutez les informations concernant le &lt;em&gt;Default gateway&lt;/em&gt; et donnez une adresse IP au serveur, selon les réponse que vous avez données à l’exercice précédent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez l’onglet &lt;em&gt;Services&lt;/em&gt; et choisissez DHCP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Configurez un &lt;em&gt;pool&lt;/em&gt; qui correspond à la plage d’adresses que vous avez précisée à l’exercice précédent (changez le nom &lt;em&gt;Pool name&lt;/em&gt;, par ex. en &lt;em&gt;network1&lt;/em&gt;).
Précisez l’adresse du &lt;em&gt;Default gateway&lt;/em&gt;, ignorez le &lt;em&gt;DNS server&lt;/em&gt;, précisez la première adresse IP de la plage (&lt;em&gt;Start IP address&lt;/em&gt;) et un masque de sous-réseau.
Ignorez tous les autres champs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Important.&lt;/strong&gt; Cliquez sur &lt;em&gt;Save&lt;/em&gt; et pensez à activer le service DHCP en cliquant sur &lt;em&gt;On&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enfin, connectez le serveur DHCP au premier réseau à l’aide d’un cable. Attendez quelques secondes pour que les triangles vers s’affichent sur ce cable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant que le serveur DHCP est configuré, il faut l’utiliser !&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sélectionnez un PC du premier réseau, ouvrez l’onglet &lt;em&gt;Desktop&lt;/em&gt; et ensuite un invite de commandes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que vous arrivez à faire un &lt;em&gt;ping&lt;/em&gt; vers le serveur DHCP &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Si cela n’est pas le cas, revenez à l’activité précédente et
corrigez les erreurs de configuration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si vous avez pu faire un &lt;em&gt;ping&lt;/em&gt; vers le serveur DCHP, ouvrez l’onglet &lt;em&gt;Config&lt;/em&gt; sur le PC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Important.&lt;/strong&gt; Pensez à noter l’adresse IP actuelle du PC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans la section &lt;em&gt;Gateway/DNS IPv4&lt;/em&gt; choisissez &lt;em&gt;DHCP&lt;/em&gt; au lieu de &lt;em&gt;Static&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez à nouveau l’onglet &lt;em&gt;Desktop&lt;/em&gt; et ouvrez un invite de commandes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;em&gt;ipconfig&lt;/em&gt; et vérifiez que maintenant votre PC a une nouvelle adresse IP et que la passerelle par défaut est correctement configurée.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Faites un &lt;em&gt;ping&lt;/em&gt; vers un autre PC du premier réseau et vers un autre PC du deuxième réseau pour vous assurer que le PC qui a obtenu l’adresse IP du serveur DHCP puisse encore communiquer avec les autres.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pensez à sauvegarder votre travail !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;etude-dun-échange-dhcp&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Etude d’un échange DHCP&lt;/h2&gt;
&lt;p&gt;Nous allons maintenant étudier plus en profondeur les échanges entre un PC et un serveur DHCP pour bien apprendre comment
le protocole DHCP marche.&lt;/p&gt;
&lt;p&gt;N’hésitez pas à revoir les transparents du cours pour avoir une idée générale des messages que vous devrez vous attendre à trouver.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Simulation&lt;/em&gt; et démarrez la simulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Selectionnez un autre PC du premier réseau (pas celui que vous avez configuré dans l’activité précédente )&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Activez le DHCP sur ce PC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Laissez dérouler la simulation. Vous la metterez en pause lorsque vous ne verrez plus d’échanges DHCP/ARP.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regardez le premier message DHCP dans le panneau de simulation.&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse MAC source ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse MAC destination ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse IP source ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse IP destination ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Regardez les destinataires des messages DHCP qui suivent immediatement le premier, est-ce que cela est cohérent avec les adresses MAC et IP destination que vous avez notées ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quel est le protocole de la couche transport utilisé dans ces messages DHCP ? Quels sont les ports source et destination ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant nous allons nous concentrer sur la réponse du serveur.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.3  &lt;/strong&gt;&lt;/span&gt;
Identifiez le premier message de réponse du DHCP dans le panneau de simulation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ignorez pour le moment les échanges ICMP et ARP.&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse MAC source ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse MAC destination ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse IP source ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse IP destination ? A quel dispositif cette adresse correspond-t-elle ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Ensuite on va voir ce qui se passe après la réponse du serveur.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quel dispositif réagit à la réponse du serveur ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle est l’adresse de destination de cette réaction ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Est-ce que le serveur répond à cette réaction ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que l’échange des messages que vous avez identifiés correspond bien aux messages décrits dans les supports de cours.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant on va se préoccuper des messages ICMP.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.5  &lt;/strong&gt;&lt;/span&gt;
A quoi sert selon vous le message ICMP (&lt;em&gt;ping&lt;/em&gt;) envoyé par le serveur ? Analysez le message pour essayer de comprendre.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Avant de continuer, pensez à terminer l’activité suivante.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Realtime&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Activez le DHCP sur tous les ordinateurs du premier réseau. Assurez-vous que chaque ordinateur a ainsi une adresse IP valide.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;serveur-dhcp-pour-le-deuxième-réseau&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Serveur DHCP pour le deuxième réseau&lt;/h2&gt;
&lt;p&gt;Maintenant on va ajouter la fonctionnalité DHCP sur le deuxième réseau.
Une solution possible consisterait à ajouter un deuxième serveur DHCP au deuxième réseau
et à le configurer avec une plage d’addresses pour le deuxième réseau.&lt;/p&gt;
&lt;p&gt;Cette solution étant inefficace (on ne va pas prévoir un serveur DHCP pour chaque réseau), on préfère ajouter une deuxième plage d’addresses sur notre serveur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ajoutez au serveur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; une deuxième plage d’addresses pour le deuxième réseau.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On essaie maintenant d’activer le DHCP sur l’un des ordinateurs du deuxième réseau.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.6  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Simulation&lt;/em&gt; et demarrez la simulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choisissez l’un des ordinateurs du deuxième réseau et activez le DCHP sur celui-ci.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observez le chemin pris par les messages DHCP.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Est-ce que les messages DHCP arrivent au serveur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;? Pourquoi ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;On vérifie la configuration réseau de l’ordinateur choisi.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.7  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur l’ordinateur choisi et ouvrez l’onglet &lt;em&gt;Desktop&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez un invite de commandes et tapez la commande &lt;em&gt;ipconfig&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quelle adresse IP cet ordinateur a-t-il reçu ? Pourriez-vous expliquer en quelques mots ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Est-ce que cet ordinateur a reçu l’adresse IP de la passerelle ? Cela vous surprend ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Pour résoudre les problèmes rencontrés dans les activités précédentes, on procède de la manière suivante :&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur le routeur &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt; et ouvrez l’onglet CLI. Cela vous permet d’accèder au terminal (CLI - Command Line Interface) de configuration du routeur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le terminal devrait afficher un invite &lt;em&gt;Router&amp;gt;&lt;/em&gt;. Si cela n’est pas le cas, appuyez sur la touche &lt;em&gt;Entrée&lt;/em&gt; de votre clavier.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Il faut maintenant saisir des commandes pour passer en mode configuration du routeur.
Les commandes à saisir sont, dans l’ordre (après chaque commande, appuyez sur la touche &lt;em&gt;Entrée&lt;/em&gt;):&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;enable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;configure terminal&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;La commande suivante sert à passer en mode configuration de l’interface du routeur reliée au deuxième réseau. &lt;br&gt; &lt;strong&gt;Important.&lt;/strong&gt; Dans la commande suivante il faut remplacer XXX par le nom de l’interface reliée au premier routeur.
Vous pouvez retrouver le nom de cette interface en passant la souris sur le cable reliant le routeur au deuxième réseau.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;interface XXX&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Ensuite, sasissez la commande suivante en remplaçant IPDHCP par l’adresse IP de votre serveur DHCP.
On vous demandera plus tard à quoi cette comment sert.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;ip helper-address IPDHCP&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;Pour sortir de la configuration saisissez la commande suivante :&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;end&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Simulation&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur l’ordinateur du deuxième réseau que vous aviez choisi avant, et ouvrez l’invite des commandes.
Cochez la case &lt;em&gt;Top&lt;/em&gt; que vous trouvez en bas à gauche de l’invite.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Décalez la fenêtre de l’invite de manière à pouvoir avoir une vision complète des deux réseaux.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saissez la commande &lt;code&gt;ipconfig /renew&lt;/code&gt; dans l’invite des commandes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Démarrez la simulation et observez attentivement le chemin des messages DHCP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quand la commande &lt;code&gt;ipconfig /renew&lt;/code&gt; se termine, elle affichera dans l’invite de commande la configuration réseau
obtenue du serveur DHCP. Si ce n’est pas le cas, il faut revoir la configuration de votre serveur DHCP et de votre routeur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Si la commande &lt;code&gt;ipconfig /renew&lt;/code&gt; se termine et que votre ordinateur a obtenu une configuration correcte,
vous pouvez mettre en pause la simulation. &lt;strong&gt;Ne passez pas en mode &lt;em&gt;Realtime&lt;/em&gt; et ne fermez pas l’invite de commandes, car sinon le panneau de simulation sera reinisialisé et
vous perdrez tous les messages&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Le moment est venu de mieux comprendre ce qui s’est passé.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-14&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.8  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Sur la base de ce que vous avez observé, quelle est utilité de la commande &lt;code&gt;ip helper-address&lt;/code&gt; que vous avez sasie
dans l’activité précédente ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pourriez-vous identifier le premier message DHCP (un DHCP discover) reçu par &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; ? Est-ce que le routeur &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt; l’a envoyé en broadcast ou
bien il a été envoyé directement au serveur DHCP ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Le service DHCP en exécution sur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (c’est à dire le logiciel qui est en charge de réaliser le service DHCP)
reçoit le message DHCP discover dépourvu des entêtes de la couche 2 et 3.
Comment ce logiciel arrive-t-il a déterminer qu’il a reçu ce message du routeur &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt; ?
&lt;strong&gt;Coup de pouce.&lt;/strong&gt; Regardez le contenu du message à la couche DHCP (application).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant il faut bien activer le DHCP sur tous les ordinateurs du deuxième réseau.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Realtime&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Activez le DHCP sur tous les ordinateurs du deuxième réseau.
Assurez-vous que chaque ordinateur a ainsi une adresse IP valide.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox curiosity&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pensez à sauvegarder votre travail !&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;serveur-http&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Serveur HTTP&lt;/h1&gt;
&lt;p&gt;Nous allons ajouter un serveur HTTP dans le deuxième réseau.&lt;/p&gt;
&lt;div class=&#34;infobox activitybox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activité&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ajoutez un nouveau serveur au deuxième réseau ; nous nommerons ce serveur &lt;em&gt;mywebserver&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reliez &lt;em&gt;mywebserver&lt;/em&gt; au commutateur du deuxième réseau.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;em&gt;mywebserver&lt;/em&gt; et ouvrez l’onglet &lt;em&gt;Services&lt;/em&gt;, puis selectionnez HTTP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vous devriez voir que le service HTTP est déjà activé. En plus, le serveur gère déjà un certain nombre de fichiers HTML (des pages Web).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Maintenant cliquez sur un ordinateur de votre réseau (l’ordinateur ne doit pas nécessairement être connecté au deuxième réseau) et ouvrez l’onglet &lt;em&gt;Desktop&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;em&gt;Web Browser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lorsque le navigateur s’ouvrira, saisissez l’adresse IP de &lt;em&gt;mywebserver&lt;/em&gt; dans le champs &lt;em&gt;URL&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur &lt;em&gt;Go&lt;/em&gt; ; vous devriez voir apparaître une page Web (c’est le contenu du fichier &lt;em&gt;index.html&lt;/em&gt; géré par &lt;em&gt;mywebserver&lt;/em&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant il faut étudier les échanges HTTP et les expliquer.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-15&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;
Passez en mode &lt;em&gt;Simulation&lt;/em&gt; et cliquez à nouveau sur &lt;em&gt;Go&lt;/em&gt;.
Expliquez le chemin des messages HTTP et leur contenu.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Lorsque nous naviguons sur le Web, nous ne sommes jamais amenés à rentrer des adresses IP dans le navigateur Web (fort heureusement !).
On va voir si cela peut être le cas ici.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-16&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Passez en mode &lt;em&gt;Realtime&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dans le champ URL du navigateur saisissez l’URL &lt;em&gt;mywebserver.com&lt;/em&gt; et cliquez sur &lt;em&gt;Go&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Qu’obtenez-vous ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Êtes-vous surpris ?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Le moment est venu de configurer un serveur DNS.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration-dun-serveur-dns&#34; class=&#34;section level1&#34; number=&#34;7&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Configuration d’un serveur DNS&lt;/h1&gt;
&lt;p&gt;Pour la configuration d’un serveur DNS, nous n’allons pas rajouter un nouvel ordinateur à notre réseau.
Rappelez-vous qu’un serveur DNS est un logiciel, et donc il n’est pas interdit de le faire tourner sur le même ordinateur sur lequel d’autres services sont
en exécution.&lt;/p&gt;
&lt;p&gt;Dans notre cas, nous allons activer le service DNS sur le serveur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, celui qui nous sert aussi de serveur DHCP.&lt;/p&gt;
&lt;p&gt;Vous devriez maintenant savoir comment configurer un service.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-17&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7.1  &lt;/strong&gt;&lt;/span&gt;
Activez le service DNS sur &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.
Ajoutez au DNS un enregistrement de ressource pour que le nom du domaine &lt;em&gt;mywebserver.com&lt;/em&gt; corresponde à l’adresse IP du serveur HTTP &lt;em&gt;mywebserver&lt;/em&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Quel type d’enregistrement de ressource allez-vous ajouter ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez un navigateur sur l’un des ordinateurs de votre réseau. Saisissez l’URL &lt;em&gt;mywebserver.com&lt;/em&gt;. Est-ce que vous arrivez à accèder à la page que vous avez vu tout à l’heure ?
Pourquoi ? Que faut-il faire pour résoudre ce problème ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Une fois le problème résolu, passez en mode &lt;em&gt;Simulation&lt;/em&gt; et saisissez à nouveau l’URL &lt;em&gt;mywebserver.com&lt;/em&gt;. Observez le chemin et le contenu des messages DNS et décrivez-les.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Normalization</title>
      <link>/courses/databases/tutorials/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/normalization/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- # Question 1

We consider a relational table $R$ with four columns $A$, $B$, $C$ and $D$.
Let $\mathcal{F}$ be the following set of functional dependencies:

1. $AB \rightarrow C$

2. $D \rightarrow BC$

3. $A \rightarrow B$

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;
Derive a minimal set $\mathcal{G}$ of functional dependencies that is equivalent to $\mathcal{F}$.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::

 --&gt;
&lt;div id=&#34;question-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 1&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; be a relational table with five columns &lt;span class=&#34;math inline&#34;&gt;\((A, B, C, D, E)\)&lt;/span&gt;.
The following set of functional dependencies hold:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(AB \rightarrow C\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(C \rightarrow D\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow E\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Find all the candidate keys of table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2  &lt;/strong&gt;&lt;/span&gt;
We assume that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 1NF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 2NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 3NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3  &lt;/strong&gt;&lt;/span&gt;
If the table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; from the previous exercise is not in 3NF,
what changes would you make to turn &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; into a 3NF table?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important.&lt;/strong&gt; For each table that you create, specify the primary key and the foreign keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;question-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 2&lt;/h1&gt;
&lt;p&gt;Each table has its own set of functional dependencies.
For the sake of simplicity, we should always identify a &lt;strong&gt;minimal set&lt;/strong&gt; of functional dependencies.&lt;/p&gt;
&lt;p&gt;A set of functional dependencies is &lt;strong&gt;minimal&lt;/strong&gt; if it complies with the following properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Every functional dependency in the set is in &lt;strong&gt;canonical form&lt;/strong&gt;: the right side consists of only one column.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Every functional dependency in the set is &lt;strong&gt;left-irreducible&lt;/strong&gt;: it is impossible to remove any of the columns on the left side without losing any information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Every functional dependency in the set is &lt;strong&gt;non-redundant&lt;/strong&gt;. We should not be able to derive one functional dependency in the set from another.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Formally, a functional dependency is &lt;strong&gt;redundant&lt;/strong&gt;
if we can derive it by successive applications of the &lt;strong&gt;Armstrong’s axioms&lt;/strong&gt; on the other functional dependencies.
Armstrong’s axioms are &lt;a href=&#34;https://en.wikipedia.org/wiki/Armstrong%27s_axioms&#34; target=&#34;_blank&#34;&gt;detailed here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4  &lt;/strong&gt;&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; be a relational table with four columns
&lt;span class=&#34;math inline&#34;&gt;\((A, B, C, D)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\{A, B\}\)&lt;/span&gt; is the only candidate key of &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Identify a minimal set of functional dependencies on &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5  &lt;/strong&gt;&lt;/span&gt;
Would the set obtained in the previous exercise still be minimal, if we added
the functional dependency &lt;span class=&#34;math inline&#34;&gt;\(B \rightarrow D\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer&lt;/strong&gt; and, in case of a negative answer, specify the minimal set of functional dependencies.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6  &lt;/strong&gt;&lt;/span&gt;
We assume that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is in 1NF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 2NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is table &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in 3NF? Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 7  &lt;/strong&gt;&lt;/span&gt;
Normalize &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; to the 3NF form.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important.&lt;/strong&gt; For each table that you create, specify the primary and foreign keys.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your choices.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;question-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question 3&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; be a relational table with five columns &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. The following set of functional dependencies is given:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A \rightarrow C\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B \rightarrow C\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D \rightarrow E\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 8  &lt;/strong&gt;&lt;/span&gt;
Is the given set of functional dependencies minimal?
If not, which functional dependencies must be removed to obtain a minimal set?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 9  &lt;/strong&gt;&lt;/span&gt;
Given the minimal set of functional dependencies obtained from the previous exercise,
determine the candidate keys of &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 10  &lt;/strong&gt;&lt;/span&gt;
Given the candidate keys that you found in the previous exercise, which columns are prime?
Which ones are non-prime?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 11  &lt;/strong&gt;&lt;/span&gt;
We assume that R is in 1NF. Which normal form is R in?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 12  &lt;/strong&gt;&lt;/span&gt;
How would you turn R into 3NF?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justify your answer.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Specify primary and foreign keys&lt;/strong&gt; of all tables that you create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Routage IPv4</title>
      <link>/courses/network/labs/routing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/network/labs/routing/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;L’objectif de cette activité est de configurer des &lt;strong&gt;tables de transfert&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Cette activité se compose de deux parties :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Première partie : configuration &lt;strong&gt;statique&lt;/strong&gt; des tables de transfert.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Deuxième partie : configuration &lt;strong&gt;dynamique&lt;/strong&gt; des tables de transfert.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!-- ::: {.infobox .warning data-latex=&#34;{warning}&#34;}
**Informations importantes**

Ce TP sera **évalué**. 

**Remise du devoir**  : il faut remettre **un seul fichier .zip** comprenant :

* Un rapport en PDF, complet d&#39;une page de garde avec vos noms et prénoms et la date. 
Ecrivez une réponse pour chaque question. Ne vous limitez pas à des phrases courtes et ambiguës. 
La qualité et l&#39;exhaustivité de vos réponses seront prises en compte dans la note finale.

* Un fichier ``static-routing.pkt`` contenant le réseau créé dans Cisco Packet Tracer avec la configuration statique des tables de transfert.

* Un fichier ``dynamic-routing.pkt`` contenant le réseau créé dans Cisco Packet Tracer avec la configuration dynamique des tables de transfert.


::: 
 --&gt;&lt;/p&gt;
&lt;div id=&#34;environnement-de-travail&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Environnement de travail&lt;/h1&gt;
&lt;p&gt;Pour cette activité nous allons utiliser Cisco Packet Tracer.
Le logiciel permet de configurer et simuler un réseau.
Il utilise des composants de la marque Cisco,
un important fabricant sur le marché des équipements réseau professionnels.
D’autre importants fabriquants sont : Huawei, HPE, Nokia, Netgear, Dell, Juniper, Aerohive.&lt;/p&gt;
&lt;p&gt;Pour cette activité, le réseau vous sera fourni sous forme d’un fichier géneré à l’aide de Cisco Packet Tracer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Téléchargez le réseau &lt;a href=&#34;/courses/network/routing.pkt&#34;&gt;à ce lien&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Faites un double click sur le fichier téléchargé pour ouvrir le réseau avec Cisco Packet Tracer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Certains cables sont rouges. Mais cela n’aura aucun impact sur l’activité.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;première-partie-routage-statique&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Première partie : routage statique&lt;/h1&gt;
&lt;p&gt;Dans un premier temps, nous allons configurer les tables de transfert des routers de manière &lt;strong&gt;statique&lt;/strong&gt;, c’est à dire manuellement.&lt;/p&gt;
&lt;p&gt;Observez attentivement le réseau et répondez aux questions suivantes.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Combien de sous-réseaux a-t-on dans ce réseau ? Pour chaque sous-réseau, précisez les interfaces réseau qui en font partie.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Restez en mode &lt;em&gt;Realtime&lt;/em&gt; et répondez aux questions suivantes.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Essayez de faire un &lt;em&gt;ping&lt;/em&gt; entre &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. Obtenez-vous une réponse de la part de &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; ?
En regardant la configuration réseau des ordinateurs &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, êtes-vous surpris par ce que vous venez d’observer ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Essayez de faire un &lt;em&gt;ping&lt;/em&gt; entre &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. Obtenez-vous une réponse de la part de &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; ?
En regardant la configuration réseau des ordinateurs &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; et &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, êtes-vous surpris par ce que vous venez d’observer ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contenu-de-la-tables-de-transfert&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Contenu de la tables de transfert&lt;/h2&gt;
&lt;p&gt;On va maintenant regarder le contenu de la table de transfert du routeur R1.&lt;/p&gt;
&lt;p&gt;Cliquez sur le routeur &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt;, sélectionnez l’onglet &lt;em&gt;CLI&lt;/em&gt; (IOS Command-Line Interface), tapez &lt;em&gt;Entrée&lt;/em&gt; sur votre clavier et ensuite
tapez la commande suivante :&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;show ip route&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
En vous aidant des informations que vous trouvez &lt;a href=&#34;https://www.ciscopress.com/articles/article.asp?p=2756479&amp;amp;seqNum=6&#34; target=&#34;_blank&#34;&gt;à cette page&lt;/a&gt;,
expliquez le contenu de la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Tout en restant en mode &lt;em&gt;Realtime&lt;/em&gt;, essayez de faire un &lt;em&gt;ping&lt;/em&gt; entre &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; et 10.1.5.2.
Il s’agit de l’adresse IP de l’interface &lt;em&gt;Gig0/0/0&lt;/em&gt; du routeur &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vous ne devriez obtenir aucune réponse. On va maintenant essayer de comprendre pourquoi.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pour répondre aux questions suivantes, passez en mode &lt;em&gt;Simulation&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;Est-ce que le message &lt;em&gt;Echo ping request&lt;/em&gt; arrive à destination ? Sur la base des informations présentes dans la table de trasfert de &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt;,
êtes-vous supris de ce que vous venez d’observer ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;Est-ce que le router &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; arrive à trouver une route pour répondre à &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; ? Justifiez votre réponse en vous aidant des informations présentes
dans la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;remplissage-des-tables-de-transfert&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Remplissage des tables de transfert&lt;/h2&gt;
&lt;p&gt;Maintenant nous allons remplir les tables de transfert pour que les ordinateurs et les routeurs de ce réseau puissent communiquer correctement.&lt;/p&gt;
&lt;p&gt;Nous commençons par le routeur &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Comment rajouter une route&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pour rajouter une route à une table de tranfert, il faut suivre la procédure suivante.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur le routeur et sélectionnez l’onglet &lt;em&gt;CLI&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assurez-vous que l’invite affiche &lt;em&gt;Router&amp;gt;&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;enable&lt;/code&gt;. L’invite devrait changer en &lt;em&gt;Router#&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;config terminal&lt;/code&gt;. L’invite devrait changer en &lt;em&gt;Router (config) #&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pour ajouter une route, il faut saisir la commande suivante :&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;code&gt;ip route PREFIX MASK NEXT-HOP&lt;/code&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;où:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PREFIX spécifie l’adresse IP du sous-réseau de destination.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MASK spécifie le masque du sous-réseau de destination.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NEXT-HOP spécifie l’adresse IP de l’interface réseau à laquelle un paquet doit être envoyé pour atteindre la destination.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pour afficher la table de transfert,
il faut saisir deux fois la commande &lt;code&gt;exit&lt;/code&gt; (pour quitter le mode configuration du routeur) et taper une fois la touche &lt;em&gt;Entrée&lt;/em&gt; pour que l’invite
affiche à nouveau &lt;em&gt;Router&amp;gt;&lt;/em&gt;. Seulement maintenant on peut saisir la commande que nous avons utilisée tout à l’heure :&lt;/p&gt;
&lt;center&gt;
&lt;code&gt;show ip route&lt;/code&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.7  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ajoutez une route à la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; de manière à ce que le routeur sache comment atteindre le sous-réseau de &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Affichez le contenu de la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.
Voyez-vous la nouvelle route que vous venez de rajouter ? Que signifie l’étiquette &lt;em&gt;S&lt;/em&gt; qui s’affiche dans la première colonne à côté de cette route ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Essayez de faire un &lt;em&gt;ping&lt;/em&gt; de &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; vers &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;. Maintenant &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; devrait être capable de répondre à &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Bon à savoir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Si le premier &lt;em&gt;ping&lt;/em&gt; échoue, ne désespérez pas !&lt;/p&gt;
&lt;p&gt;Réessayez la commande &lt;em&gt;ping&lt;/em&gt; une deuxième fois.
Cisco Packet Tracer ne gère pas bien les temps de réponse de la commande &lt;em&gt;ping&lt;/em&gt; lorsque celle-ci déclenche
des requêtes ARP pour obtenir les adresses MAC des appareils concernés.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Maintenant que vous avez compris le mécanisme, vous devez compléter les tables de tranfert des autres routeurs.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.8  &lt;/strong&gt;&lt;/span&gt;
Complétez les tables de transfert des autres routeurs afin que tous les appareils puissent communiquer correctement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que vous pouvez faire un &lt;em&gt;ping&lt;/em&gt; entre tous les ordinateurs du réseau.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sauvegardez votre travail et fermez Cisco Packet Tracer.
&lt;strong&gt;Renommez le fichier&lt;/strong&gt; sur lequel vous avez travaillé en &lt;em&gt;static-routing.pkt&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;routage-dynamique&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Routage dynamique&lt;/h1&gt;
&lt;p&gt;Dans cette section nous allons étudier comment remplir les tables de transfert de manière &lt;strong&gt;dynamique&lt;/strong&gt;, c’est à dire à l’aide
d’un &lt;strong&gt;algorithme de routage&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Téléchargez&lt;/strong&gt; à nouveau le réseau d’origine &lt;a href=&#34;/courses/network/routing.pkt&#34;&gt;à ce lien&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Renommez&lt;/strong&gt; le fichier en &lt;em&gt;dynamic-routing.pkt&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ouvrez le fichier avec Cisco Packet Tracer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;présentation-du-protocole-rip&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Présentation du protocole RIP&lt;/h2&gt;
&lt;p&gt;Dans cette section nous allons utiliser le protocole de routage dynamique &lt;strong&gt;RIP&lt;/strong&gt; (Routing Information Protocol).
Il s’agit d’un protocole de routage couramment utilisé dans les réseaux TCP/IP de petite et moyenne taille.
Il s’agit d’un protocole stable qui utilise un algorithme de vecteur de distance pour calculer les routes.&lt;/p&gt;
&lt;p&gt;Le protocole RIP (Routing Information Protocol)
envoie des paquets de données UDP en diffusion
pour échanger des informations de routage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration-de-rip&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Configuration de RIP&lt;/h2&gt;
&lt;p&gt;Pour que les routeurs s’échangent des routes via RIP, il faut activer RIP sur tous les routeurs.&lt;/p&gt;
&lt;p&gt;Nous commençons en l’activant sur le routeur R1 à titre d’exemple.&lt;/p&gt;
&lt;div class=&#34;infobox warning&#34;&gt;
&lt;p&gt;&lt;strong&gt;Activation de RIP&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cliquez sur le routeur et sélectionnez l’onglet &lt;em&gt;CLI&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;enable&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;config terminal&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Saisissez la commande &lt;code&gt;router rip&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Maintenant il faut saisir la commande &lt;code&gt;network&lt;/code&gt; pour chaque sous-réseau qui est attaché au routeur.
Dans le cas du routeur &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt; il faudra saisir :&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;network 192.168.1.0

network 192.168.2.0

network 10.1.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Saisissez maintenant la commande &lt;code&gt;exit&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Si vous regardez le contenu de la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; (le voisin de &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt;), elle ne devrait pas être changée.
En effet, il faut activer le RIP sur &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; pour que &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; reçoive les routes annoncées par &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
En utilisant les commandes détaillées ci-dessus, activez le RIP sur &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.
Attendez une trentaine de secondes et vérifiez que le contenu de la table de routage de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; contient bien
les routes annoncées par &lt;span class=&#34;math inline&#34;&gt;\(R1\)&lt;/span&gt; (les routes vers 192.168.1.0 et 192.168.2.0).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.2  &lt;/strong&gt;&lt;/span&gt;
Regardez plus attentivement les routes ajoutées dans la table de transfert de &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Que signifie l’étiquette &lt;em&gt;R&lt;/em&gt; qui apparaît dans la première colonne ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Que signifie l’information 120/X qui apparaît dans la troisième colonne ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.3  &lt;/strong&gt;&lt;/span&gt;
Activez le RIP sur tous les routeurs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ecrivez dans votre rapport le contenu de la table de transfert de chaque routeur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Vérifiez que chaque routeur connaisse les routes vers les différents sous-réseaux.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assurez-vous que tous les ordinateurs arrivent à communiquer à l’aide de la commande &lt;em&gt;ping&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Passez maintenant en mode &lt;em&gt;Simulation&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-12&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.4  &lt;/strong&gt;&lt;/span&gt;
Faites un &lt;em&gt;ping&lt;/em&gt; de la machine &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; vers la machine &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;.
Laissez dérouler la simulation jusqu’à ce que vous ne voyez plus de messages ICMP passer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Est-ce que les messages ICMP suivent toujours le même chemin ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Est-ce que vous êtes surpris par ce que vous venez d’observer ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Passez un mode &lt;em&gt;Realtime&lt;/em&gt; et supprimez le lien entre le routeur &lt;span class=&#34;math inline&#34;&gt;\(R2\)&lt;/span&gt; et le routeur &lt;span class=&#34;math inline&#34;&gt;\(R4\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-13&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.5  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Comment les tables de tranfert de chaque routeur ont-elles changé ?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Est-ce que les ordinateurs arrivent toujours à communiquer correctement ?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Running Spark programs on a cluster</title>
      <link>/courses/bdia/tutorials/spark-programming-dce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia/tutorials/spark-programming-dce/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ --&gt;
&lt;!-- The group to which users in the cluster belong--&gt;
&lt;!-- User accounts are numbered from the lower to the upper limit--&gt;
&lt;!-- ############ END OF MODIFICATIONS ############ --&gt;
&lt;div id=&#34;computing-averages&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Computing averages&lt;/h1&gt;
&lt;p&gt;We consider a collection of CSV files containing temperature measurements in the following format:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;year,month,day,hours,minutes,seconds,temperature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;you can find the files under the directory &lt;code&gt;hdfs://sar01:9000/data/temperatures/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here are the details for each file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86400.csv&lt;/code&gt; contains one measurement per day in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_2880.csv&lt;/code&gt; contains one measurement every 2880 seconds in the years 1980 - 2018.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_86.csv&lt;/code&gt; contains one measurement every 86 seconds for the year 1980 alone.&lt;/li&gt;
&lt;li&gt;File &lt;code&gt;temperatures_10.csv&lt;/code&gt; contains one measurement every 10 seconds for the years 1980 - 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We intend to implement a Spark algorithm to generate pairs &lt;span class=&#34;math inline&#34;&gt;\((y, t_{avg})\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the year and &lt;span class=&#34;math inline&#34;&gt;\(t_{avg}\)&lt;/span&gt; is the average temperature in the year.&lt;/p&gt;
&lt;div id=&#34;first-implementation&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; First implementation&lt;/h2&gt;
&lt;p&gt;Copy the file &lt;code&gt;~cpu_vialle/DCE-Spark/template_temperatures.py&lt;/code&gt;
to your home directory by typing
the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Open the file &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt;.
The file contains the implementation of the function &lt;code&gt;avg_temperature_slow&lt;/code&gt;
that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;takes in an RDD, where each item is a line of the input text file;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;returns an RDD, where each item is a key-value pair &lt;code&gt;(year, avg_temp)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the same file, locate the two variables &lt;code&gt;input_path&lt;/code&gt; and &lt;code&gt;output-path&lt;/code&gt;
and write the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input_path = &amp;quot;hdfs://sar01:9000/data/temperatures/&amp;quot;
output_path = &amp;quot;hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace &lt;code&gt;X&lt;/code&gt; with the number corresponding to your account!
&lt;strong&gt;Don’t forget the / at the end of the file paths!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Execute the following actions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the script &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; by using &lt;code&gt;temperatures_86400.csv&lt;/code&gt; as an input.
To this extent, use the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;You should find the output of the program under the following folder:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Type the following command to verify it :&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -ls hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/temperatures_86400.out&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;If you want to read the result of the computation, you can execute the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;hdfs dfs -cat hdfs://sar01:9000/bdiaspark2024/bdiaspark2024_X/temperatures_86400.out/*&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the output of Spark on the command line you should see a line that reads something similar to the following phrase:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
   &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note the execution time that you obtained.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_2880.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? Does it seem reasonable compared with the execution time that you observed before?
Justify your answer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the same script by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the execution time? How would you justify it, knowing that
the files &lt;code&gt;temperatures_2880.csv&lt;/code&gt; and &lt;code&gt;temperatures_86.csv&lt;/code&gt; have a similar size (11 MB the former, 9 MB the latter)?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;second-implementation&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Second implementation&lt;/h2&gt;
&lt;p&gt;We want to implement a better version of the program.
You can draft your ideas on paper before you write any code.&lt;/p&gt;
&lt;p&gt;When you’re ready, create a copy of &lt;code&gt;avg_temperatures_slow.py&lt;/code&gt; and rename it as &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt;, with the following command:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ./avg_temperatures_slow.py ./avg_temperatures_fast.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.2  &lt;/strong&gt;&lt;/span&gt;
Open the file and implement the function &lt;code&gt;avg_temperature_fast&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE.&lt;/strong&gt; Remember to comment the call
to &lt;code&gt;avg_temperature_slow&lt;/code&gt; and to uncomment the call to &lt;code&gt;avg_temperature_fast&lt;/code&gt; at the end of the file.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-3&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run the script &lt;code&gt;avg_temperatures_fast.py&lt;/code&gt; by using &lt;code&gt;temperatures_86.csv&lt;/code&gt; as an input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What’s the execution time? Compare it with the execution time obtained in the previous exercise and
comment the difference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the same script by using &lt;code&gt;temperatures_10.csv&lt;/code&gt; (3 GB!) as an input.
Do you think that the program takes too long? Why?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-friends-in-a-social-network&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Common friends in a social network&lt;/h1&gt;
&lt;p&gt;Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line &lt;span class=&#34;math inline&#34;&gt;\(A,B,C,D\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is friend with &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.
An excerpt of the file looks like as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;B,A,D
A,B,C,D
D,A,B,C
C,A,D
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppose that the friendship relation is symmetric: &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We want to obtain the list of the common friends for each pair of individuals&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(B, C), [A, D] 
(A, D), [B, C] 
(C, D), [A]
(A, C), [D] 
(B, D), [A] 
(A, B), [D]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an additional constraint, we want to represent a couple only once and avoid
to represent the symmetric couple.
In other words, if we output &lt;span class=&#34;math inline&#34;&gt;\((A, B)\)&lt;/span&gt;, we don’t want to output &lt;span class=&#34;math inline&#34;&gt;\((B, A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We use the following input files available in folder &lt;code&gt;hdfs://sar01:9000/data/social-network/&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_tiny.csv&lt;/code&gt;. Small social network, that you can use to test your implementation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_10k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^4\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_100k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1k_100k.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^3\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^5\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;sn_1m_1m.csv&lt;/code&gt;. Social network with &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; individuals and &lt;span class=&#34;math inline&#34;&gt;\(10^6\)&lt;/span&gt; links.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Implementation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Get the code template with the following command:&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_common_friends.py .&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Write an implementation that uses a &lt;code&gt;groupByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write an implementation that uses a &lt;code&gt;reduceByKey&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Test both implementations on file &lt;code&gt;sn_tiny.csv&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-and-performance-measures&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Tests and performance measures&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run both implementations on the other files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fill in a table where you indicate: the name and size of each file and
the measured running times of both implementations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-maximum-and-average-degree&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Minimum, maximum and average degree&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add a function to file &lt;code&gt;template_common_friends.py&lt;/code&gt; that returns a tuple containing the minimum, the maximum and the average degree of
a node in the social network. You must use RDDs to do so, &lt;strong&gt;don’t try to &lt;code&gt;collect()&lt;/code&gt; the
content of the RDDs and so compute the values on Python lists&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the function for all the given input files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete the table that you created in the previous exercise by adding the
minimum, maximum and average number of friends.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-analysis&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Performance analysis&lt;/h2&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We suppose that each node has a number of friends that is equal to the
average number of friends.
Compute (with pencil or paper, no need to write a code for that) the number of
intermediate pairs &lt;span class=&#34;math inline&#34;&gt;\(((A, B), X)\)&lt;/span&gt; generated by your code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete the table by writing down the number of intermediate pairs for each file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot three graphs, where the y-axis has the program running times and the x-axis
has: the number of intermediate pairs, the average degree and the file size respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which graphs best predict the evolution of the computational time?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;average-and-standard-deviation&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Average and standard deviation&lt;/h1&gt;
&lt;p&gt;We use the same files as in the first question.
Our objective is to write a Spark program that produces
triples &lt;span class=&#34;math inline&#34;&gt;\((y, t_{\mu}, t_{\sigma})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_{\mu}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(t_{\sigma}\)&lt;/span&gt; are the year, the average temperature in the year and the
standard deviation respectively.&lt;/p&gt;
&lt;p&gt;We can express the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(x_1 \ldots x_n\)&lt;/span&gt; with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Type the following command to get a code template:&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;code&gt;cp ~cpu_vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Define a new function &lt;code&gt;avg_stddev_temperature&lt;/code&gt; in file &lt;code&gt;avg_stddev_temp.py&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Execute the script and observe the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Spark programming</title>
      <link>/courses/bdia_old/tutorials/spark-programming-dce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdia_old/tutorials/spark-programming-dce/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/courses/plp/overview/cluster-connection&#34; target=&#34;_blank&#34;&gt;Refer to this documentation&lt;/a&gt; to learn how to connect and
interact with the cluster.&lt;/p&gt;
&lt;!--

# Computing averages

We consider a collection of  CSV files containing temperature measurements in the following format:

``year,month,day,hours,minutes,seconds,temperature`` 

you can find the files under the directory ``hdfs://sar01:9000/data/temperatures/``

Here are the details for each file:

* File ``temperatures_86400.csv`` contains one measurement per day in the years 1980 - 2018. 
* File ``temperatures_2880.csv`` contains one measurement every 2880 seconds in the years 1980 - 2018. 
* File ``temperatures_86.csv`` contains one measurement every 86 seconds for the year 1980 alone. 
* File ``temperatures_10.csv`` contains one measurement every 10 seconds for the years 1980 - 2018. 

We intend to implement a Spark algorithm to generate pairs $(y, t_{avg})$, where 
$y$ is the year and $t_{avg}$ is the average temperature in the year.

## First implementation

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` 
to your home directory by typing 
the following command:

``
cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py
``

Open the file ``avg_temperatures_first.py`` 
and write the following function:

```
def avg_temperature(theTextFile): 
    temperatures = theTextFile \ 
                        .map(lambda line: line.split(&#34;,&#34;)) \ 
                        .map(lambda term: (term[0],   [float(term[6])])) \ 
                        .reduceByKey(lambda x, y: x+y) \ 
                        .mapValues(lambda lv: sum(lv)/len(lv)) 
    return temperatures 
```

In the same file, locate the  two variables ``input_path`` and ``output-path``.
and write the following code:

```
input_path = &#34;hdfs://sar01:9000/data/temperatures/&#34;
output_path = &#34;hdfs://sar01:9000/cpupsmia1/your_username/&#34;
```

Always remember to replace ``your_username`` with your username!
**Don&#39;t forget the / at the end of the file paths!**.


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-1&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-1) &lt;/strong&gt;&lt;/span&gt;

* Run the script ``avg_temperatures_first.py`` by using ``temperatures_86400.csv`` as an input.
To this extent, use the following command:

    &lt;div align=&#34;left&#34;&gt;
    ``
    spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv
    ``
    &lt;/div&gt;

    You should find the output of the program under the folder ``hdfs://sar01:9000/cpupsmia1/your_username/temperatures_86400.out``


* What&#39;s the execution time? 
   * In the output of Spark on the command line you should see a line that mentions something along the following line:
       
       ``
       INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
       ``

* Run the same script by using ``temperatures_2880.csv`` as an input.

* What is the execution time? Does it seem reasonable compared with the execution time that you observed before?
Justify your answer.

* Execute the same script by using ``temperatures_86.csv`` as an input.

* What is the execution time? How would you justify it, knowing that 
the files  ``temperatures_2880.csv`` and ``temperatures_86.csv`` have a similar size (11 MB the former, 9 MB the latter)?
&lt;/div&gt;\EndKnitrBlock{exercise}

:::





## Second implementation

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing 
the following command:

&lt;div align=&#34;left&#34;&gt;
``
cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py
``
&lt;/div&gt;

::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-2&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-2) &lt;/strong&gt;&lt;/span&gt;
Based on the observations made in the 
previous exercise, write an improved implementation of the 
function ``avg_temperature``.
&lt;/div&gt;\EndKnitrBlock{exercise}

:::



::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;

* Run the script ``avg_temperatures_second.py`` by using ``temperatures_86.csv`` as an input.

* What&#39;s the execution time? Compare it with the execution time obtained in the previous exercise and 
comment the difference.
   
* Run the same script by using ``temperatures_10.csv`` (3 GB!) as an input. 
Do you think that the program takes too long? Why?

&lt;/div&gt;\EndKnitrBlock{exercise}

:::



# Average and standard deviation

We use the same files as in the first question.
Our objective is to write a Spark program that produces 
triples $(y, t_{\mu}, t_{\sigma})$, where $y$, $t_{\mu}$ and 
$t_{\sigma}$ are the year, the average temperature in the year and the 
standard deviation respectively.

We can express the standard deviation of $n$ values $x_1 \ldots x_n$ with the following formula:

$$
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
$$

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing 
the following command:

``
cp ~vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py
``


::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**


\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-4&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-4) &lt;/strong&gt;&lt;/span&gt;

* Complete the definition of the function ``avg_temperature`` in file ``avg_stddev_temp.py``.
   
* Run the script by using  ``temperatures_86400.csv``  and ``temperatures_2880.csv`` 
as input files (small files).

* Run the script by using  ``temperatures_86.csv``  and ``temperatures_10.csv`` as input files (large files).

&lt;/div&gt;\EndKnitrBlock{exercise}

:::



--&gt;
</description>
    </item>
    
    <item>
      <title>Advanced SQL queries</title>
      <link>/courses/bdalbert/tutorials/sql-advanced/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdalbert/tutorials/sql-advanced/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Which last names are &lt;strong&gt;not&lt;/strong&gt; repeated in table &lt;em&gt;actor&lt;/em&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is a copy of the movie ACADEMY DINOSAUR available for rent from store 1?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the title and the release year of all films in one of the following categories:
&lt;em&gt;Family&lt;/em&gt;, &lt;em&gt;Animation&lt;/em&gt;, &lt;em&gt;Documentary&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_in.asp&#34; target=&#34;_blank&#34;&gt;operator IN&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find all customers (id, last name, first name) whose last name starts with the letter L.
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_like.asp&#34; target=&#34;_blank&#34;&gt;operator LIKE&lt;/a&gt;
&lt;/details&gt;&lt;/li&gt;
&lt;li&gt;Return the total paid by each customer. For each customer, display a single
column containing first and last name and another column with the total amount paid.
Order the result alphabetically by last name&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/func_sqlserver_concat.asp&#34; target=&#34;_blank&#34;&gt;operator CONCAT&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the total revenue from the rentals across the stores in each country.
Order by descending revenue.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;The first and last name of the actor that played in the most films.
If two or more actors are tied, the query must return the names of all of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select last_name
from actor
group by last_name
having count(*) = 1
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select distinct i.inventory_id
from film f join inventory i using(film_id)
join rental r using(inventory_id)
where f.title=&#39;ACADEMY DINOSAUR&#39; 
    and i.store_id=1 
    and r.return_date is not null  
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select  distinct f.title, f.release_year
from film f join film_category using(film_id)
join category cat using(category_id)
where cat.name in (&#39;Family&#39;, &#39;Animation&#39;, &#39;Documentary&#39;)
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select customer_id, first_name, last_name
from customer
where last_name LIKE &#39;L%&#39;
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select concat(first_name, &#39; &#39;, last_name), sum(amount)
from customer join payment using (customer_id)
group by customer_id
order by last_name asc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select country, sum(amount) as revenue
from payment join rental using (rental_id)
  join inventory using (inventory_id)
  join store using (store_id)
  join address using (address_id)
  join city using (city_id)
  join country using (country_id)
group by country_id
order by revenue desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;select actor_id, first_name, last_name, count(&lt;em&gt;)
from film_actor join actor using(actor_id)
group by actor_id
having count(&lt;/em&gt;) =
(select max(nb_films)
from (select count(*) as nb_films
from film_actor
group by actor_id) t)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to SQL</title>
      <link>/courses/bdalbert/tutorials/sql-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/bdalbert/tutorials/sql-intro/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key integrity constraints in a relational database.&lt;/li&gt;
&lt;li&gt;Basic SQL queries.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;description-of-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Description of the data&lt;/h1&gt;
&lt;p&gt;We consider the database of a DVD rental store chain containing data on films, actors,
customers and the transactions of the store.&lt;/p&gt;
&lt;p&gt;This database goes by the name &lt;strong&gt;Sakila&lt;/strong&gt; and was developed by Mike Hillyer, a former member of the MySQL team.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The conceptual schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: The conceptual schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The tables of the database are &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-structure-tables.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;documented on this page&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this lab, we’ll use SQLite as a relational DBMS, due to the unavailability of the PostgreSQL servers.
SQLite is a simple relational DBMS; data is stored in just one file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download and install &lt;a href=&#34;https://sqlitebrowser.org/dl/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;DB Browser for SQLite&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;/courses/databases/tutorial-4/sakila_sqlite.db&#34;&gt;&lt;strong&gt;database file here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open DB Browser for SQLite and select “Open Database”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the file that you’ve just downloaded. In the “Database Structure” tab you should see
the tables of the database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on the tab “Execute SQL” to access a text area where you can type SQL queries.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;integrity-constraints&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Integrity constraints&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
insert into film_actor (actor_id, film_id) values(1, 25)
&lt;/pre&gt;
&lt;p&gt;What is this statement supposed to do?
What is the reaction of the DBMS?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;This statement should insert a new row in table &lt;TT&gt;film_actor&lt;/TT&gt;,
where the value of &lt;TT&gt;actor_id&lt;/TT&gt; is 1 and the value of
&lt;TT&gt;film_id&lt;/TT&gt; is 25.&lt;/p&gt;
&lt;p&gt;The DBMS returns an error because there is already a row
with these values and the two columns
&lt;TT&gt;film_actor&lt;/TT&gt;, &lt;TT&gt;film_id&lt;/TT&gt; form the primary key.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Write the statement to delete the film with
&lt;em&gt;film_id&lt;/em&gt;=1 from
the table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;Execute the command. What happens?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;PRE&gt;
delete from film where film_id=1
&lt;/PRE&gt;
&lt;p&gt;The statement is rejected because there are rows in other tables
that reference the row that we intend to delete.
This is the effect of the foreign key constraint.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;!--
::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;
Look at the definition of the foreign key constraints
in table &lt;TT&gt;film_actor&lt;/TT&gt; (right-click on the table, 
select *Constraints*, *foreign key*).

Is the definition of the foreign key constraint 
to table &lt;TT&gt;film&lt;/TT&gt; coherent with the behavior observed
in the previous exercise?

**NB** In order to see the options of a foreign key, click on the 
edit button on the left of the constraint. Then look at the 
tab *Action*.&#34;

&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Look at the definition of the foreign key constraints
in table &lt;TT&gt;film_actor&lt;/TT&gt; (see in the Database structure tab).&lt;/p&gt;
&lt;p&gt;Is the definition of the foreign key constraint
to table &lt;TT&gt;film&lt;/TT&gt; coherent with the behavior observed
in the previous exercise?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The foreign key linking table &lt;TT&gt;film_actor&lt;/TT&gt; to table
&lt;TT&gt;film&lt;/TT&gt; is defined with the option RESTRICT on delete.
This is coherent with the behavior that we observed
in the previous exercise. Referenced rows cannot be deleted if
there are still referencing rows.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Execute the following query:&lt;/p&gt;
&lt;pre&gt;
SELECT f.film_id as film_id_in_table_film, 
       fa.film_id AS film_id_in_table_film_actor, 
       fa.actor_id as actor_id, f.title as title
FROM film f  JOIN film_actor fa  USING (film_id)
WHERE  f.title=&#39;ACE GOLDFINGER&#39;
&lt;/pre&gt;
&lt;p&gt;What does the query? Note down the identifier of the film in both tables
&lt;TT&gt;film&lt;/TT&gt; and &lt;TT&gt;film_actor&lt;/TT&gt;.
(columns &lt;TT&gt;film_id_in_table_film&lt;/TT&gt; and &lt;TT&gt;film_id_in_table_film_actor&lt;/TT&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The query returns the list of all actors in the film titled Ace Goldfinger.
We note that the identifier of the film in both tables is identical (2), as it should
because the query joins the two tables on the equality of these two values.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;
Write and execute a statement to set the value 10000 to the identifier of the
film ACE GOLDFINGER in table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;After the modification, execute the query of the previous exercise.
What changed? Explain in details what happened.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The statement to modify the &lt;TT&gt;film_id&lt;/TT&gt; of the given film is as follows:&lt;/p&gt;
&lt;p&gt;&lt;TT&gt;
UPDATE film
SET film_id=10000
WHERE title=‘ACE GOLDFINGER’
&lt;/TT&gt;&lt;/p&gt;
&lt;p&gt;After executing the same query as the previous exercise, we see that
the identifier of the film has changed in the table &lt;TT&gt;film_actor&lt;/TT&gt; too.
This is expected, because the foreign key constraint between the
colum &lt;TT&gt;film_id&lt;/TT&gt; in table &lt;TT&gt;film_actor&lt;/TT&gt; and the column
&lt;TT&gt;film_id&lt;/TT&gt; in table &lt;TT&gt;film&lt;/TT&gt; has the option ON UPDATE CASCADE.
This means that if we modify the identifier of the film in table
&lt;TT&gt;film&lt;/TT&gt;, the modification is propagated to all the referencing columns.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
UPDATE film_actor
SET film_id=2
WHERE film_id=10000
&lt;/pre&gt;
&lt;p&gt;What does it? What happens? Explain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;p&gt;The statement intends to set the identifier of the film
titled Ace Goldfinger (in the previous exercise we gave it the identifier 10000)
back to its original value.
However, we execute the statement on the table &lt;TT&gt;film_actor&lt;/TT&gt;.
The action is not allowed, as the identifier 2 does not correspond to
any film in table &lt;TT&gt;film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;The foreign key enforces the referential integrity constraint.
A row cannot refer to a non-existing entity in the referenced table.&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the first and last name of all customers of the store with identifier 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select * 
from customer
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-and-paginating&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Sorting and paginating&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the last and first name of all customers. Sort by last name in ascending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 1., but only return the first 100 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the last and first name of all customers of the store with identifier 1.
Sort by last name in ascending order and by first name in descending order.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select last_name, first_name 
from customer
order by last_name asc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select last_name, first_name 
from customer
order by last_name asc, first_name DESC
limit 100
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name 
from customer
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregating-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Aggregating queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Count the number of films in the database (expected result: 1000).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How many distinct actor last names are there?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the total amount of payments across all rentals (expected result: 67416.51).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the average, minimum and maximum duration of rental across all films
(expected result: 4.9850000000000000, 3, 7).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of copies of each film in each store (table &lt;TT&gt;inventory&lt;/TT&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as 6., but only returns the pairs (film, store) if the number of copies is
greater than or equal to 3.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select count(*) 
from film
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select  count (distinct last_name) 
from actor
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select sum(amount) 
from payment
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select avg(rental_duration), min(rental_duration), max(rental_duration) 
from film
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, count(*) as nb_actors
from film_actor
group by film_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, store_id, count(*) as nb_films
from inventory
group by film_id, store_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select film_id, store_id, count(*) as nb_films
from inventory
group by film_id, store_id
having count(*) &gt;=3
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;join-queries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Join queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the actors in the film ACE GOLDFINGER.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return first and last name of each actor and the number of films in which they played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 3., but order by number of films in descending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 4., but only return actors who played a role in at least 10 films.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Return the identifier, the first and family name of the customers who have rented between 5 and 10
movies in the category &lt;em&gt;Family&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Solution&lt;/summary&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exosolution&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name
from staff join store using(store_id)
where store_id=1
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name
from film join film_actor using(film_id) join actor using(actor_id)
where title=&#39;ACE GOLDFINGER&#39;
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
order by nb_films desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select first_name, last_name, count(*) as nb_films
from actor join film_actor using(actor_id)
group by actor_id
having count(*) &gt;= 10
order by nb_films desc
&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;pre&gt;
select cust.customer_id, first_name, last_name, count(*) as nb_films
from customer cust join rental using(customer_id) 
  join inventory using(inventory_id)
    join film_category using(film_id)
    join category cat using(category_id)
where cat.name=&#39;Family&#39;
group by customer_id
having count(*) between 5 and 10
&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning SQL queries</title>
      <link>/courses/databases/tutorials/sql-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/sql-intro/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you’ll learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key integrity constraints in a relational database.&lt;/li&gt;
&lt;li&gt;Basic SQL queries.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;description-of-the-data&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Description of the data&lt;/h1&gt;
&lt;p&gt;We consider the database of a DVD rental store chain containing data on films, actors,
customers and the transactions of the store.&lt;/p&gt;
&lt;p&gt;This database goes by the name &lt;strong&gt;Sakila&lt;/strong&gt; and was developed by Mike Hillyer, a former member of the MySQL team.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:cnm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/courses/databases/tutorial-4/sakila-3nf.png&#34; alt=&#34;The conceptual schema of the Sakila database&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: The conceptual schema of the Sakila database
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The tables of the database are &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-structure-tables.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;documented on this page&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sakila has been ported from MySQL to PostgreSQL under the name &lt;strong&gt;pagila&lt;/strong&gt;.
In pgAdmin, &lt;strong&gt;Open a new query tool&lt;/strong&gt; to do the exercises.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integrity-constraints&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Integrity constraints&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.1  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
insert into film_actor (actor_id, film_id) values(1, 25)
&lt;/pre&gt;
&lt;p&gt;What is this statement supposed to do?
What is the reaction of the DBMS?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-2&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.2  &lt;/strong&gt;&lt;/span&gt;
Write the statement to delete the film with
&lt;em&gt;film_id&lt;/em&gt;=1 from
the table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;Execute the command. What happens?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!--
::: {.infobox .exercisebox data-latex=&#34;{exercisebox}&#34;}
**Exercise**

\BeginKnitrBlock{exercise}&lt;div class=&#34;exercise&#34;&gt;&lt;span class=&#34;exercise&#34; id=&#34;exr:unnamed-chunk-3&#34;&gt;&lt;strong&gt;(\#exr:unnamed-chunk-3) &lt;/strong&gt;&lt;/span&gt;
Look at the definition of the foreign key constraints
in table &lt;TT&gt;film_actor&lt;/TT&gt; (right-click on the table, 
select *Constraints*, *foreign key*).

Is the definition of the foreign key constraint 
to table &lt;TT&gt;film&lt;/TT&gt; coherent with the behavior observed
in the previous exercise?

**NB** In order to see the options of a foreign key, click on the 
edit button on the left of the constraint. Then look at the 
tab *Action*.&#34;

&lt;/div&gt;\EndKnitrBlock{exercise}

:::
--&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-4&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.3  &lt;/strong&gt;&lt;/span&gt;
Look at the definition of the foreign key constraints
in table &lt;TT&gt;film_actor&lt;/TT&gt; (see in the Database structure tab).&lt;/p&gt;
&lt;p&gt;Is the definition of the foreign key constraint
to table &lt;TT&gt;film&lt;/TT&gt; coherent with the behavior observed
in the previous exercise?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-5&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.4  &lt;/strong&gt;&lt;/span&gt;
Execute the following query:&lt;/p&gt;
&lt;pre&gt;
SELECT f.film_id as film_id_in_table_film, 
       fa.film_id AS film_id_in_table_film_actor, 
       fa.actor_id as actor_id, f.title as title
FROM film f  JOIN film_actor fa  USING (film_id)
WHERE  f.title=&#39;ACE GOLDFINGER&#39;
&lt;/pre&gt;
&lt;p&gt;What does the query? Note down the identifier of the film in both tables
&lt;TT&gt;film&lt;/TT&gt; and &lt;TT&gt;film_actor&lt;/TT&gt;.
(columns &lt;TT&gt;film_id_in_table_film&lt;/TT&gt; and &lt;TT&gt;film_id_in_table_film_actor&lt;/TT&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-6&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.5  &lt;/strong&gt;&lt;/span&gt;
Write and execute a statement to set the value 10000 to the identifier of the
film ACE GOLDFINGER in table &lt;TT&gt;Film&lt;/TT&gt;.&lt;/p&gt;
&lt;p&gt;After the modification, execute the query of the previous exercise.
What changed? Explain in details what happened.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-7&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 2.6  &lt;/strong&gt;&lt;/span&gt;
Execute the following statement:&lt;/p&gt;
&lt;pre&gt;
UPDATE film_actor
SET film_id=2
WHERE film_id=10000
&lt;/pre&gt;
&lt;p&gt;What does it? What happens? Explain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-queries&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basic queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-8&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 3.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return all the information on all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of all customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of all customers of the store with identifier 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-and-paginating&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Sorting and paginating&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-9&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 4.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the last and first name of all customers. Sort by last name in ascending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 1., but only return the first 100 customers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the last and first name of all customers of the store with identifier 1.
Sort by last name in ascending order and by first name in descending order.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregating-queries&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Aggregating queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-10&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 5.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Count the number of films in the database (expected result: 1000).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How many distinct actor last names are there?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the total amount of payments across all rentals (expected result: 67416.51).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the average, minimum and maximum duration of rental across all films
(expected result: 4.9850000000000000, 3, 7).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of actors for each film.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the number of copies of each film in each store (table &lt;TT&gt;inventory&lt;/TT&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as 6., but only returns the pairs (film, store) if the number of copies is
greater than or equal to 3.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;join-queries&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Join queries&lt;/h1&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-11&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 6.1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the first and last name of the actors in the film ACE GOLDFINGER.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return first and last name of each actor and the number of films in which they played a role.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 3., but order by number of films in descending order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same as in 4., but only return actors who played a role in at least 10 films.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the identifier, the first and family name of the customers who have rented between 5 and 10
movies in the category &lt;em&gt;Family&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning SQL queries</title>
      <link>/courses/databases/tutorials/sql-advanced/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/courses/databases/tutorials/sql-advanced/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/course.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/styles/cloud-computing.css&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;infobox exercisebox&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&#34;exercise&#34;&gt;
&lt;p&gt;&lt;span id=&#34;exr:unnamed-chunk-1&#34; class=&#34;exercise&#34;&gt;&lt;strong&gt;Exercise 1  &lt;/strong&gt;&lt;/span&gt;
Write the following SQL queries:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Which last names are &lt;strong&gt;not&lt;/strong&gt; repeated in table &lt;em&gt;actor&lt;/em&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is a copy of the movie ACADEMY DINOSAUR available for rent from store 1?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the title and the release year of all films in one of the following categories:
&lt;em&gt;Family&lt;/em&gt;, &lt;em&gt;Animation&lt;/em&gt;, &lt;em&gt;Documentary&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_in.asp&#34; target=&#34;_blank&#34;&gt;operator IN&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find all customers (id, last name, first name) whose last name starts with the letter L.
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/sql_like.asp&#34; target=&#34;_blank&#34;&gt;operator LIKE&lt;/a&gt;
&lt;/details&gt;&lt;/li&gt;
&lt;li&gt;Return the total paid by each customer. For each customer, display a single
column containing first and last name and another column with the total amount paid.
Order the result alphabetically by last name&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Tip&lt;/summary&gt;
You can use the &lt;a href=&#34;https://www.w3schools.com/sql/func_sqlserver_concat.asp&#34; target=&#34;_blank&#34;&gt;operator CONCAT&lt;/a&gt;
&lt;/details&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Return the total revenue from the rentals across the stores in each country.
Order by descending revenue.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;The first and last name of the actor that played in the most films.
If two or more actors are tied, the query must return the names of all of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
