---
title: "Spark structured streaming"
heading: Platforms and languages â€” Lab assignment 3
linktitle: spark-sstream-lab
output:
  bookdown::pdf_book:
    toc: false
    includes:
      in_header: ../latex-files/preamble.tex
    number_sections: no
  blogdown::html_page:
    number_sections: no
  html_document:
    df_print: paged
menu:
  plp:
    name: Lab 3. Spark structured streaming
    parent: tutorials
    identifier: spark-sstream-lab
    weight: 7
params:
  solution: true
summary: Description of the lab structured streaming.
draft: no
type: docs
---

<link rel="stylesheet" href="/styles/course.css">
<link rel="stylesheet" href="/styles/cloud-computing.css">

<!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ -->

<!-- The link to the Edunao page where the students submit their work -->
`r submission_page <- "https://centralesupelec.edunao.com/mod/quiz/view.php?id=48267"`

<!-- The submission deadline -->
`r submission_date <- "Tuesday, March 30, 2021 8:00 AM"`


<!-- The group to which users in the cluster belong-->
`r user_group <- "cpuasi1"`


<!-- User accounts are numbered from the lower to the upper limit-->

`r user_account_lower <- "1"`

`r user_account_upper <- "28"`


<!-- ############ END OF MODIFICATIONS ############ -->


<!--


# Introduction 

The goal of this lab assignment is to learn how to analyze streams of data with the Spark Structured Streaming API.
[Refer to this documentation](/courses/plp/overview/cluster-connection){target="_blank"} to learn how to connect and 
interact with the cluster.

::: {.infobox .warning data-latex="{warning}"}
**Assignment submission**

This lab assignment will be **evaluated**.

In order to finalize the submission of the assignment, 
complete [this form available on Edunao](`r submission_page`){target="_blank"}.

The submission deadline is **`r submission_date`**.

:::


::: {.infobox .warning data-latex="{warning}"}
**Documentation**

In order to answer the questions and do the exercises, you might want to refer to the following 
documentation:

*  The [Structured Streaming programing guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html){target="_blank"}.

* The [Spark SQL API reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html){target="_blank"}.

:::



# Warming up

Consider the following program.

```
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

port_number = COMPLETE HERE
checkpoint_location = COMPLETE HERE

spark = (SparkSession.builder.appName("Structured Streaming - exo1").getOrCreate())

lines = (spark\
        .readStream.format("socket")\
        .option("host", "localhost")\
        .option("port", port_number)\
        .load())

streamingQuery = lines.writeStream\
                .option("checkpointLocation", checkpoint_location)\
                .format("console").start()

streamingQuery.awaitTermination()
```

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. Where does this program get its input from?

2. What object type does the variable ``lines`` contain? 

3. Where does this program write its output?

4. What is the output of this program?

5. What is the option  ``checkpointLocation`` intended for?

6. What does the instruction ``streamingQuery.awaitTermination()``?


```

:::

You can now verify your answers to the previous questions by **executing the program**. 

::: {.infobox .activitybox data-latex="{exercisebox}"}
**Activity**



1. Connect to the cluster, if you haven't done so yet. [Refer to this documentation](/courses/plp/overview/cluster-connection){target="_blank"}.

2. After running the command ``srun ...``, you should be connected to a machine on the cluster Kyle. 
Note the name of this machine (you should see it at the terminal prompt).

3. Create a checkpoint directory for the first exercise (e.g., ``checkpoint_exo1``) under your home directory
``hdfs://sar01:9000/`r user_group`/`r user_group`_X`` **in HDFS**. 

4. Copy and paste the code into a Python file (e.g., ``exo1.py``) that you'll save into your home directory **in the local filesystem** 
of the cluster machine.
   * Change the value of the variable ``checkpoint_location`` so that it points to the directory that you created at point 3.
   * Change the value of the variable ``port_number`` to any value in the range [49152, 65535].

5. Open a new terminal window, connect to ``phome.metz.supelec.fr`` and then to the same machine that you noted at point 2.

6. In the new terminal, start a **netcat server** listening on the port number that you selected at point 4. Use the following command:

``
nc -lk port_number
``

7. Run the Python code with the command ``spark-submit``. Wait until Spark does not display any more messages on screen.
   * In case the program stops for an error, read the box "What to do in case of errors" below.

8. In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output.

:::

::: {.infobox .warning data-latex="{warning}"}
**What to do in case of errors**

If any error arises, **before** running the ``spark-submit`` again it would be better to remove 
all files from the checkpoint directory.

:::


::: {.infobox .warning data-latex="{warning}"}
**Stop the program**

* When you're done with your experiments, you can stop the Spark program by simply 
typing CTRL-C in the terminal where Spark is running. 

* Don't stop the netcat server, you'll need it in the next exercise.

:::


# Triggering policy

In a Structured Streaming program we can choose a **triggering policy**.


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. What is a triggering policy?

2. What is the triggering policy in the previous program?

3. Modify the code of the previous program in order to set  the 
``Fixed interval micro-batch`` triggering policy.

4. Run the program. How is the behaviour of this program different from before?


```

:::


# Checkpoint location and output mode

We're now going to see the impact of  the checkpoint location and the output modes on a streaming query.

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. What is an output mode and what are the available options?

2. What is the output mode of the previous program?

```

:::


We're now going to write a new streaming query.




::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. Create  a new checkpoint location in HDFS. 
You may also keep the same directory as before; in this case, make sure you **remove all files** from that 
directory.

2. Write a new program that reads a streaming text from a TCP socket and 
counts the number of occurrences of each word.

3. Which output mode are you going to choose and why?

4. Run the program. Write few lines on the netcat server and observe the output.

5. Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output.
What can you say about the word counts?

6. Stop the program and remove the files in the checkpoint location. Run the program again and write few lines 
on the netcat terminal. What can you say about the word counts?

7. Play with the different output modes and observe how the output changes.


```

:::


# Window operations on event time



::: {.infobox .warning data-latex="{warning}"}
**Netcat and checkpoint**

1. You can stop the netcat server now. 

2. Remember to create a new checkpoint location for this exercise. 
Alternatively, you can also use the same directory as in the previous exercises, but
you should remove all its files.


:::


We're now going to find out how to perform aggregations over  a sliding event-time window.

A given data source generates some words for a certain time interval.
Each word is accompanied with a timestamp that indicates the exact moment when the word is 
generated. This timestamp is the **event time**.

After generating a word, the data source saves the word and its timestamp into 
a CSV file in a directory on HDFS. 
For convenience, we'll refer to this directory as the **source directory**.

At any given moment, the source  will contain zero to many CSV files, 
where each file only contains exactly one line in the format 
``word,timestamp`` (no whitespace before nor after the comma).


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


Write a Spark program that:

1. Reads the stream of data from the source directory.

2. Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes.

3. Print the output counts to the console. Use triggers of 5 seconds.

```

:::

We now test the new Spark program.

::: {.infobox .warning data-latex="{warning}"}
**Data source and timeline visualization**

We provide two Python programs for this exercise: a data generator and 
a tool for visualizing words in a timeline. Instructions to get and run
these two programs are given in the activity below.

The **data generator** is our data source. 
It generates two words every second for a certain 
amount of time. 
Each word is saved in a separate CSV file in source directory.
It also saves the list of all generated 
words to a summary CSV file. 

The **visualization tool** takes as its input the summary CSV file 
written by the data generator and visualizes the words 
on a timeline.

:::


::: {.infobox .activitybox data-latex="{exercisebox}"}
**Activity**

1. Create the source directory under your home directory
``hdfs://sar01:9000/`r user_group`/`r user_group`_X`` **in HDFS**. 

2. Copy to your home directory 
in the local filesystem 
the data generator that you find at the following path

``/usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py``. 

3. Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly.

4. In another terminal, run the Python script ``tempws_gen.py``. Use the 
following command to learn how to run this program:

``python3 tempws_gen.py --help``

For this exercise, do not introduce any delay 
(keep the default values of the parameters ``--delay``, ``--mindelay``, ``--maxdelay``).

5. After launching the data generator, you should 
see some output in the terminal where you launched the Spark program. 
**Wait for the script ``tempws_gen.py`` to terminate the data generation**.
The output might be a bit overwhelming. Scroll up to identify the results 
on each micro-batch.

6. If you need to rerun the Spark program and the data generator, make sure you delete all the files in the 
checkpoint location and the source directory.


:::

We now want to analyze the output of the program.


* The script ``tempws_gen.py`` has generated 
a file ``gen_words.csv`` in your home directory. 
This file contains the list of all words generated with the relative timestamps. 
**Download the file to your computer**.

* Download the visualization tool ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py`` 
to your computer.


::: {.infobox .warning data-latex="{warning}"}
**Visualization tool**

Use the following command to learn how to run the visualization tool:

``
python timeline_visualization.py --help
``
 
The visualization tool displays a vertical blue bar at each trigger.
To this purpose, you'll need to pass the tool the timestamps associated to the first 
and last trigger and the interval (in seconds) between two consecutive triggers.

You can get the timestamps associated to the first and last trigger by analyzing the output of Spark.
More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you'll need 
to look at the timestamp associated to the first and last micro-batch.



:::



::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. Analyze the output of your Spark program and the timeline of the generated words.

2. Describe how the counts are updated by the Spark program.

```

:::

# Late data and watermarking

We're now going to learn how Structured Streaming handles 
late data in windowed aggregations.


::: {.infobox .warning data-latex="{warning}"}
**Remove generated files**

Remove all the files in the directory ``tempws``.

:::

The data generator ``tempws_gen.py``
can generate a stream of words, some of which 
might be written to the directory ``tempws`` with some amount of 
delay. In other words, there is a gap between the event time (when the word is generated)
and the arrival time (when the word is written to the directory).

Use the following command to learn how to do it:

``python3 tempws_gen.py --help``


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. Write a Spark program that does the same aggregation as in the previous exercise.
Additionally, the program must use watermarking to handle late data.

2. Start the Spark program.

3. Generate some data with delay with the program ``tempws_gen.py``. 
Once the data generation stops, you can stop the Spark program.

4. Visualize the generated words with the visualization tool. 
Late words have the delay indicated between parentheses.

5. Observe the output of the Spark program and describe how 
the watermarking mechanism works on this example.

```

:::


-->








