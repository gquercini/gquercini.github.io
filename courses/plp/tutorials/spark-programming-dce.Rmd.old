---
title: "Introduction to Spark programming"
heading: Big data algorithms, techniques and platforms â€” Tutorial 4
linktitle: tutorial-spark-dce
output:
  bookdown::pdf_book:
    toc: false
    includes:
      in_header: ../latex-files/preamble.tex
    number_sections: no
  blogdown::html_page:
    number_sections: no
  html_document:
    df_print: paged
menu:
  plp:
    name: Introduction to Spark programming
    parent: tutorials
    identifier: tutorial-spark-dce
    weight: 3
params:
  solution: false
summary: Description of the tutorial on Spark programming on a cluster
draft: no
type: docs
---

<link rel="stylesheet" href="/styles/course.css">
<link rel="stylesheet" href="/styles/cloud-computing.css">

[Refer to this documentation](/courses/plp/overview/cluster-connection){target="_blank"} to learn how to connect and 
interact with the cluster.

<!--

# Computing averages

We consider a collection of  CSV files containing temperature measurements in the following format:

``year,month,day,hours,minutes,seconds,temperature`` 

you can find the files under the directory ``hdfs://sar01:9000/data/temperatures/``

Here are the details for each file:

* File ``temperatures_86400.csv`` contains one measurement per day in the years 1980 - 2018. 
* File ``temperatures_2880.csv`` contains one measurement every 2880 seconds in the years 1980 - 2018. 
* File ``temperatures_86.csv`` contains one measurement every 86 seconds for the year 1980 alone. 
* File ``temperatures_10.csv`` contains one measurement every 10 seconds for the years 1980 - 2018. 

We intend to implement a Spark algorithm to generate pairs $(y, t_{avg})$, where 
$y$ is the year and $t_{avg}$ is the average temperature in the year.

## First implementation

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` 
to your home directory by typing 
the following command:

``
cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py
``

Open the file ``avg_temperatures_first.py`` 
and write the following function:

```
def avg_temperature(theTextFile): 
    temperatures = theTextFile \ 
                        .map(lambda line: line.split(",")) \ 
                        .map(lambda term: (term[0],   [float(term[6])])) \ 
                        .reduceByKey(lambda x, y: x+y) \ 
                        .mapValues(lambda lv: sum(lv)/len(lv)) 
    return temperatures 
```

In the same file, locate the  two variables ``input_path`` and ``output-path``.
and write the following code:

```
input_path = "hdfs://sar01:9000/data/temperatures/"
output_path = "hdfs://sar01:9000/cpupsmia1/your_username/"
```

Always remember to replace ``your_username`` with your username!
**Don't forget the / at the end of the file paths!**.


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**


```{exercise}


* Run the script ``avg_temperatures_first.py`` by using ``temperatures_86400.csv`` as an input.
To this extent, use the following command:

    <div align="left">
    ``
    spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv
    ``
    </div>

    You should find the output of the program under the folder ``hdfs://sar01:9000/cpupsmia1/your_username/temperatures_86400.out``


* What's the execution time? 
   * In the output of Spark on the command line you should see a line that mentions something along the following line:
       
       ``
       INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s 
       ``

* Run the same script by using ``temperatures_2880.csv`` as an input.

* What is the execution time? Does it seem reasonable compared with the execution time that you observed before?
Justify your answer.

* Execute the same script by using ``temperatures_86.csv`` as an input.

* What is the execution time? How would you justify it, knowing that 
the files  ``temperatures_2880.csv`` and ``temperatures_86.csv`` have a similar size (11 MB the former, 9 MB the latter)?

```

:::


`r if(params$solution) {"<details>
<summary>Solution</summary>

::: {.infobox .exosolution data-latex=\"{exercisebox}\"}

* The execution time on file \x60\x60temperatures_86400.csv\x60\x60 is around 1.73 seconds.

* The execution time on file  \x60\x60temperatures_2880.csv\x60\x60 is around 6.23 seconds.
This is normal,  as we have considerably more measurements per year compared to the first file.

* The execution time on file \x60\x60temperatures_86.csv\x60\x60  is around 74.96 seconds.
Although \x60\x60temperatures_86.csv\x60\x60 and 
\x60\x60temperatures_2880.csv\x60\x60 have a comparable size, in the first file 
all measurements are associated to the same key (1980), and therefore  the application of \x60\x60reduceByKey\x60\x60 takes much longer than before.


:::

</details>

"}`


## Second implementation

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing 
the following command:

<div align="left">
``
cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py
``
</div>

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}

Based on the observations made in the 
previous exercise, write an improved implementation of the 
function ``avg_temperature``.

```

:::

`r if(params$solution) {"<details>
<summary>Solution</summary>

\x60\x60\x60
temperatures = text_file.map(lambda line: line.split(\",\"))\
                    .map(lambda triple: (triple[0], (float(triple[6]), 1)))\
		            .reduceByKey(lambda t1, t2: (t1[0]+t2[0], t1[1] + t2[1]))\
                    .mapValues(lambda t: t[0]/t[1])
\x60\x60\x60



</details>

"}`

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**


```{exercise}


* Run the script ``avg_temperatures_second.py`` by using ``temperatures_86.csv`` as an input.

* What's the execution time? Compare it with the execution time obtained in the previous exercise and 
comment the difference.
   
* Run the same script by using ``temperatures_10.csv`` (3 GB!) as an input. 
Do you think that the program takes too long? Why?


```

:::

`r if(params$solution) {"<details>
<summary>Solution</summary>

* The execution time on file \x60\x60temperatures_86.csv\x60\x60 is around 3.38 seconds.
This is much more efficient than the first implementation. 
In fact, we don't create here a huge list of values associated with a given key.

* The execution time on file \x60\x60temperatures_10.csv\x60\x60 is around 33 seconds.
This is entirely acceptable given that file contains 3 GB of data and we have a measurement 
every 10 seconds for each year.

</details>

"}`

# Average and standard deviation

We use the same files as in the first question.
Our objective is to write a Spark program that produces 
triples $(y, t_{\mu}, t_{\sigma})$, where $y$, $t_{\mu}$ and 
$t_{\sigma}$ are the year, the average temperature in the year and the 
standard deviation respectively.

We can express the standard deviation of $n$ values $x_1 \ldots x_n$ with the following formula:

$$
\sigma = \sqrt{\overline{x^2} - \overline{x}^2} = \sqrt{\frac{\sum_{i=1}^n (x_i)^2}{n} - \Bigg(\frac{\sum_{i=1}^n x_i}{n}\Bigg)^2} 
$$

Copy the file  ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing 
the following command:

``
cp ~vialle/DCE-Spark/template_temperatures.py  ./avg_stddev_temp.py
``


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**


```{exercise}


* Complete the definition of the function ``avg_temperature`` in file ``avg_stddev_temp.py``.
   
* Run the script by using  ``temperatures_86400.csv``  and ``temperatures_2880.csv`` 
as input files (small files).

* Run the script by using  ``temperatures_86.csv``  and ``temperatures_10.csv`` as input files (large files).


```

:::

`r if(params$solution) {"<details>
<summary>Solution</summary>

\x60\x60\x60
temperatures = text_file.map(lambda line: line.split(\",\")) \
	.map(lambda t: (t[0],(float(t[6]), float(t[6])*float(t[6]),1))) \
    .reduceByKey(lambda f, s: ((f[0]+s[0]),(f[1]+s[1]),(f[2]+s[2])))\
    .mapValues(lambda t : (t[0]/t[2], math.sqrt(t[1]/t[2] - (t[0]/t[2])*(t[0]/t[2]))))
\x60\x60\x60

</details>

"}`

-->






