---
title: "Advanced Spark programming"
heading: Platforms and languages â€” Lab assignment 1
linktitle: spark-dce-lab
output:
  bookdown::pdf_book:
    toc: false
    includes:
      in_header: ../latex-files/preamble.tex
    number_sections: no
  blogdown::html_page:
    number_sections: no
  html_document:
    df_print: paged
menu:
  plp:
    name: Lab 1. Advanced Spark programming
    parent: tutorials
    identifier: spark-dce-lab
    weight: 5
params:
  solution: false
summary: Description of the lab advanced Spark programming.
draft: no
type: docs
---

<link rel="stylesheet" href="/styles/course.css">
<link rel="stylesheet" href="/styles/cloud-computing.css">


<!-- ############ IMPORTANT: CHANGE THESE VALUES BEFORE ANY LAB ############ -->

<!-- The link to the Edunao page where the students submit their work -->
`r submission_page <- "https://centralesupelec.edunao.com/mod/quiz/view.php?id=47098"`

<!-- The submission deadline -->
`r submission_date <- "Tuesday, March 9, 2021 8:00 AM"`

<!-- ############ END OF MODIFICATIONS ############ -->

<!--

# Introduction 


The goal of this Spark lab assignment is to write Spark programs and run them on a cluster.
[Refer to this documentation](/courses/plp/overview/cluster-connection){target="_blank"} to learn how to connect and 
interact with the cluster.




::: {.infobox .warning data-latex="{warning}"}
**Assignment submission**

This lab assignment will be **evaluated**.

In order to finalize the submission of the assignment, 
complete [this form available on Edunao](`r submission_page`){target="_blank"}.

The submission deadline is **`r submission_date`**.

:::


# Common friends in a social network

Consider a social network described by a graph encoded in a text file.
Each line of the file is a list of identifiers separated by commas.
For instance, the line $A,B,C,D$ means that $A$ is friend with $B$, $C$ and $D$. 
An excerpt of the file looks like as follows:

```
A,B,C,D
B,A,D
C,A,D
D,A,B,C
...
```

We suppose that the friendship relation is symmetric: $(A, B)$ implies
$(B, A)$.

**We want to obtain the list of the common friends for each pair of individuals**: 

```
(A, B), [D]
(A, C), [D] 
(A, D), [B, C] 
(B, C), [D] 
(B, D), [A] 
(C, D), [A]
```

As an additional constraint, we want to represent a couple only once and avoid 
to represent the symmetric couple. 
In other words, if we output $(A, B)$, we don't want to output $(B, A)$.


We use the following input files available in folder ``hdfs://sar01:9000/data/sn/``:

* ``sn_tiny.csv``. Small social network, that you can use to test your implementation.

* ``sn_10k_100k.csv``. Social network with $10^4$ individuals and $10^5$ links. 

* ``sn_100k_100k.csv``. Social network with $10^5$ individuals and $10^5$ links. 

* ``sn_1k_100k.csv``. Social network with $10^3$ individuals and $10^5$ links. 

* ``sn_1m_1m.csv``. Social network with $10^6$ individuals and $10^6$ links. 


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}

Write an implementation in Spark.
**Test your implementation on file ``sn_tiny.csv``.**

```

:::

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}

Run your implementation on the other files and write down the execution times. 
Comment on the execution times considering the file sizes, the number of nodes and links 
and the number of pairs $((A, B), X)$ generated by the algorithm.

```

:::

::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}

Execute your implementation on the file ``sn_1m_1m.csv`` by varying the 
number of cores used by the Spark executors. 
You can specify the total number of cores with the option ``--total-executor-cores``
of the command ``spark-submit`` (you can also refer [to the Spark documentation](https://spark.apache.org/docs/latest/submitting-applications.html){target="_blank"}).

* What is the impact of the number of cores on the execution time? Make a graph and comment.



```

:::


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}


1. By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of 
a node in a given graph.

2. Compute the minimum, maximum and average degree on all the given input files.

3. Do these values confirm or invalidate the considerations that you made on the execution times of the 
algorithm in the first exercise? Justify your answer.



```

:::



# Creating an inverted index


In folder ``hdfs://sar01:9000/data/bbc/`` you'll find a collection of 50 
articles obtained from the BBC website (2004-2005) organized into five subfolders:
*business*, *entertainment*, *politics*, *sport* and *technology*. 

We want to create an **inverted index**, which associates each word with the list of the 
files in which the word occurs. 
More specifically, for each word, the inverted index will have a list of the 
names of the files (path relative to the folder ``/data/bbc``) that contain the word.  

The inverted index:

* must not contain the same word twice; 

* must not contain any stopwords (the list of stopwords is provided in the 
``hdfs://sar01:9000/data/stopwords.txt`` file); 

Moreover:

* Words in the inverted index must only contain letters.

* Words in the inverted index must be lowercase.


::: {.infobox .exercisebox data-latex="{exercisebox}"}
**Exercise**

```{exercise}

Write a Spark program to create an inverted index and execute it on the 
input folder.
You can use the template available at ``~vialle/DCE-Spark/template_inverted_index.py``.

```

:::

-->
