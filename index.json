[{"authors":["admin"],"categories":null,"content":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire de Recherche en Informatique (LRI).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire de Recherche en Informatique (LRI).","tags":null,"title":"Gianluca Quercini","type":"authors"},{"authors":null,"categories":null,"content":" Overview Nowadays, the marketing strategies of most companies are based on the analysis of massive and heterogeneous data that need a considerable amount of computational power. Instead of purchasing new hardware and software infrastructures, companies often resort to the computational and storage power offered by cloud computing platforms over the Internet.\nThe objective of this course is to present the fundamental principles of distributed computing that are at the heart of cloud computing. The course will cover the principles of virtualization and containerization and the methods and tools used for distributed processing (MapReduce and Spark).\n Prerequisites The main prerequisite is the course Information systems and programming:\n Python programming.\n Basic networking notions.\n Basic data management notions.\n  Previous experience with working the command-line terminal and Linux is desired but not essential.\nA beginner introduction to Linux is available here.\n Teaching staff  Gianluca Quercini\n Francesca Bugiotti\n Marc-Antoine Weisser\n Idir Ait Sadoune\n   Required software  Docker. See the installation guide.   Course summary Introduction  Context and applications. Service models (SaaS, PaaS, IaaS). Architectural design. Public cloud platforms. Cloud economic model.  Virtualization  Virtualization principles. Containerization (Docker). Orchestrators (Docker Swarm and Kubernetes).  Cloud programming and software environments  Parallel computing, programming paradigms. Hadoop MapReduce. Apache Spark.    Exam  The Lab assignment 1 will be graded (\\(g_1\\)).\n A mini project will conclude the course (\\(g_2\\)).\n  The final grade will be the average of \\(g_1\\) and \\(g_2\\).\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e76a719efd772c3d82d45e1d0c32b856","permalink":"/courses/cloud-computing/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/","section":"courses","summary":"Presentation of the course Distributed and Cloud Computing","tags":null,"title":"Cloud Computing","type":"docs"},{"authors":null,"categories":null,"content":" Overview All individuals who own a personal computer need to store data on a daily basis. The operating system provides the file system as a way to organize and access files easily, masking the complexities of physical data storage. Although the file system is a simple and efficient data management system, many applications need more sophisticated features, which a file system does not provide, such as data integrity, indexes and fine-grained access control mechanisms.\nThis is where databases come into play.\nThe objective of this course is to introduce the basic notions on which all modern database management systems are grounded. The course is divided into two parts. The first part focuses on relational databases and presents the notions of integrity constraints, data consistency, transactions and normalization, both in single-server and distributed environments. The second part will introduce the four families of NoSQL databases, with a special focus on document (MongoDB) and graph databases (Neo4j).\n Prerequisites Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Teaching staff  Gianluca Quercini   Microsoft Teams group Students are required to join the course team on Microsoft Teams.\n Download and install Microsoft Teams on your computer.\n Log in with your CentraleSupelec credentials.\n Join the team by using the following code: z0ixc50.\n  Each lecture will be broadcast live on Microsoft Teams for all students that are not on campus due to Covid-19 restrictions.\n Required software  DB Browser for SQLite. Tool to manage SQLite databases. Click here to download it and install it.\n MongoDB. A document database management system. Click here to download it and install it.\n Neo4j. A graph database management system. Click here to download it and install it. You’ll be requested to fill in a form.\n   Course summary 1. An introduction to database systems and data modeling.\n Database systems: definitions and motivations. Data models. Introduction to the relational data model. Database design with the Entity-Relationship model (ER).  2. Normalization theory.\n Normalization: definitions and motivations. Notion of functional dependency. 1st, 2nd and 3rd normal forms. Further normal forms: Boyce-Codd, 4th and 5th normal forms. Normalization in real life.  3. Relational database management systems (DBMS).\n Types of relational DBMS: client-server vs. embedded. An embedded relational DBM: SQLite. Interacting with a relational DBMS: the SQL language.  4. Advanced database concepts.\n Overview of a database management system. Transactions: definitions and motivations. Transaction management and processing. Query processing. Indexing.  5. Distributed databases.\n Distributed databases: definitions and motivations. Data replication and sharding. Data consistency in distributed databases. The CAP theorem.  6. NoSQL database systems.\n NoSQL database systems: definitions and motivations. Overview of NoSQL databases systems. Document-oriented databases: MongoDB. Graph databases: Neo4j.   Exam The evaluation is based on:\nA project (40% of the final grade). A written examination (60% of the final grade).  The project is a teamwork assignment where the students use the notions learned in classroom to design and create a database for a travel reservation system. Each team will be composed of 3 students. The students will start the project during the last three hours of the course (Tuesday 24 November 2020, 9:00 AM - 12 PM).\nThe written examination lasts 3 hours and consists of two parts:\n A multiple choice questionnaire (1 hour). A series of exercises on relational/NoSQL database modeling and querying (2 hours).   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c50fbd0dd8e5954b528c8988ad1693b6","permalink":"/courses/databases/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/overview/","section":"courses","summary":"Presentation of the course Introduction to Databases","tags":null,"title":"Introduction to Databases","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB and Neo4j as case studies).\n Prerequisites  Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Good knowledge of relational database management systems.\n   Teaching staff  Gianluca Quercini   Microsoft Teams group Students are required to join the course team on Microsoft Teams.\n Download and install Microsoft Teams on your computer.\n Log in with your CentraleSupelec credentials.\n Join the team by using the following code: hxud77w.\n   Course summary 1. Introduction and MapReduce programming.\n Basic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce.  2. Hadoop and its ecosystem: storage and data processing.\n Introduction to the Hadoop ecosystem.\n In-depth description of the Hadoop Distributed File System (HDFS).\n  3. Hadoop and its ecosystem: data access.\n Hadoop data access tools: Pig, Hive, HBase, Kafka, Flume.  4. Introduction to Apache Spark.\n Apache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions. Dataframes.  5. Advanced Apache Spark.\n SparkSQL, Spark streaming, machine learning, graph analysis.  6. NoSQL database systems: MongoDB.\n Distributed database. Overview of NoSQL databases systems. Document-oriented databases: MongoDB.  7. Graph-oriented database systems: Neo4j.\n Exam The evaluation is based on:\nTwo lab assignments will be graded. A written examination (70% of the final grade).  The written examination lasts 3 hours and consists of two parts:\n A multiple choice questionnaire (1 hour). A series of exercises on relational/NoSQL database modeling and querying (2 hours).   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"aa867f88f796b12dfa5207b07de94be4","permalink":"/courses/plp/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/","section":"courses","summary":"Presentation of the course Platforms and Languages","tags":null,"title":"Platforms and languages","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Getting started with Docker\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Wednesday 6 May, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n Tutorial 2 Title: MapReduce - Hadoop\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Thursday 14 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d41cadecab8bb09746ea9ea01e2a42d0","permalink":"/courses/cloud-computing/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Data modeling\nDate and time: Tuesday 15 September 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 2 Title: Normalization\nDate and time: Tuesday 29 September 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 3 Title: SQL queries\nDate and time: Tuesday 6 October 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 4 Title: Document-based databases: MongoDB.\nDate and time: Tuesday 3 November 2020, 10 AM - 12 AM\nLink: available soon.\n Tutorial 5 Title: Graph databases: Neo4j.\nDate and time: Tuesday 10 November 2020, 10 AM - 12 AM\nLink: available soon.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40c351607858b41143809fa1fb45464e","permalink":"/courses/databases/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nDate and time: Wednesday 23 September 2020, 10:15 AM - 11:45 AM\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89eb8f3c35dc5558cefc06fa0d79071d","permalink":"/courses/plp/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":"  1 Windows 1.1 Docker Desktop for Windows 1.2 Docker Toolbox for Windows  2 MacOS 2.1 Docker Desktop for Mac. 2.2 Docker Toolbox for Mac  3 Linux 4 Alternative options 4.1 Docker playground 4.2 Docker in an already prepared virtual machine 4.3 Docker in a Ubuntu Linux virtual machine  5 Verify your installation 6 Interacting with Docker 6.1 Graphical interface  7 Troubleshooting   This document contains information on how to install Docker on your computer.\nAlthough you can access an online Docker environment without installing anything on your computer (see Section 4.1), you should consider this option only if you really cannot install Docker.\nThe installation procedure depends on the operating system that your computer runs.\n1 Windows The installation procedure depends on the Windows version running on your computer.\n1.1 Docker Desktop for Windows If your computer runs Windows 10 64 bits (Pro, Enterprise, or Education, build 15063 or later), you can install Docker Desktop for Windows (recommended).\n Show me more\nHardware prerequisites\n 64 bit processor.\n 4GB system RAM.\n BIOS-level hardware virtualization support must be enabled in the BIOS settings. For more information, see Virtualization.\n  VirtualBox users\nDocker for Windows uses Hyper-V as a virtual machine to run containers. Unfortunately, Hyper-V and VirtualBox are not compatible; when Hyper-V is enabled, VirtualBox will stop working.\nHowever:\n The existing VirtualBox images will not be removed.\n When you want to use VirtualBox, you can turn Hyper-V off.\n  Cannot/don’t want to install Docker Desktop for for Windows\nIf your computer doesn’t meet the hardware requirements, or you don’t want to install Docker Desktop for Windows because you don’t want to mess up your VirtualBox installation (although you shouldn’t really worry about the latter), you have two options:\n Install Docker Toolbox for Windows (Section 1.2).\n See the alternative options (Section 4).\n  Installation procedure\n Download Docker Desktop for Windows.\n Follow the installation instructions. You might need to restart the system to enable Hyper-V.\n Verify your installation (see Section 5).\n    1.2 Docker Toolbox for Windows If your computer runs Windows 7 or higher, and doesn’t meet the hardware requirements for Docker for Windows, you can install Docker Toolbox for Windows.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     2 MacOS The installation procedure depends on the version of MacOS running on your computer.\n2.1 Docker Desktop for Mac. If your computer runs MacOS 10.13 or higher, you can install Docker Desktop for Mac (recommended).\n Show me more\nHardware requirements\n Your computer hardware must be a 2010 or a newer model. Verify that your computer is compatible with Docker Desktop for Mac:  Open a terminal. Run the following command: sysctl kern.hv_support. If the output of the command is kern.hv_support: 1 your computer is compatible.  At least 4GB of RAM.  VirtualBox users\nIf you have a version of VirtualBox older than 4.3.30, you should consider upgrading it, as it would not be compatible with Docker Desktop.\nCannot install Docker Desktop for Mac\nIf your computer doesn’t meet the hardware requirements, you have two options:\n Install Docker Toolbox for Mac (Section 2.2).\n See the alternative options (Section 4).\n  Installation instructions\n Download Docker Desktop for Mac.\n Follow the installation instructions.\n Verify your installation (see Section 5).\n    2.2 Docker Toolbox for Mac If your computer runs MacOs 10.8 or higher, and doesn’t meet the hardware requirements for Docker Desktop for Mac, you can install Docker Toolbox for Mac.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     3 Linux You can install Docker on the following Linux distributions:\n CentOS (installation instructions).\n Debian (installation instructions).\n Fedora (installation instructions).\n Ubuntu (installation instructions).\n  Make sure to read the post-installation steps for Linux and to take the necessary steps to be able to run Docker as a non-root user.\n 4 Alternative options If you’re unable to install Docker on your computer, you have two options left: using the Docker playground or installing Docker in a Linux virtual machine.\n4.1 Docker playground The Docker playground is an online Docker environment that you can play with for free.\n The advantage is that you don’t have anything to install on your computer.\n The disadvantage is that you might be unable to open a session depending on the number of active sessions.\n  In order to connect to the playground, you need to create an account on DockerHub.\n 4.2 Docker in an already prepared virtual machine We provide an Alpine Linux virtual machine with Docker already installed (size: 743MB). The virtual machine will give you a simple command-line interface where you can type the Docker commands.\nDownload the virtual machine and import it into VirtualBox, as shown in the following video (Safari users: watch on YouTube for fullscreen mode; all users: select HD quality for a better experience).\n The username and password to log into the virtual machine are both root. In the video, you’ll be directed to create a folder in your computer called docker_files. There, you’ll place all files that you’ll need to play with Docker. Don’t hesitate to create subdirectories to organize your files (e.g., td-1, final-project). You’ll be able to access this folder from the virtual machine from the folder /mnt/docker_files. This way, you can manipulate your files by using the file system manager of your computer and you’ll just use the terminal of the virtual machine to type the Docker commands.\n 4.3 Docker in a Ubuntu Linux virtual machine  Install VirtualBox on your computer.\n Download the ISO image of Ubuntu Desktop.\n Open VirtualBox and select New to install a new operating system.\n Choose Linux as the operating system type and Ubuntu (64-bit) as the version.\n Set the memory size (1024 MB or higher).\n Create a virtual hard disk with the recommended size.\n Select VDI (VirtualBox Disk Image) as the hard disk type.\n Select the option Fixed size and create the new virtual machine.\n Double-click on the new virtual machine and, when prompted, select the ISO image of Ubuntu.\n Follow the instructions to install Ubuntu.\n When Ubuntu is finally installed, follow the instructions in Section 3 to install Docker in Ubuntu.\n    5 Verify your installation Open a terminal and type the following command:\ndocker run hello-world If everything is OK, you should see the output in the following figure.\n 6 Interacting with Docker In this course, we’ll learn how to interact with the Docker engine by using the command-line terminal. This option might seem a bit tedious (nobody likes to remember textual commands), but it offers a great flexibility.\nThis is the option that we recommend and for which we’ll provide a full support throughout the course.\n6.1 Graphical interface If you really want to use a graphical interface, you might want to look at Portainer, which is itself run as a Docker container.\n Linux or MacOS users\nOpen a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer   Windows users Open a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart always -v \\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine -v portainer_data:C:\\data portainer/portainer   When the container is up and running, the interface is available at the following URL: http://localhost:9000.\n Choose a password and create the user admin.\n Select Local to manage the Docker environment installed on your computer and click on Connect.\n Click on the endpoint Local (figure below) to access the dashboard.\n   The menu on the left of the dashboard allows you to manage the different components of your Docker environment (e.g., containers, images, volumes and networks).  A user guide of Portainer is very much out of the scope of this course. However, the interface is rather intuitive and you should easily find out how to create, run, stop and remove containers, build images and create volumes and networks.\n  7 Troubleshooting In this section we’ll document the installation issues that you might experience.\nDon’t hesitate to contact us to report your installation problems.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1009f24f18a9853858a69a79adeb4f0c","permalink":"/courses/cloud-computing/overview/installing-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/installing-docker/","section":"courses","summary":"Docker installation instructions","tags":null,"title":"Installing Docker","type":"docs"},{"authors":[],"categories":[],"content":"","date":1581344688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581344688,"objectID":"4e459ff727a9e6a0c795e8d8f3d69232","permalink":"/project/data-for-you/","publishdate":"2020-02-10T15:24:48+01:00","relpermalink":"/project/data-for-you/","section":"project","summary":"","tags":[],"title":"Data for You","type":"project"},{"authors":["Fatiha Saïs","Joana E. Gonzales Malaverri","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"24e4602ce4c2a42bac26f3019386ecbf","permalink":"/publication/sais-2020/","publishdate":"2020-04-08T14:26:24.211279Z","relpermalink":"/publication/sais-2020/","section":"publication","summary":"","tags":null,"title":"MOMENT: Temporal Meta-Fact Generation and Propagation in Knowledge Graphs","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Coriane Nana Jipmo","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3685ef29403450add392048ea8b1e19d","permalink":"/publication/seghouani-2019/","publishdate":"2020-02-10T15:32:10.718712Z","relpermalink":"/publication/seghouani-2019/","section":"publication","summary":"","tags":null,"title":"Determining the interests of social media users: two approaches","type":"publication"},{"authors":["Suela Isaj","Nacéra Bennacer Seghouani","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f0a2e1e20345b7ff2748dd0ff24c29fd","permalink":"/publication/isaj-2019/","publishdate":"2020-02-10T15:37:10.960652Z","relpermalink":"/publication/isaj-2019/","section":"publication","summary":"","tags":null,"title":"Profile Reconciliation Through Dynamic Activities Across Social Networks","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed97c59bbeebf6f67506ea911cd4d01b","permalink":"/publication/seghouani-2018/","publishdate":"2020-02-10T15:32:10.719498Z","relpermalink":"/publication/seghouani-2018/","section":"publication","summary":"","tags":null,"title":"A frequent named entities-based approach for interpreting reputation in Twitter","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9cdf7741acd87116627b5dd0bc25a27c","permalink":"/publication/seghouani-2018-a/","publishdate":"2020-02-10T15:33:43.343039Z","relpermalink":"/publication/seghouani-2018-a/","section":"publication","summary":"","tags":null,"title":"Élimination des liens inter-langues erronés dans Wikipédia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"edcf86d1dd80092db4864af2aa551e98","permalink":"/publication/bennacer-2017-a/","publishdate":"2020-02-10T15:32:10.723551Z","relpermalink":"/publication/bennacer-2017-a/","section":"publication","summary":"","tags":null,"title":"Eliminating Incorrect Cross-Language Links in Wikipedia","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"048dafc42b622ff1cee4c115e5409243","permalink":"/publication/jipmo-2017/","publishdate":"2020-02-10T15:32:10.722099Z","relpermalink":"/publication/jipmo-2017/","section":"publication","summary":"","tags":null,"title":"Frisk: A multilingual approach to find twitteR InterestS via wiKipedia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1e3d10b83c3c7b1807d422d7d251cca3","permalink":"/publication/bennacer-2017/","publishdate":"2020-02-10T15:32:10.7231Z","relpermalink":"/publication/bennacer-2017/","section":"publication","summary":"","tags":null,"title":"Interpreting reputation through frequent named entities in twitter","type":"publication"},{"authors":["Gianluca Quercini","Nacéra Bennacer","Mohammad Ghufran","Coriane Nana Jipmo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c02251b35ab73929914bbe9b772e5248","permalink":"/publication/quercini-2017/","publishdate":"2020-02-10T15:32:10.720054Z","relpermalink":"/publication/quercini-2017/","section":"publication","summary":"","tags":null,"title":"Liaison: reconciliation of individuals profiles across social networks","type":"publication"},{"authors":["Mohammad Ghufran","Nacéra Bennacer","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c1928a9ee227111e1b27ba31fdbb6a86","permalink":"/publication/ghufran-2017/","publishdate":"2020-02-10T15:32:10.722652Z","relpermalink":"/publication/ghufran-2017/","section":"publication","summary":"","tags":null,"title":"Wikipedia-based extraction of key information from resumes","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"49508dab832a1726fe7da77e8604a7f8","permalink":"/publication/jipmo-2016/","publishdate":"2020-02-10T15:32:10.724083Z","relpermalink":"/publication/jipmo-2016/","section":"publication","summary":"","tags":null,"title":"Catégorisation et Désambiguı̈sation des Intérêts des Individus dans le Web Social.","type":"publication"},{"authors":["Nacéra Bennacer","Mia Johnson Vioulès","Maximiliano Ariel López","Gianluca Quercini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ce3927111f9fc9f8ef041600fa6e6126","permalink":"/publication/bennacer-2015/","publishdate":"2020-02-10T15:32:10.725565Z","relpermalink":"/publication/bennacer-2015/","section":"publication","summary":"","tags":null,"title":"A multilingual approach to discover cross-language links in Wikipedia","type":"publication"},{"authors":["Mohammad Ghufran","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"482b66b1b2e201edf7dfa2685afa010a","permalink":"/publication/ghufran-2015/","publishdate":"2020-02-10T15:32:10.724645Z","relpermalink":"/publication/ghufran-2015/","section":"publication","summary":"","tags":null,"title":"Toponym disambiguation in online social network profiles","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"2bce9aaaae3b0248796b29334c06054f","permalink":"/publication/bennacer-2014/","publishdate":"2020-02-10T15:32:10.726341Z","relpermalink":"/publication/bennacer-2014/","section":"publication","summary":"","tags":null,"title":"Matching user profiles across social networks","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ff35f567785c6703d81172b874c7531b","permalink":"/publication/bennacer-2014-a/","publishdate":"2020-02-10T15:32:10.727368Z","relpermalink":"/publication/bennacer-2014-a/","section":"publication","summary":"","tags":null,"title":"Réconciliation des profils dans les réseaux sociaux.","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"41c8a22b08bd7e057a05ad86a744cac5","permalink":"/publication/quercini-2014/","publishdate":"2020-02-10T15:32:10.728416Z","relpermalink":"/publication/quercini-2014/","section":"publication","summary":"","tags":null,"title":"Uncovering the spatial relatedness in Wikipedia","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"9086beba183fe687c8f4c462cd3153ca","permalink":"/publication/quercini-2013/","publishdate":"2020-02-10T15:32:10.72918Z","relpermalink":"/publication/quercini-2013/","section":"publication","summary":"","tags":null,"title":"Entity discovery and annotation in tables","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cc5f1500c80ecfe28e62ab5e9b608e81","permalink":"/publication/quercini-2012-a/","publishdate":"2020-02-10T15:32:10.737022Z","relpermalink":"/publication/quercini-2012-a/","section":"publication","summary":"","tags":null,"title":"Des données tabulaires à RDF: l’extraction de données de Google Fusion Tables","type":"publication"},{"authors":["Massimo Ancona","Betty Bronzini","Davide Conte","Gianluca Quercini"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"62840715b27d999325c6799de42c8994","permalink":"/publication/ancona-2012/","publishdate":"2020-02-10T15:32:10.717657Z","relpermalink":"/publication/ancona-2012/","section":"publication","summary":"","tags":null,"title":"Developing Attention-Aware and Context-Aware User Interfaces on Handheld Devices","type":"publication"},{"authors":["Antonio Penta","Gianluca Quercini","Chantal Reynaud","Nigel Shadbolt"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"fa3cbdff0934af7e1ad34c9d906d46b4","permalink":"/publication/penta-2012/","publishdate":"2020-02-10T15:32:10.72995Z","relpermalink":"/publication/penta-2012/","section":"publication","summary":"","tags":null,"title":"Discovering Cross-language Links in Wikipedia through Semantic Relatedness.","type":"publication"},{"authors":["Gianluca Quercini","Jochen Setz","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1eb03ccdfc068b0af2eee6b98febab4f","permalink":"/publication/quercini-2012/","publishdate":"2020-02-10T15:32:10.736267Z","relpermalink":"/publication/quercini-2012/","section":"publication","summary":"","tags":null,"title":"Facetted Browsing on Extracted Fusion Tables Data for Digital Cities.","type":"publication"},{"authors":["Jochen Setz","Gianluca Quercini","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"152a779726400c4afb268da398df2025","permalink":"/publication/setz-2012/","publishdate":"2020-02-10T15:32:10.730492Z","relpermalink":"/publication/setz-2012/","section":"publication","summary":"","tags":null,"title":"Facetted search on extracted fusion tables data for digital cities","type":"publication"},{"authors":["Laura Papaleo","Gianluca Quercini","Viviana Mascardi","Massimo Ancona","A Traverso","Henry de Lumley"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2951f449fe1a9a559bd320ee66d7a4df","permalink":"/publication/papaleo-2011/","publishdate":"2020-02-10T15:32:10.7311Z","relpermalink":"/publication/papaleo-2011/","section":"publication","summary":"","tags":null,"title":"Agents and Ontologies for Understanding and Preserving the Rock Art of Mount Bego.","type":"publication"},{"authors":["Gianluca Quercini","Massimo Ancona"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"712870e00c1980526965a476c61c6a9f","permalink":"/publication/quercini-2010/","publishdate":"2020-02-10T15:32:10.731853Z","relpermalink":"/publication/quercini-2010/","section":"publication","summary":"","tags":null,"title":"Confluent drawing algorithms using rectangular dualization","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet","Jagan Sankaranarayanan","Michael D Lieberman"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"c8ae118f82887db05d951390d2ae7695","permalink":"/publication/quercini-2010-a/","publishdate":"2020-02-10T15:33:43.347923Z","relpermalink":"/publication/quercini-2010-a/","section":"publication","summary":"","tags":null,"title":"Determining the spatial reader scopes of news sources using local lexicons","type":"publication"},{"authors":["Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Anton Bogdanovych","H De Lumley","Laura Papaleo","Simeon Simoff","Antonella Traverso"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"6456e6f4265092c5466fcdd21d7bb4eb","permalink":"/publication/ancona-2010/","publishdate":"2020-02-10T15:32:10.73288Z","relpermalink":"/publication/ancona-2010/","section":"publication","summary":"","tags":null,"title":"Virtual institutions for preserving and simulating the culture of Mount Bego's ancient people","type":"publication"},{"authors":["Anton Bogdanovych","Laura Papaleo","Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Simeon Simoff","Alex Cohen","Antonella Traverso"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"c71e7154cbc2dfbe25039ed19758c530","permalink":"/publication/bogdanovych-2009/","publishdate":"2020-02-10T15:32:10.737689Z","relpermalink":"/publication/bogdanovych-2009/","section":"publication","summary":"","tags":null,"title":"Integrating agents and virtual institutions for sharing cultural heritage on the Web","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e93eb6b5f5c5cf19230762a04c9abc05","permalink":"/publication/ancona-2009/","publishdate":"2020-02-10T15:32:10.720588Z","relpermalink":"/publication/ancona-2009/","section":"publication","summary":"","tags":null,"title":"Text Entry in PDAs with WtX","type":"publication"},{"authors":["Massimo Ancona","Davide Conte","Donatella Pian","Sonia Pini","Gianluca Quercini","Antonella Traverso"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"e650a879a57f2a6c286aa2d365cb9a7d","permalink":"/publication/ancona-2008/","publishdate":"2020-02-10T15:32:10.721064Z","relpermalink":"/publication/ancona-2008/","section":"publication","summary":"","tags":null,"title":"Wireless networks in archaeology and cultural heritage","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini","Luca Dominici"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5da2e1599d08bf6b66c9260251f7f87d","permalink":"/publication/ancona-2007-a/","publishdate":"2020-02-10T15:32:10.734718Z","relpermalink":"/publication/ancona-2007-a/","section":"publication","summary":"","tags":null,"title":"An Improved Text Entry Tool for PDAs","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0bb01482186a36901efd9bc59287ade5","permalink":"/publication/ancona-2007-b/","publishdate":"2020-02-10T15:33:43.342472Z","relpermalink":"/publication/ancona-2007-b/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5de935c6dca2f9dc45d3fcc539d7a23e","permalink":"/publication/ancona-2007/","publishdate":"2020-02-10T15:32:10.721486Z","relpermalink":"/publication/ancona-2007/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["Massimo Ancona","Marco Cappello","Marco Casamassima","Walter Cazzola","Davide Conte","Massimiliano Pittore","Gianluca Quercini","Naomi Scagliola","Matteo Villa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"956e4fbb19a8e3db3caf66b9ca3ce215","permalink":"/publication/ancona-2006-a/","publishdate":"2020-02-10T15:33:43.349611Z","relpermalink":"/publication/ancona-2006-a/","section":"publication","summary":"","tags":null,"title":"Mobile vision and cultural heritage: the agamemnon project","type":"publication"},{"authors":["Massimo Ancona","Walter Cazzola","Sara Drago","Gianluca Quercini"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"152d79fb2becb0b67888c71837db1203","permalink":"/publication/ancona-2006/","publishdate":"2020-02-10T15:32:10.733963Z","relpermalink":"/publication/ancona-2006/","section":"publication","summary":"","tags":null,"title":"Visualizing and managing network topologies via rectangular dualization","type":"publication"},{"authors":["M Ancona","S Locati","M Mancini","A Romagnoli","G Quercini"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"a5f6251a4b15ec8ddae9685ad605d62a","permalink":"/publication/ancona-2005/","publishdate":"2020-02-10T15:32:10.735534Z","relpermalink":"/publication/ancona-2005/","section":"publication","summary":"","tags":null,"title":"Comfortable textual data entry for PocketPC: the WTX system","type":"publication"},{"authors":null,"categories":null,"content":"  The Lab assignment 1 will be graded (\\(g_1\\)).\n A mini project will conclude the course (\\(g_2\\)).\n  The final grade will be the average of \\(g_1\\) and \\(g_2\\).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"924a1a1e7867769d4fc29cef00146e05","permalink":"/courses/cloud-computing/exam/exam-presentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/exam/exam-presentation/","section":"courses","summary":"Overview of the exam.","tags":null,"title":"Grading","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The objective of this project is to assess your ability of building and sharing images in Docker. The project consists of five steps:\nChoose an application. Package your application into one or more Docker images. Ship your image(s) to the DockerHub registry. Write a documentation. Submit your project.  In the following sections, you’ll find the details of each step.\nIf you have questions, doubts, need of help, don’t hesitate to contact me anytime in the MS Teams space.\n Choose an application In this step, you’ll need to select an application of your choice. Examples include:\n An application that you developed in the past in another course (does the phrase coding weeks ring a bell?).\n An application that you develop from scratch specifically for this project. That’s a bit ambitious, but if you feel like challenging yourself, why not?\n An application developed by a friend of yours.\n An application that you found on the internet. A good place to look at is GitHub.\n  The application may be written in any programming language.\n Package your application Your application must be packaged as a Docker image, or a set of Docker images, if the application is composed of many independent services.\nRecommendations  Create a directory on your computer named after your application (e.g., sample-app).\n If your application is monolithic (i.e., it’s not composed of many services), put everything into this directory (including the Dockerfile).\n If your application consists of many independent services, create a subdirectory for each service, containing everything needed to run that service (as well as the Dockerfile).\n  In case your application is composed of many independent services, you might also want to create a docker-compose file with the instructions to run the application (you’d greatly simplify my life!). However, writing a docker-compose is not mandatory.\n  Ship your image(s). Upload your image(s) to the DockerHub registry. In the following instructions I show the Docker commands to upload an image named test-image to my DockerHub registry (my username is quercinigia). Of course, when you’ll type these Docker commands yourselves, you’ll have to use your username and your image’s name.\nFor your convenience, in my last lecture I showed a full demo of the procedure, that you can watch in in this video, starting at minute 37.\n If you haven’t done so yet, create an account on DockerHub.\n From the command-line terminal, I log into my DockerHub account. One way to do so is the following:\n I created a file docker_pwd.txt containing my password.\n In the terminal, I place myself in the same directory as the file docker_pwd.txt and I type the following command:\n   cat docker_pwd.txt | docker login --username quercinigia --password-stdin  I tag the image to upload with my DockerHub username, as follows:  docker image tag test_image quercinigia/test_image:1.0  Finally, I push the image to the DockerHub registry:  docker image push quercinigia/test_image:1.0  Write a documentation The documentation must be concise (max 4 pages). It must include:\n Your name and family name. Short description of the application that you chose. The description of the Dockerfile, specifying the meaning of each instruction. The instructions that I have to follow to pull your image(s) from the DockerHub registry and run them.  The last point is particularly important as I have to verify that everything works as expected.\n Submit your project You’ll submit your project on Edunao (click on this link).\nDeadline. June 6, 2020, 23:59.\nDepending on the application that you chose, two cases are possible:\nYour application consists of only one Docker image. In this case, you may just upload the documentation in PDF.\n You application consists of many images and you have created a docker-compose file, or you need to send additional files. In this case, you can upload an archive file (accepted formats include .zip .7z .bdoc .cdoc .ddoc .gtar .gz .gzip .hqx .rar .sit .tar .tgz), containing the documentation in PDF as well as the other files.\n  In any case, do not submit the source code of your application.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c1fa1ef2c9d9118abb69e237496d95f","permalink":"/courses/cloud-computing/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Project","type":"docs"},{"authors":null,"categories":null,"content":" Lab assignment 1 Title: MapReduce - Spark\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Tuesday 19 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nRemark: This lab assignment will be graded\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6e75e11c1ee6ba55693a9622583beaf9","permalink":"/courses/cloud-computing/labs/cc-labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/labs/cc-labs/","section":"courses","summary":"Presentation of the lab assignments of the course.","tags":null,"title":"Lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Cloud Computing Introduction\nLecturer: Wilfried Kirschenmann (ANEO)\nDate and time: Friday 24 April 2020, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 2 Title: Virtualization and Containerization\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Thursday 30 April, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 3 Title: Introduction to MapReduce\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Thursday 7 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao (Part 1, Part 2, Part 3)\n Lecture 4 Title: Introduction to Hadoop and Spark\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Friday 15 May, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao (Part 1, Part 2, Part 3, Part 4, Part 5).\n Lecture 5 Title: Multi-service applications\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Wednesday 20 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f5a608bccc37bec35254441f7fcae9a","permalink":"/courses/cloud-computing/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Class material  Lecture slides, available here.\n An introduction to Docker.\n A Docker cheat sheet, with a summary of the most important Docker commands.\n An introduction to Linux, useful to understand how Docker works and how to interact with Docker.\n   Books  Schenker, Gabriel. Learn Docker - Fundamentals of Docker 18.x. Packt Publishing,. Print.\n Surianarayanan, C., \u0026amp; Chelliah, P. R. (2019). Essentials of Cloud Computing: A Holistic Perspective. Springer Nature.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d57f199c44c3345bf5285f7abd8f4c9","permalink":"/courses/cloud-computing/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Soon available.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85d2d631269d7307bfde5c4b0669c155","permalink":"/courses/databases/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":" Introduction Slides: Available here.\n Lecture 1 Title: An introduction to database systems and data modeling.\nDate and time: Tuesday 15 September 2020, 9:00 AM - 12 PM.\nSlides: Available here.\n Lecture 2 Title: Normalization theory.\nDate and time: Tuesday 29 September 2020, 9:00 AM - 12 PM.\nSlides: Available here.\n Lecture 3 Title: Relational database management systems.\nDate and time: Tuesday 6 October 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n Lecture 4 Title: Advanced database concepts.\nDate and time: Tuesday 13 October 2020, 9:00 AM - 12 AM.\nSlides: Available soon.\n Lecture 5 Title: Distributed databases and NoSQL.\nDate and time: Tuesday 20 October 2020, 9:00 AM - 12 AM.\nSlides: Available soon.\n Lecture 6 Title: Document-oriented databases: MongoDB.\nDate and time: Tuesday 3 November 2020, 9:00 AM - 12 AM.\nSlides: Available soon.\n Lecture 7 Title: Graph databases: Neo4j.\nDate and time: Tuesday 10 November 2020, 9:00 AM - 12 AM.\nSlides: Available soon.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f967524fae94aa5965fe18163acab25","permalink":"/courses/databases/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Date, Christopher John. An introduction to database systems. Pearson Education India, 2004.\n Hoffer, Jeffrey A., Venkataraman Ramesh, and Heikki Topi. Modern database management. Pearson, 2016\n Garcia-Molina, Hector, Jeffrey D. Ullman and Jennifer Widom. Database systems: the complete book. Pearson Education India, 2008.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Robinson, Ian, Jim Webber, and Emil Eifrem. Graph databases. \u0026quot; O’Reilly Media, Inc.\u0026quot;, 2013.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"074436047baef381bdf6909058220908","permalink":"/courses/databases/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL\nExample. Building an image from a Dockerfile in the current directory: docker build .\n The command assumes that a file named Dockerfile is in the current directory.  Common options:\n  Tag the image   Name of the Dockerfile       docker build -t my-image:latest .   docker build -f my-dockerfile .       Manage images   List all images   List images (no intermediate)   Remove an image     docker image ls -a   docker image ls   docker image rm my-image     Remove dangling images   Remove unused images   Show the history of an image     docker image prune   docker image prune -a   docker history my-image     Dockerfile In a Dockerfile the following main keywords are used:\n  FROM base-image   FROM scratch   RUN cmd     Specifies the base image   No base image used   Runs a command     COPY src dst   ADD src dst   WORKDIR dir     Copy source file to destination   Copy source file (including URL and TAR) to destination   Sets the working directory     ENTRYPOINT cmd   CMD params   EXPOSE port     Command to execute when container is run   Parameters of the entrypoint command   Exposes a container port     Volumes   Create a volume   Remove a volume   Remove unused volumes     docker volume create my-volume   docker volume rm my-volume   docker volume prune     List volumes         docker volume ls         Networks   Create a network   Remove a network   Remove unused networks     docker network create my-network   docker network rm my-network   docker network prune     List all the networks   Inspect a network   Connect a container to a network     docker network ls   docker network inspect my-network   docker network connect my-network my-container     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f5dc854a8f25cad521a576f1c221397","permalink":"/courses/cloud-computing/references/docker-cheat-sheet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/docker-cheat-sheet/","section":"courses","summary":"Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL","tags":null,"title":"Docker Cheat Sheet","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nDate and time: Wednesday 23 September 2020, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\n Lecture 2 Title: Hadoop and its ecosystem: storage and data processing.\nDate and time: Wednesday 7 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n Lecture 3 Title: Hadoop and its ecosystem: data access.\nDate and time: Wednesday 14 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n Lecture 4 Title: Introduction to Apache Spark.\nDate and time: Monday 19 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n Lecture 5 Title: Advanced Apache Spark.\nDate and time: Wednesday 21 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n Lecture 6 Title: NoSQL database systems: MongoDB.\nDate and time: Monday 25 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n Lecture 7 Title: Graph-oriented database systems: Neo4j.\nDate and time: Wednesday 28 October 2020, 8:30 AM - 11:45 AM.\nSlides: Available soon.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2df8cc01c3dc820657e60bdae93d56cd","permalink":"/courses/plp/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\n Mehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\n Karau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\n Giamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Scifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c41cb45dee07917944a15794098e978b","permalink":"/courses/plp/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model.  Prerequisites:\n Having attended Lecture 1.  1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1  Give the conceptual schema of the database with an ER diagram.\n   Solution\n  Exercise\nExercise 1.2  Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\n   Solution\nThe collection of tables is the following:\n UserAccount (nickname, first_name, last_name, city, country) Post (post_id, content, date, time, lat, long, nickname) EmailAddress (email_address, nickname) Relationship (nickname_src, nickname_dst, type, date) Tag (post_id, nickname)  The foreign keys are the following:\nPost(nickname) → UserAccount(nickname).\nEmailAddress(nickname) → EmailAddress(nickname).\nRelationship(nickname_src) → UserAccount(nickname).\nRelationship(nickname_dst) → UserAccount(nickname).\nTag(post_id) → Post(post_id).\nTag(nickname) → UserAccount(nickname).\n    2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\n Figure 2.1: The conceptual schema of the bank database  Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer.    Solution\nSince no two banks have the same code_bank or name, either property can be chosen as the primary key of the entity Bank. Both can be considered as valid candidate keys.\n  Exercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer.    Solution\nThe answer is no. While there aren’t any banks that have the same value for {code_bank, name}, two subsets ({code_bank} and {name}) are candidate keys.\n  Exercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises.    Solution\n  Exercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.    Solution\nThe collection of tables is the following:\n Bank (code_bank, name, address) Branch (branch_id, address, code_bank) Account (acct_nbr, acct_type, balance, branch_id, ssn) Loan (loan_nbr, loan_type, amount, branch_id, ssn) Customer (ssn, first_name, last_name, telephone, address)  The foreign keys are the following:\nBranch(code_bank) → Bank(code_bank).\nAccount(branch_id) → Branch(branch_id).\nAccount(ssn) → Customer(ssn).\nLoan(branch_id) → Branch(branch_id).\nLoan(ssn) → Customer(ssn).\n    3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, colour, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\n A salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys.  When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram.    Solution\n  Exercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.     Solution\nThe collection of tables is the following:\n Car (serial_number, make, model, colour, is_new, owner_id) Customer (cust_id, cust_first_name, cust_last_name, cust_phone) Invoice (invoice_number, date, price, car_serial_number, sp_id) Salesperson (sp_id, sp_first_name, sp_last_name) Mechanic (mec_id, mec_first_name, mec_last_name) Ticket (ticket_number, date_open, date_return, car_serial_number) Repair (ticket_number, mec_id)  The foreign keys are the following:\nCar(owner_id) → Customer(cust_id).\nInvoice(car_serial_number) → Car(serial_number).\nInvoice(sp_id) → Salesperson(sp_id).\nTicket(car_serial_number) → Car(serial_number).\nRepair(ticket_number) → Ticket(ticket_number).\nRepair(mec_id) → Mechanic(mec_id).\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85c2215a0f57e47f0e3f4cc4fba8d167","permalink":"/courses/databases/tutorials/data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/data-modeling/","section":"courses","summary":"Description of the data modeling tutorial.","tags":null,"title":"Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to run containers. How to define and build images. How to create and use volumes. How to define and use networks.  Prerequisites:\n Having installed Docker on your computer. See the installation guide. Being familiar with the notions of containers, images, volumes and networks in Docker. See the Docker primer for an introduction. Being familiar with the basic notions of Linux. Don’t hesitate to look at the Docker cheat sheet to verify the syntax of the Docker commands.  Terminology\n You’ll use the terminal to run Docker commands. Referring to the Docker architecture, the terminal is the client that communicates with the Docker daemon.\n Docker runs containers on your computer. We’ll refer to your computer as the host, the containers being the guests.\n   A containerized application is an application running in a container.    1 Running containers The command used to run a container is docker run followed by four parameters:\ndocker run [options] image-name [command] [arg] The four parameters are:\n options. List of options. image-name. The fully qualified name of the image used to run the container. command. The command to be executed in the container. arg. The arguments taken by the command executed in the container.  Only the parameter image-name is mandatory. The fully qualified name of an image is specified as a sequence of four fields, formatted as follows:\nregistry_url/user/name:tag where:\n registry_url (optional). The URL of the registry that provides the image. If its value is not specified, the image will be looked up for in the DockerHub registry. user (optional). The identifier of the user or organization that created the image. The default value is library. name (mandatory). The name of the image. tag (optional). It specifies the image version. If its value is not specified, the tag latest is used, pointing to the latest image version.  Exercise\nExercise 1.1  For each of the following images, specify the registry name, the user, the name and the tag.\nregistry.redhat.io/rhel8/mysql-80\n alpine:3.11\n  alpine      Solution\nRegistry: registry.redhat.io, user: rhel8, name: mysql-80, tag: latest\n Registry: DockerHub, user: library, name: alpine, tag: 3.11\n Registry: DockerHub, user: library, name: alpine, tag: latest    Exercise\nExercise 1.2 What’s the difference between the following image names?\nalpine:latest\n registry.hub.docker.com/library/alpine\n  alpine      Solution\nThere’s no difference. They all point to the same image, that is the latest version of alpine in the DockerHub registry.\n  We now learn how to use the command docker run and some of its options. In the following exercises, we’ll run containers from the image named alpine that is available on the DockerHub registry. This image provides a lightweight distribution (i.e., it doesn’t contain many features) of Linux.\nExercise\nExercise 1.3 You want to run the container from the latest version of the image alpine. Which command would you write in the terminal?    Solution\nThe goal of this exercise is to start playing with the docker run command. Since the question doesn’t say anything about the options, nor does it mention the command to run inside the container, we’d type:\ndocker run alpine   Exercise\nExercise 1.4 Execute the command that you proposed in the previous exercise, observe the output in the terminal and explain the actions taken by Docker to run the container.    Solution\nThe output obtained from executing the command should look like as follows:\nUnable to find image \u0026#39;alpine:latest\u0026#39; locally latest: Pulling from library/alpine aad63a933944: Pull complete Digest: sha256:b276d875eeed9c7d3f1cfa7edb06b22ed22b14219a7d67c52c56612330348239 Status: Downloaded newer image for alpine:latest Here’s what happens under the hood:\nDocker looks for an image named alpine:latest in the host computer and cannot find it.\n Docker pulls (i.e., downloads) the image from the DockerHub registry.\n    Okay but where’s the result of running the container?\nFirst of all, let’s see if the container is still running in the first place. In order to list all containers still running on the host, type the following command:\ndocker container ls  Your container shouldn’t appear in the output, because it’s not running. In order to see all containers, including those that are not running, type the following command:\ndocker container ls -a Exercise\nExercise 1.5 What information is displayed for each container?    Solution\n The identifier of the container.\n The name of the image used to run the container (it should be alpine for your container).\n The command executed within the container (it should be /bin/sh for your container).\n When the container has been created.\n The container current status (it should be exited (0) x seconds ago for your container).\n The network ports used by the container (we’ll study them later).\n The name of the container. If you don’t specify any when you run the container (as is our case), Docker generates a random name by concatenating an adjective and a famous scientist’s name (e.g., agitated_newton).\n    Exercise\nExercise 1.6 By looking at the command executed within the container (/bin/sh), can you tell why the container stopped without giving any output?    Solution\nThe command is /bin/sh; the container runs a Linux terminal. But since we didn’t specify what to do with that terminal (we didn’t run any Linux command, nor we tried to access the terminal), the container stopped.\n  We’re now going to do something useful with the image alpine. But first, we start with some good practices that you should adopt while playing with images and containers.\n1.1 Good practices Name your containers. Although Docker assigns a default name to a new container, it’s usually a good practice to give a container a name of your choice to make it easily distinguishable. You can do it by using the option --name. Try the following:  docker run --name my-alpine alpine As before, the container stops immediately. If you list all your containers by typing again:\ndocker container ls -a you should see a container named my-alpine.\nRemove automatically a container if you use it once. Unless you want to reuse your container later, you can ask Docker to automatically remove it when it stops by using the option --rm. This will prevent unused containers from taking up too much disk space.  Try the following:\ndocker run --rm --name container-to-remove alpine If you list all the containers you should see that there is no container named container-to-remove.\nRemove unused containers. Stopped containers that have been run without using the option --rm are still stored in the host. If you want to remove a specific container (e.g., my-alpine), use the following command:  docker container rm my-alpine If you want to remove all stopped containers, use the following command:\ndocker container prune Remove unused images. Images can take up a lot of disk space. As a result, you should remember to remove those that you don’t intend to use any longer. The commands to remove a specific image and prune unused ones are docker image rm and docker image prune -a respectively.   1.2 Pass a command to the containerized application Remember that the template of docker run is the following:\ndocker run [options] image-name [command] [arg] The optional parameter command refers to a command that you can pass the containerized application, possibly with some arguments (parameter arg).\nLet’s see an example. As we saw before, when we run a container from the image alpine, a Linux terminal /bin/sh is launched.\nNotice\nThe Linux terminal /bin/sh is run within the container. Henceforth, we’ll use the following terms:\n Host terminal. The terminal that you use to interact with the operating system of your computer.   Guest terminal. The terminal that is run within the container.    By using the optional parameter command, we can run a command in the guest terminal.\nExercise\nExercise 1.7  Run a container from the image alpine and execute the Linux command ls that lists the content of the current directory.\n Where are the listed files stored? In the host or in the container?      Solution\ndocker run --rm --name ls-test alpine ls  The command ls is run in the guest terminal, therefore what we see in the output is a list of files stored in the container.    Notice\nIn Exercise 1.7 the command ls is executed in the guest terminal, but its output is redirected to the host terminal.\nIn other words, when we run the container, we don’t interact directly with the guest terminal; we just send a command and the output is redirected to the host terminal.\n Now let’s see how to execute a command in the guest terminal that also requires an argument.\nExercise\nExercise 1.8 By using the Linux utility ping, check whether the Web site www.centralesupelec.fr is reachable.     Solution\ndocker run --rm --name ping-test alpine ping www.centralesupelec.fr In order to interrupt ping just type the key combination that you’s use to interrupt any other command in your terminal. (typically Ctrl-C on Windows and Cmd-C in MacOs).\n   1.3 Interacting with a container An application running in a container might need to interact with the user. For instance, the Linux command rev reverses whatever the user types on the keyboard. In order to interact with a container, you should use the option -it of docker run.\nExercise\nExercise 1.9 Run a container from the image alpine to execute the Linux command rev and interact with it. You can stop interacting with rev by typing Ctrl+C at any time.    Solution\ndocker run --rm --name rev -it alpine rev After typing the command, type a word on your keyboard (e.g., deeps), press Return and you should see the same word reversed (e.g., speed).\nThe option -t opens a guest terminal (so we can see its output); the option -i allows you to write directly into the guest terminal.\nIn order to stop using the guest terminal, you’ll need to press Ctrl+D (both in Windows and MacOs).\n  Now run the following command:\ndocker run --name my-alpine -it alpine Note: we didn’t use the option --rm (the container will not be removed when we stop it, we’re going to use it again). Moreover, we didn’t specify any command to run in the guest terminal.\nExercise\nExercise 1.10 What do you obtain?    Solution\nWhen we run a container from the image alpine, the command /bin/sh is executed within the container. Since we specified the option -it, what we obtain is an access to the Linux terminal running in the container.\n   1.4 Starting and stopping containers. docker run is a shorthand for two Docker commands, namely docker create, that creates a container from an image, and docker start, that starts the container after its creation.\nSuppose now that you want to download a Web page by using Linux Alpine. You can use the Linux command wget followed by the URL of the page that you want to download.\nExercise\nExercise 1.11 By using the guest terminal in the container my-alpine, download this Web page.\n Where will the Web page be saved? The host computer or the container?      Solution\nJust type in my-alpine guest terminal the following command:\nwget https://www.centralesupelec.fr/fr/presentation  The Web page will be saved in the current directory of the container. You can verify that the file is there by typing ls in the guest terminal.    In my-alpine guest terminal type exit. This closes the guest terminal and, as a result, stops the container.\nNOTICE\nStopping the container will not erase any of the files stored in the container. Removing the container will.\n If you want to start the container my-alpine again, you can use the following command:\ndocker container start -ai my-alpine This will open the guest terminal of the container again; type ls to verify that the Web page that you downloaded before is still there.\n 1.5 A simple use case Suppose that you need to download all the figures of this Web page. The Linux utility wget comes in handy. However, you don’t have Linux and you’d like to avoid the hassle of installing it on your computer, or in a virtual machine, just for this task.\nA great alternative is to run Linux in a Docker container. Unfortunately, the Alpine distribution that we’ve been playing with doesn’t provide an implementation of wget with all the options that we need.\nWe turn to another Linux distribution, Ubuntu, for which DockerHub has several images.\nExercise\nExercise 1.12 Run a container with Ubuntu 19.10 and open a guest terminal. Call the container dl-figures, and avoid the option --rm, we’ll use this container later.    Solution\ndocker run --name dl-figures -it ubuntu:19.10 If you look at the DockerHub registry Web page describing Ubuntu, you’ll see that the version 19.10 has many tags, including 19.10, eoan-20200313, eoan, rolling. You can use any of these tags to download the image. Another way to write the previous command is:\ndocker run --name dl-figures -it ubuntu:eoan   From now on, we’ll be interacting with the guest Ubuntu terminal. If you type the command wget, you’ll get an error (bash: wget: command not found).\nNotice\nThe image Ubuntu doesn’t include all the commands that you’d find in a full-blown Ubuntu distribution; the reason is to keep the size of the image small, a necessary constraint given that images are transferred over the Internet.\n Luckily, there’s a way to install wget in our Ubuntu distribution. Ubuntu provides a powerful command-line package manager called Advanced Package Tool (APT). First, you need to run the following command:\napt-get update which fetches the available packages from a list of sources available in file /etc/apt/sources.list.\nThen, you can install wget by running the following command:\napt-get install -y wget In order to obtain all the figures from a Web page, type the following command:\nwget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation You should see in the current directory a new folder named my-figures containing the downloaded figures; verify it by typing ls my-figures.\nBefore terminating, don’t forget to read your fortune cookie. In the shell, run the following command:\napt-get install -y fortune and then:\n/usr/games/fortune -s When you’re done, you can simply type the command exit to quit the guest terminal and stop the container.\nNotice\nYou might wonder how you can transfer the downloaded figures from the container to the host computer. We’ll see that later when we introduce the notion of volumes.\n   2 Creating Images A Docker image can be thought of as a template to create and run a container. An image is a file that contains a layered filesystem with each layer being immutable; this means that the files that belong to a layer cannot be modified or deleted, nor can files be added to a layer.\nWhen a container is created from an image, it will be composed of all the image read-only layers and, on top of them, a writable layer (termed the container layer), where all the new files created in the container will be written. For example, the figures that we downloaded in the container dl-figures were stored in the writable layer of that container.\n2.1 Interactive image creation When we run the container dl-figures in Section 1.5, we modified the container to install the command wget. You can see the modifications by typing the following command:\ndocker diff dl-figures The output consists of a list of files tagged with the letter A, C or D, indicating respectively that the file has been added (A), changed (C) or deleted (D). In this list you’ll find the downloaded figures, as well as other files that have been added or modified or deleted when we installed wget.\nExercise\nExercise 2.1 If layers, except the top one, are immutable, how can files that belong to the lower layers be modified or deleted?    Solution\nAll files marked with A are new and therefore are added to the writable layer of the container.\nAs for the existing files, they live in the immutable layers of the image, and therefore cannot be touched directly. Instead, they are copied from the bottom layers to the writable layer where they are modified. This strategy is called copy-on-write.\nThe structure of layers generates a layered filesystem in the image; if different copies of the same file exist in different layers, the copy in the uppermost layer overwrites the others.\n  We can create a new image from the container dl-figures, one that provides a Ubuntu distribution with the command wget already installed, with the following command:\ndocker commit dl-figures ubuntu-with-wget The command creates a new image called ubuntu-with-wget.\nExercise\nExercise 2.2 Run a container from the image ubuntu-with-wget and verify that the command wget is actually installed.    Solution\nJust type the following command:\ndocker run --rm -it ubuntu-with-wget In the guest terminal type wget: you should see the following output:\nwget: missing URL Usage: wget [OPTION]... [URL]... Try `wget --help` for more options.    2.2 Dockerfiles The interactive creation of an image is a manual, and therefore inefficient, process. The most common way to create an image is to use a Dockerfile, a text file that contains all the instructions necessary to build the image. The advantage of the Dockerfile is that it can be interpreted by the Docker engine, which makes the creation of images an automated and repeatable task.\nInspired by the previous example, suppose that we want to create a containerized application to download figures from a Web page. As a template for this application, we need to build a new image, that we’ll call fig-downloader.\nThe Dockerfile containing the instructions to build the image fig-downloader is as follows:\nFROM ubuntu:eoan RUN apt-get update RUN apt-get install -y wget RUN mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] Here’s the explanation:\nWe use the image ubuntu:eoan as the base image. This corresponds to the instruction FROM ubuntu:eoan.\n We install the utility wget in the base image. This corresponds to the instructions RUN apt-get update and RUN apt-get install -y wget.\n We create a directory my-figures under the root directory of the image. This corresponds to the instruction RUN mkdir -p /my-figures.\n We set the newly created directory /my-figures as the working directory of the image. This corresponds to the instruction WORKDIR /my-figures.\n We specify the command to be executed when a container is run from this image. This corresponds to the instruction ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;]. This instruction means: execute wget with the options -nd, -r, -A; the last option takes a list of file extensions (jpg,jpeg,bmp,png,gif) as its argument.\n Remember that the utility wget takes the URL of the Web page as an argument. The URL will be specified when we run the container from the image fig-downloader. Optionally, we can specify a default argument by using the keyword CMD. The meaning of the instruction CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] is: if we don’t give any URL when we run the container, the figures will be downloaded from https://www.centralesupelec.fr/fr/presentation.\n  Exercise\nExercise 2.3 What’s the relation between the Dockerfile lines and the image layers?    Solution\nEach line corresponds to a new layer. The first line corresponds to the bottom layer; the last line to the top layer.\n  Exercise\nExercise 2.4 Could you identify a problem in this Dockerfile? Modify the Dockerfile accordingly.    Solution\nWhen creating an image, we should keep the number of layers relatively small; in fact, the more the layers, the bigger the image will be. Here we create three separate layers with three RUN commands; we can simply merge the three layers. The resulting Dockerfile will be:\nFROM ubuntu:eoan RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y wget \u0026amp;\u0026amp; \\ mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;]    2.3 Building an image We’re now going to build an image from a Dockerfile.\nCreate a directory named fig-downloader in your computer with a file named Dockerfile inside.\n In the Dockerfile write the set of instructions that you proposed in Exercise 2.4.\n In the terminal, set the working directory to fig-downloader.\n Build an image called fig-downloader by executing the following command:\n  docker build -t fig-downloader . The . at the end of the command means that the Docker engine will look for a file named Dockerfile in the working directory.\nExercise\nExercise 2.5 Once the image is built, type the command docker image ls -a. What are the images with repository and tag \u0026lt;none\u0026gt;? Why are there three of such images?     Solution\nThese are the intermediate images. Once a layer is compiled, an intermediate image is created that contains that layer and all the layers underneath. In other words, the intermediate image corresponding to the layer \\(i\\) contains all files up to the layer \\(i\\), including layers 1 through \\(i-1\\).\nThe intermediate layers are used by the build cache, of which we’ll see an example later.\nAlthough there are five layers in the new image, there are only three intermediate images because:\n the base image is ubuntu:eoan. the image corresponding to the top layer is the final image fig-downloader.    Good to know\nIf you give the Dockerfile a different name (say, Dockerfile-fig-downloader), the command to build the image will be:\ndocker build -t fig-downloader -f Dockerfile-fig-downloader . The option -f is used to specify the name of the Dockerfile.\n Let’s dive deeper into the anatomy of an image.\nExercise\nExercise 2.6 Run the following command:\ndocker history fig-downloader\nand analyze the layers of the new image.\n Why do some layers have an ID, while others are marked as missing?   Can you find the identifiers of the intermediate images?      Solution\nThe layers with an ID correspond to the layers of the new image, including the top layer and the base image. The layers marked as missing are those that compose the base image. Those layers are not stored in\nyour computer, simply because they belong to an image that hasn’t been built on your computer and you downloaded from the DockerHub registry.\nBy looking at the output of docker image ls -a and the output of this command, we see that the layers between the base image and the top layer have the same identifiers as the intermediate images.\n  Exercise\nExercise 2.7 Run the following command:\ndocker run --name dl-1 fig-downloader\nWhat does it do? Where are the downloaded pictures?    Solution\nWe downloaded the figures of the page https://www.centralesupelec.fr/fr/presentation. The downloaded pictures are in the folder /my-figures of the container dl-1. For now, don’t worry about accessing them.\n  Exercise\nExercise 2.8 Run the following command:\ndocker run --name dl-2 fig-downloader https://www.centralesupelec.fr/fr/nos-campus What does it do? Where are the downloaded pictures?    Solution\nWe downloaded the figures of the page https://www.centralesupelec.fr/fr/nos-campus. We basically overwrote the URL specified by the CMD keyword with a new one. The downloaded pictures are in the folder /my-figures of the container dl-2. For now, don’t worry about accessing these figures.\n   2.4 Containerized Python application Download this archive file and unzip it into your working directory. In this archive you’ll find:\n A Dockerfile. A Python script main.py that asks the user to enter the URL and the language of a Web page, and prints the 10 most frequent words occurring in that page. A file requirements.txt with the list of the Python packages needed to run the given script.  The content of the Dockerfile is as follows:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./main.py ./requirements.txt /app/ RUN pip install -r requirements.txt ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.9 Describe what this Dockerfile does.    Solution\n Takes python:3.7-slim as the base image. Creates a new folder app in the image under the root directory. Changes the working directory to /app. Copies the files main.py and requirements.txt from the local computer to the directory /app in the image. Runs the command pip install to install the Python libraries specified in the file requirements.txt. Executes the command python main.py.    Exercise\nExercise 2.10 Build an image called wordfreq from this Dockerfile.    Solution\ndocker build -t wordfreq .   Exercise\nExercise 2.11 Without changing the Dockerfile, rebuild the same image. What do you notice?    Solution\nThe build is very fast. Since we didn’t change the Dockerfile, the image is rebuilt by using the image layers created previously. This is clearly indicated by the phrase using cache written at each layer. Using the already stored layers is called build cache.\n  Exercise\nExercise 2.12 What happens if you modify a line in the Python script and you rebuild the image? Is the build cache still used?    Solution\nAdd any instruction at the end of main.py, such as:\nprint(\u0026quot;I added this line\u0026quot;) then rebuild the image. The three bottom layers are not affected by the modification, therefore they benefit from the build cache. Layer 4 is the first affected by the modification. This layer, and those above, need therefore to be rebuilt.\n  Exercise\nExercise 2.13 Considering how the build cache is used in Docker, can you tell what’s wrong with this Dockerfile? Modify the Dockerfile accordingly and rebuild the image.    Solution\nEach time we modify main.py and we rebuild the image, the layer 4 and 5 are recreated, meaning that all the Python packages are downloaded and installed. Depending on the size and number of the packages, this might take some while. A better way to structure the Dockerfile is to install the packages before copying the Python script to the image. Here is how we should modify the Dockerfile:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./requirements.txt /app/ RUN pip install -r requirements.txt COPY ./main.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;]   Exercise\nExercise 2.14 Modify main.py by adding a new line of code and rebuild the image. What changed?    Solution\nThe Python packages are not reinstalled, as a result rebuilding the image\ntakes much less time than before.\n    3 Data Volumes In Exercise 2.7 you’ve been asked to run a container named dl-1 to download some figures from a Web page. The figures were downloaded into the directory /my-figures of the container. But we left a question unanswered.\nHow do we transfer those figures from the container to the host computer?\nOne way to go about that is to run the following command in the host terminal:\ndocker cp dl-1:/my-figures . This will copy the directory /my-figures from the container dl-1 to the host computer working directory. You can verify it by yourself.\nExercise\nExercise 3.1 Can you tell why this solution is less than ideal?    Solution\nAfter running the container we need to do an additional action to copy the figures from the container to the host computer.\n The container is created and run only to download some figures. We’d like to remove it automatically (with the option --rm) when its execution is over. However, if we do so, the pictures will be lost before we can copy them to the host computer.\n    3.1 Using a host volume A better solution is to mount (i.e., attach) a directory of the host computer at the container’s directory /my-figures when we run it. Let’s see how it works.\nStep 1. Create a directory named figs-volume in your working directory.\nStep 2. Type and execute the following command:\ndocker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader This command runs a container from the image fig-downloader.\n With the option -v we specify that we want to mount the directory $(pwd)/figs-volume ($(pwd) indicates the host working directory) at the directory figs-volume in the container;\n The option --rm indicates that we want the container to be removed when its execution is over.\n  Step 3. Verify that the pictures are in the folder figs-volume.\nIn this example, we’ve used the directory figs-volume as a volume (essentially, an external storage area) of the container; when the container is destroyed, the volume remains with all its data.\n 3.2 Docker volumes In the example that we’ve just described, we’ve used a host directory as a volume. This is useful when we, or an application running on the host, need to access the files produced by a container. In all the other cases, a container should use a Docker volume, which is managed directly by the Docker engine.\nLet’s create a new Docker volume called data-volume:\ndocker volume create data-volume Good to know (advanced notion)\nWhere the data will be actually stored?\nYou can inspect the new volume by typing the following command:\ndocker volume inspect data-volume A mount point is indicated; that’s the folder where the data will be actually stored. If your computer runs Linux, that folder will be available on the host; if your computer runs Windows or MacOS, you’ll not find that folder on your computer. Instead, it will be available in the virtual machine that Docker use on MacOS and Windows.\nDo you want to see the directory? (Instructions for MacOS)\nOne way to look into the hidden VM is to run the following containerized application:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1 This application will open a guest terminal into the VM. You can then use the commands cd and ls to browse to the directory indicated as the mount path of the new volume.\n 3.2.1 Sharing data A Docker volume can be used to share data between containers.\nExercise\nExercise 3.2 Run a container from the image ubuntu:eoan, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume at the container’s directory /data.      Solution\ndocker run --rm -it -v data-volume:/data ubuntu:eoan    Exercise\nExercise 3.3 Type a command in the guest Linux terminal to create a file test-file.txt in the directory /data. Verify that the file is created.    Solution\nThe following command:\necho \u0026quot;This is a new file\u0026quot; \u0026gt; /data/test-file.txt creates a file test-file.txt with the line of text “This is a new file”.\nIn order to verify that the file is created, we can type the following command:\nls /data/test-file.txt To see the content of the file, we can type:\ncat /data/test-file.txt   Exercise\nExercise 3.4 Run a container from the image alpine:latest, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume to the directory /my-data of the container.      Solution\ndocker container run --rm -it -v data-volume:/my-data alpine   Exercise\nExercise 3.5 Verify that you can read the file test-file.txt. Which folder would you look in?    Solution\nWe need to look in the folder /my-data because this is where we mounted data-volume.\ncat /my-data/test-file.txt   In the guest terminals of both containers type exit. This will terminate and destroy (since we used the option --rm) the containers.\nExercise\nExercise 3.6 Will the file test-file.txt be removed? Why?    Solution\nNo. The file that we created before has been saved in the volume data-volume. Volumes are a way to persist data beyond the life span of a container.\n     4 Single-Host Networking In order to let containers communicate and, therefore, co-operate, Docker defines a simple networking model known as the container network model\nExercise\nExercise 4.1 Describe the output of the following command:\ndocker network ls\n   Solution\nThe command lists all the networks created by Docker on your computer. For each network, the values of four attributes are shown:\n The identifier. The name. The driver used by the network. The scope of the network (local or global). A local scope means that the network connects containers running on the same host, as opposed to a global scope that means that containers on different hosts can communicate.  Depending on the containers that you used in the past, you might see different networks. However, three networks are worth noting:\n The network named bridge, that uses the driver bridge and a local scope. By default, any new container is attached to this network. The network named host, that uses the driver host and a local scope. It’s used when we want a container to directly use the network interface of the host. It’s important to remember that this network should only be used when analyzing the host’s network traffic. In the other cases, using this network exposes the container to all sorts of security risks. The network named none, that uses the driver null and a local scope. Attaching a container to this network means that the container isn’t connected to any network, and therefore it’s completely isolated.    Exercise\nExercise 4.2 The following command:\ndocker network inspect bridge\noutputs the configuration of the network bridge. By looking at this configuration, can you tell what IP addresses will be given to the containers attached to this network? What’s the IP address of the router of this network?    Solution\nThe information is specified in the field named IPAM, more specifically:\n Subnet indicates the range of IP addresses used by the network. The value of this field should be 172.17.0.0/16; the addresses range from 172.17.0.1 to 172.17.255.255.\n Gateway indicates the IP address of the router of the network. The value should be 172.17.0.1\n    4.1 Creating networks By default, any new container is attached to the network named bridge. As a result, all new containers will be able to communicate over this network. This is not a good idea. If a hacker can compromise any of these containers, s/he might be able to attack the other containers as well. As a rule of thumb, we should attach two containers to the same network only on a need-to-communicate basis.\nExercise 4.3 What if a container doesn’t need to use the network at all? Try to run a container from the image alpine disconnected from all networks and verify that you cannot ping the URL www.google.com.\nLook at the Docker cheat sheet to learn how to attach a container to a network.    Solution\nWe should attach the container to the network none. As an example, we run the following command:\ndocker run --rm -it --network none alpine /bin/sh Then we try to ping www.google.com as follows:\nping www.google.com We should obtain the following message:\nbad address \u0026#39;www.google.com\u0026#39; Type the command exit to quit the container.\nInstead, if we run Linux Alpine without specifying the network (meaning that the container will be attached to the network bridge):\ndocker run --rm -it alpine /bin/sh and we try to ping www.google.com, we should get an answer. In some cases, the command ping would just hang and show no output; this is usually fixed by restarting Docker.\n  In order to create a new network, you can use the following command:\ndocker network create network_name Exercise\nExercise 4.4 Create two networks named buckingham and rochefort that use the driver bridge. By using the docker network inspect command, look at the IP addresses of the new networks and write them down.    Solution\nJust run the following commands:\ndocker network create buckingham docker network create rochefort The IP addresses for the network buckingham are 172.18.0.0/16 (addresses from 172.18.0.1 to 172.18.255.255); The IP addresses for the network rochefort are: 172.19.0.0/16 (assuming that you create buckingham before rochefort).\nThe IP addresses may be different on your machines.\n  Exercise\nExercise 4.5 Create three containers athos, porthos and aramis and attach them to the two networks buckingham and rochefort as displayed in this figure. The three containers will open a Linux Alpine shell. You’ll need to launch the commands in three separate tabs of your terminal window.\n What will the IP addresses of the three containers be in the two networks? Remember that porthos is attached to two networks, therefore it’ll have two network interfaces (endpoints) and, as a result, two IP addresses.   Verify your answers by inspecting the two networks (use the command docker network inspect).      Solution\nHere are the commands to run athos and aramis while connecting them to buckingham and rochefort respectively.\ndocker run --rm -it --name athos --network buckingham alpine /bin/sh docker run --rm -it --name aramis --network rochefort alpine /bin/sh Here’s the command to run porthos and attach it to buckingham:\ndocker run --rm -it --name porthos --network buckingham alpine /bin/sh The following command attaches porthos to the second network rochefort:\ndocker network connect rochefort porthos As for the IP addresses, each network has IP addresses in the range 172.x.0.0/16, where x is 18 in the network buckingham and 19 in the network rochefort. The address 172.x.0.1 is reserved for the router. Therefore, the containers will be assigned IP addresses from 172.x.0.2. In this solution, we created athos, aramis and portos in this order. Therefore, the IP addresses will be:\n In network buckingham:  athos: 172.18.0.2 porthos: 172.18.0.3  In network rochefort:  aramis: 172.19.0.2 porthos: 172.19.0.3   You can actually verify this configuration by inspecting the two networks with the following commands:\ndocker network inspect buckingham docker network inspect rochefort The IP addresses might be different on your machines.\n   4.2 Communication between containers Let’s see if and when the three containers can communicate.\nExercise\nExercise 4.6 Which containers are able to communicate? Justify your answer.    Solution\nThe only containers that cannot communicate are athos and aramis, because they’re not connected to the same network.\n  Exercise\nExercise 4.7 Try to ping porthos from athos by using its IP address.\n Which IP address of porthos would you use?      Solution\nWe need to use the IP address assigned to the endpoint linking porthos to the network buckingham, to which athos is connected. In our case, this is 172.18.0.3.\n  Exercise\nExercise 4.8 Try to ping porthos from athos by using its name. Do you succeed? Are you surprised?    Solution\nWe succeed. Indeed, the network buckingham provides a DNS server, that can translate names into IP addresses.\n  You can now exit the three containers.\n 4.3 A containerized chat room We developed a simple chat room in Python that you can download here.\nParticipants use a client program to connect to the chat room; the chat room is managed by a server application that receives the client connections and forwards the messages between the users. The archive contains the following files:\n client.py. Implementation of the chat room client. server.py. Implementation of the chat room server. utils.py. Library with utility functions used in both client.py and server.py.  Exercise\nExercise 4.9 By using Dockerfiles, create two images chat-client and chat-server that will be used to run the client and the server in Docker.    Solution\nThe Dockerfile for the client (let’s call it Dockerfile-client) is as follows.\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./client.py ./utils.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;client.py\u0026quot;] We build the image with the following command:\ndocker build -t chat-client -f Dockerfile-client . The Dockerfile for the server (let’s call it Dockerfile-server) is as follows.\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./server.py ./utils.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;server.py\u0026quot;] We build the image with the following command:\ndocker build -t chat-server -f Dockerfile-server . Good to know\nThe first three layers in both images are identical. Therefore, when building the second image chat-server the Docker engine reuses the cached layers created for the first image. This is indicated in the output of the docker build command with the phrase using cache.\n   We’ll now run both containers. Since they need to communicate, they need to be attached to the same network (e.g., buckingham).\nExercise\nExercise 4.10 Run a container from the image server-chat. Set the options to:\n Automatically remove the container once its execution is over.\n Give the container a name (e.g., server-chat).\n The server will print messages on the screen. In order to see them, you must use the option -t.\n  Also, keep in mind that server.py takes an argument that is the port number where the server will listen to incoming connections. Choose a random port number in the interval [49152-65535].\nWhat is the IP address of the server?    Solution\nWe execute the following command:\ndocker container run --rm -t --name chat-server --network buckingham chat-server 64903 In my case, the IP address is 172.18.0.2 and port number is 64903\n  Exercise\nExercise 4.11  Run a container from the image client-chat. Set the options to:\n Automatically remove the container once its execution is over.\n Give the container a name (e.g., client-chat).\n Since you’ll use the client to write messages in the chat room, remember to set the option -it.\n  The client takes two arguments: the host where the server is running and the port which the server is listening to.    Solution\ndocker run --rm -it --name chat-client --network buckingham chat-client 172.18.0.2 64903 Instead of the server host IP address, we can use the server container name (the network buckingham has a DNS server).\nAs a result, we can run the client as follows:\ndocker run --rm -it --name chat-client --network buckingham chat-client chat-server 64903   Once the client is started, you’ll be prompted to enter your name. Then you can start writing messages.\nNotice\n You can type #quit at any moment to exit the chat room (client-side).\n Type Ctrl-C to stop the server.\n   Now, suppose that one of your classmates wants to join the chat room, but s/he’s on another computer.\nExercise\nExercise 4.12 Do you think your classmate can connect to the containerized server running in your machine? Justify your answer.    Solution\nNo, s/he can’t. The two containers can communicate only if they’re connected to the same network.\n  What we need to do here is to expose our server to the outside world. The server runs in a container \\(c\\) that, in turns, runs on the host machine \\(h\\). The server listens to port \\(p_c\\) that is opened inside the container. We need to map port \\(p_c\\) to a port \\(p_h\\) in the host computer. This way, the classmate client will connect to the server by specifying the IP address of the host \\(h\\) (not \\(c\\)) and \\(p_h\\) as the port number.\nExercise\nExercise 4.13 Stop both the server and the client.\nRun the server by specifying the option to map port \\(p_c\\) (e.g., 64903) to port \\(p_h\\) (e.g., 8080). As before, attach the server to the network buckingham.    Solution\nWe need to use the option -p. The command to run the container is:\ndocker run --rm -it --name chat-server --network buckingham -p 8080:64903 chat-server 64903   Notice\nAssuming that the name of the server container is chat-server, run the following command:\ndocker container port chat-server The output should look like as follows:\n64903/tcp -\u0026gt; 0.0.0.0:8080 This means that the port 64903 (\\(p_c\\)) in the container is mapped to the port 8080 (\\(p_h\\)) of the host computer. When a remote client wants to connect to the server, it’ll use port 8080. The IP address 0.0.0.0 means that a client can connect to the server by using any of its IP addresses.\n If you’re on the same local network as one of your classmate, do Exercise 4.14\n If none of your classmates is on your local network, do Exercise 4.15\n  Exercise\nExercise 4.14 Ask your classmate to connect to your server. For this, you’ll need to tell your classmate the IP address of your machine and the port number \\(p_h\\).    Solution\nAssuming that the IP address of your machine is 192.168.1.8, the command will be the following:\ndocker run --rm -it --name chat-client chat-client 192.168.1.8 8080   Exercise\nExercise 4.15 Since none of your classmates is on your local network (COVID-19 be damned!), you’ll need to simulate a distant connection to the server running on your machine.\nSimply run an instance of the client without attaching it to the network buckingham. The IP address of the server will be the IP address of your machine; the port will be \\(p_h\\).\n   Solution\nAssuming that the IP address of your machine is 192.168.1.8, the command will be the following:\ndocker run --rm -it --name chat-client chat-client 192.168.1.8 8080     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a74fda64fd4d3f15083e937e46612453","permalink":"/courses/cloud-computing/tutorials/tutorial-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-docker/","section":"courses","summary":"Text of the lab assignment on Docker","tags":null,"title":"Getting started with Docker","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1  Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\n   Solution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).    Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2  What is the maximum number of measurements in a year?\n   Solution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\n  Exercise\nExercise 1.3  Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\n   Solution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\n  Exercise\nExercise 1.4  Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n   Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).     2 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 2.1  Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\n   Solution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\n  Exercise\nExercise 2.2  Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year.    Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n \\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\)     3 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 3.1  Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n   Solution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n \\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\).  We note that the reduce function is the identity.\n   4 Creating an inverted index We have a collection of \\(n\\) documents in a directory and we want to create an inverted index, one that associates each word to the list of the files the word occurs in. More precisely, for each word, the inverted index will have a list of the names of the documents that contain the word.\nExercise\nExercise 4.1  Propose a MapReduce implementation to create an inverted index over a collection of documents.\n   Solution\nThe input to the map will be a key-value pair, where the key is the name of a file \\(f\\) and the value is the content \\(C\\) of the file.\nmap: \\((f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]\\)\nreduce: \\((w, L) \\rightarrow (w, L)\\)\nwhere \\(L\\) is the list of the files containing the word \\(w\\).\nWe note that the reduce function is the identity.\nNote also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22d83831482b2a6a4aa5d6546c3dd9cd","permalink":"/courses/plp/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to obtain a non-redundant set of functional dependencies. How to determine the candidate keys of a table given its functional dependencies. How to determine the normal form of a table.  Prerequisites:\n Having attended Lecture 2.  1 Non-redundant functional dependencies Consider the following set \\(\\mathcal{F}\\) of functional dependencies:\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B, C\\)\n \\(A \\rightarrow B\\)\n  Exercise\nExercise 1.1  Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).\n   Solution\nFirst, we rewrite the FDs such that each has a singleton right side.\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n  The first has two attributes in the determinant, therefore we need to check whether we can eliminate one of the two columns.\nWe have \\(A \\rightarrow B\\). By augmentation we obtain:\n\\[A \\rightarrow A, B\\]\nWe have \\(A, B \\rightarrow C\\). By transitivity we obtain:\n\\[A \\rightarrow C\\]\nTherefore, the column \\(B\\) is not useful and we can drop it.\nThe set \\(\\mathcal{G}\\) consists of the following FDs:\n \\(A \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n     2 Candidate keys and normal forms (1) We consider the following table:\nPatient (ssn, first_name, last_name, phone_number, insurance_number, insurance_expiration_date) where the following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\[ \\begin{align} ssn \\rightarrow first\\_name, \u0026amp; last\\_name, phone\\_number, insurance\\_number, \\\\ \u0026amp; insurance\\_expiration\\_date \\end{align} \\]\n\\[ insurance\\_number \\rightarrow insurance\\_expiration\\_date \\]\nExercise\nExercise 2.1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).    Solution\nFirst, we need to rewrite the FDs such that each has a singleton right side.\n\\(ssn \\rightarrow first\\_name\\) \\(ssn \\rightarrow last\\_name\\) \\(ssn \\rightarrow phone\\_number\\) \\(ssn \\rightarrow insurance\\_number\\) \\(ssn \\rightarrow insurance\\_expiration\\_date\\) \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\)  The determinant of each FD is composed of only one column, therefore it is already irreducible. It is easy to see that all FDs with ssn as a determinant must be kept (otherwise we lose some information).\nBy transitivity from 4. and 6. we obtain:\n\\[ssn \\rightarrow insurance\\_expiration\\_date\\]\nTherefore, \\(\\mathcal{G}\\) is obtained from \\(\\mathcal{F}\\) by removing 5.\n  Exercise\nExercise 2.2 Given \\(\\mathcal{G}\\), identify the candidate keys in the table Patient.    Solution\nFrom the functional dependencies in \\(\\mathcal{F}\\), it’s easy to see that that the only column that implies all the others is ssn. Therefore, {ssn} is the only candidate key in this table.\n  Exercise\nExercise 2.3 Specify the normal form of the table Patient. Justify your answer.    Solution\n It is immediate to verify that the table is 1NF.\n The table is 2NF because there is only one candidate key, which is composed of only one column.\n The table is not in 3NF. Indeed, there is a functional dependency between two non-prime columns \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\).\n    Exercise\nExercise 2.4 How would you obtain two or more tables in BCNF from the table Patient?    Solution\nWe need to split the data relative to the patient from the data relative to the insurance. Therefore, we propose the following two tables:\n Patient (ssn, first_name, last_name, phone_number, insurance_number) Insurance (insurance_number, insurance_expiration_date)  Note that the column insurance_number in table Patient is foreign key to the column insurance_number in table Insurance.\n   3 Candidate keys and normal forms (2) Let \\(R\\) be a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\(A, B \\rightarrow C\\) \\(C \\rightarrow A\\) \\(C \\rightarrow B\\) \\(C \\rightarrow D\\) \\(D \\rightarrow E\\)  Exercise\nExercise 3.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nFirst, let’s try sets composed of only one column: \\(\\{A\\}\\), \\(\\{B\\}\\), \\(\\{C\\}\\), \\(\\{D\\}\\) and \\(\\{E\\}\\).\nWe have the following:\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D, E\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  Therefore, \\(\\{C\\}\\) is a candidate key because it implies all the other columns.\nFrom the functional dependency 1., we obtain that \\(\\{A, B\\}\\) imply \\(C\\); therefore, by transitivity they imply all the other columns.\nIn conclusion, we have two candidate keys: \\(\\{C\\}\\) and \\(\\{A, B\\}\\).\n  Exercise\nExercise 3.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is in 2NF. In fact, all non-prime columns depend entirely on both candidate keys.\n \\(R\\) is not in 3NF. In fact, the functional dependency \\(D \\rightarrow E\\) is between two non-prime columns.\n     4 Candidate keys and normal forms (3) Let \\(R\\) be the a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n \\(A \\rightarrow C\\) \\(A, B \\rightarrow D\\) \\(A, B \\rightarrow E\\)  Exercise\nExercise 4.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nThe following hold:\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A, C\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{C\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  It is clear that the only possible candidate key could be {A, B}; let’s verify it by computing the closure of {A, B} under \\(\\mathcal{F}\\):\n \\(\\{A, B\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)  Indeed, {A, B} is the candidate key.\n  Exercise\nExercise 4.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is not in 2NF. In fact, the non-prime column \\(C\\) is functionally dependent on only one part of the candidate key.\n \\(R\\) is not in 3NF, because it doesn’t fulfill the first condition, that is being in 2NF.\n     5 Candidate keys and normal forms (4) Consider the following table Student:\nStudent (stud_id, stud_ssn, course) The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n \\(stud\\_id \\rightarrow stud\\_ssn\\)\n \\(stud\\_ssn \\rightarrow stud\\_id\\)\n  Exercise\nExercise 5.1  Specify the candidate keys of the table Student.\n   Solution\nIf we consider each column independently we have:\n \\(\\{stud\\_id\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)\n \\(\\{stud\\_ssn\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)\n  No single column implies all the other columns. Therefore, the candidate key must be composed of at least 2 columns.\nIt is also easy to verify that:\n \\(\\{stud\\_id, stud\\_ssn\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)  We are left with two possibilities:\n \\(\\{stud\\_id, course\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn, course\\}\\)\n \\(\\{stud\\_ssn, course\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn, course\\}\\)\n  In conclusion, there are two candidate keys: \\(\\{stud\\_id, course\\}\\) and \\(\\{stud\\_ssn, course\\}\\)\n  Exercise\nExercise 5.2  We assume that Student is in 1NF.\n Is table Student in 2NF? Justify your answer.\n Is table Student in 3NF? Justify your answer.\n Is table Student in BCNF? Justify your answer.     Solution\n Student is in 2NF. There are no non-prime columns.\n Student is in 3NF. There are no non-prime columns.\n Student is not in BCNF. In both functional dependencies the determinant is not a superkey.\n     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd4b11df4fa072aaae4db3251396b08d","permalink":"/courses/databases/tutorials/normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/normalization/","section":"courses","summary":"Description of the normalization tutorial.","tags":null,"title":"Normalization","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to write SQL queries.  Prerequisites:\n Having attended Lecture 3.  1 Database of a social network platform We consider the following collection of relational tables:\nUserAccount (nickname, first_name, last_name, city, country)\nPost (post_id, content, date, time, lat, long, nickname)\nEmailAddress (email_address, nickname)\nRelationship (nickname_src, nickname_dst, type, date)\nTag (post_id, nickname)\nExercise\nExercise 1.1  Write the following queries in SQL:\nQ1. Count the number of posts by nickname. Sort by number of posts in descending order.\nQ2. Get the identifier and the content of all the posts of the user with nickname nick_BLANCO.\nQ3. Get the number of user accounts by country. Order by number of countries in descending order.\nQ4. Get the identifier of the posts where the user with nickname nick_ANIS is tagged.\nQ5. Get the content of all the posts where the user with nickname nick_ANIS is tagged.\nQ6. Get the email addresses of the user with nickname nick_BLANCO.\nQ7. Get the nicknames of the friends of the user with nickname nick_BLANCO.\nQ8. Get the the first and last names of the friends of the user with nickname nick_BLANCO.\nQ9. For each user, get the number of followers by country. Rank the countries by the number of followers in descending order.\nQ10. Get the first and family name of the users tagged by the user with nickname nick_CAMERON.\nQ11. Get the nickname, first and last name of the common friends of the users with nickname nick_CAMERON and nick_ANIS.\nQ12. Rank the users by the number of their followers. The user with the most followers must appear at the top of the list.\n   2 Database of a banking system We consider the following collection of relational tables:\nBank (code_bank, name, street_number, street_name, postal_code)\nBranch (branch_id, street_number, street_name, postal_code, code_bank)\nAccount (acct_nbr, acct_type, balance, branch_id, ssn)\nLoan (loan_nbr, loan_type, amount, branch_id, ssn)\nCustomer (ssn, first_name, last_name, telephone, street_number, street_name, postal_code)\nAddress (postal_code, city, country)\nExercise\nExercise 2.1  Write the following queries in SQL:\nQ1. Get the name, street name, street number and postal code of all the banks in the database.\nQ2. Get the city and country of all banks in the database.\nQ3. Count the number of accounts by customer.\nQ4. Get the amount of money owned by the customer.\nQ5. Return the average amount of money loaned by any bank for a mortgage.\nQ6. Get the amount of money loaned by each bank.\nQ7. Get the social security number, the first and last name of all customers of the bank with name “Bank of America”.\nQ8. Get all the data (including the full address) on the customers that have a checking account with a negative balance.\nQ9. Get the SSN of all customers that have no loan at the “Bank of America”.\nQ10. Get the name of the bank that has the most branches.\nQ11. Get the SSN of all customers having an account in more than one bank.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82410733cad4d5a26e04b28fe7e8b2ec","permalink":"/courses/databases/tutorials/sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sql/","section":"courses","summary":"Description of the SQL tutorial.","tags":null,"title":"SQL queries","type":"docs"}]