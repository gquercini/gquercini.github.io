[{"authors":["admin"],"categories":null,"content":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).","tags":null,"title":"Gianluca Quercini","type":"authors"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB as a case study).\n Prerequisites  Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Good knowledge of relational database management systems.\n   Teaching staff  Gianluca Quercini   Course summary 1. Introduction and MapReduce programming.\n Basic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce.  2. Hadoop and its ecosystem: HDFS.\n In-depth description of the Hadoop Distributed File System (HDFS).  3. Introduction to Apache Spark.\n Apache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions.  4. SparkSQL\n4. Spark streaming\n6. Distributed databases and NoSQL.\n Data distribution (replication, sharding, the CAP theorem).\n Overview of NoSQL databases.\n  7. Document oriented databases: MongoDB.\n Presentation of MongoDB.   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"770e6c3c2f07c415eb5d845260384b4c","permalink":"/courses/bdia/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Big data algorithms, techniques and platforms","type":"docs"},{"authors":null,"categories":null,"content":" Overview Nowadays, the marketing strategies of most companies are based on the analysis of massive and heterogeneous data that need a considerable amount of computational power. Instead of purchasing new hardware and software infrastructures, companies often resort to the computational and storage power offered by cloud computing platforms over the Internet.\nThe objective of this course is to present the fundamental principles of distributed computing that are at the heart of cloud computing. The course will cover the principles of virtualization and containerization and the methods and tools used for distributed processing (MapReduce and Spark).\n Prerequisites The main prerequisite is the course Information systems and programming:\n Python programming.\n Basic networking notions.\n Basic data management notions.\n  Previous experience with working the command-line terminal and Linux is desired but not essential.\nA beginner introduction to Linux is available here.\n Teaching staff Instructors  Gianluca Quercini\n Francesca Bugiotti\n   Teaching assistants  Arpad Rimmel\n Idir Ait Sadoune\n Marc-Antoine Weisser\n    Required software  Sign up for a free Student subscription on Microsoft Azure\n Docker and Kubernetes. See the installation guide.\n   Course summary Introduction  Context and motivation. Definition of Cloud Computing. Cloud deployment models (public, private, hybrid). Service models (SaaS, PaaS, IaaS). Introduction to public cloud platforms. Getting started with Microsoft Azure.  Virtualization and containerization  Virtualization: definition and implementation. Containerization (Docker). Orchestrators (Docker Swarm and Kubernetes).  Cloud programming and software environments  Parallel computing, programming paradigms. MapReduce - Apache Spark. Arrow - Spark + Argo    Exam Two lab assignments are graded. The final grade will be the average of the two grades.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e76a719efd772c3d82d45e1d0c32b856","permalink":"/courses/cloud-computing/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/","section":"courses","summary":"Presentation of the course Distributed and Cloud Computing","tags":null,"title":"Cloud Computing","type":"docs"},{"authors":null,"categories":null,"content":" Overview All individuals who own a personal computer need to store data on a daily basis. The operating system provides the file system as a way to organize and access files easily, masking the complexities of physical data storage. Although the file system is a simple and efficient data management system, many applications need more sophisticated features, which a file system does not provide, such as data integrity, indexes and fine-grained access control mechanisms.\nThis is where databases come into play.\nThe objective of this course is to introduce the basic notions on which all modern database management systems are grounded. The course is divided into two parts. The first part focuses on relational databases and presents the notions of integrity constraints, data consistency, transactions and normalization, both in single-server and distributed environments. The second part will introduce the four families of NoSQL databases, with a special focus on document databases (MongoDB)  Prerequisites Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Teaching staff  Gianluca Quercini   Course summary 1. An introduction to database systems and data modeling.\n Database systems: definitions and motivations. Data models. Introduction to the relational data model. Database design with the Entity-Relationship model (ER).  2. Normalization theory.\n Normalization: definitions and motivations. Notion of functional dependency. 1st, 2nd and 3rd normal forms. Further normal forms: Boyce-Codd, 4th and 5th normal forms. Normalization in real life.  3. Relational database management systems (RDBMS).\n Types of relational DBMS: client-server vs. embedded. An embedded relational DBM: SQLite. Interacting with a relational DBMS: the SQL language.  4. Advanced database concepts.\n Overview of a database management system. Transactions: definitions and motivations. Transaction management and processing. Query processing. Indexing.  5. Distributed databases.\n Distributed databases: definitions and motivations. Data replication and sharding. Data consistency in distributed databases. The CAP theorem.  6. NoSQL database systems.\n NoSQL database systems: definitions and motivations. Overview of NoSQL databases systems. Document-oriented databases: MongoDB.    Exam The evaluation is based on:\nA project (40% of the final grade). A written examination (60% of the final grade).  The project is a teamwork assignment where the students use the notions learned in classroom to design and create a database for a travel reservation system. Each team will be composed of 2-3 students.\nThe written examination lasts 1.5 hours and consists of two parts:\n A multiple choice questionnaire. A series of exercises on relational/NoSQL database modeling and querying.   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c50fbd0dd8e5954b528c8988ad1693b6","permalink":"/courses/databases/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/overview/","section":"courses","summary":"Presentation of the course Introduction to Databases","tags":null,"title":"Introduction to Databases","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB as a case study).\n Prerequisites  Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Good knowledge of relational database management systems.\n   Teaching staff  Gianluca Quercini   Course summary 1. Introduction and MapReduce programming.\n Basic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce.  2. Hadoop and its ecosystem: HDFS.\n In-depth description of the Hadoop Distributed File System (HDFS).  3. Introduction to Apache Spark.\n Apache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions.  4. Spark Structured APIs and Structured Streaming\n SparkSQL, Spark streaming.  5. Distributed databases and NoSQL.\n Data distribution (replication, sharding, the CAP theorem).\n Overview of NoSQL databases.\n  6. Document oriented databases: MongoDB.\n Presentation of MongoDB.   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"143a84392a4b8526c86e86f4828e51f0","permalink":"/courses/big-data-marseille/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Big data","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB as a case study).\n Prerequisites  Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Good knowledge of relational database management systems.\n   Teaching staff  Gianluca Quercini   Course summary 1. Introduction and MapReduce programming.\n Basic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce.  2. Hadoop and its ecosystem: HDFS.\n In-depth description of the Hadoop Distributed File System (HDFS).  3. Introduction to Apache Spark.\n Apache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions.  4. Advanced Apache Spark.\n SparkSQL, Spark streaming  5. Distributed databases and NoSQL.\n Data distribution (replication, sharding, the CAP theorem).\n Overview of NoSQL databases.\n  6. Document oriented databases: MongoDB.\n Presentation of MongoDB.   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"aa867f88f796b12dfa5207b07de94be4","permalink":"/courses/plp/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Platforms and languages","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f5464f3121c926385e0e40cd5209fa7","permalink":"/courses/bdia/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Getting started with Docker\nSupervisors: Francesca Bugiotti, Gianluca Quercini, Arpad Rimmel, Idir Ait Sadoune, Marc-Antoine Weisser\nDate and time: Thursday 22 April, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n Tutorial 2 Title: Introduction to Kubernetes\nSupervisors: Francesca Bugiotti, Gianluca Quercini, Arpad Rimmel, Idir Ait Sadoune, Marc-Antoine Weisser\nDate and time: Thursday 6 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n Tutorial 3 Title: Introduction to Kubernetes\nSupervisors: Francesca Bugiotti, Gianluca Quercini, Arpad Rimmel, Idir Ait Sadoune, Marc-Antoine Weisser\nDate and time: Friday 21 May, 1:45 AM - 5 PM\nRoom: Remotely on Microsoft Teams\nLink: Available soon\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d41cadecab8bb09746ea9ea01e2a42d0","permalink":"/courses/cloud-computing/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Data modeling\nDate and time: Tuesday, 14 September 2021, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 2 Title: Normalization\nDate and time: Tuesday, 21 September 2021, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 3 Title: SQL queries\nDate and time: Tuesday, 28 September 2021, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 4 Title: Document-based databases: MongoDB.\nDate and time: Tuesday, 23 November 2021, 10 AM - 12 AM\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40c351607858b41143809fa1fb45464e","permalink":"/courses/databases/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nDate and time: Monday 3 May 2021\nLink: Click here\n Tutorial 2 Title: Introduction to Spark RDD programming\nDate and time: Wednesday 5 May 2021\nLink: Click here\n Tutorial 3 Title: Introduction to DataFrames and SparkSQL\nDate and time: Monday 10 May 2021\nLinks:\n DataFrames\n DataFrames + SQL\n   Lab assignment 1 Title: Apache Spark programming\nDate and time: Wednesday 12 May 2021\nLink: Click here\n Tutorial 4 Title: Apache Structured Streaming\nDate and time: Wednesday 12 May 2021\nLink: Click here\n Lab assignment 2 Title: MongoDB\nDate and time: Wednesday 19 May 2021\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f85579f8755ff2ff8b6533862868e6d2","permalink":"/courses/big-data-marseille/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nLink: Click here\n Tutorial 2 Title: MapReduce in Python\nLink: Click here\n Tutorial 3 Title: MapReduce in Spark\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89eb8f3c35dc5558cefc06fa0d79071d","permalink":"/courses/plp/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":null,"categories":null,"content":"  1 Windows 1.1 Docker Desktop for Windows 1.2 Docker Toolbox for Windows  2 MacOS 2.1 Docker Desktop for Mac. 2.2 Docker Toolbox for Mac  3 Linux 4 Alternative options 4.1 Docker playground 4.2 Docker in an already prepared virtual machine  5 Verify your installation 6 Interacting with Docker 6.1 Graphical interface  7 Troubleshooting   This document contains information on how to install Docker on your computer.\nAlthough you can access an online Docker environment without installing anything on your computer (see Section 4.1), you should consider this option only if you really cannot install Docker.\nThe installation procedure depends on the operating system that your computer runs.\n1 Windows The installation procedure depends on the Windows version running on your computer.\n1.1 Docker Desktop for Windows If your computer runs Windows 10 64 bits (Pro, Enterprise, or Education, build 15063 or later), you can install Docker Desktop for Windows (recommended).\n Show me more\nHardware prerequisites\n 64 bit processor.\n 4GB system RAM.\n BIOS-level hardware virtualization support must be enabled in the BIOS settings. For more information, see Virtualization.\n  VirtualBox users\nDocker for Windows uses Hyper-V as a virtual machine to run containers. Unfortunately, Hyper-V and VirtualBox are not compatible; when Hyper-V is enabled, VirtualBox will stop working.\nHowever:\n The existing VirtualBox images will not be removed.\n When you want to use VirtualBox, you can turn Hyper-V off.\n  Cannot/don’t want to install Docker Desktop for for Windows\nIf your computer doesn’t meet the hardware requirements, or you don’t want to install Docker Desktop for Windows because you don’t want to mess up your VirtualBox installation (although you shouldn’t really worry about the latter), you have two options:\n Install Docker Toolbox for Windows (Section 1.2).\n See the alternative options (Section 4).\n  Installation procedure\n Download Docker Desktop for Windows.\n Follow the installation instructions. You might need to restart the system to enable Hyper-V.\n Verify your installation (see Section 5).\n    1.2 Docker Toolbox for Windows If your computer runs Windows 7 or higher, and doesn’t meet the hardware requirements for Docker for Windows, you can install Docker Toolbox for Windows.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     2 MacOS The installation procedure depends on the version of MacOS running on your computer.\n2.1 Docker Desktop for Mac. If your computer runs MacOS 10.13 or higher, you can install Docker Desktop for Mac (recommended).\n Show me more\nHardware requirements\n Your computer hardware must be a 2010 or a newer model. Verify that your computer is compatible with Docker Desktop for Mac:  Open a terminal. Run the following command: sysctl kern.hv_support. If the output of the command is kern.hv_support: 1 your computer is compatible.  At least 4GB of RAM.  VirtualBox users\nIf you have a version of VirtualBox older than 4.3.30, you should consider upgrading it, as it would not be compatible with Docker Desktop.\nCannot install Docker Desktop for Mac\nIf your computer doesn’t meet the hardware requirements, you have two options:\n Install Docker Toolbox for Mac (Section 2.2).\n See the alternative options (Section 4).\n  Installation instructions\n Download Docker Desktop for Mac.\n Follow the installation instructions.\n Verify your installation (see Section 5).\n    2.2 Docker Toolbox for Mac If your computer runs MacOs 10.8 or higher, and doesn’t meet the hardware requirements for Docker Desktop for Mac, you can install Docker Toolbox for Mac.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     3 Linux You can install Docker on the following Linux distributions:\n CentOS (installation instructions).\n Debian (installation instructions).\n Fedora (installation instructions).\n Ubuntu (installation instructions).\n  Make sure to read the post-installation steps for Linux and to take the necessary steps to be able to run Docker as a non-root user.\n 4 Alternative options If you’re unable to install Docker on your computer, you have two options left: using the Docker playground or installing Docker in a Linux virtual machine.\n4.1 Docker playground The Docker playground is an online Docker environment that you can play with for free.\n The advantage is that you don’t have anything to install on your computer.\n The disadvantage is that you might be unable to open a session depending on the number of active sessions.\n  In order to connect to the playground, you need to create an account on DockerHub.\n 4.2 Docker in an already prepared virtual machine \nThe **username** and **password** to log into the virtual machine are both *root*. In the video, you'll be directed to create a folder in your computer called *docker_files*. There, you'll place all files that you'll need to play with Docker. Don't hesitate to create subdirectories to organize your files (e.g., *td-1*, *final-project*). You'll be able to access this folder from the virtual machine from the folder */mnt/docker_files*. This way, you can manipulate your files by using the file system manager of your computer and you'll just use the terminal of the virtual machine to type the Docker commands.-- We provide an Ubuntu Linux virtual machine (5GB) with Docker and Kubernetes already installed. Download the virtual machine. You’ll need your CentraleSupélec credentials to access the file. Follow the installation procedure that is detailed in this video.\n  5 Verify your installation Open a terminal and type the following command:\ndocker run hello-world If everything is OK, you should see the output in the following figure.\n 6 Interacting with Docker In this course, we’ll learn how to interact with the Docker engine by using the command-line terminal. This option might seem a bit tedious (nobody likes to remember textual commands), but it offers a great flexibility.\nThis is the option that we recommend and for which we’ll provide a full support throughout the course.\n6.1 Graphical interface If you really want to use a graphical interface, you might want to look at Portainer, which is itself run as a Docker container.\n Linux or MacOS users\nOpen a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer   Windows users Open a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart always -v \\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine -v portainer_data:C:\\data portainer/portainer   When the container is up and running, the interface is available at the following URL: http://localhost:9000.\n Choose a password and create the user admin.\n Select Local to manage the Docker environment installed on your computer and click on Connect.\n Click on the endpoint Local (figure below) to access the dashboard.\n   The menu on the left of the dashboard allows you to manage the different components of your Docker environment (e.g., containers, images, volumes and networks).  A user guide of Portainer is very much out of the scope of this course. However, the interface is rather intuitive and you should easily find out how to create, run, stop and remove containers, build images and create volumes and networks.\n  7 Troubleshooting In this section we’ll document the installation issues that you might experience.\nDon’t hesitate to contact us to report your installation problems.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1009f24f18a9853858a69a79adeb4f0c","permalink":"/courses/cloud-computing/overview/installing-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/installing-docker/","section":"courses","summary":"Docker installation instructions","tags":null,"title":"Installing Docker","type":"docs"},{"authors":[],"categories":[],"content":"","date":1581344688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581344688,"objectID":"4e459ff727a9e6a0c795e8d8f3d69232","permalink":"/project/data-for-you/","publishdate":"2020-02-10T15:24:48+01:00","relpermalink":"/project/data-for-you/","section":"project","summary":"","tags":[],"title":"Data for You","type":"project"},{"authors":["Armita Khajeh Nassiri","Nathalie Pernelle","Fatiha Saı̈s","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"77dcf9b15a66ddb64a7a8426a803dd21","permalink":"/publication/nassiri-2020/","publishdate":"2020-11-05T15:51:37.231059Z","relpermalink":"/publication/nassiri-2020/","section":"publication","summary":"","tags":null,"title":"Generating Referring Expressions from RDF Knowledge Graphs for Data Linking","type":"publication"},{"authors":["Fatiha Saïs","Joana E. Gonzales Malaverri","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"24e4602ce4c2a42bac26f3019386ecbf","permalink":"/publication/sais-2020/","publishdate":"2020-04-08T14:26:24.211279Z","relpermalink":"/publication/sais-2020/","section":"publication","summary":"","tags":null,"title":"MOMENT: Temporal Meta-Fact Generation and Propagation in Knowledge Graphs","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Coriane Nana Jipmo","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3685ef29403450add392048ea8b1e19d","permalink":"/publication/seghouani-2019/","publishdate":"2020-02-10T15:32:10.718712Z","relpermalink":"/publication/seghouani-2019/","section":"publication","summary":"","tags":null,"title":"Determining the interests of social media users: two approaches","type":"publication"},{"authors":["Suela Isaj","Nacéra Bennacer Seghouani","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f0a2e1e20345b7ff2748dd0ff24c29fd","permalink":"/publication/isaj-2019/","publishdate":"2020-02-10T15:37:10.960652Z","relpermalink":"/publication/isaj-2019/","section":"publication","summary":"","tags":null,"title":"Profile Reconciliation Through Dynamic Activities Across Social Networks","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed97c59bbeebf6f67506ea911cd4d01b","permalink":"/publication/seghouani-2018/","publishdate":"2020-02-10T15:32:10.719498Z","relpermalink":"/publication/seghouani-2018/","section":"publication","summary":"","tags":null,"title":"A frequent named entities-based approach for interpreting reputation in Twitter","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9cdf7741acd87116627b5dd0bc25a27c","permalink":"/publication/seghouani-2018-a/","publishdate":"2020-02-10T15:33:43.343039Z","relpermalink":"/publication/seghouani-2018-a/","section":"publication","summary":"","tags":null,"title":"Élimination des liens inter-langues erronés dans Wikipédia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"edcf86d1dd80092db4864af2aa551e98","permalink":"/publication/bennacer-2017-a/","publishdate":"2020-02-10T15:32:10.723551Z","relpermalink":"/publication/bennacer-2017-a/","section":"publication","summary":"","tags":null,"title":"Eliminating Incorrect Cross-Language Links in Wikipedia","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"048dafc42b622ff1cee4c115e5409243","permalink":"/publication/jipmo-2017/","publishdate":"2020-02-10T15:32:10.722099Z","relpermalink":"/publication/jipmo-2017/","section":"publication","summary":"","tags":null,"title":"Frisk: A multilingual approach to find twitteR InterestS via wiKipedia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1e3d10b83c3c7b1807d422d7d251cca3","permalink":"/publication/bennacer-2017/","publishdate":"2020-02-10T15:32:10.7231Z","relpermalink":"/publication/bennacer-2017/","section":"publication","summary":"","tags":null,"title":"Interpreting reputation through frequent named entities in twitter","type":"publication"},{"authors":["Gianluca Quercini","Nacéra Bennacer","Mohammad Ghufran","Coriane Nana Jipmo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c02251b35ab73929914bbe9b772e5248","permalink":"/publication/quercini-2017/","publishdate":"2020-02-10T15:32:10.720054Z","relpermalink":"/publication/quercini-2017/","section":"publication","summary":"","tags":null,"title":"Liaison: reconciliation of individuals profiles across social networks","type":"publication"},{"authors":["Mohammad Ghufran","Nacéra Bennacer","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c1928a9ee227111e1b27ba31fdbb6a86","permalink":"/publication/ghufran-2017/","publishdate":"2020-02-10T15:32:10.722652Z","relpermalink":"/publication/ghufran-2017/","section":"publication","summary":"","tags":null,"title":"Wikipedia-based extraction of key information from resumes","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"49508dab832a1726fe7da77e8604a7f8","permalink":"/publication/jipmo-2016/","publishdate":"2020-02-10T15:32:10.724083Z","relpermalink":"/publication/jipmo-2016/","section":"publication","summary":"","tags":null,"title":"Catégorisation et Désambiguı̈sation des Intérêts des Individus dans le Web Social.","type":"publication"},{"authors":["Nacéra Bennacer","Mia Johnson Vioulès","Maximiliano Ariel López","Gianluca Quercini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ce3927111f9fc9f8ef041600fa6e6126","permalink":"/publication/bennacer-2015/","publishdate":"2020-02-10T15:32:10.725565Z","relpermalink":"/publication/bennacer-2015/","section":"publication","summary":"","tags":null,"title":"A multilingual approach to discover cross-language links in Wikipedia","type":"publication"},{"authors":["Mohammad Ghufran","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"482b66b1b2e201edf7dfa2685afa010a","permalink":"/publication/ghufran-2015/","publishdate":"2020-02-10T15:32:10.724645Z","relpermalink":"/publication/ghufran-2015/","section":"publication","summary":"","tags":null,"title":"Toponym disambiguation in online social network profiles","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"2bce9aaaae3b0248796b29334c06054f","permalink":"/publication/bennacer-2014/","publishdate":"2020-02-10T15:32:10.726341Z","relpermalink":"/publication/bennacer-2014/","section":"publication","summary":"","tags":null,"title":"Matching user profiles across social networks","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ff35f567785c6703d81172b874c7531b","permalink":"/publication/bennacer-2014-a/","publishdate":"2020-02-10T15:32:10.727368Z","relpermalink":"/publication/bennacer-2014-a/","section":"publication","summary":"","tags":null,"title":"Réconciliation des profils dans les réseaux sociaux.","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"41c8a22b08bd7e057a05ad86a744cac5","permalink":"/publication/quercini-2014/","publishdate":"2020-02-10T15:32:10.728416Z","relpermalink":"/publication/quercini-2014/","section":"publication","summary":"","tags":null,"title":"Uncovering the spatial relatedness in Wikipedia","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"9086beba183fe687c8f4c462cd3153ca","permalink":"/publication/quercini-2013/","publishdate":"2020-02-10T15:32:10.72918Z","relpermalink":"/publication/quercini-2013/","section":"publication","summary":"","tags":null,"title":"Entity discovery and annotation in tables","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cc5f1500c80ecfe28e62ab5e9b608e81","permalink":"/publication/quercini-2012-a/","publishdate":"2020-02-10T15:32:10.737022Z","relpermalink":"/publication/quercini-2012-a/","section":"publication","summary":"","tags":null,"title":"Des données tabulaires à RDF: l’extraction de données de Google Fusion Tables","type":"publication"},{"authors":["Massimo Ancona","Betty Bronzini","Davide Conte","Gianluca Quercini"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"62840715b27d999325c6799de42c8994","permalink":"/publication/ancona-2012/","publishdate":"2020-02-10T15:32:10.717657Z","relpermalink":"/publication/ancona-2012/","section":"publication","summary":"","tags":null,"title":"Developing Attention-Aware and Context-Aware User Interfaces on Handheld Devices","type":"publication"},{"authors":["Antonio Penta","Gianluca Quercini","Chantal Reynaud","Nigel Shadbolt"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"fa3cbdff0934af7e1ad34c9d906d46b4","permalink":"/publication/penta-2012/","publishdate":"2020-02-10T15:32:10.72995Z","relpermalink":"/publication/penta-2012/","section":"publication","summary":"","tags":null,"title":"Discovering Cross-language Links in Wikipedia through Semantic Relatedness.","type":"publication"},{"authors":["Gianluca Quercini","Jochen Setz","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1eb03ccdfc068b0af2eee6b98febab4f","permalink":"/publication/quercini-2012/","publishdate":"2020-02-10T15:32:10.736267Z","relpermalink":"/publication/quercini-2012/","section":"publication","summary":"","tags":null,"title":"Facetted Browsing on Extracted Fusion Tables Data for Digital Cities.","type":"publication"},{"authors":["Jochen Setz","Gianluca Quercini","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"152a779726400c4afb268da398df2025","permalink":"/publication/setz-2012/","publishdate":"2020-02-10T15:32:10.730492Z","relpermalink":"/publication/setz-2012/","section":"publication","summary":"","tags":null,"title":"Facetted search on extracted fusion tables data for digital cities","type":"publication"},{"authors":["Laura Papaleo","Gianluca Quercini","Viviana Mascardi","Massimo Ancona","A Traverso","Henry de Lumley"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2951f449fe1a9a559bd320ee66d7a4df","permalink":"/publication/papaleo-2011/","publishdate":"2020-02-10T15:32:10.7311Z","relpermalink":"/publication/papaleo-2011/","section":"publication","summary":"","tags":null,"title":"Agents and Ontologies for Understanding and Preserving the Rock Art of Mount Bego.","type":"publication"},{"authors":["Gianluca Quercini","Massimo Ancona"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"712870e00c1980526965a476c61c6a9f","permalink":"/publication/quercini-2010/","publishdate":"2020-02-10T15:32:10.731853Z","relpermalink":"/publication/quercini-2010/","section":"publication","summary":"","tags":null,"title":"Confluent drawing algorithms using rectangular dualization","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet","Jagan Sankaranarayanan","Michael D Lieberman"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"c8ae118f82887db05d951390d2ae7695","permalink":"/publication/quercini-2010-a/","publishdate":"2020-02-10T15:33:43.347923Z","relpermalink":"/publication/quercini-2010-a/","section":"publication","summary":"","tags":null,"title":"Determining the spatial reader scopes of news sources using local lexicons","type":"publication"},{"authors":["Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Anton Bogdanovych","H De Lumley","Laura Papaleo","Simeon Simoff","Antonella Traverso"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"6456e6f4265092c5466fcdd21d7bb4eb","permalink":"/publication/ancona-2010/","publishdate":"2020-02-10T15:32:10.73288Z","relpermalink":"/publication/ancona-2010/","section":"publication","summary":"","tags":null,"title":"Virtual institutions for preserving and simulating the culture of Mount Bego's ancient people","type":"publication"},{"authors":["Anton Bogdanovych","Laura Papaleo","Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Simeon Simoff","Alex Cohen","Antonella Traverso"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"c71e7154cbc2dfbe25039ed19758c530","permalink":"/publication/bogdanovych-2009/","publishdate":"2020-02-10T15:32:10.737689Z","relpermalink":"/publication/bogdanovych-2009/","section":"publication","summary":"","tags":null,"title":"Integrating agents and virtual institutions for sharing cultural heritage on the Web","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e93eb6b5f5c5cf19230762a04c9abc05","permalink":"/publication/ancona-2009/","publishdate":"2020-02-10T15:32:10.720588Z","relpermalink":"/publication/ancona-2009/","section":"publication","summary":"","tags":null,"title":"Text Entry in PDAs with WtX","type":"publication"},{"authors":["Massimo Ancona","Davide Conte","Donatella Pian","Sonia Pini","Gianluca Quercini","Antonella Traverso"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"e650a879a57f2a6c286aa2d365cb9a7d","permalink":"/publication/ancona-2008/","publishdate":"2020-02-10T15:32:10.721064Z","relpermalink":"/publication/ancona-2008/","section":"publication","summary":"","tags":null,"title":"Wireless networks in archaeology and cultural heritage","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini","Luca Dominici"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5da2e1599d08bf6b66c9260251f7f87d","permalink":"/publication/ancona-2007-a/","publishdate":"2020-02-10T15:32:10.734718Z","relpermalink":"/publication/ancona-2007-a/","section":"publication","summary":"","tags":null,"title":"An Improved Text Entry Tool for PDAs","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0bb01482186a36901efd9bc59287ade5","permalink":"/publication/ancona-2007-b/","publishdate":"2020-02-10T15:33:43.342472Z","relpermalink":"/publication/ancona-2007-b/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5de935c6dca2f9dc45d3fcc539d7a23e","permalink":"/publication/ancona-2007/","publishdate":"2020-02-10T15:32:10.721486Z","relpermalink":"/publication/ancona-2007/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["Massimo Ancona","Marco Cappello","Marco Casamassima","Walter Cazzola","Davide Conte","Massimiliano Pittore","Gianluca Quercini","Naomi Scagliola","Matteo Villa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"956e4fbb19a8e3db3caf66b9ca3ce215","permalink":"/publication/ancona-2006-a/","publishdate":"2020-02-10T15:33:43.349611Z","relpermalink":"/publication/ancona-2006-a/","section":"publication","summary":"","tags":null,"title":"Mobile vision and cultural heritage: the agamemnon project","type":"publication"},{"authors":["Massimo Ancona","Walter Cazzola","Sara Drago","Gianluca Quercini"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"152d79fb2becb0b67888c71837db1203","permalink":"/publication/ancona-2006/","publishdate":"2020-02-10T15:32:10.733963Z","relpermalink":"/publication/ancona-2006/","section":"publication","summary":"","tags":null,"title":"Visualizing and managing network topologies via rectangular dualization","type":"publication"},{"authors":["M Ancona","S Locati","M Mancini","A Romagnoli","G Quercini"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"a5f6251a4b15ec8ddae9685ad605d62a","permalink":"/publication/ancona-2005/","publishdate":"2020-02-10T15:32:10.735534Z","relpermalink":"/publication/ancona-2005/","section":"publication","summary":"","tags":null,"title":"Comfortable textual data entry for PocketPC: the WTX system","type":"publication"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\nAssignment submission\nThis lab assignment will be evaluated.\nYou need to submit a .zip file containing the following files:\n Source code of the programs that you write.\n A PDF document with the answer to the questions that you find in this document.\n  Please send me the zip file by email.\nThe submission deadline is Thursday, May 20, 2021 8:00 AM.\n 1 Computing averages We consider a collection of CSV files containing temperature measurements in the following format:\nyear,month,day,hours,minutes,seconds,temperature\nyou can find the files under the directory hdfs://sar01:9000/data/temperatures/\nHere are the details for each file:\n File temperatures_86400.csv contains one measurement per day in the years 1980 - 2018. File temperatures_2880.csv contains one measurement every 2880 seconds in the years 1980 - 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds for the years 1980 - 2018.  We intend to implement a Spark algorithm to generate pairs \\((y, t_{avg})\\), where \\(y\\) is the year and \\(t_{avg}\\) is the average temperature in the year.\n1.1 First implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py Open the file avg_temperatures_first.py and write the following function:\ndef avg_temperature(theTextFile): temperatures = theTextFile \\ .map(lambda line: line.split(\u0026quot;,\u0026quot;)) \\ .map(lambda term: (term[0], [float(term[6])])) \\ .reduceByKey(lambda x, y: x+y) \\ .mapValues(lambda lv: sum(lv)/len(lv)) return temperatures  In the same file, locate the two variables input_path and output-path. and write the following code:\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/cpuecm1/cpuecm1_XX/\u0026quot; Don’t forget the / at the end of the file paths and to replace XX with the number at the end of your username.\nExercise\nExercise 1.1 \n Run the script avg_temperatures_first.py by using temperatures_86400.csv as an input. To this extent, use the following command:   spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv You should find the output of the program under the folder\nhdfs://sar01:9000/cpuecm1/cpuecm1_XX/temperatures_86400.out\n What’s the execution time?  In the output of Spark on the command line you should see a line that mentions something along the following line:   INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s   Run the same script by using temperatures_2880.csv as an input.\n What is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer.\n Execute the same script by using temperatures_86.csv as an input.\n What is the execution time? How would you justify it, knowing that the files temperatures_2880.csv and temperatures_86.csv have a similar size (11 MB the former, 9 MB the latter)?\n  ```\n  1.2 Second implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your working directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py Exercise\nExercise 1.2  Based on the observations made in the previous exercise, write an improved implementation of the function avg_temperature.\n  Exercise\nExercise 1.3 \n Run the script avg_temperatures_second.py by using temperatures_86.csv as an input.\n What’s the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference.\n Run the same script by using temperatures_10.csv (3 GB!) as an input. Do you think that the program takes too long? Why?\n      2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nWe use the following input files available in folder hdfs://sar01:9000/data/sn/:\n sn_tiny.csv. Small social network, that you can use to test your implementation.\n sn_10k_100k.csv. Social network with \\(10^4\\) individuals and \\(10^5\\) links.\n sn_100k_100k.csv. Social network with \\(10^5\\) individuals and \\(10^5\\) links.\n sn_1k_100k.csv. Social network with \\(10^3\\) individuals and \\(10^5\\) links.\n sn_1m_1m.csv. Social network with \\(10^6\\) individuals and \\(10^6\\) links.\n  Exercise\nExercise 2.1  Write an implementation in Spark. Test your implementation on file sn_tiny.csv.\n  Exercise\nExercise 2.2  Run your implementation on the other files and write down the execution times. Comment on the execution times considering the file sizes, the number of nodes and links and the number of pairs \\(((A, B), X)\\) generated by the algorithm.\n  Exercise\nExercise 2.3 \nBy using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of a node in a given graph.\n Compute the minimum, maximum and average degree on all the given input files.\n Do these values confirm or invalidate the considerations that you made on the execution times of the algorithm in the first exercise? Justify your answer.\n     3 Creating an inverted index In folder hdfs://sar01:9000/data/bbc/ you’ll find a collection of 50 articles obtained from the BBC website (2004-2005) organized into five subfolders: business, entertainment, politics, sport and technology.\nWe want to create an inverted index, which associates each word with the list of the files in which the word occurs. More specifically, for each word, the inverted index will have a list of the names of the files (path relative to the folder /data/bbc) that contain the word.\nThe inverted index:\n must not contain the same word twice;\n must not contain any stopwords (the list of stopwords is provided in the hdfs://sar01:9000/data/stopwords.txt file);\n  Moreover:\n Words in the inverted index must only contain letters.\n Words in the inverted index must be lowercase.\n  Exercise\nExercise 3.1  Write a Spark program to create an inverted index and execute it on the input folder. You can use the template available at ~vialle/DCE-Spark/template_inverted_index.py.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76158ebb08d6aa7e40a693d348db00e7","permalink":"/courses/big-data-marseille/tutorials/spark-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/spark-assignment/","section":"courses","summary":"Lab assignment Spark RDD.","tags":null,"title":"Apache Spark — Programming with RDD","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nSlides: Available on Edunao.\n Lecture 2 Title: Hadoop and storage: HDFS.\nSlides: Available on Edunao.\n Lecture 3 Title: Introduction to Apache Spark.\nSlides: Available on Edunao.\n Lecture 4 Title: Spark SQL.\n Lecture 5 Title: Spark Streaming\n Lecture 6 Title: Distributed and NoSQL databases\n Lecture 7 Title: Document-oriented database systems: MongoDB.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6083f96b0b4ec7adb864b2f008bd261f","permalink":"/courses/bdia/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Both lab assignments will be graded.\nLab assignment 1 Title: Multi-service applications in the Cloud\nSupervisors: Francesca Bugiotti Gianluca Quercini, Arpad Rimmel, Idir Ait Sadoune, Marc-Antoine Weisser\nDate and time: Friday 7 May, 1:45 PM - 5 PM\nRoom: On Microsoft Teams\nLink: Click here\n Lab assignment 2 Title: MapReduce - Apache Spark\nSupervisors: Francesca Bugiotti Gianluca Quercini, Arpad Rimmel, Idir Ait Sadoune, Marc-Antoine Weisser\nDate and time: Friday 28 May, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nLink: available soon\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6e75e11c1ee6ba55693a9622583beaf9","permalink":"/courses/cloud-computing/labs/cc-labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/labs/cc-labs/","section":"courses","summary":"Presentation of the lab assignments of the course.","tags":null,"title":"Lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction to Cloud Computing\nLecturer: Gianluca Quercini\nDate and time: Wednesday 14 April, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 2 Title: Virtualization and Containerization\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Friday 16 April, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 3 Title: Multi-service applications and orchestration\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Friday 23 April, 1:45 PM - 5:00 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 4 Title: MapReduce\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Friday 14 May, 1:45 PM - 5:00 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available here\n Lecture 5 Title: Apache Spark\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Tuesday 18 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available here\n Lecture 6 Title: Arrow - Spark + Argo\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Wednesday 26 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available soon\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f5a608bccc37bec35254441f7fcae9a","permalink":"/courses/cloud-computing/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Class material  Lecture slides, available here.\n An introduction to Docker.\n A Docker cheat sheet, with a summary of the most important Docker commands.\n An introduction to Linux, useful to understand how Docker works and how to interact with Docker.\n   Books  Schenker, Gabriel. Learn Docker - Fundamentals of Docker 18.x. Packt Publishing,. Print.\n Surianarayanan, C., \u0026amp; Chelliah, P. R. (2019). Essentials of Cloud Computing: A Holistic Perspective. Springer Nature.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d57f199c44c3345bf5285f7abd8f4c9","permalink":"/courses/cloud-computing/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\n Figure 1: Cluster architecture (image credit: Stéphane Vialle)  In order to use the Spark cluster, you’ll need to go through the following fours steps:\nChoose a username on Edunao. The password will be communicated by the teacher during the tutorial.\n Open a SSH connection to the machine phome.metz.supelec.fr from your local machine.\n Open a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\n On the machine slurm1 allocate resources for the job from a named reservation.\n  Steps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\n Connect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\n In the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\n In the connection panel, set Enable TCP keepalives and set 30s between keepalives.\n Click on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\n A command-line terminal should pop up, prompting you to enter your username and password.\n   Connect to slurm1 In the command-line terminal type the following command:\nssh slurm1\n Allocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\n Use the cluster only in the evening in weekdays or during the weekends.\n In order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks1 -N 1 --ntasks-per-node=4 --pty bash\n    Source file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuasi1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!).  or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!).  The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\n Windows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\n  Creating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuasi1_X, where X is between 1 and 28.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuasi1/cpuasi1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuasi1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuasi1/\n Preliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\n Copy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\n If you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\n Open the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\n Locate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\n Similarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuasi1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\n Run the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\n When the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\nAs usual, remember to replace cpuasi1_X with your username.\n In order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out/*   Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"69c94965ddb76cf5352cebe8a39b6dc2","permalink":"/courses/bdia/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\n Figure 1: Cluster architecture (image credit: Stéphane Vialle)  In order to use the Spark cluster, you’ll need to go through the following fours steps:\nUse your username in the spreadsheet communicated by the teacher before the tutorial. The password will be communicated by the teacher during the tutorial.\n Open a SSH connection to the machine phome.metz.supelec.fr from your local machine.\n Open a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\n On the machine slurm1 allocate resources for the job from a named reservation.\n  Steps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\n Connect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\n In the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\n In the connection panel, set Enable TCP keepalives and set 30s between keepalives.\n Click on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\n A command-line terminal should pop up, prompting you to enter your username and password.\n   Connect to slurm1 In the command-line terminal type the following command:\nssh slurm1\n Allocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\n Use the cluster only in the evening in weekdays or during the weekends.\n In order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks2 -N 1 --ntasks-per-node=4 --pty bash\n    Source file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuecm1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!).  or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!).  The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\n Windows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\n  Creating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuecm1_X, where X is between 1 and 10.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuecm1/cpuecm1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuecm1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuecm1/\n Preliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\n Copy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\n If you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\n Open the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\n Locate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\n Similarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuecm1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\n Run the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\n When the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\nAs usual, remember to replace cpuecm1_X with your username.\n In order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out/*   Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56cde29729b5f8b5acea77062bfe0c80","permalink":"/courses/big-data-marseille/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\n Figure 1: Cluster architecture (image credit: Stéphane Vialle)  In order to use the Spark cluster, you’ll need to go through the following fours steps:\nChoose a username on Edunao. The password will be communicated by the teacher during the tutorial.\n Open a SSH connection to the machine phome.metz.supelec.fr from your local machine.\n Open a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\n On the machine slurm1 allocate resources for the job from a named reservation.\n  Steps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\n Connect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\n In the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\n In the connection panel, set Enable TCP keepalives and set 30s between keepalives.\n Click on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\n A command-line terminal should pop up, prompting you to enter your username and password.\n   Connect to slurm1 In the command-line terminal type the following command:\nssh slurm1\n Allocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\n Use the cluster only in the evening in weekdays or during the weekends.\n In order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks1 -N 1 --ntasks-per-node=4 --pty bash\n    Source file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuasi1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!).  or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!).  The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\n Windows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\n  Creating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuasi1_X, where X is between 1 and 28.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuasi1/cpuasi1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuasi1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuasi1/\n Preliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\n Copy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\n If you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\n Open the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\n Locate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\n Similarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuasi1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\n Run the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\n When the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\nAs usual, remember to replace cpuasi1_X with your username.\n In order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out/*   Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c22db20918c1b3dd3408cafa3febaca8","permalink":"/courses/plp/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this project is to assess your knowledge of the main notions presented in classroom.\nThe project must be implemented by students working in groups.\nCreating the groups\nClick here to create your group.\n This project consists in designing a relational database for the given application context, importing the data of a given dataset into a SQLite database and querying the data in SQL.\nThe deadline to submit the work is Friday December 3rd, 11:59 PM.\nSubmit your project\nThe submission site is available at this link.\nPlease note that:\n You must be member of a group to submit your work. Only one submission per group is possible. Two members of the same group cannot submit two different versions.  You must submit three files, please follow these instructions:\n A report in PDF containing the answers to all the questions and exercises, except the SQL queries (max 10 pages). Please name the file groupXX_report.pdf, where XX is your group number.\n A file containing the SQLite database that you generated. Please name the file groupXX_db.txt. In order to create this file, select File \\(\\rightarrow\\) Export \\(\\rightarrow\\) Database to SQL file. Make sure that in the options field the “Export everything” option is selected. You’ll need to save the file with the extension .sql. Once the file is saved, you’ll rename it to .txt.\n A file with all the SQL queries. Please name the file groupXX_queries.txt. Separate each query in the file by a blank line.\n  Example:\nSELECT * FROM Airport SELECT * FROM Hotel  Each group must submit an original work. Edunao is equipped with an anti-plagiarism tool that is able to check the originality of each submission. Submissions that are flagged as too similar by the tool are likely to be discarded and will not be evaluated.\n  Application context We intend to manage the data of a travel reservation system with clients all over the world. Upon registration, customers are automatically given a numeric identifier and they are asked to indicate their first and family names, their gender, date of birth, a phone number, an email address and their country of residence.\nAny customer can book a trip that includes the reservation of one or more flights and, possibly, one or more hotels.\nExample\nAlice wants to fly from Paris, France to New York City (NYC), USA and she intends to stay in NYC for 10 days. Her trip includes two flights: an outbound flight from Paris to NYC and an inbound flight from NYC to Paris; and an hotel in NYC.\n A flight is operated by an airline company, of which the system keeps its name (e.g., British Airways), the country where the airline is incorporated and, when available, its IATA code (e.g., BA, a two-letter code identifying the airline), its ICAO code (e.g., BAW, a three-letter code identifying the airline) and alternate name or alias (e.g., British).\nA flight connects two airports, each having a name (e.g., London Heathrow Airport), and, possibly, a IATA (e.g., LHR) and ICAO code (e.g., EGLL); an airport serves a specific location (e.g., London, UK) and its precise position is given by its geographic coordinates (latitude and longitude).\nA flight connecting two airports at specific departure and arrival times is identified by a flight number. Two flights operated by two different airline companies cannot have the same flight number, but the same flight number can denote two flights operated by the same airline company on different days.\nExample\nEmirates flight EK074 leaves Paris, France at 10 AM and arrives at Dubai, UAE at 7:40 PM (regardless of the departure day).\n For each flight booked by a customer, the system keeps the seat number, the travel class (e.g., economy or business), the price and the date of the flight. Usually, airlines include details on the type of aircraft they plan to use on their flight schedules; these details include the name of the aircraft (e.g., Boeing 787-8) and, when available, the IATA code (e.g., 788, a unique three-letter identifier for the aircraft) and the ICAO code (e.g., B788, a unique four-letter identifier for the aircraft).\nThe system maintains a list of hotels, with their names, addresses and an average review score, which is a real number denoting the average grade assigned to the hotel by its customers. Customers can write a review for an hotel; in which case the system stores the text of the review, the date and its author. When a customer books an hotel, the system keeps the price paid, the check-in and check-out dates and whether the breakfast is included.\n The dataset You can download the dataset by clicking here. The dataset consists of 7 CSV files: aircrafts.csv, airlines.csv, airports.csv, hotels.csv, customers.csv, hotel_bookings.csv, flight_bookings.csv. The separator character in each file is the tab character (‘’).\nNotice\nTake some time to look at the content of the files to understand the structure that your tables will have.\n  Creation of a relational database You’ll now proceed to the definition of a relational database for our travel reservation system. First, you need to define the conceptual schema and then you’ll define the tables that compose the database.\nThe conceptual schema Before defining the logical schema of the database, answer the following questions:\nCan you use the name of the hotel as a primary key? Justify your answer.\n Can you use the flight number as a primary key to identify a flight? Justify your answer and, in case of a negative answer, propose a solution.\n Knowing that it is unlikely that two reviews have the same textual content, would you use it as a primary key? Justify your answer.\n Knowing that the IATA code uniquely identifies an airport, would you choose it as a primary key for the entity Airport? Justify your answer.\n  Exercise\nExercise 1  Propose an Entity-Relationship diagram describing the conceptual model of a relational database for the given application context.\n Specify all the attributes for each entity and relation.\n For each entity, underline the attributes composing the primary key.\n For each relation, clearly indicate the minimum and maximum cardinality.     Normalization Exercise\nExercise 2  Translate the logical schema into a collection of tables. For each table:\n Indicate its name and the names of the columns (but not their types).\n Underline the columns that are part of the primary key.\n Indicate the entity in the ER diagram to which the table corresponds.\n  To make sure you choose the right data types for the columns, you can also check the values in the dataset.\n  Which normal form are your tables in?\nExercise\nExercise 3  For each table:\n Indicate a minimal set of functional dependencies.\n Indicate the normal form that the table satisfies. Justify your answer.    Normalize your tables.\nExercise\nExercise 4  Normalize each table up to the Boyce-Codd Normal Form (BCNF).\n   The physical schema You can now define the physical schema of your database.\nExercise\nExercise 5  Write the SQL code to create the tables. For each table:\n Indicate the primary key.\n Indicate the foreign keys.\n Indicate NOT NULL and UNIQUE constraints, if needed.     Database creation and data import In order to create a database in SQLite, use DB Browser for SQLite:\n Open the program and click on New Database.\n The program will ask you to specify a name for the database file and the folder where you want to store it.\n The program will display a pop-up window where you can create the tables. Alternatively, you can directly execute the SQL code that you wrote before to create your tables.\n  Exercise\nExercise 6  Create a SQLite database with the tables that you defined in the previous exercise.\n  In order to import the data into a table, you can select File \\(\\rightarrow\\) Import \\(\\rightarrow\\) Table from CSV file. The given CSV file might not correspond exactly to the tables that you created, so you’ll need to edit them a bit in order to fit the schema of your tables.\n Running queries Write the following queries in SQL.\nExercise\nExercise 7 \nGet the average ticket price on Air France flights.\n Count the number of customers by country.\n Select the names of the airports in Paris, France.\n Select the name of the cities (city and country name) with the most airports.\n Select the name of the airline companies that use an Airbus A300.\n Select the identifier, first and family names of all customers who flew to Sydney, Australia.\n Select the identifier, name, city and country of the busiest airport (the one with more outbound and inbound flights).\n Select the average price in the economy class.\n Select the average price in the business class.\n Select the name, city and country of the destination airport of french customers.\n Select the destination cities (specify city and country name) preferred by women.\n Select the destination cities (specify city and country name) preferred by men.\n Count the number of customers by country flying to Paris.\n Count the number of hotels by city.\n Determine the amount of money spent by Tatiana REZE in flights.\n     Indexes Exercise\nExercise 8 \nWrite a query to get all the information of a customer with a given family name. Run the query multiple times and note the average running time of the query.\n Create an index on the column containing the family name of a customer.\n Rerun the same query multiple times and note the average running times of the query.\n Do you observe any difference? Can you explain what is going on here?      ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85d2d631269d7307bfde5c4b0669c155","permalink":"/courses/databases/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":" Introduction Slides: Click here\n Lecture 1 Title: An introduction to database systems and data modeling.\nDate and time: Tuesday, 14 September 2021, 9:00 AM - 12 PM.\nSlides: Click here\n Lecture 2 Title: Normalization theory.\nDate and time: Tuesday, 21 September 2021, 9:00 AM - 12 PM.\nSlides: Click here\n Lecture 3 Title: Relational databases: SQL.\nDate and time: Tuesday, 28 September 2021, 9:00 AM - 12 AM.\nSlides: Click here\n Lecture 4 Title: Relational databases: indexing and transactions.\nDate and time: Tuesday 19 October 2021, 9:00 AM - 12 AM.\nSlides: Click here\n Lecture 5 Title: Distributed databases and NoSQL.\nDate and time: Tuesday, 26 October 2021 - 9 November 2021, 9:00 AM - 12 AM.\nSlides: Click here\n Lecture 6 Title: Document-oriented databases: MongoDB.\nDate and time: Tuesday, 23 November 2021, 9:00 AM - 12 AM.\nSlides: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f967524fae94aa5965fe18163acab25","permalink":"/courses/databases/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Date, Christopher John. An introduction to database systems. Pearson Education India, 2004.\n Hoffer, Jeffrey A., Venkataraman Ramesh, and Heikki Topi. Modern database management. Pearson, 2016\n Garcia-Molina, Hector, Jeffrey D. Ullman and Jennifer Widom. Database systems: the complete book. Pearson Education India, 2008.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Robinson, Ian, Jim Webber, and Emil Eifrem. Graph databases. \u0026quot; O’Reilly Media, Inc.\u0026quot;, 2013.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"074436047baef381bdf6909058220908","permalink":"/courses/databases/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL\nExample. Building an image from a Dockerfile in the current directory: docker build .\n The command assumes that a file named Dockerfile is in the current directory.  Common options:\n  Tag the image   Name of the Dockerfile       docker build -t my-image:latest .   docker build -f my-dockerfile .       Manage images   List all images   List images (no intermediate)   Remove an image     docker image ls -a   docker image ls   docker image rm my-image     Remove dangling images   Remove unused images   Show the history of an image     docker image prune   docker image prune -a   docker history my-image     Dockerfile In a Dockerfile the following main keywords are used:\n  FROM base-image   FROM scratch   RUN cmd     Specifies the base image   No base image used   Runs a command     COPY src dst   ADD src dst   WORKDIR dir     Copy source file to destination   Copy source file (including URL and TAR) to destination   Sets the working directory     ENTRYPOINT cmd   CMD params   EXPOSE port     Command to execute when container is run   Parameters of the entrypoint command   Exposes a container port     Volumes   Create a volume   Remove a volume   Remove unused volumes     docker volume create my-volume   docker volume rm my-volume   docker volume prune     List volumes         docker volume ls         Networks   Create a network   Remove a network   Remove unused networks     docker network create my-network   docker network rm my-network   docker network prune     List all the networks   Inspect a network   Connect a container to a network     docker network ls   docker network inspect my-network   docker network connect my-network my-container     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f5dc854a8f25cad521a576f1c221397","permalink":"/courses/cloud-computing/references/docker-cheat-sheet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/docker-cheat-sheet/","section":"courses","summary":"Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL","tags":null,"title":"Docker Cheat Sheet","type":"docs"},{"authors":null,"categories":null,"content":"  1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server  2 Instructions for Windows users   In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites  Install XCode. You’ll find it for free in the Mac App Store.\n Install Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n  /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot;  Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page.   1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\n  Tap the official MongoDB Homebrew tap with the following command:  brew tap mongodb/brew  Type the following command to install MongoDB:  brew install mongodb-community@4.4  1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4  1.4 Launch MongoDB compass  Open MongoDB Compass.\n After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.   1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n   2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\n  Go to the MongoDB download page and download the installer (.msi file).\n Double-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\n Once the installation procedure is over, the MongoDB server is automatically started.\n MongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.  If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"30ec4dcfab1236f37e20039aa093e144","permalink":"/courses/bdia/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":"  1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server  2 Instructions for Windows users   In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites  Install XCode. You’ll find it for free in the Mac App Store.\n Install Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n  /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot;  Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page.   1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\n  Tap the official MongoDB Homebrew tap with the following command:  brew tap mongodb/brew  Type the following command to install MongoDB:  brew install mongodb-community@4.4  1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4  1.4 Launch MongoDB compass  Open MongoDB Compass.\n After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.   1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n   2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\n  Go to the MongoDB download page and download the installer (.msi file).\n Double-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\n Once the installation procedure is over, the MongoDB server is automatically started.\n MongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.  If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ffc657bc33e600d8a2856451c403fd03","permalink":"/courses/big-data-marseille/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":"  1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server  2 Instructions for Windows users   In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites  Install XCode. You’ll find it for free in the Mac App Store.\n Install Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n  /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot;  Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page.   1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\n  Tap the official MongoDB Homebrew tap with the following command:  brew tap mongodb/brew  Type the following command to install MongoDB:  brew install mongodb-community@4.4  1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4  1.4 Launch MongoDB compass  Open MongoDB Compass.\n After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.   1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n   2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\n  Go to the MongoDB download page and download the installer (.msi file).\n Double-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\n Once the installation procedure is over, the MongoDB server is automatically started.\n MongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.  If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bee18108026333e44d7372a340146c8","permalink":"/courses/plp/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  By using the MongoDB shell, execute the following queries. Q1. The release year of the movie \"Le parrain III\". Q2. The title of the movies released between 1980 and 1990. Q3. Same query as b. with the titles must be sorted by alphabetical order. Q4. The titles of the french movies. Q5. The title of the \"crime\" or \"drama\" movies. Q6. The names and birth dates of the directors of french movies. Q7. The title of the movies in which Sofia Coppola played. Q8. The title and the genre of the movies of which Hitchcock is director. \\EndKnitrBlock{exercise} ::: # Querying the data with the aggregation framework ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  By using the MongoDB shell, execute the following queries by using **aggregation pipelines**. Q1. The number of movies by country. Show by decresing number. Q2. The name of the actor in the role \"Mary Corleone\" in the movie \"Le parrain III\". Q3. The number of actors by movie. Sort by decreasing number. Q4. The average number of actors in a film. \\EndKnitrBlock{exercise} ::: # Join in MongoDB In the database *cinema*, create a new collection called *movies_boffice*. Import the documents in file *moviesBoxOffice.json* into this collection. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  By using the operator *$lookup* on the collections *movies* and *movies_boffice*, find the box office of the movie \"Le parrain III\". \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"12699c180fbbdd5c767809d89744e184","permalink":"/courses/bdia/tutorials/mongodb-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/mongodb-tutorial/","section":"courses","summary":"Description of the MongoDB tutorial.","tags":null,"title":"MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  By using the MongoDB shell, execute the following queries. Q1. The release year of the movie \"Le parrain III\". Q2. The title of the movies released between 1980 and 1990. Q3. Same query as b. with the titles must be sorted by alphabetical order. Q4. The titles of the french movies. Q5. The title of the \"crime\" or \"drama\" movies. Q6. The names and birth dates of the directors of french movies. Q7. The title of the movies in which Sofia Coppola played. Q8. The title and the genre of the movies of which Hitchcock is director. \\EndKnitrBlock{exercise} ::: # Querying the data with the aggregation framework ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  By using the MongoDB shell, execute the following queries by using **aggregation pipelines**. Q1. The number of movies by country. Show by decresing number. Q2. The name of the actor in the role \"Mary Corleone\" in the movie \"Le parrain III\". Q3. The number of actors by movie. Sort by decreasing number. Q4. The average number of actors in a film. \\EndKnitrBlock{exercise} ::: # Join in MongoDB In the database *cinema*, create a new collection called *movies_boffice*. Import the documents in file *moviesBoxOffice.json* into this collection. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  By using the operator *$lookup* on the collections *movies* and *movies_boffice*, find the box office of the movie \"Le parrain III\". \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b82eb6b804bb0bc2e6736e5b5eac3d54","permalink":"/courses/plp/tutorials/mongodb-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/mongodb-tutorial/","section":"courses","summary":"Description of the MongoDB tutorial.","tags":null,"title":"MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nDate and time: Monday 3 May 2021, 1:45 PAM - 5 PM.\nSlides: Available here\n Lecture 2 Title: Hadoop and its ecosystem: HDFS.\nDate and time: Wednesday 5 May 2021, 8:30 AM - 10 AM.\nSlides: Available here\n Lecture 3 Title: Introduction to Apache Spark.\nDate and time: Wednesday 5 May 2021, 10 AM - 11:30 AM.\nSlides: Available here\nSpark RDD programming demo: Available here\n Lecture 4 Title: Apache Spark’s Structured APIs and Structured Streaming.\nDate and time: Monday 10 May 2021, 8:30 AM - 11:30 AM.\nSlides: Available here\nDataFrames: Notebook available here\nSpark SQL: Notebook available here\n Lecture 5 Title: Distributed and NoSQL databases\nDate and time: Monday 17 May 2021, 10:30 AM - 11:30 AM / 14 PM - 16 PM\nSlides: Available here\n Lecture 6 Title: Document-oriented database systems: MongoDB.\nDate and time: Monday 17 May 2021, 16 PM - 17PM\nNotebook: Available here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2392a0bcf324168321fd0099e6a6b96","permalink":"/courses/big-data-marseille/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nSlides: Available on Edunao.\n Lecture 2 Title: Hadoop and storage: HDFS.\nSlides: Available on Edunao.\n Lecture 3 Title: Introduction to Apache Spark.\nSlides: Available on Edunao.\n Lecture 4 Title: Spark SQL.\n Lecture 5 Title: Spark Streaming\n Lecture 6 Title: Distributed and NoSQL databases\n Lecture 7 Title: Document-oriented database systems: MongoDB.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2df8cc01c3dc820657e60bdae93d56cd","permalink":"/courses/plp/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\n Mehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\n Karau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\n Giamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Scifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"47dae0ded442f0aa29b22737adf719b5","permalink":"/courses/bdia/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":"  Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\n Mehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\n Karau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\n Giamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Scifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"63bc28b1c571ba26f8b80edb6882bae4","permalink":"/courses/big-data-marseille/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":"  Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\n Mehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\n Karau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\n Giamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Scifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c41cb45dee07917944a15794098e978b","permalink":"/courses/plp/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  Write an implementation in Spark. **Test your implementation on file ``sn_tiny.csv``.** \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Run your implementation on the other files and write down the execution times. Comment on the execution times considering the file sizes, the number of nodes and links and the number of pairs $((A, B), X)$ generated by the algorithm. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  Execute your implementation on the file ``sn_1m_1m.csv`` by varying the number of cores used by the Spark executors. You can specify the total number of cores with the option ``--total-executor-cores`` of the command ``spark-submit`` (you can also refer [to the Spark documentation](https://spark.apache.org/docs/latest/submitting-applications.html){target=\"_blank\"}). * What is the impact of the number of cores on the execution time? Make a graph and comment. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  1. By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of a node in a given graph. 2. Compute the minimum, maximum and average degree on all the given input files. 3. Do these values confirm or invalidate the considerations that you made on the execution times of the algorithm in the first exercise? Justify your answer. \\EndKnitrBlock{exercise} ::: # Creating an inverted index In folder ``hdfs://sar01:9000/data/bbc/`` you'll find a collection of 50 articles obtained from the BBC website (2004-2005) organized into five subfolders: *business*, *entertainment*, *politics*, *sport* and *technology*. We want to create an **inverted index**, which associates each word with the list of the files in which the word occurs. More specifically, for each word, the inverted index will have a list of the names of the files (path relative to the folder ``/data/bbc``) that contain the word. The inverted index: * must not contain the same word twice; * must not contain any stopwords (the list of stopwords is provided in the ``hdfs://sar01:9000/data/stopwords.txt`` file); Moreover: * Words in the inverted index must only contain letters. * Words in the inverted index must be lowercase. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write a Spark program to create an inverted index and execute it on the input folder. You can use the template available at ``~vialle/DCE-Spark/template_inverted_index.py``. \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"81ec3d3af71c75dda8a5d582d22ac1fc","permalink":"/courses/bdia/tutorials/spark-lab-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/spark-lab-assignment/","section":"courses","summary":"Description of the lab advanced Spark programming.","tags":null,"title":"Advanced Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  Write an implementation in Spark. **Test your implementation on file ``sn_tiny.csv``.** \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Run your implementation on the other files and write down the execution times. Comment on the execution times considering the file sizes, the number of nodes and links and the number of pairs $((A, B), X)$ generated by the algorithm. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  Execute your implementation on the file ``sn_1m_1m.csv`` by varying the number of cores used by the Spark executors. You can specify the total number of cores with the option ``--total-executor-cores`` of the command ``spark-submit`` (you can also refer [to the Spark documentation](https://spark.apache.org/docs/latest/submitting-applications.html){target=\"_blank\"}). * What is the impact of the number of cores on the execution time? Make a graph and comment. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  1. By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of a node in a given graph. 2. Compute the minimum, maximum and average degree on all the given input files. 3. Do these values confirm or invalidate the considerations that you made on the execution times of the algorithm in the first exercise? Justify your answer. \\EndKnitrBlock{exercise} ::: # Creating an inverted index In folder ``hdfs://sar01:9000/data/bbc/`` you'll find a collection of 50 articles obtained from the BBC website (2004-2005) organized into five subfolders: *business*, *entertainment*, *politics*, *sport* and *technology*. We want to create an **inverted index**, which associates each word with the list of the files in which the word occurs. More specifically, for each word, the inverted index will have a list of the names of the files (path relative to the folder ``/data/bbc``) that contain the word. The inverted index: * must not contain the same word twice; * must not contain any stopwords (the list of stopwords is provided in the ``hdfs://sar01:9000/data/stopwords.txt`` file); Moreover: * Words in the inverted index must only contain letters. * Words in the inverted index must be lowercase. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write a Spark program to create an inverted index and execute it on the input folder. You can use the template available at ``~vialle/DCE-Spark/template_inverted_index.py``. \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dc442fca81a29582331cefd6a42cbd11","permalink":"/courses/plp/tutorials/spark-lab-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/spark-lab-assignment/","section":"courses","summary":"Description of the lab advanced Spark programming.","tags":null,"title":"Advanced Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  1. Where does this program get its input from? 2. What object type does the variable ``lines`` contain? 3. Where does this program write its output? 4. What is the output of this program? 5. What is the option ``checkpointLocation`` intended for? 6. What does the instruction ``streamingQuery.awaitTermination()``? \\EndKnitrBlock{exercise} ::: You can now verify your answers to the previous questions by **executing the program**. ::: {.infobox .activitybox data-latex=\"{exercisebox}\"} **Activity** 1. Connect to the cluster, if you haven't done so yet. [Refer to this documentation](/courses/plp/overview/cluster-connection){target=\"_blank\"}. 2. After running the command ``srun ...``, you should be connected to a machine on the cluster Kyle. Note the name of this machine (you should see it at the terminal prompt). 3. Create a checkpoint directory for the first exercise (e.g., ``checkpoint_exo1``) under your home directory ``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 4. Copy and paste the code into a Python file (e.g., ``exo1.py``) that you'll save into your home directory **in the local filesystem** of the cluster machine. * Change the value of the variable ``checkpoint_location`` so that it points to the directory that you created at point 3. * Change the value of the variable ``port_number`` to any value in the range [49152, 65535]. 5. Open a new terminal window, connect to ``phome.metz.supelec.fr`` and then to the same machine that you noted at point 2. 6. In the new terminal, start a **netcat server** listening on the port number that you selected at point 4. Use the following command: `` nc -lk port_number `` 7. Run the Python code with the command ``spark-submit``. Wait until Spark does not display any more messages on screen. * In case the program stops for an error, read the box \"What to do in case of errors\" below. 8. In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output. ::: ::: {.infobox .warning data-latex=\"{warning}\"} **What to do in case of errors** If any error arises, **before** running the ``spark-submit`` again it would be better to remove all files from the checkpoint directory. ::: ::: {.infobox .warning data-latex=\"{warning}\"} **Stop the program** * When you're done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running. * Don't stop the netcat server, you'll need it in the next exercise. ::: # Triggering policy In a Structured Streaming program we can choose a **triggering policy**. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  1. What is a triggering policy? 2. What is the triggering policy in the previous program? 3. Modify the code of the previous program in order to set the ``Fixed interval micro-batch`` triggering policy. 4. Run the program. How is the behaviour of this program different from before? \\EndKnitrBlock{exercise} ::: # Checkpoint location and output mode We're now going to see the impact of the checkpoint location and the output modes on a streaming query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  1. What is an output mode and what are the available options? 2. What is the output mode of the previous program? \\EndKnitrBlock{exercise} ::: We're now going to write a new streaming query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  1. Create a new checkpoint location in HDFS. You may also keep the same directory as before; in this case, make sure you **remove all files** from that directory. 2. Write a new program that reads a streaming text from a TCP socket and counts the number of occurrences of each word. 3. Which output mode are you going to choose and why? 4. Run the program. Write few lines on the netcat server and observe the output. 5. Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output. What can you say about the word counts? 6. Stop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts? 7. Play with the different output modes and observe how the output changes. \\EndKnitrBlock{exercise} ::: # Window operations on event time ::: {.infobox .warning data-latex=\"{warning}\"} **Netcat and checkpoint** 1. You can stop the netcat server now. 2. Remember to create a new checkpoint location for this exercise. Alternatively, you can also use the same directory as in the previous exercises, but you should remove all its files. ::: We're now going to find out how to perform aggregations over a sliding event-time window. A given data source generates some words for a certain time interval. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the **event time**. After generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we'll refer to this directory as the **source directory**. At any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format ``word,timestamp`` (no whitespace before nor after the comma). ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write a Spark program that: 1. Reads the stream of data from the source directory. 2. Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes. 3. Print the output counts to the console. Use triggers of 5 seconds. \\EndKnitrBlock{exercise} ::: We now test the new Spark program. ::: {.infobox .warning data-latex=\"{warning}\"} **Data source and timeline visualization** We provide two Python programs for this exercise: a data generator and a tool for visualizing words in a timeline. Instructions to get and run these two programs are given in the activity below. The **data generator** is our data source. It generates two words every second for a certain amount of time. Each word is saved in a separate CSV file in source directory. It also saves the list of all generated words to a summary CSV file. The **visualization tool** takes as its input the summary CSV file written by the data generator and visualizes the words on a timeline. ::: ::: {.infobox .activitybox data-latex=\"{exercisebox}\"} **Activity** 1. Create the source directory under your home directory ``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 2. Copy to your home directory in the local filesystem the data generator that you find at the following path ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py``. 3. Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly. 4. In another terminal, run the Python script ``tempws_gen.py``. Use the following command to learn how to run this program: ``python3 tempws_gen.py --help`` For this exercise, do not introduce any delay (keep the default values of the parameters ``--delay``, ``--mindelay``, ``--maxdelay``). 5. After launching the data generator, you should see some output in the terminal where you launched the Spark program. **Wait for the script ``tempws_gen.py`` to terminate the data generation**. The output might be a bit overwhelming. Scroll up to identify the results on each micro-batch. 6. If you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory. ::: We now want to analyze the output of the program. * The script ``tempws_gen.py`` has generated a file ``gen_words.csv`` in your home directory. This file contains the list of all words generated with the relative timestamps. **Download the file to your computer**. * Download the visualization tool ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py`` to your computer. ::: {.infobox .warning data-latex=\"{warning}\"} **Visualization tool** Use the following command to learn how to run the visualization tool: `` python timeline_visualization.py --help `` The visualization tool displays a vertical blue bar at each trigger. To this purpose, you'll need to pass the tool the timestamps associated to the first and last trigger and the interval (in seconds) between two consecutive triggers. You can get the timestamps associated to the first and last trigger by analyzing the output of Spark. More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you'll need to look at the timestamp associated to the first and last micro-batch. ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-6)  1. Analyze the output of your Spark program and the timeline of the generated words. 2. Describe how the counts are updated by the Spark program. \\EndKnitrBlock{exercise} ::: # Late data and watermarking We're now going to learn how Structured Streaming handles late data in windowed aggregations. ::: {.infobox .warning data-latex=\"{warning}\"} **Remove generated files** Remove all the files in the directory ``tempws``. ::: The data generator ``tempws_gen.py`` can generate a stream of words, some of which might be written to the directory ``tempws`` with some amount of delay. In other words, there is a gap between the event time (when the word is generated) and the arrival time (when the word is written to the directory). Use the following command to learn how to do it: ``python3 tempws_gen.py --help`` ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-7)  1. Write a Spark program that does the same aggregation as in the previous exercise. Additionally, the program must use watermarking to handle late data. 2. Start the Spark program. 3. Generate some data with delay with the program ``tempws_gen.py``. Once the data generation stops, you can stop the Spark program. 4. Visualize the generated words with the visualization tool. Late words have the delay indicated between parentheses. 5. Observe the output of the Spark program and describe how the watermarking mechanism works on this example. \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"040db908ca821f5f89f33cc7749ec154","permalink":"/courses/bdia/tutorials/structured-streaming-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/structured-streaming-lab/","section":"courses","summary":"Description of the lab structured streaming.","tags":null,"title":"Spark structured streaming","type":"docs"},{"authors":null,"categories":null,"content":" (\\#exr:unnamed-chunk-1)  1. Where does this program get its input from? 2. What object type does the variable ``lines`` contain? 3. Where does this program write its output? 4. What is the output of this program? 5. What is the option ``checkpointLocation`` intended for? 6. What does the instruction ``streamingQuery.awaitTermination()``? \\EndKnitrBlock{exercise} ::: You can now verify your answers to the previous questions by **executing the program**. ::: {.infobox .activitybox data-latex=\"{exercisebox}\"} **Activity** 1. Connect to the cluster, if you haven't done so yet. [Refer to this documentation](/courses/plp/overview/cluster-connection){target=\"_blank\"}. 2. After running the command ``srun ...``, you should be connected to a machine on the cluster Kyle. Note the name of this machine (you should see it at the terminal prompt). 3. Create a checkpoint directory for the first exercise (e.g., ``checkpoint_exo1``) under your home directory ``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 4. Copy and paste the code into a Python file (e.g., ``exo1.py``) that you'll save into your home directory **in the local filesystem** of the cluster machine. * Change the value of the variable ``checkpoint_location`` so that it points to the directory that you created at point 3. * Change the value of the variable ``port_number`` to any value in the range [49152, 65535]. 5. Open a new terminal window, connect to ``phome.metz.supelec.fr`` and then to the same machine that you noted at point 2. 6. In the new terminal, start a **netcat server** listening on the port number that you selected at point 4. Use the following command: `` nc -lk port_number `` 7. Run the Python code with the command ``spark-submit``. Wait until Spark does not display any more messages on screen. * In case the program stops for an error, read the box \"What to do in case of errors\" below. 8. In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output. ::: ::: {.infobox .warning data-latex=\"{warning}\"} **What to do in case of errors** If any error arises, **before** running the ``spark-submit`` again it would be better to remove all files from the checkpoint directory. ::: ::: {.infobox .warning data-latex=\"{warning}\"} **Stop the program** * When you're done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running. * Don't stop the netcat server, you'll need it in the next exercise. ::: # Triggering policy In a Structured Streaming program we can choose a **triggering policy**. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  1. What is a triggering policy? 2. What is the triggering policy in the previous program? 3. Modify the code of the previous program in order to set the ``Fixed interval micro-batch`` triggering policy. 4. Run the program. How is the behaviour of this program different from before? \\EndKnitrBlock{exercise} ::: # Checkpoint location and output mode We're now going to see the impact of the checkpoint location and the output modes on a streaming query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  1. What is an output mode and what are the available options? 2. What is the output mode of the previous program? \\EndKnitrBlock{exercise} ::: We're now going to write a new streaming query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  1. Create a new checkpoint location in HDFS. You may also keep the same directory as before; in this case, make sure you **remove all files** from that directory. 2. Write a new program that reads a streaming text from a TCP socket and counts the number of occurrences of each word. 3. Which output mode are you going to choose and why? 4. Run the program. Write few lines on the netcat server and observe the output. 5. Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output. What can you say about the word counts? 6. Stop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts? 7. Play with the different output modes and observe how the output changes. \\EndKnitrBlock{exercise} ::: # Window operations on event time ::: {.infobox .warning data-latex=\"{warning}\"} **Netcat and checkpoint** 1. You can stop the netcat server now. 2. Remember to create a new checkpoint location for this exercise. Alternatively, you can also use the same directory as in the previous exercises, but you should remove all its files. ::: We're now going to find out how to perform aggregations over a sliding event-time window. A given data source generates some words for a certain time interval. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the **event time**. After generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we'll refer to this directory as the **source directory**. At any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format ``word,timestamp`` (no whitespace before nor after the comma). ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write a Spark program that: 1. Reads the stream of data from the source directory. 2. Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes. 3. Print the output counts to the console. Use triggers of 5 seconds. \\EndKnitrBlock{exercise} ::: We now test the new Spark program. ::: {.infobox .warning data-latex=\"{warning}\"} **Data source and timeline visualization** We provide two Python programs for this exercise: a data generator and a tool for visualizing words in a timeline. Instructions to get and run these two programs are given in the activity below. The **data generator** is our data source. It generates two words every second for a certain amount of time. Each word is saved in a separate CSV file in source directory. It also saves the list of all generated words to a summary CSV file. The **visualization tool** takes as its input the summary CSV file written by the data generator and visualizes the words on a timeline. ::: ::: {.infobox .activitybox data-latex=\"{exercisebox}\"} **Activity** 1. Create the source directory under your home directory ``hdfs://sar01:9000/cpuasi1/cpuasi1_X`` **in HDFS**. 2. Copy to your home directory in the local filesystem the data generator that you find at the following path ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py``. 3. Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly. 4. In another terminal, run the Python script ``tempws_gen.py``. Use the following command to learn how to run this program: ``python3 tempws_gen.py --help`` For this exercise, do not introduce any delay (keep the default values of the parameters ``--delay``, ``--mindelay``, ``--maxdelay``). 5. After launching the data generator, you should see some output in the terminal where you launched the Spark program. **Wait for the script ``tempws_gen.py`` to terminate the data generation**. The output might be a bit overwhelming. Scroll up to identify the results on each micro-batch. 6. If you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory. ::: We now want to analyze the output of the program. * The script ``tempws_gen.py`` has generated a file ``gen_words.csv`` in your home directory. This file contains the list of all words generated with the relative timestamps. **Download the file to your computer**. * Download the visualization tool ``/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py`` to your computer. ::: {.infobox .warning data-latex=\"{warning}\"} **Visualization tool** Use the following command to learn how to run the visualization tool: `` python timeline_visualization.py --help `` The visualization tool displays a vertical blue bar at each trigger. To this purpose, you'll need to pass the tool the timestamps associated to the first and last trigger and the interval (in seconds) between two consecutive triggers. You can get the timestamps associated to the first and last trigger by analyzing the output of Spark. More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you'll need to look at the timestamp associated to the first and last micro-batch. ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-6)  1. Analyze the output of your Spark program and the timeline of the generated words. 2. Describe how the counts are updated by the Spark program. \\EndKnitrBlock{exercise} ::: # Late data and watermarking We're now going to learn how Structured Streaming handles late data in windowed aggregations. ::: {.infobox .warning data-latex=\"{warning}\"} **Remove generated files** Remove all the files in the directory ``tempws``. ::: The data generator ``tempws_gen.py`` can generate a stream of words, some of which might be written to the directory ``tempws`` with some amount of delay. In other words, there is a gap between the event time (when the word is generated) and the arrival time (when the word is written to the directory). Use the following command to learn how to do it: ``python3 tempws_gen.py --help`` ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-7)  1. Write a Spark program that does the same aggregation as in the previous exercise. Additionally, the program must use watermarking to handle late data. 2. Start the Spark program. 3. Generate some data with delay with the program ``tempws_gen.py``. Once the data generation stops, you can stop the Spark program. 4. Visualize the generated words with the visualization tool. Late words have the delay indicated between parentheses. 5. Observe the output of the Spark program and describe how the watermarking mechanism works on this example. \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f1505088605c90177c426a77cc242db","permalink":"/courses/plp/tutorials/structured-streaming-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/structured-streaming-lab/","section":"courses","summary":"Description of the lab structured streaming.","tags":null,"title":"Spark structured streaming","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this lab assignment is to learn how to analyze streams of data with the Spark Structured Streaming API. Refer to this documentation to learn how to connect and interact with the cluster.\nDocumentation\nIn order to answer the questions and do the exercises, you might want to refer to the following documentation:\n The Structured Streaming programing guide.\n The Spark SQL API reference.\n    1 Warming up Consider the following program.\nfrom pyspark.sql import SparkSession from pyspark.sql.types import * import pyspark.sql.functions as F port_number = COMPLETE HERE checkpoint_location = COMPLETE HERE spark = (SparkSession.builder.appName(\u0026quot;Structured Streaming - exo1\u0026quot;).getOrCreate()) lines = (spark\\ .readStream.format(\u0026quot;socket\u0026quot;)\\ .option(\u0026quot;host\u0026quot;, \u0026quot;localhost\u0026quot;)\\ .option(\u0026quot;port\u0026quot;, port_number)\\ .load()) streamingQuery = lines.writeStream\\ .option(\u0026quot;checkpointLocation\u0026quot;, checkpoint_location)\\ .format(\u0026quot;console\u0026quot;).start() streamingQuery.awaitTermination() Exercise\nExercise 1.1 \nWhere does this program get its input from?\n What object type does the variable lines contain?\n Where does this program write its output?\n What is the output of this program?\n What is the option checkpointLocation intended for?\n What does the instruction streamingQuery.awaitTermination()?\n     Solution\nThe input is a socket, where a program generates some data.\n lines is a dataframe.\n To the console.\n Just copies the input to the output.\n It is where the Spark program writes the progress of the streaming query.\n The instructions means that the program will block on this instruction. The program won’t stop until there is data to process.\n    You can now verify your answers to the previous questions by executing the program.\nActivity\nConnect to the cluster, if you haven’t done so yet. Refer to this documentation.\n After running the command srun ..., you should be connected to a machine on the cluster Kyle. Note the name of this machine (you should see it at the terminal prompt).\n Create a checkpoint directory for the first exercise (e.g., checkpoint_exo1) under your home directory hdfs://sar01:9000/cpuecm1/cpuecm1_X in HDFS.\n Copy and paste the code into a Python file (e.g., exo1.py) that you’ll save into your home directory in the local filesystem of the cluster machine.  Change the value of the variable checkpoint_location so that it points to the directory that you created at point 3. Change the value of the variable port_number to any value in the range [49152, 65535].  Open a new terminal window, connect to phome.metz.supelec.fr and then to the same machine that you noted at point 2.\n In the new terminal, start a netcat server listening on the port number that you selected at point 4. Use the following command:\n  nc -lk port_number Run the Python code with the command spark-submit. Wait until Spark does not display any more messages on screen.  In case the program stops for an error, read the box “What to do in case of errors” below.  In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output.   What to do in case of errors\nIf any error arises, before running the spark-submit again it would be better to remove all files from the checkpoint directory.\n Stop the program\n When you’re done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running.\n Don’t stop the netcat server, you’ll need it in the next exercise.\n Remove all files from the checkpoint location.\n    2 Triggering policy In a Structured Streaming program we can choose a triggering policy.\nExercise\nExercise 2.1 \nWhat is a triggering policy?\n What is the triggering policy in the previous program?\n Modify the code of the previous program in order to set the Fixed interval micro-batch triggering policy.\n Run the program. How is the behaviour of this program different from before?\n     Solution\nIt is a policy that dictates the timing of streaming data processing.\n No triggering policy is specified, so the default is chosen. In practice, as soon as the previous micro-batch finishes processing, the next one is read in.\n Here is the code. We need to specify a `trigger.\n  streamingQuery = lines.writeStream .trigger(processingTime = \u0026quot;15 seconds\u0026quot;) .format(\u0026quot;console\u0026quot;).start() The new data is checked at the specified interval. It is possible that within the specified interval (15 seconds), we write many lines, so the program will get multiple lines in a micro-batch (unlike before, when the processing is triggered as soon as there is data available).     3 Checkpoint location and output mode We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.\nExercise\nExercise 3.1 \nWhat is an output mode and what are the available options?\n What is the output mode of the previous program?     Solution\nThe output mode tells Spark how the output is presented. There are several options: append (only the new rows added to the Result Table since the last trigger are visible in the output), complete (the whole Result Table is visible after each trigger) and update (only the rows that were updated since the last trigger are visible in the output).\n In the previous program we didn’t specify any output mode, so the default mode (append) is selected.    We’re now going to write a new streaming query.\nExercise\nExercise 3.2 \nCreate a new checkpoint location in HDFS. You may also keep the same directory as before; in this case, make sure you remove all files from that directory.\n Write a new program that reads a streaming text from a TCP socket and counts the number of occurrences of each word.\n Which output mode are you going to choose and why?\n Run the program. Write few lines on the netcat server and observe the output.\n Stop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output. What can you say about the word counts?\n Stop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts?\n Play with the different output modes and observe how the output changes.\n     Solution\nThe new program is as follows:\nlines = (spark\\ .readStream.format(\u0026quot;socket\u0026quot;)\\ .option(\u0026quot;host\u0026quot;, \u0026quot;localhost\u0026quot;)\\ .option(\u0026quot;port\u0026quot;, port_number)\\ .load()) lines = lines.select(F.explode(F.split(lines.value, \u0026quot; \u0026quot;))\\ .alias(\u0026quot;word\u0026quot;))\\ .groupBy(\u0026quot;word\u0026quot;).count() streamingQuery = lines.writeStream\\ .trigger(processingTime = \u0026quot;15 seconds\u0026quot;)\\ .option(\u0026quot;checkpointLocation\u0026quot;, checkpoint_location)\\ .outputMode(\u0026quot;update\u0026quot;)\\ .format(\u0026quot;console\u0026quot;)\\ .start() streamingQuery.awaitTermination() The append mode doesn’t work, because aggregating function might modify previous lines of the ResultTable. So, the only options left are update and complete. We choose update to just have the values that changed since the last trigger.\n   4 Window operations on event time Netcat and checkpoint\nYou can stop the netcat server now.\n Remember to create a new checkpoint location for this exercise. Alternatively, you can also use the same directory as in the previous exercises, but you should remove all its files.\n   We’re now going to find out how to perform aggregations over a sliding event-time window.\nA given data source generates some words for a certain time interval. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the event time.\nAfter generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we’ll refer to this directory as the source directory.\nActivity\nCreate the source directory under your home directory hdfs://sar01:9000/cpuecm1/cpuecm1_X in HDFS.\n At any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format word,timestamp (no whitespace before nor after the comma).\nExercise\nExercise 4.1 \nWrite a Spark program that:\nReads the stream of data from the source directory.\n Counts the number of occurrences of each word within 10 minute windows that slide every 5 minutes.\n Print the output counts to the console. Use triggers of 5 seconds.     Solution\nThe new program is as follows:\n source_directory = hdfs://.... words = (spark .readStream.format(\u0026quot;csv\u0026quot;) .schema(\u0026quot;word STRING, timestamp TIMESTAMP\u0026quot;) .load(source_directory)) windowedCount = words.groupBy(F.window(words.timestamp, \u0026quot;10 seconds\u0026quot;, \u0026quot;5 seconds\u0026quot;, startTime=0), words.word).count() windowedQuery = windowedCount.withColumn(\u0026quot;trigger_timestamp\u0026quot;, F.expr(\u0026quot;get_current_timestamp()\u0026quot;)).writeStream\\ .trigger(processingTime=\u0026quot;5 seconds\u0026quot;)\\ .outputMode(\u0026quot;update\u0026quot;)\\ .format(\u0026quot;console\u0026quot;)\\ .option(\u0026quot;truncate\u0026quot;, False)\\ .start() streamingQuery.awaitTermination()   We now test the new Spark program.\nData source and timeline visualization\nWe provide two Python programs for this exercise: a data generator and a tool for visualizing words in a timeline. Instructions to get and run these two programs are given in the activity below.\nThe data generator is our data source. It generates two words every second for a certain amount of time. Each word is saved in a separate CSV file in source directory. It also saves the list of all generated words to a summary CSV file.\nThe visualization tool takes as its input the summary CSV file written by the data generator and visualizes the words on a timeline.\n Activity\nCopy to your home directory in the local filesystem the data generator that you find at the following path  /usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly.\n In another terminal, run the Python script tempws_gen.py. Use the following command to learn how to run this program:\n  python3 tempws_gen.py --help For this exercise, do not introduce any delay (keep the default values of the parameters --delay, --mindelay, --maxdelay).\nAfter launching the data generator, you should see some output in the terminal where you launched the Spark program. Wait for the script tempws_gen.py to terminate the data generation. The output might be a bit overwhelming. Scroll up to identify the results on each micro-batch.\n If you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory.\n   We now want to analyze the output of the program.\n The script tempws_gen.py has generated a file gen_words.csv in your home directory. This file contains the list of all words generated with the relative timestamps. Download the file to your computer.\n Download the visualization tool that you find at the following path:\n  /usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py to your computer.\nVisualization tool\nUse the following command to learn how to run the visualization tool:\npython timeline_visualization.py --help The visualization tool displays a vertical blue bar at each trigger. To this purpose, you’ll need to pass the tool the timestamps associated to the first and last trigger and the interval (in seconds) between two consecutive triggers.\nYou can get the timestamps associated to the first and last trigger by analyzing the output of Spark. More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you’ll need to look at the timestamp associated to the first and last micro-batch.\n Exercise\nExercise 4.2 \nAnalyze the output of your Spark program and the timeline of the generated words.\n Describe how the counts are updated by the Spark program.     Solution\nUnlike the previous exercise, here the number of occurrences of each word is counted based on a time window of 10 seconds that slides every 5 seconds. Each word is associated with an event time that is used to compute the number of occurrences. The time window starts from the first trigger, say at 12:00. If a word arrives at 12:07, the count associated to this word are updated in two time windows, 12:00 - 12:10 and 12:05 - 12:15.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"96a6833d49e7a19d1120d1cc206d0e1a","permalink":"/courses/big-data-marseille/tutorials/spark-streaming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/spark-streaming/","section":"courses","summary":"Tutorial Structured Streaming.","tags":null,"title":"Apache Spark — Structured Streaming","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model.  Prerequisites:\n Having attended Lecture 1.  1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1  Give the conceptual schema of the database with an ER diagram.\n   Solution\n  Exercise\nExercise 1.2  Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\n   Solution\nThe collection of tables is the following:\n UserAccount (nickname, first_name, last_name, city, country) Post (post_id, content, date, time, lat, long, nickname) EmailAddress (email_address, nickname) Relationship (nickname_src, nickname_dst, type, date) Tag (post_id, nickname)  The foreign keys are the following:\nPost(nickname) → UserAccount(nickname).\nEmailAddress(nickname) → EmailAddress(nickname).\nRelationship(nickname_src) → UserAccount(nickname).\nRelationship(nickname_dst) → UserAccount(nickname).\nTag(post_id) → Post(post_id).\nTag(nickname) → UserAccount(nickname).\n    2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\n Figure 2.1: The conceptual schema of the bank database  Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer.    Solution\nSince no two banks have the same code_bank or name, either property can be chosen as the primary key of the entity Bank. Both can be considered as valid candidate keys.\n  Exercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer.    Solution\nThe answer is no. While there aren’t any banks that have the same value for {code_bank, name}, two subsets ({code_bank} and {name}) are candidate keys.\n  Exercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises.    Solution\n  Exercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.    Solution\nThe collection of tables is the following:\n Bank (code_bank, name, address) Branch (branch_id, address, code_bank) Account (acct_nbr, acct_type, balance, branch_id, ssn) Loan (loan_nbr, loan_type, amount, branch_id, ssn) Customer (ssn, first_name, last_name, telephone, address)  The foreign keys are the following:\nBranch(code_bank) → Bank(code_bank).\nAccount(branch_id) → Branch(branch_id).\nAccount(ssn) → Customer(ssn).\nLoan(branch_id) → Branch(branch_id).\nLoan(ssn) → Customer(ssn).\n    3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, colour, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\n A salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys.  When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram.    Solution\n  Exercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.     Solution\nThe collection of tables is the following:\n Car (serial_number, make, model, colour, is_new) Customer (cust_id, cust_first_name, cust_last_name, cust_phone) Invoice (invoice_number, date, price, car_serial_number, sp_id, cust_id) Salesperson (sp_id, sp_first_name, sp_last_name) Mechanic (mec_id, mec_first_name, mec_last_name) Ticket (ticket_number, date_open, date_return, car_serial_number) Repair (ticket_number, mec_id)  The foreign keys are the following:\nInvoice(cust_id) → Customer(cust_id).\nInvoice(car_serial_number) → Car(serial_number).\nInvoice(sp_id) → Salesperson(sp_id).\nTicket(car_serial_number) → Car(serial_number).\nRepair(ticket_number) → Ticket(ticket_number).\nRepair(mec_id) → Mechanic(mec_id).\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85c2215a0f57e47f0e3f4cc4fba8d167","permalink":"/courses/databases/tutorials/data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/data-modeling/","section":"courses","summary":"Description of the data modeling tutorial.","tags":null,"title":"Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to run containers. How to define and build images. How to create and use volumes. How to define and use networks.  Prerequisites:\n Having installed Docker on your computer or having imported the Linux virtual machine either with VirtualBox or with Multipass.   Using a virtual machine with Multipass? Click here for more info\nMultipass commands\n To start the virtual machine, type multipass start cloudvm.\n The folder /home/ubuntu/labs in the virtual machine is mounted (i.e., linked) to the folder on YOUR computer (let’s call it LAB) where you’ll store all your lab material. You specified this folder when you installed the virtual machine.\n If you don’t remember the path to the folder linked to /home/ubuntu/labs, type the command multipass info cloudvm.\n To open a terminal into the virtual machine, type multipass shell cloudvm.\n When the terminal opens, the current working directory is /home/ubuntu.\n Type cd labs to move to the folder where you’ll find your lab material.\n You’ll use the virtual machine terminal only to type the Docker commands. You can use all the tools installed on your machine to create and manage the files in the directory LAB directly from your computer.\n Once the lab is over, type exit to leave the virtual machine terminal. This will lead you back to the terminal of your computer.\n In the terminal of your computer, type multipass stop cloudvm to stop the virtual machine. This will NOT destroy your files! It just stops the virtual machine from needlessly using the resources of your computer.\n     Being familiar with the notions of containers, images, volumes and networks in Docker. See the Lecture 2 for an introduction. Being familiar with the basic notions of Linux. Don’t hesitate to look at the Docker cheat sheet to verify the syntax of the Docker commands.  Terminology\n You’ll use the terminal to run Docker commands. Referring to the Docker architecture, the terminal is the client that communicates with the Docker daemon.\n Docker runs containers on your computer. We’ll refer to your computer as the host, the containers being the guests.\n   A containerized application is an application running in a container.    1 Running containers (\\#exr:unnamed-chunk-1)  For each of the following images, specify the registry name, the user, the name and the tag. 1. registry.redhat.io/rhel8/mysql-80 2. alpine:3.11 ::::: {.last-child} 3. alpine ::::: \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2) What's the difference between the following image names? 1. alpine:latest 2. registry.hub.docker.com/library/alpine ::::: {.last-child} 3. alpine ::::: \\EndKnitrBlock{exercise} ::: -- We now learn how to use the command docker run and some of its options. In the following exercises, we’ll run containers from the image named alpine that is available on the DockerHub registry. This image provides a lightweight distribution (i.e., it doesn’t contain many features) of Linux.\nExercise\nExercise 1.1 You want to run the container from the latest version of the image alpine. Which command would you write in the terminal? Execute it.   (\\#exr:unnamed-chunk-4) Execute the command that you proposed in the previous exercise, observe the output in the terminal and explain the actions taken by Docker to run the container.\\EndKnitrBlock{exercise} ::: -- Exercise\nExercise 1.2 Is the container still running?   (\\#exr:unnamed-chunk-6) What information is displayed for each container?\\EndKnitrBlock{exercise} ::: -- Exercise\nExercise 1.3 By looking at the command executed within the container (/bin/sh), can you tell why the container stopped without giving any output?   We’re now going to do something useful with the image alpine. Make sure you read the good practices that you should adopt while playing with images and containers.\nGood practices\nName your containers. Although Docker assigns a default name to a new container, it’s usually a good practice to give a container a name of your choice to make it easily distinguishable. You can do it by using the option --name. Try the following:  docker run --name my-alpine alpine\nAs before, the container stops immediately. If you list all your containers by typing again:\ndocker container ls -a\nyou should see a container named my-alpine.\nRemove automatically a container if you use it once. Unless you want to reuse your container later, you can ask Docker to automatically remove it when it stops by using the option --rm. This will prevent unused containers from taking up too much disk space.  Try the following:\ndocker run --rm --name container-to-remove alpine\nIf you list all the containers you should see that there is no container named container-to-remove.\nRemove unused containers. Stopped containers that have been run without using the option --rm are still stored in the host. If you want to remove a specific container (e.g., my-alpine), use the following command:  docker container rm my-alpine\nIf you want to remove all stopped containers, use the following command:\ndocker container prune\nRemove unused images. Images can take up a lot of disk space. As a result, you should remember to remove those that you don’t intend to use any longer. The commands to remove a specific image and prune unused ones are docker image rm and docker image prune -a respectively.   1.1 Pass a command to the containerized application Remember that the template of docker run is the following:\ndocker run [options] image-name [command] [arg]\nThe optional parameter command refers to a command that you can pass the containerized application, possibly with some arguments (parameter arg).\nLet’s see an example. As we saw before, when we run a container from the image alpine, a Linux terminal /bin/sh is launched.\nNotice\nThe Linux terminal /bin/sh is run within the container. Henceforth, we’ll use the following terms:\n Host terminal. The terminal that you use to interact with the operating system of your computer.   Guest terminal. The terminal that is run within the container.    By using the optional parameter command, we can run a command in the guest terminal.\nExercise\nExercise 1.4  Run a container from the image alpine and execute the Linux command ls that lists the content of the current directory.\n Where are the listed files stored? In the host or in the container?     Notice\nIn Exercise 1.4 the command ls is executed in the guest terminal, but its output is redirected to the host terminal.\nIn other words, when we run the container, we don’t interact directly with the guest terminal; we just send a command and the output is redirected to the host terminal.\n Now let’s see how to execute a command in the guest terminal that also requires an argument.\nExercise\nExercise 1.5 By using the Linux utility ping, check whether the Web site www.centralesupelec.fr is reachable.    (\\#exr:unnamed-chunk-9) Run a container from the image *alpine* to execute the Linux command `rev` and interact with it. You can stop interacting with ``rev`` by typing Ctrl+C at any time.\\EndKnitrBlock{exercise} ::: -- Now run the following command:\ndocker run --name my-alpine -it alpine\nNote: we didn’t use the option --rm (the container will not be removed when we stop it, we’re going to use it again). Moreover, we didn’t specify any command to run in the guest terminal.\nExercise\nExercise 1.6 What do you obtain?    1.2 Starting and stopping containers. docker run is a shorthand for two Docker commands, namely docker create, that creates a container from an image, and docker start, that starts the container after its creation.\nSuppose now that you want to download a Web page by using Linux Alpine. You can use the Linux command wget followed by the URL of the page that you want to download.\nExercise\nExercise 1.7 By using the guest terminal in the container my-alpine, download this Web page.\n Where will the Web page be saved? The host computer or the container?      Want to copy the donwloaded file from the container to your computer?\nTo copy the file presentation to your working directory, type the following command in the host terminal:\n docker cp \u0026lt;containerId\u0026gt;:/presentation . In the previous command, replace \u0026lt;containerId\u0026gt; with the identifier of your container. You can obtain the identifier of the container from the output of the command docker container ls -a.\n Where are the containers and image files stored?\n If you use MacOS or Windows\nThe files managed by Docker are not stored directly in your computer, but in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).\nTherefore, you need to open a terminal inside that Linux virtual machine by typing the following command:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1 Once the terminal is opened, you can follow the instructions given below for Linux users.\n  If you use Docker on Linux\n All files managed by Docker are stored under folder /var/lib/docker.\n To access that folder, you need to be root (i.e., administrator). Type the command sudo su.\n If you type ls /var/lib/docker you can look at the folders stored under this directory. You’ll see that there are folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).\n To locate the files of a specific container, you first need to get the container identifier by typing docker container ls -a.\n Type the command docker inspect \u0026lt;container-id\u0026gt; (replace \u0026lt;container-id\u0026gt; with the identifier of the container that you intend to inspect).\n Locate the field UpperDir. The value of this field is the path to the directory (let’s call it CONTAINER_DIR) that contains the upper layer of the container (the writable layer). It should be a path that ends by /diff.\n If you type cd CONTAINER_DIR (replace CONTAINER_DIR with the value of the field UpperDir) you can finally see the files stored in your container.\n    In my-alpine guest terminal type exit. This closes the guest terminal and, as a result, stops the container.\nNOTICE\nStopping the container will not erase any of the files stored in the container. Removing the container will.\n If you want to start the container my-alpine again, you can use the following command:\ndocker container start -ai my-alpine\nThis will open the guest terminal of the container again; type ls to verify that the Web page that you downloaded before is still there.\n Homework (optional)\nSuppose that you need to download all the figures of this Web page. The Linux utility wget comes in handy. However, you don’t have Linux and you’d like to avoid the hassle of installing it on your computer, or in a virtual machine, just for this task.\nA great alternative is to run Linux in a Docker container. Unfortunately, the Alpine distribution that we’ve been playing with doesn’t provide an implementation of wget with all the options that we need.\nWe turn to another Linux distribution, Ubuntu, for which DockerHub has several images.\nExercise\nExercise 1.8 Run a container with Ubuntu (latest) and open a guest terminal. Call the container dl-figures, and avoid the option --rm, we’ll use this container later.   From now on, we’ll be interacting with the guest Ubuntu terminal. If you type the command wget, you’ll get an error (bash: wget: command not found).\nNotice\nThe image Ubuntu doesn’t include all the commands that you’d find in a full-blown Ubuntu distribution; the reason is to keep the size of the image small, a necessary constraint given that images are transferred over the Internet.\n Luckily, there’s a way to install wget in our Ubuntu distribution. Ubuntu provides a powerful command-line package manager called Advanced Package Tool (APT). First, you need to run the following command:\napt update\nwhich fetches the available packages from a list of sources available in file /etc/apt/sources.list.\nThen, you can install wget by running the following command:\napt install -y wget\nIn order to obtain all the figures from a Web page, type the following command:\nwget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation You should see in the current directory a new folder named my-figures containing the downloaded figures; verify it by typing ls my-figures.\nBefore terminating, don’t forget to read your fortune cookie. In the shell, run the following command:\napt-get install -y fortune\nand then:\n/usr/games/fortune -s\nWhen you’re done, you can simply type the command exit to quit the guest terminal and stop the container.\n   2 Creating Images A Docker image can be thought of as a template to create and run a container. An image is a file that contains a layered filesystem with each layer being immutable; this means that the files that belong to a layer cannot be modified or deleted, nor can files be added to a layer.\nWhen a container is created from an image, it will be composed of all the image read-only layers and, on top of them, a writable layer (termed the container layer), where all the new files created in the container will be written. For example, the Web page that you downloaded in Exercise 1.7 were stored in the writable layer of that container (cf Slide 64).\n(\\#exr:unnamed-chunk-12) If layers, except the top one, are immutable, how can files that belong to the lower layers be modified or deleted?\\EndKnitrBlock{exercise} ::: We can create a new image from the container *dl-figures*, one that provides a Ubuntu distribution with the command ``wget`` already installed, with the following command: ```shell docker commit dl-figures ubuntu-with-wget ``` The command creates a new image called *ubuntu-with-wget*. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-14) Run a container from the image *ubuntu-with-wget* and verify that the command *wget* is actually installed. \\EndKnitrBlock{exercise} ::: -- 2.1 Dockerfiles The most common way to create an image is to use a Dockerfile, a text file that contains all the instructions necessary to build the image. The advantage of the Dockerfile is that it can be interpreted by the Docker engine, which makes the creation of images an automated and repeatable task.\nSuppose that we want to create a containerized application to download figures from a Web page. As a template for this application, we need to build a new image, that we’ll call fig-downloader.\nThe Dockerfile containing the instructions to build the image fig-downloader is as follows:\nFROM ubuntu RUN apt-get update RUN apt-get install -y wget RUN mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] Here’s the explanation:\nWe use the image ubuntu as the base image. This corresponds to the instruction FROM ubuntu.\n We install the utility wget in the base image. This corresponds to the instructions RUN apt-get update and RUN apt-get install -y wget.\n We create a directory my-figures under the root directory of the image. This corresponds to the instruction RUN mkdir -p /my-figures.\n We set the newly created directory /my-figures as the working directory of the image. This corresponds to the instruction WORKDIR /my-figures.\n We specify the command to be executed when a container is run from this image. This corresponds to the instruction\n  ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;]\nThis instruction means: execute wget with the options -nd, -r, -A; the last option takes a list of file extensions (jpg,jpeg,bmp,png,gif) as its argument.\nRemember that the utility wget takes the URL of the Web page as an argument. The URL will be specified when we run the container from the image fig-downloader. Optionally, we can specify a default argument by using the keyword CMD. The meaning of the instruction:  CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;]\nis: if we don’t give any URL when we run the container, the figures will be downloaded from https://www.centralesupelec.fr/fr/presentation.\nExercise\nExercise 2.1 What’s the relation between the Dockerfile lines and the image layers?   Exercise\nExercise 2.2 Could you identify a problem in this Dockerfile? Modify the Dockerfile accordingly.    2.2 Building an image We’re now going to build an image from a Dockerfile.\nCreate a directory named fig-downloader in your computer with a file named Dockerfile inside.\n In the Dockerfile write the set of instructions that you proposed in Exercise 2.2.\n In the terminal, set the working directory to fig-downloader.\n Build an image called fig-downloader by executing the following command:\n  docker build -t fig-downloader .\nThe . at the end of the command means that the Docker engine will look for a file named Dockerfile in the working directory.\n(\\#exr:unnamed-chunk-16) Once the image is built, type the command ``docker image ls -a``. What are the images with repository and tag ````? Why are there three of such images?\\EndKnitrBlock{exercise} :::  -- Good to know\nIf you give the Dockerfile a different name (say, Dockerfile-fig-downloader), the command to build the image will be:\ndocker build -t fig-downloader -f Dockerfile-fig-downloader .\nThe option -f is used to specify the name of the Dockerfile.\n In order to verify that the new image has been created, type the following command:\ndocker images\n(\\#exr:unnamed-chunk-17) Run the following command: `` docker history fig-downloader `` and analyze the layers of the new image. * Why do some layers have an ID, while others are marked as missing? ::::: {.last-child} * Can you find the identifiers of the intermediate images? ::::: \\EndKnitrBlock{exercise} ::: -- Exercise\nExercise 2.3 Run the following command:\ndocker run --name dl-1 fig-downloader\nWhat does it do? Where are the downloaded pictures?   Exercise\nExercise 2.4 Run the following command:\ndocker run --name dl-2 fig-downloader https://www.centralesupelec.fr/ What does it do? Where are the downloaded pictures?    2.3 Containerized Python application Download this archive file and unzip it into your working directory. In this archive you’ll find:\n A Dockerfile. A Python script main.py that asks the user to enter the URL and the language of a Web page, and prints the 10 most frequent words occurring in that page. A file requirements.txt with the list of the Python packages needed to run the given script.  The content of the Dockerfile is as follows:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./main.py ./requirements.txt /app/ RUN pip install -r requirements.txt ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.5 Describe what this Dockerfile does.   Exercise\nExercise 2.6 Build an image called wordfreq from this Dockerfile.   Exercise\nExercise 2.7 Without changing the Dockerfile, rebuild the same image. What do you notice?   Exercise\nExercise 2.8 What happens if you modify a line in the Python script and you rebuild the image?   Exercise\nExercise 2.9 Based on the previous considerations, can you tell what’s wrong with this Dockerfile? Modify the Dockerfile accordingly and rebuild the image.   Exercise\nExercise 2.10 Modify main.py by adding a new line of code and rebuild the image. What changed?   Play with the application by running the following command:\ndocker run --rm -it wordfreq\nThe containerized application will prompt you to insert the URL of a webpage and the language of the page (in English). The output will be the 20 most used words in the webpage.\n  3 Data Volumes In Exercise 2.3 you’ve been asked to run a container named dl-1 to download some figures from a Web page. The figures were downloaded into the directory /my-figures of the container. But we left a question unanswered.\nHow do we transfer those figures from the container to the host computer?\nThe solution is to use volumes.\n(\\#exr:unnamed-chunk-25) Can you tell why this solution is less than ideal?\\EndKnitrBlock{exercise} ::: ## Using a host volume A better solution is to **mount** (i.e., attach) a directory of the host computer at the container's directory */my-figures* when we run it. Let's see how it works. **Step 1.** Create a directory named *figs-volume* in your working directory. **Step 2.** Type and execute the following command: ```shell docker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader ``` This command runs a container from the image *fig-downloader*. * With the option ``-v`` we specify that we want to mount the directory *\\$(pwd)/figs-volume* (*\\$(pwd)* indicates the host working directory) at the directory *figs-volume* in the container; * The option ``--rm`` indicates that we want the container to be removed when its execution is over. **Step 3.** Verify that the pictures are in the folder *figs-volume*. In this example, we've used the directory *figs-volume* as a **volume** (essentially, an external storage area) of the container; when the container is destroyed, the volume remains with all its data.-- 3.1 Docker volumes A volume can be seen as a virtual storage device attached to a container. All files there are written to a volume survive the containerized application that created them. In other words, when a container is destroyed, all the files created by the application in the container remain in the volume.\nLet’s create a new Docker volume called data-volume:\ndocker volume create data-volume\nGood to know (advanced notion)\nWhere the data will be actually stored?\nYou can inspect the new volume by typing the following command:\ndocker volume inspect data-volume\nA mount point is indicated; that’s the folder where the data will be actually stored. If your computer runs Linux, that folder will be available on the host; if your computer runs Windows or MacOS, you’ll not find that folder on your computer. Instead, it will be available in the virtual machine that Docker use on MacOS and Windows.\nDo you want to see the directory? (Instructions for Windows and MacOS)\nOne way to look into the hidden VM is to run the following containerized application:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1\nThis application will open a guest terminal into the VM. You can then use the commands cd and ls to browse to the directory indicated as the mount path of the new volume.\n 3.1.1 Sharing data A Docker volume can be used to share data between containers.\nExercise\nExercise 3.1 Run a container from the image ubuntu, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume at the container’s directory /data.   Feel free to use the Docker cheat sheet.\n  Type the following command in the guest Linux terminal to create a file test-file.txt in the directory /data:  echo \u0026quot;This is a new file\u0026quot; \u0026gt; /data/test-file.txt\nPrint to the console the content of the file with the following command:  cat /data/test-file.txt\nType exit to leave the guest terminal. Since we’ve specified the option --rm, the container is destroyed. Now we’re going to verify that test-file.txt is still accessible.  Exercise\nExercise 3.2 Run a container from the image alpine:latest, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume to the directory /my-data of the container.     Exercise\nExercise 3.3 Verify that you can read the file test-file.txt. Which folder would you look in?   Type exit to exit the guest terminal and terminate the container.\n   4 Single-Host Networking In order to let containers communicate and, therefore, co-operate, Docker defines a simple networking model known as the container network model\n If you use a Linux virtual machine with Multipass In this section, you’ll need to open several terminals in the virtual machine. You can do it easily by using byobu, an advanced window manager already available in your virtual machine.\n Just type byobu to launch the window manager.\n If you want to open a new terminal, just press F2.\n If you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4 (to move to next terminal).\n If you want to close a terminal, just type exit.\n When you close all terminals, byobu will stop executing.\n   Exercise\nExercise 4.1 Describe the output of the following command:\ndocker network ls\n  Exercise\nExercise 4.2 The following command:\ndocker network inspect bridge\noutputs the configuration of the network bridge. By looking at this configuration, can you tell what IP addresses will be given to the containers attached to this network? What’s the IP address of the router of this network?   4.1 Creating networks By default, any new container is attached to the network named bridge.\nExercise 4.3  Explain why it is not a good practice to attach all our containers to the same network.\n  In order to create a new network, you can use the following command:\ndocker network create network_name\nExercise\nExercise 4.4 Create two networks named buckingham and rochefort that use the driver bridge. By using the docker network inspect command, look at the IP addresses of the new networks and write them down.   Exercise\nExercise 4.5 Create three containers athos, porthos and aramis and attach them to the two networks buckingham and rochefort as displayed in this figure. The three containers will open a Linux Alpine shell. You’ll need to launch the commands in three separate tabs of your terminal window.\n What will the IP addresses of the three containers be in the two networks? Remember that porthos is attached to two networks, therefore it’ll have two network interfaces (endpoints) and, as a result, two IP addresses.   Verify your answers by inspecting the two networks (use the command docker network inspect).      4.2 Communication between containers Let’s see if and when the three containers can communicate.\nExercise\nExercise 4.6 Which containers are able to communicate? Justify your answer.   Exercise\nExercise 4.7 Try to ping porthos from athos by using its IP address.\n Which IP address of porthos would you use?     Exercise\nExercise 4.8 Try to ping porthos from athos by using its name. Do you succeed? Are you surprised?   You can now exit the three containers.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a74fda64fd4d3f15083e937e46612453","permalink":"/courses/cloud-computing/tutorials/tutorial-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-docker/","section":"courses","summary":"Text of the lab assignment on Docker","tags":null,"title":"Getting started with Docker","type":"docs"},{"authors":null,"categories":null,"content":" Warning\nIf you use the Linux VM, you need to change the hardware configuration of the VM before booting it. You can also watch this video. Here are the necessary modifications:\n Increase the main memory to 4GB.\n Set the number of CPUs to 2.\n  Without these modifications you’re not going to have a good experience with Kubernetes.\n In this tutorial you’ll learn:\n How to build and deploy a multi-service application with Docker Compose.\n How to push an image to DockerHub.\n How to deploy a multi-service application with Kubernetes.\n  Warning\nThis tutorial is adapted from the examples presented in Chapters 14 and 20 of the book G. Schenker, Learn Docker - Fundamentals of Docker 19.x (March 2020).\n 1 Docker Compose In this section, you’re going to build and deploy a multi-service application by using Docker Compose.\nDownload this archive file and unzip it into a folder on your own computer. The archive contains all the necessary files to build and run a web application consisting of two services:\n web. This is the frontend (the part the user interacts with) of the application. It consists of HTML and JavaScript code.\n db. This is the backend (the part hidden to the user). It is a PostgreSQL (relational) database.\n  The structure of the application is shown in Figure 1.1. The root directory of the application contains two subdirectories, one for each service (database and web).\n Figure 1.1: Structure of the application  Good to know\n The files with extension .conf under the directory database contain configuration parameters of the PostgreSQL database.\n The file init-db.sql under the directory database contains the SQL queries to populate the database with some data (photos of cats).\n The directory web contains the HTML and JavaScript code of the web application. The file package.json contains the dependencies to install.\n   Both directories contain a Dockerfile. The one in directory database is as follows:\nFROM postgres:10.2-alpine COPY init-db.sql /docker-entrypoint-initdb.d/ RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql ENV POSTGRES_USER dockeruser ENV POSTGRES_PASSWORD dockerpass ENV POSTGRES_DB pets The Dockerfile builds on an existing image called postgres:10-2-alpine, that is documented here.\nExercise\nExercise 1.1  Consider the following line in the Dockerfile:\nCOPY init-db.sql /docker-entrypoint-initdb.d/\nBy looking at the documentation of the image postgres:10-2-alpine, answer the two following questions:\n Where is the directory /docker-entrypoint-initdb.d/?\n Why do we copy the file init-db.sql to this directory?\n    The last three lines of the Dockerfile contain a keyword (ENV) that we never came across before.\nExercise\nExercise 1.2  Look again at the documentation of the image postgres:10-2-alpine and try to explain the meaning of the last three lines of the Dockerfile.\n  The Dockerfile of the web application is as follows:\nFROM node:9.6-alpine RUN mkdir /app WORKDIR /app COPY package.json /app/ RUN npm install COPY ./src /app/src EXPOSE 3000 CMD node src/server.js The Dockerfile builds on the image node:9.6-alpine that contains a Node.js environment, a JavaScript-based platform for server-side applications. The instructions in the Dockerfile look like the ones of the examples that we’ve seen in the first tutorial and in the lectures.\nGood to know\n The command npm install installs all the dependencies specified in the file package.json from the software registry npm.\n The instruction EXPOSE 3000 informs that the container listens on port 3000 when it is executed from the image.\n  From the Docker documentation we learn that the “EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published”.\nIn order to actually publish the port, we’ll use the file docker-compose.yml.\n 1.1 Describing the application The application showcases a building block of a pet store. In the current version, the application lets users look at few pictures of cats.\nThe root directory of the application contains a file named docker-compose.yml that contains the declarative configuration of the application. The content of the file is as follows. It is a sequence of key-value pairs.\nversion: \u0026quot;3.6\u0026quot; services: web: build: web image: pet-store-web networks: - backend ports: - 5000:3000 db: build: database image: pet-store-db networks: - backend volumes: - pets-data:/var/lib/postgresql/data networks: backend: volumes: pets-data: There are three main sections:\n services. Defines the services of the application. Here two services are defined: web and db.\n networks. Defines the networks used by the application. Here one network is defined: backend.\n volumes. Defines the volumes used by the application. Here one volume is defined: pets-data. The volume is attached to the directory /var/lib/postgresql/data (that is in the container).\n  Exercise\nExercise 1.3  What key informs docker compose where to find the Dockerfile of the two services?\n  When we’ll build the application from the file docker-compose.yml, two images will be created, one for each service.\nExercise\nExercise 1.4  What will the name of the two images be? What key in the file docker-compose.yml gives you this information?\n   1.2 Building the application We now build the application.\n Open the command-line terminal and by using the command cd position yourself in the root directory pet-store of the application.\n Execute the following command:\n  docker-compose build\n When the build is complete, verify that the two images corresponding to the two services have been created (which docker command do you need here?).   1.3 Executing the application We now execute the application with the following command:\ndocker-compose up -d\nGood to know\nThe option -d in the previous command means that the application is executed in the background or, in other words, the containers are run as daemons (hence, the -d). This means that any output of the application is not shown in the terminal.\n Exercise\nExercise 1.5  Verify that the network and the volumes associated with the application have been correctly created.\n  Exercise\nExercise 1.6  How many containers do you expect to be associated to the application?\n  You can verify the answer to the previous question by typing the following command:\ndocker-compose ps\nThis command is exactly equivalent to docker container ls, except that it only shows the containers associated with the application that we’ve just executed.\nExercise\nExercise 1.7  In the output of the command docker-compose ps, can you explain the meaning of the following?\n0.0.0.0:5000-\u0026gt;3000/tcp\n  Exercise\nExercise 1.8  Can you tell which URL you have to type in your web browser to access the application?\n  Warning\nIf you want to see the cats, you need to append /pet to the URL that you found in the previous question. This is determined in file server.js.\n  1.4 Shutting down the application You can shut down the application by typing the following command:\ndocker-compose down\nExercise\nExercise 1.9  Does shutting down the application remove the networks created for the application? What about the volumes?\n   1.5 Scaling a service When we launch the application, we can specify the number of instances of each service. This is useful when we expect our application to be solicited by many users; the workload will be automatically balanced across all the instances of the service.\nLet’s launch our application with 3 instances of the service web:\ndocker-compose up --scale web=3 -d\nExercise\nExercise 1.10  Running the previous command results in an error.\n Can you explain why?\n What fix would you propose in file docker-compose.yml?    Warning\nYou need to shut down the application before relaunching it again. In fact, even if we got an error, an instance of the db service and an instance of the service web are still running.\n  Modify the file docker-compose.yml and relaunch the application.\n Run the following command and verify that you actually have three running instances of the service web.\n  docker-compose ps\n Try to connect to the application by using the three port numbers indicated in the output of the previous command.  Warning\nDon’t forget to shut down the application before you go on.\n   2 Pushing an application Once we have built an application, we might want to share it by pushing it to the DockerHub registry or any other container registry, be it private or public.\n2.1 Creating an account on DockerHub You need to create an account on the DockerHub website in order to perform this activity.\n 2.2 Renaming the images In order to push your images to the registry, you need to rename them, so as the new name has the following structure:\nyourusername/image-name:image-tag\nFor instance, since my username is quercinigia, I’ll rename the two images pet-store-web and pet-store-db with the docker tag command as follows:\ndocker tag pet-store-web quercinigia/pet-store-web:1.0\ndocker tag pet-store-db quercinigia/pet-store-db:1.0\nI chosen 1.0 as a tag, but feel free to pick another one.\n 2.3 Logging in You need to log in to your DockerHub account.\n Instructions for Docker Desktop (macOS and Windows)\n macOS users: to see how to log in click here.\n Windows users: to see how to log in click here.\n    Instructions for the Linux VM\nLog in to DockerHub by typing the following command in the terminal (replace YOUR-USERNAME with your actual username).\ndocker login -u YOUR-USERNAME\nWarning\nYou might get a warning that your password will be stored unencrypted. There are methods to prevent this from happening. These methods being out of the scope of this course, the interested reader can look them up at this link.\n   2.4 Pushing the images Once you’re successfully logged in, you can type the following commands in the terminal to push the two images of your application:\ndocker push YOUR-USERNAME/pet-store-web:1.0\ndocker push YOUR-USERNAME/pet-store-db:1.0\nRemember to replace YOUR-USERNAME with your actual username in the commands above.\nAfter the task is completed, verify that the images appear in your Docker registry.\nGood to know\nHere we manually tagged and uploaded the two images. This method becomes quickly annoying when the application consists of more than two images. Another way to go about this task is:\n Specify the image names with your username in file docker-compose.yml. For instance, in my docker-compose.yml I would write:  services: web: build: web image: quercinigia/pet-store-web:1.0 ... db: build: database image: quercinigia/pet-store-db:1.0 ...  Build the application with the following command:  docker-compose build\n Push the images of the application with the following command:  docker-compose push\nThis way, with just two commands we push to the registry all the images of the application, no matter how many they are.\n   3 Introduction to Kubernetes Kubernetes is the most popular orchestrator to date. It is used to manage a multi-service application, usually deployed across multiple hosts in a cluster.\nWhile using Docker Compose, a service corresponds to a Docker image and a service instance to a Docker container.\nAs opposed to that, in Kubernetes the computation unit is a pod, that is a collection of containers. In other words, we don’t reason in terms of containers anymore, but in terms of pods. Of course, a pod can also consist of just one container.\nGood to know\nIn practice, we rarely have to manipulate pods directly in Kubernetes, as there are higher-level objects that manage them. We’ll use these objects in the next sections.\n 3.1 Activate Kubernetes  Instructions for Docker Desktop (macOS and Windows)\nYou need to follow all the instructions documented on this page\nBefore moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated\nkubectl get nodes\nBefore moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated\nkubectl get nodes\nYou should see a node called docker-desktop whose status is set to READY. If the status is NOT READY, just wait few seconds before typing the command again.\nWarning\nThis solution might not be working for you. In that case, disable Kubernetes in Docker Desktop and install minikube, by following these instructions.\n   Instructions for the Linux VM\nIn the Linux VM you’ll find Minikube, a single-node Kubernetes cluster in VirtualBox. Please follow all the instructions:\n Start the cluster, type the following command:  minikube start\n Verify that Kubernetes is correctly activated by typing the following command:  kubectl get nodes\nYou should see a node called minikube whose status is set to READY. If the status is NOT READY, just wait few seconds before typing the command again.\n Open a new terminal and type the command:  minikube tunnel\nWhen prompted to enter a password, just type ENTER. The command will start to produce some output. Leave that terminal open and go back to the previous terminal.\nTunnel\nThe Minikube tunnel is used to create a route to services deployed with type LoadBalancer. If you don’t activate the tunnel, you won’t be able to use these services in the exercises below.\n   3.2 Deploying a pod In order to deploy a pod, we first have to give its specification, basically its name and the containers that compose it with their settings. Similarly to Docker Compose, a pod is specified in a declarative way with a YAML configuration file.\nConsider the following specification.\napiVersion: v1 kind: Pod metadata: name: web-pod labels: app: web-pod spec: containers: - name: web image: nginx:alpine ports: - containerPort: 80 Here is an explanation of the properties in the specification.\n apiVersion. Defines the versioned schema of this representation.\n kind. The type of the resource that we intend to create.\n metadata. The resource metadata. The list of all metadata is specified here.\n spec. The specification of the desired behaviour of the pod. The list of the possible specifications can be found here.\n  Essentially, the previous specification defines a pod with a container that is launched from the image nginx:alpine (a Web server) and listens to ports 80 and 443.\nActivity\n Copy the previous specification to a file named sample-pod.yaml (or any other name of your liking).\n By using the command cd in the terminal, place yourself in the directory where the file sample-pod.yaml is stored.\n Deploy the pod by typing the following command:\n  kubectl create -f sample-pod.yaml\n Verify that the pod is deployed by typing the following command:  kubectl get pods\nThe first time you run the last command, you might see that the pod is not ready yet. You need to wait for the image nginx:alpine to be pulled from DockerHub. Wait few seconds and try the command again until the pod is marked as running.\nYou can also get more information on the running pod (e.g., its assigned IP address) by typing the following command:\nkubectl get pod -o wide web-pod\n Exercise\nExercise 3.1  Open a web browser and type http://localhost:80.\n What do you get? What if you try to use the IP of the pod instead of locahost?\n What should we define in order to fix the problem?    The following configuration defines a Service object of type LoadBalancer:\napiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer ports: - port: 8080 targetPort: 80 protocol: TCP name: http selector: app: web-pod Good to know\nA LoadBalancer service is a NodePort service (cf slide 64 in Lecture 3) that offers load balancing capabilities. It is intended to expose an IP address that client applications (external to the Kubernetes cluster) can use to access the service.\n Exercise\nExercise 3.2  What is the field selector in the definition of the service? (cf slide 57 in Lecture 3)\n  Exercise\nExercise 3.3  In the specification of the service, what port and targetPort mean?\n  Activity\n Copy and paste the service specification into a file named sample-service.yaml (or any other name of your liking).\n By using the command cd in the terminal, place yourself in the directory where the file sample-service.yaml is stored.\n Deploy the service by typing the following command:\n  kubectl create -f sample-service.yaml\n Verify that the service is deployed by typing the following command:  kubectl get services\nThis command returns all services running in Kubernetes. For each service, you also get a name. In order to only target the service that you’ve just created, simply type the following command:\nkubectl get svc/nginx-service\nYour service’s name is nginx-service (as specified in file sample-service.yml); svc is only the namespace where all Kubernetes services are put.\n Exercise\nExercise 3.4  By looking at the output of the command kubectl get svc/nginx-service, which URL do you need to type in the Web browser in order to access the service?\n  Warning\nStop the pod and the service before moving on. Here are the commands to do so:\nkubectl delete po/web-pod\nkubectl delete svc/nginx-service\n   4 Kubernetes: deploying an application In this section, we’re going to deploy our pet store in Kubernetes. As a reminder, our application consists of two services: web and db.\nIn order to define an application in Kubernetes, we need to use two types of objects for each service of the application:\n A workload resource that gives the specification of the service, such as the pods that make up the service itself (images, network settings) and metadata, such as the desired number of instances.\n A Service object. As we have seen previously, a service object exposes an IP address that allows client applications, both inside and outside the Kubernetes cluster, to connect to the service.\n  When we define an application in Kubernetes, we rarely, if ever, need to play directly with pods. Instead, we can resort to higher-level objects, called Controllers, for an easier definition of the desired state of the application itself. The type of the controller that we need to use depends on the nature of the service itself: stateless or stateful (slides 14-15 of Lecture 3.).\nExercise\nExercise 4.1 \n Is the service web of our application stateless or stateful?\n Is the service db of our application stateless of stateful?    4.1 The web service: deployment For stateless application services, we can use a Deployment for the deployment of a set of identical pods that are launched across several nodes in the Kubernetes cluster.\nDeployment and ReplicaSet\nDeployments and ReplicaSets have been introduced in slides 51-52-53 of the third lecture.\n We here define the specification of a Deployment corresponding to the service web.\napiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP Here is the explanation of the specification:\n A Deployment named web is created, as indicated by the field metadata.name.\n The Deployment creates three replicated pods, as indicated by the field spec.replicas.\n The Deployment considers that the pods with both labels app: pets and service: web are part of the deployment. This is indicated by the field spec.selector.matchLabels. This field is used to let the Deployment know how to find its pods.\n The configuration of the pods that are part of the Deployment is given in the field spec.template and its subfields.\n Each pod of the Deployment is given labels app: pets and service: web. This is indicated by the field spec.template.metadata.labels. Note that here we specify exactly the same values as in spec.selector.matchLabels. This field is used to give pods labels so that they can be identified and located in Kubernetes.\n Each pod has exactly one container, named web, run from the image quercinigia/pet-store-web:1.0 stored in the DockerHub. The container listens on port 3000 and uses the TCP protocol. This is specified in the field spec.template.spec.containers.\n  Activity\n Copy and paste the previous specification to a new file web-deployment.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-web:1.0 with the one that you pushed to your DockerHub registry.\n Use the command cd in the terminal to position yourself in the directory where the file web-deployment.yaml is stored.\n Deploy this Deployment in Kubernetes by typing the following command:\n  kubectl create -f web-deployment.yaml\n Exercise\nExercise 4.2  Type the following command:\nkubectl get all\nWhich objects have been created following the creation of the Deployment?\n  Exercise\nExercise 4.3  Get the name of one of the running pods and kill it by using the following command\nkubectl delete name-of-pod\nIf the command hangs in the terminal, feel free to type Ctrl-C to get back control of the terminal.\nType again following command:\nkubectl get all\nHow many pods do you see? Is it surprising?\n   4.2 The web service: service Now we need to define a Service in order to expose the web service to the public. Here is the definition:\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web Exercise\nExercise 4.4  Describe the specification of this service.\n  Activity\n Copy and paste the previous specification to a new file web-service.yaml (or any name of your liking).\n Use the command cd in the terminal to position yourself in the directory where the file web-service.yaml is stored.\n Create the service with the following command:\n  kubectl create -f web-service.yaml\n Verify that the service has been created with the following command:  kubectl get services\n Locate the external IP (let’s call it EXTERNAL-IP) of the web service and type the URL http://EXTERNAL-IP:8080 in your Web browser. You should see a Web page where the phrase Pet store appears.    4.3 The db service: StatefulSet Kubernetes has defined a special type of ReplicaSet for stateful services that is called StatefulSet.\nLet’s define a StatefulSet to give the specification of the db service.\napiVersion: apps/v1 kind: StatefulSet metadata: name: db spec: selector: matchLabels: app: pets service: db serviceName: db template: metadata: labels: app: pets service: db spec: containers: - image: quercinigia/pet-store-db:1.0 name: db ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: pets-data volumeClaimTemplates: - metadata: name: pets-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi By now you shouldn’t have any problem understanding the meaning of the fields in this specification. The fields are also documented in the official Kubernetes documentation. The only novelty is the field volumeClaimTemplates. It describes additional constraints on the volumes defined in the specification, in this case the volume named pets-data (where PostgreSQL keeps the data). In particular, two claims are given:\n The access mode ReadWriteOnce means that the volume can be mounted as read-write by a single node in the Kubernetes cluster. Access modes are documented here.\n We request at least 100MB of storage for the database.\n  Activity\n Copy and paste the previous specification to a new file db-stateful-set.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-db:1.0 with the one that you pushed to your DockerHub registry.\n Use the command cd in the terminal to position yourself in the directory where the file db-stateful-set.yaml is stored.\n Deploy this StatefulSet in Kubernetes by typing the following command:\n  kubectl create -f db-stateful-set.yaml\n Exercise\nExercise 4.5  Type the command:\nkubectl get all\nWhich objects have been created when you deployed the StatefulSet?\n  Exercise\nExercise 4.6  Open the Web browser and type the following URL:\n If you’re using Docker Desktop\nhttp://localhost:8080/pet\n  If you’re using Minikube on the Linux VM\nhttp://minikube-ip:8080/pet\nwhere you’ll replace minikube-ip with the external IP address associated with the web service.\n Right after, type the command kubectl get all. What do you observe? Can you explain the reason?\n  In the output of the command get kubectl all, look at the names of the pods that are instances of the web service. Take the name of any these pods, and put it in place of NAME-OF-POD in the following command:\nkubectl logs NAME-OF-POD --previous\nExercise\nExercise 4.7  Does the output of the previous command confirm the explanation given in the previous question?\n    5 The db service: Service object From the above observations, we understand that we need to define a service to expose the database to the clients.\nExercise\nExercise 5.1  Should the db service be accessible to client applications that are external to the Kubernetes cluster?\n  Exercise\nExercise 5.2  Given the answer to the previous question, what should the type of this service be?\n  Exercise\nExercise 5.3 \nWrite the specification of the db service in a file named db-service.yaml (or any other name of your liking).\nCaution. In the file db-stateful-set.yaml the field spec.serviceName indicates the name that the service must have.\n  Activity\n Deploy the service by typing the following command:  kubectl create -f db-service.yaml\n Verify that the service has been created with the following command:  kubectl get all\n Verify that you can reach the application at http://localhost:8080/pet (Minikube users: replace localhost with the external IP address associated to the web service!). It might happen that the database service is not ready yet, and so you’ll get a connection error. Just wait and retry later.   5.1 Shutting down the application After you’re done with the application, you can shut it down with the following commands:\nkubectl delete svc/web\nkubectl delete svc/db\nkubectl delete deploy/web\nkubectl delete statefulset/db\nThe order in which you type these commands doesn’t matter.\nIf you type multiple times the command:\nkubectl get all\nyou should see that the resources progressively disappear.\n 5.2 Conclusion In the previous exercises, we deployed an application with two services, for which we had to create four files and type as many commands. For larger applications this gets a bit annoying. We can write all the definitions in a single file (e.g., pets.yaml) where each specification is terminated by - - -.\nHere is an example, where we write the specification of the Service and Deployment associated with the service web.\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web --- apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f5b5b01a686c8fdb871e8a71fba8ba3","permalink":"/courses/cloud-computing/tutorials/tutorial-kube/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-kube/","section":"courses","summary":"Text of the lab assignment on multi-service applications","tags":null,"title":"Multi-service applications","type":"docs"},{"authors":null,"categories":null,"content":" Objectives In this lab assignment you will:\n Gain a deeper understanding of the Docker networking model.\n Build and deploy a multi-service application with Docker Compose.\n Deploy a multi-service application on a local Kubernetes cluster.\n Deploy a multi-service application on a Kubernetes cluster in Microsoft Azure.\n   Submission  Each submission must be individual.\n In order to submit your work, you need to answer all the questions that you find here. Each question corresponds to an exercise that you find in this page.\n You can answer either in French or in English. Your pick!\n Deadline (extended): 24 May 2021, 23h59\n  Warning\nThis lab assignment is evaluated. You won’t have access to the solutions. You can ask questions anytime (even after the Friday 7 May lab session), should you run into problems with the implementation of the assignment.\n  1 Docker networking model We developed a simple chat room in Python that you can download here. The code has been adapted from this GitHub project.\nParticipants use a client program to connect to the chat room; the chat room is managed by a server application that receives the client connections and forwards the messages between the users. The archive contains the following files:\n client.py. Implementation of the chat room client. server.py. Implementation of the chat room server. utils.py. Library with utility functions used in both client.py and server.py.  Warning\nOnly one instance of the server is running. Several instances of the client can run at the same time. Client instances might not be running on the same host.\nIn order to execute the server, we need to specify the port number as a parameter.\n Exercise\nExercise 1.1 Explain the methodology that you’re going to follow to containerize the client and the server by using Docker. In particular explain:\nCan you create only one image for both server and client? Justify your answer.\n Is it a good idea to use Docker compose in this case ? Justify your answer.\n What you need to build the images in Docker.\n Write the exact command that you run to build an image.\n    Exercise\nExercise 1.2 Containerize the application by following the methodology that you outlined in Exercise 1.1.\nUpload the Dockerfiles in the corresponding question on Edunao.  Warning\nThe size of the images should be kept as small as possible. Remember to include in the images only what you need to build and run the application.\n  Good to know\nIf you need to inspect the file system of an image that you create (e.g., to verify that the files that you copied are actually there), you can open a terminal in the image by using the following command:\ndocker run -it --entrypoint sh IMAGE_NAME\n Exercise\nExercise 1.3  If you built more than one image:\nDo they have some common layers?\n If so, is it something that has an impact on the build time and how?    1.1 Client and server on the same network We want to execute:\n One instance of the server.\n Two instances of the client.\n  The server, as well as the clients, will run in Docker containers attached to the same network.\nExercise\nExercise 1.4  We want to make sure that:\nThe server and the clients can communicate with each other.\n Other Docker containers cannot communicate neither with the server nor with the clients.\n  How can you satisfy both requirements in Docker? Explain your solution and justify it.\n  Exercise\nExercise 1.5  Execute the server and the two clients by using the network configuration that you explained in the previous exercise.\n The server writes messages on the terminal. Remember to launch the container with the appropriate options in order to actually see those messages.\n Users need to interact with the client, that is: read and write messages. Remember to launch the container with the appropriate options in order to actually see those messages.\n    Shut down the application!\nShut down both the clients and the server before you move on.\n On the client-side, type #quit at any moment to exit the chat room.\n Type Ctrl-C to stop the server.\n    1.2 Client and server on different networks We want to execute:\n One instance of the server.\n Two instances of the client.\n  However, neither client is connected to the same network as the server.\nExercise\nExercise 1.6  Why can’t you launch the containers with the same settings as in Exercise 1.5?\nWhat do you propose as a solution instead?\nHINT. You might want to look at slide 77 of the second lecture.\n  Exercise\nExercise 1.7  Execute the server and the two clients by making sure that neither client is connected to the same network as the server.\nWrite the exact commands that you typed for both configuring the network and launching the server and the clients. Explain these commands.\n  Shut down the application!\nShut down both the clients and the server before you move on.\n On the client-side, type #quit at any moment to exit the chat room.\n Type Ctrl-C to stop the server.\n     2 Multi-service application: Docker Compose We intend to build and deploy the application TripMeal by using Docker Compose. You can download the application here. The source code has been readapted from this GitHub repository.\nThe downloaded file is an archive. Extracting the archive will create a folder named tripmeal_sujet.\nExercise\nExercise 2.1  Explain the structure of the content of the folder tripmeal_sujet.\n  Exercise\nExercise 2.2  How many services has the application? Specify the technologies (a.k.a., programming languages) used to implement each service.\n  The Dockerfile in the directory db is already implemented.\nIn case you get Exec format error\nIn order to avoid an Exec format error when you launch the application, you’ll need to add to your Dockerfile an instruction:\nRUN chmod -x path_to_file_app\nwhere path_to_file_app is the path to the file app.py inside the image.\n Exercise\nExercise 2.3  Write the Dockerfile in the directory web. You need a Python 3.7 environment to run the application.\nUpload the Dockerfile to Edunao.\n  In the folder tripmeal_sujet, you’ll find a file named tripmeal.env. It contains the definition of environment variables that are used in the application.\nExercise\nExercise 2.4  By reading the values of the environment variables, you can already know the values to assign to some of the keys in docker-compose.yml. Can you tell which ones?\n  Exercise\nExercise 2.5  What do you need to define in file docker-compose.yml to enable the communication between the different services?\n  The database management system used by the application is MySQL. You can see it by reading the Dockerfile in the directory tripmeal_sujet/db.\nExercise\nExercise 2.6  Which base image is used to build a container for the database? Where is this base image stored? Can you find the documentation of this image in the Internet? Write down the link to the documentation page in the answer.\n  Exercise\nExercise 2.7  By looking at the documentation of the database image, what do you need to define in docker-compose.yml to make sure that the data is not deleted once the application is taken down?\n  You’re finally ready to complete the file docker-compose.yml.\nWarning\nRemember that you need to pass the environment variables to your application.\nAs a documentation of Docker Compose you can use:\n The examples that we’ve seen together.\n The overview presented on the official Docker website.\n You can also find the full specification of Compose here.\n   Exercise\nExercise 2.8  Write the file docker-compose.yml. Build, deploy and test your application.\nUpload the file docker-compose.yml to Edunao.\n  Exercise\nExercise 2.9  Push all the images that you built in this section to your DockerHub registry.\nIn the answer to this exercise write the complete names of these images.\n  Shut down the application!\nShut down both the application before you move on.\n  3 Local Kubernetes cluster We intend to deploy the application TripMeal on a local Kubernetes cluster (either Docker Desktop or Minikube).\nExercise\nExercise 3.1  For each service of the application TripMeal, specify which are stateless and which are stateful. Justify your answer.\n  Exercise\nExercise 3.2  For each service of the application TripMeal, specify which Kubernetes objects you need to create and their types. Justify your answers.\n  Exercise\nExercise 3.3  For each Kubernetes object:\nConfigure. Write a Yaml configuration file. Remember that you need to pass the application the environment variables.\n Create. Create the object in Kubernetes. Which command are you using?\n Analyze. Execute the command kubectl get all and explain in detail which objects appear in the output as the result of step 2.\n    Activity\nPlay with the application in order to verify that everything works as expected.\n Shut down the application!\nShut down the application by removing all the Kubernetes objects.\n Exercise\nExercise 3.4  Create a new file called tripmeal.yml that contains the definition of all the Kubernetes objects of the application, as we have seen in the conclusion of the tutorial 2.\nUpload this file to Edunao.\n   4 Kubernetes cluster on Microsoft Azure You’re now going to create a Kubernetes cluster on Microsoft Azure and deploy TripMeal on that cluster.\nWarning\nIn order to perform this activity, you need a student subscription on Microsoft Azure. If you haven’t activated it yet:\n Connect to this webpage.\n Click on Start free.\n Sign in by using your CentraleSupélec credentials.\n Follow the instructions.\n   During this activity, you’ll need to answer some questions that encourage you to gain a deeper knowledge of the Azure platform and better understand the commands that you type. Feel free to read the documentation on the Azure platform in order to answer the questions.\n4.1 Installing the Azure CLI In order to create the Kubernetes cluster and deploy the application, you need to install the Azure Command Line Interface (CLI). It’s a terminal where you’ll type the commands to create the resources on Azure.\n Windows users Follow the instructions at this Web page.   macOS users Follow the instructions at this Web page.   VM Linux users Open a terminal and type the following command:\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\nWhen prompted for a password, just type ENTER and wait for the installation to finish.\n  4.2 Login to Azure through CLI Whether you’re on Windows, macOS or Linux, you need to open a terminal.\nIn order to log in to your Azure account, type the following command:\naz login A Web page will open in your default Web browser, where you can type your username and password (your CentraleSupélec credentials). After authenticating your Azure account, you can go back to the terminal, where you should see some information about your account, such as:\n[ { \u0026quot;cloudName\u0026quot;: \u0026quot;AzureCloud\u0026quot;, \u0026quot;homeTenantId\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;isDefault\u0026quot;: true, \u0026quot;managedByTenants\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;Azure for Students\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;Enabled\u0026quot;, \u0026quot;tenantId\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;user\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;\u0026lt;email-address\u0026gt;\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;user\u0026quot; } } ]  4.3 Deploy and use Azure Container Registry When we run TripMeal on a local Kubernetes cluster, we assumed that our computer could have a direct access to the Internet and, therefore, to the DockerHub registry, where we could pull the images of the TripMeal services. When we run a containerized application in production, we cannot make this assumption as the production servers have often no direct access to the Internet.\nExercise\nExercise 4.1  In your view, why isn’t it a good idea to have production servers directly connected to the Internet?\n  We need to place the images in a container registry that is in the same context as our production servers. Since we’re going to run the application on Azure, we can use the Azure Container Registry (ACR).\n4.3.1 Registry creation First, we need to create a resource group.\nExercise\nExercise 4.2  What is a resource group in Azure and how is it useful?\n  The command to create a new resource group is the following (replace RES_GRP_NAME with a name of your choice).\naz group create --name RES_GRP_NAME --location eastus Exercise\nExercise 4.3  In the previous command, what does the location refer to? Briefly describe this notion (presented in Lecture 1).\n  Next, we need to create a container registry with the following command (replace RES_GRP_NAME with the name of your resource group and REG_NAME with a name of your choice for the registry).\naz acr create --resource-group RES_GRP_NAME --name REG_NAME --sku Basic Exercise\nExercise 4.4  In the previous command, what does the sku refer to? Feel free to look up on the Azure website to find the answer.\n   4.3.2 Registry login After creating the registry, we can log into it with the following command:\naz acr login --name REG_NAME  4.3.3 Image tagging We’re almost ready to push the images that compose the application TripMeal to the registry. In order to do that, we need to tag (i.e., rename) our images so that their names are preceded by the login server name of the registry.\nIn order to get the name of the login server name (that we denote here as acrloginserver) of your registry, you can type the following command:\naz acr list --resource-group RES_GRP_NAME --query \u0026quot;[].{acrLoginServer:loginServer}\u0026quot; --output table Your acrloginserver will be something like xxx.azurecr.io.\nExercise\nExercise 4.5  Tag the images that correspond to the services of the application TripMeal so that their name is similar to: acrloginserver/nameofimage:latest\nWrite the exact Docker command that you use to tag the images.\n   4.3.4 Pushing the images We can push the images with the following command (use your image name instead of xxx.azurecr.io/imagename:latest).\ndocker push xxx.azurecr.io/imagename:latest Make sure to type this command for each image that you want to push.\nWarning\nIt might take few minutes before the images are completely pushed to the registry.\n Finally, verify that the images are actually in the registry with the following command:\naz acr repository list --name REG_NAME --output table   4.4 Deploy a Kubernetes cluster We now deploy a Kubernetes cluster on Azure.\n4.4.1 Cluster creation We create the cluster with the following command (replace CLUSTER_NAME with a name of your choice. As before, RES_GRP_NAME is the name of your resource group and REG_NAME is the name of the container registry).\naz aks create \\ --resource-group RES_GRP_NAME \\ --name CLUSTER_NAME \\ --node-count 2 \\ --generate-ssh-keys \\ --attach-acr REG_NAME The cluster will take a while to start. Time for a coffee! But before, answer the following question!\nExercise\nExercise 4.6  Can you tell the meaning of the options in the previous command?\n   4.4.2 Connect to the cluster We can configure kubectl to connect to the newly created cluster. You need to type the following command:\naz aks get-credentials --resource-group RES_GRP_NAME --name CLUSTER_NAME Now, your Kubernetes cluster should be visible locally. To verify it, type the following command:\nkubectl config get-contexts Here context refers to the Kubernetes clusters that kubectl has access to. The Kubernetes cluster that you created on Azure should be visible in the output; an asterisk should appear in front of its name, indicating that it is the current context (that is the Kubernetes cluster being currently referenced by kubectl).\nType the following command:\nkubectl get nodes You should see that the Kubernetes cluster has two nodes.\n  4.5 Deploy the application We’re almost done! The images are in the registry, the Kubernetes cluster is up and running. The only missing piece of the puzzle is our application TripMeal.\nFirst thing to do is to slightly modify the file tripmeal.yml that you created at the end of the previous section.\nExercise\nExercise 4.7  Look at the names of the images of each service in that file. How must these names change? Modify the file accordingly (no need to upload it on Edunao).\n  It is the moment of truth! Deploy your application by typing the following command:\nkubectl apply -f tripmeal.yml Look at Kubernetes objects created after this command:\nkubectl get all Get the external IP address of the web service and try to connect to the application, in the same way you did in the previous section.\nIf you can play with the application like you did in your local deployment it means that you reached the conclusion of this assignment! Bravo!\nExercise\nExercise 4.8  Submit a video like that the one that you can see here.\nThe video must clearly show that:\n You’re connected to your Azure portal (Email address on the top right corner of the portal).\n All the passages that you see in the sample video: you need to show the public IP address of the application that you deployed, use that address to connect to your application and play with the application to show that it works correctly.\n    The next exercise is optional. Before you leave, make sure to read the conclusion of this document!!.\nExercise\nExercise 4.9  The title of this exercise is Cerise sur le gâteau (aka, this is optional!).\nAt this point, you can connect to TripMeal by using an IP address. It would be nice if you could use a URL, like in the real websites. Can you find a way to assign a URL to your web service? Describe your procedure.\nYou need a bit of Googling here….\n    Conclusion Make sure you follow these instructions:\n Take down your application in Kubernetes by using the kubectl delete command on each object that you created.\n Change the context of the kubectl command so that it points back to a local Kubernetes cluster.\n  If your local Kubernetes cluster is docker-desktop type the following command:\nkubectl config use-context docker-desktop If your local Kubernetes cluster is minikube type the following command:\nkubectl config use-context minikube  On your Azure portal destroy all the resources linked to the application TripMeal, otherwise you’ll get billed even if you don’t use them!\n You can check the balance of your Azure credit here.\n You can stop Docker and Kubernetes if you don’t need it anymore.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"427e5cadc0e40c7a1fa326dad07bcb5a","permalink":"/courses/cloud-computing/labs/kube-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/labs/kube-lab/","section":"courses","summary":"Text of the lab assignment on Docker + Kubernetes","tags":null,"title":"Multi-service applications in the Cloud","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1  Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\n   Solution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).    Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2  What is the maximum number of measurements in a year?\n   Solution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\n  Exercise\nExercise 1.3  Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\n   Solution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\n  Exercise\nExercise 1.4  Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n   Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).     2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 2.1  Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n   Solution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n \\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\).  We note that the reduce function is the identity.\n   3 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 3.1  Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\n   Solution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\n  Exercise\nExercise 3.2  Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year.    Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n \\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\)    (\\#exr:unnamed-chunk-8)  Propose a MapReduce implementation to create an inverted index over a collection of documents. \\EndKnitrBlock{exercise} :::  Solution ::: {.infobox .exosolution data-latex=\"{exercisebox}\"} The input to the map will be a key-value pair, where the key is the name of a file $f$ and the value is the content $C$ of the file. map: $(f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]$ reduce: $(w, L) \\rightarrow (w, L)$ where $L$ is the list of the files containing the word $w$. We note that the reduce function is the identity. Note also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols. :::  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0cbba47479da04631bd931db00bad3ff","permalink":"/courses/bdia/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1  Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\n   Solution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).    Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2  What is the maximum number of measurements in a year?\n   Solution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\n  Exercise\nExercise 1.3  Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\n   Solution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\n  Exercise\nExercise 1.4  Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n   Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).     2 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 2.1  Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\n   Solution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\n  Exercise\nExercise 2.2  Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year.    Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n \\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\)     3 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 3.1  Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n   Solution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n \\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\).  We note that the reduce function is the identity.\n   4 Creating an inverted index We have a collection of \\(n\\) documents in a directory and we want to create an inverted index, one that associates each word to the list of the files the word occurs in. More precisely, for each word, the inverted index will have a list of the names of the documents that contain the word.\nExercise\nExercise 4.1  Propose a MapReduce implementation to create an inverted index over a collection of documents.\n   Solution\nThe input to the map will be a key-value pair, where the key is the name of a file \\(f\\) and the value is the content \\(C\\) of the file.\nmap: \\((f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]\\)\nreduce: \\((w, L) \\rightarrow (w, L)\\)\nwhere \\(L\\) is the list of the files containing the word \\(w\\).\nWe note that the reduce function is the identity.\nNote also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a59fa830506ab61c12736f4ea6f887f4","permalink":"/courses/big-data-marseille/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1  Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\n   Solution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).    Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2  What is the maximum number of measurements in a year?\n   Solution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\n  Exercise\nExercise 1.3  Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\n   Solution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\n  Exercise\nExercise 1.4  Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n   Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).     2 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 2.1  Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\n   Solution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\n  Exercise\nExercise 2.2  Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year.    Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n \\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\)     3 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 3.1  Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n   Solution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n \\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\).  We note that the reduce function is the identity.\n  (\\#exr:unnamed-chunk-8)  Propose a MapReduce implementation to create an inverted index over a collection of documents. \\EndKnitrBlock{exercise} :::  Solution ::: {.infobox .exosolution data-latex=\"{exercisebox}\"} The input to the map will be a key-value pair, where the key is the name of a file $f$ and the value is the content $C$ of the file. map: $(f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]$ reduce: $(w, L) \\rightarrow (w, L)$ where $L$ is the list of the files containing the word $w$. We note that the reduce function is the identity. Note also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols. :::  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22d83831482b2a6a4aa5d6546c3dd9cd","permalink":"/courses/plp/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv' as row MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)}) RETURN count(m)  2. Create an **index** on the property *movie_id* of the nodes with label **Movie** with the following command:  create index movie_idx for (m:Movie) on (m.movie_id)  3. Import the **nodes** corresponding to the **actors** (label **Actor**) by using the following command (it took 62 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv' as row MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name}) RETURN count(a)  4. Create an **index** on the property *actor_id* of the nodes with label **Actor** with the following command:  create index actor_idx for (a:Actor) on (a.actor_id)  5. Import the **nodes** corresponding to the **directors** (label **Director**) by using the following command (it took 4 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv' as row MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name}) RETURN count(d)  6. Create an **index** on the property *director_id* of the nodes with label **Director** with the following command:  create index director_idx for (d:Director) on (d.director_id)  7. Import the **nodes** corresponding to the **genres** (label **Genre**) by using the following command (it took 197 ms on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv' as row MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name}) RETURN count(g)  8. Create an **index** on the property *genre_id* of the nodes with label **Genre** with the following command:  create index genre_idx for (g:Genre) on (g.genre_id)  9. Import the **nodes** corresponding to the **users** (label **User**) by using the following command (it took 347 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv' as row MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname}) RETURN count(u)  10. Create an **index** on the property *user_id* of the nodes with label **User** with the following command:  create index user_idx for (u:User) on (u.user_id)  11. Import the links of type **ACTED_IN** between actors and movies with the following command (it took 2.5 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (a:Actor {actor_id: toInteger(row.actor_id)}) MERGE (a)-[r:ACTED_IN]-(m) RETURN count(r)  12. Import the links of type **DIRECTED** between directors and movies with the following command (it took 688 ms on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (d:Director {director_id: toInteger(row.director_id)}) MERGE (d)-[r:DIRECTED]-(m) RETURN count(r)  13. Import the links of type **HAS_GENRE** between movies and genres with the following command (it took 1 second on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (g:Genre {genre_id: toInteger(row.genre_id)}) MERGE (m)-[r:HAS_GENRE]-(g) RETURN count(r)  14. Import the links of type **RATED** between users and movies with the following command (it took 5.9 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (u:User {user_id: toInteger(row.user_id)}) MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-(m) RETURN count(r)  # Exploratory queries If you looked at the commands used to import the data, you might already have an idea as to the structure of the graph. You can get a glimpse on the node labels, the relationship types and the property keys by clicking on the button circled in the following figure: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-1)  Write and execute the following query:  MATCH (m:Movie {title_en:\"Toy Story\"}) RETURN m;  What do you obtain? What are the properties associated to a node with label *Movie*? Click once on the node to display its properties. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Double-click on the node displayed as the result of the previous query. Analyze the neighbouring nodes (their labels and properties) and the incident links (direction, type and properties). You can move around the node by dragging it in the window. \\EndKnitrBlock{exercise} ::: # Queries ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  Write and execute the following queries: Q1. The genres of the movies in the database. Q2. The number of movies in the database. Q3. The title of the movies released in 2015. Q4. The number of directors by movie. Sort in decreasing order. Q5. The names of the directors and the title of the movies that they directed and in which they also played. Q6. The genres of the movies in which Tom Hanks played. Q7. The title and the rate of all the movies that the user with identifier 3 rated. Sort by rate in decreasing order.\\EndKnitrBlock{exercise} ::: ## Query chaining Cypher allows the specification of complex queries composed of several queries that are concatenated with the clause **WITH**. We are now going to see an example to obtain the titles of the movies that have been rated by at least 100 users. At a first glance, the following query looks like a good solution:  MATCH (n:Movie)= 100 RETURN n.title_en LIMIT 5;  However, executing this query returns the following error:  Invalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41)) \"MATCH (n:Movie)= 100\"  Similarly to SQL, we cannot use aggregating functions in the clause WHERE. A correct formulation of the query requires the use of the clause WITH to concatenate two queries: the first will count the number of rates for each movie:  MATCH (n:Movie)The second will take in the output of the first and will filter all the movies where nb_rates MATCH (n:Movie)= 100 RETURN n.title_en  ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  Write and execute a query to obtain the five movies that obtained the best average rate among the movies that have been rated by at least 100 users. \\EndKnitrBlock{exercise} ::: # Movie recommendation We are now going to see how Neo4j can be effectively used in a real application by implementing queries that form the basis of a simple **movie recommendation system**. This system is based on the notion of **collaborative filtering**. This consists in recommending a user $u$ some films that s/he hasn’t rated yet and other users with similar preferences have loved. In our context, we say that a user loves a movie if s/he rated the movie at least 3. This concept is explained in the following figure. The user $u$ loves 6 movies, 3 of which are also loved by the user $v$ (the black nodes); it is reasonable to think that $u$ may also love the two movies that $v$ loved and $u$ hasn’t rated yet. The principle of collaborative filtering is based on the computation of a **similarity score** between two users. Several similarity scores are possible in this context; here, we are going to use the **Jaccard coefficient**. Let $L(u)$ and $L(v)$ be the sets of movies that $u$ and $v$ love respectively; the similarity score $J(u,v)$ between $u$ and $v$ is given by: $$ J(u, v) = \\frac{|L(u) \\cap L(v)|}{|L(u) \\cup L(v)|} $$ In order to recommend movies to a target user $v$, the recommender system computes the similarity score between $v$ and all the other users of the system and proposes to $v$ the movies that s/he hasn’t rated yet and that the $k$ most similar users loved. We are now going to incrementally write a query to recommend some movies to the target user 3. The first step consists in determining the value $|L(v)|$. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write and execute the query to obtain the number of movies that the user 3 loved. This query must return the target user and the number of movies that s/he loves. \\EndKnitrBlock{exercise} ::: Next, we are going to determine the value $|L(u)|$, for all users $u$ except $v$. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-6)  Write and execute the query to obtain the number of movies that each user $u$ loves, except the target user 3. This query must return each user $u$ and the number of movies that s/he loves. \\EndKnitrBlock{exercise} ::: We put the two queries together with the clause WITH. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-7)  Compose the two previous queries with the clause WITH. This query must return the target user 3, the number of movies that s/he loves, the other users $u$ and the number of movies that they love. \\EndKnitrBlock{exercise} ::: Now, we need to determine the value $L(u)\\cup L(v)$, for each user $u$, and compute the similarity score with the Jaccard coefficient. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-8)  Append (by using WITH) to the query written in the previous exercise a query that obtains the number of movies that any user $u$ loved and that the target user 3 loved too, and computes the similarity score between the target user 3 and $u$. This query must return the five most similar users to the target user and the similarity scores.  Hint Multiply the numerator of the equation by 1.0, otherwise Cypher will compute an integer division.  \\EndKnitrBlock{exercise} ::: The last step consists in recommending some movies to the target user. From the previous query, take the identifier of the user $w$ with the highest similarity to the target user. You are going to use this identifier directly in the new query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-9)  Write and execute the query to obtain the list of the movies that the user $w$ loved and that the target user hasn't rated yet. Sort this list by decreasing rate.  Hint * First, write a query to obtain the list of the movies that the target user rated. In the MATCH clause, use the variable $m$ to indicate a movie that the target user rated. Conclude the query with:  RETURN collect(m.title_en) AS movies  The function *collect* creates a list called *movies*. * Replace RETURN with WITH in the previous query and add a second query to select the titles of the movies $m$ that the user $w$ loved and the target user did not rate. In order to exclude the films that the target user did not rate, use the following predicate:  none(x in movies where x=m.title_en)  in the WHERE clause.  \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a2efe08ebb4ff1aac075391eda851b4f","permalink":"/courses/bdia/tutorials/neo4j-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/neo4j-tutorial/","section":"courses","summary":"Description of the Neo4j tutorial.","tags":null,"title":"Neo4j","type":"docs"},{"authors":null,"categories":null,"content":" :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv' as row MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)}) RETURN count(m)  2. Create an **index** on the property *movie_id* of the nodes with label **Movie** with the following command:  create index movie_idx for (m:Movie) on (m.movie_id)  3. Import the **nodes** corresponding to the **actors** (label **Actor**) by using the following command (it took 62 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv' as row MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name}) RETURN count(a)  4. Create an **index** on the property *actor_id* of the nodes with label **Actor** with the following command:  create index actor_idx for (a:Actor) on (a.actor_id)  5. Import the **nodes** corresponding to the **directors** (label **Director**) by using the following command (it took 4 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv' as row MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name}) RETURN count(d)  6. Create an **index** on the property *director_id* of the nodes with label **Director** with the following command:  create index director_idx for (d:Director) on (d.director_id)  7. Import the **nodes** corresponding to the **genres** (label **Genre**) by using the following command (it took 197 ms on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv' as row MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name}) RETURN count(g)  8. Create an **index** on the property *genre_id* of the nodes with label **Genre** with the following command:  create index genre_idx for (g:Genre) on (g.genre_id)  9. Import the **nodes** corresponding to the **users** (label **User**) by using the following command (it took 347 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv' as row MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname}) RETURN count(u)  10. Create an **index** on the property *user_id* of the nodes with label **User** with the following command:  create index user_idx for (u:User) on (u.user_id)  11. Import the links of type **ACTED_IN** between actors and movies with the following command (it took 2.5 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (a:Actor {actor_id: toInteger(row.actor_id)}) MERGE (a)-[r:ACTED_IN]-(m) RETURN count(r)  12. Import the links of type **DIRECTED** between directors and movies with the following command (it took 688 ms on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (d:Director {director_id: toInteger(row.director_id)}) MERGE (d)-[r:DIRECTED]-(m) RETURN count(r)  13. Import the links of type **HAS_GENRE** between movies and genres with the following command (it took 1 second on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (g:Genre {genre_id: toInteger(row.genre_id)}) MERGE (m)-[r:HAS_GENRE]-(g) RETURN count(r)  14. Import the links of type **RATED** between users and movies with the following command (it took 5.9 seconds on my computer):  :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (u:User {user_id: toInteger(row.user_id)}) MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-(m) RETURN count(r)  # Exploratory queries If you looked at the commands used to import the data, you might already have an idea as to the structure of the graph. You can get a glimpse on the node labels, the relationship types and the property keys by clicking on the button circled in the following figure: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-1)  Write and execute the following query:  MATCH (m:Movie {title_en:\"Toy Story\"}) RETURN m;  What do you obtain? What are the properties associated to a node with label *Movie*? Click once on the node to display its properties. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Double-click on the node displayed as the result of the previous query. Analyze the neighbouring nodes (their labels and properties) and the incident links (direction, type and properties). You can move around the node by dragging it in the window. \\EndKnitrBlock{exercise} ::: # Queries ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  Write and execute the following queries: Q1. The genres of the movies in the database. Q2. The number of movies in the database. Q3. The title of the movies released in 2015. Q4. The number of directors by movie. Sort in decreasing order. Q5. The names of the directors and the title of the movies that they directed and in which they also played. Q6. The genres of the movies in which Tom Hanks played. Q7. The title and the rate of all the movies that the user with identifier 3 rated. Sort by rate in decreasing order.\\EndKnitrBlock{exercise} ::: ## Query chaining Cypher allows the specification of complex queries composed of several queries that are concatenated with the clause **WITH**. We are now going to see an example to obtain the titles of the movies that have been rated by at least 100 users. At a first glance, the following query looks like a good solution:  MATCH (n:Movie)= 100 RETURN n.title_en LIMIT 5;  However, executing this query returns the following error:  Invalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41)) \"MATCH (n:Movie)= 100\"  Similarly to SQL, we cannot use aggregating functions in the clause WHERE. A correct formulation of the query requires the use of the clause WITH to concatenate two queries: the first will count the number of rates for each movie:  MATCH (n:Movie)The second will take in the output of the first and will filter all the movies where nb_rates MATCH (n:Movie)= 100 RETURN n.title_en  ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  Write and execute a query to obtain the five movies that obtained the best average rate among the movies that have been rated by at least 100 users. \\EndKnitrBlock{exercise} ::: # Movie recommendation We are now going to see how Neo4j can be effectively used in a real application by implementing queries that form the basis of a simple **movie recommendation system**. This system is based on the notion of **collaborative filtering**. This consists in recommending a user $u$ some films that s/he hasn’t rated yet and other users with similar preferences have loved. In our context, we say that a user loves a movie if s/he rated the movie at least 3. This concept is explained in the following figure. The user $u$ loves 6 movies, 3 of which are also loved by the user $v$ (the black nodes); it is reasonable to think that $u$ may also love the two movies that $v$ loved and $u$ hasn’t rated yet. The principle of collaborative filtering is based on the computation of a **similarity score** between two users. Several similarity scores are possible in this context; here, we are going to use the **Jaccard coefficient**. Let $L(u)$ and $L(v)$ be the sets of movies that $u$ and $v$ love respectively; the similarity score $J(u,v)$ between $u$ and $v$ is given by: $$ J(u, v) = \\frac{|L(u) \\cap L(v)|}{|L(u) \\cup L(v)|} $$ In order to recommend movies to a target user $v$, the recommender system computes the similarity score between $v$ and all the other users of the system and proposes to $v$ the movies that s/he hasn’t rated yet and that the $k$ most similar users loved. We are now going to incrementally write a query to recommend some movies to the target user 3. The first step consists in determining the value $|L(v)|$. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-5)  Write and execute the query to obtain the number of movies that the user 3 loved. This query must return the target user and the number of movies that s/he loves. \\EndKnitrBlock{exercise} ::: Next, we are going to determine the value $|L(u)|$, for all users $u$ except $v$. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-6)  Write and execute the query to obtain the number of movies that each user $u$ loves, except the target user 3. This query must return each user $u$ and the number of movies that s/he loves. \\EndKnitrBlock{exercise} ::: We put the two queries together with the clause WITH. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-7)  Compose the two previous queries with the clause WITH. This query must return the target user 3, the number of movies that s/he loves, the other users $u$ and the number of movies that they love. \\EndKnitrBlock{exercise} ::: Now, we need to determine the value $L(u)\\cup L(v)$, for each user $u$, and compute the similarity score with the Jaccard coefficient. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-8)  Append (by using WITH) to the query written in the previous exercise a query that obtains the number of movies that any user $u$ loved and that the target user 3 loved too, and computes the similarity score between the target user 3 and $u$. This query must return the five most similar users to the target user and the similarity scores.  Hint Multiply the numerator of the equation by 1.0, otherwise Cypher will compute an integer division.  \\EndKnitrBlock{exercise} ::: The last step consists in recommending some movies to the target user. From the previous query, take the identifier of the user $w$ with the highest similarity to the target user. You are going to use this identifier directly in the new query. ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-9)  Write and execute the query to obtain the list of the movies that the user $w$ loved and that the target user hasn't rated yet. Sort this list by decreasing rate.  Hint * First, write a query to obtain the list of the movies that the target user rated. In the MATCH clause, use the variable $m$ to indicate a movie that the target user rated. Conclude the query with:  RETURN collect(m.title_en) AS movies  The function *collect* creates a list called *movies*. * Replace RETURN with WITH in the previous query and add a second query to select the titles of the movies $m$ that the user $w$ loved and the target user did not rate. In order to exclude the films that the target user did not rate, use the following predicate:  none(x in movies where x=m.title_en)  in the WHERE clause.  \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0acfabbff71d33e6b9759e499c4634ed","permalink":"/courses/plp/tutorials/neo4j-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/neo4j-tutorial/","section":"courses","summary":"Description of the Neo4j tutorial.","tags":null,"title":"Neo4j","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to obtain a non-redundant set of functional dependencies. How to determine the candidate keys of a table given its functional dependencies. How to determine the normal form of a table.  Prerequisites:\n Having attended Lecture 2.  1 Non-redundant functional dependencies Consider the following set \\(\\mathcal{F}\\) of functional dependencies:\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B, C\\)\n \\(A \\rightarrow B\\)\n  Exercise\nExercise 1.1  Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).\n   Solution\nBased on the definition of a set of minimal functional dependencies (slide 18 in Lecture 2), we can rewrite the functional dependencies so that the right side consists of only one attribute. \\(D \\rightarrow B, C\\) is equivalent to \\(D \\rightarrow B\\) and \\(D \\rightarrow C\\).\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n  All functional dependencies except the first one is trivially left-irreducible (the determinant consists of only one column). The first has two attributes in the determinant, therefore we need to check whether we can eliminate one of the two columns and still preserve an equivalent set of functional dependencies.\nWe try to apply Armstrong’s axioms to compute the closure of this set of functional dependencies.\nWe have \\(A \\rightarrow B\\). By augmentation we obtain:\n\\[A \\rightarrow A, B\\]\nWe have \\(A, B \\rightarrow C\\). By transitivity we obtain:\n\\[A \\rightarrow C\\]\nTherefore, the column \\(B\\) in \\(A, B \\rightarrow C\\) is not useful and we can drop it.\nThe set \\(\\mathcal{G}\\) consists of the following FDs:\n \\(A \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n     2 Candidate keys and normal forms (1) We consider the following table:\nPatient (ssn, first_name, last_name, phone_number, insurance_number, insurance_expiration_date) where the following set \\(\\mathcal{F}\\) of functional dependencies holds:\n\\[ \\begin{align} ssn \\rightarrow first\\_name, \u0026amp; last\\_name, phone\\_number, insurance\\_number, \\\\ \u0026amp; insurance\\_expiration\\_date \\end{align} \\]\n\\[ insurance\\_number \\rightarrow insurance\\_expiration\\_date \\]\nExercise\nExercise 2.1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).    Solution\nFirst, we need to rewrite the FDs in canonical form.\n\\(ssn \\rightarrow first\\_name\\) \\(ssn \\rightarrow last\\_name\\) \\(ssn \\rightarrow phone\\_number\\) \\(ssn \\rightarrow insurance\\_number\\) \\(ssn \\rightarrow insurance\\_expiration\\_date\\) \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\)  The determinant of each FD is composed of only one column, therefore it is already irreducible. It is easy to see that all FDs with ssn as a determinant must be kept (otherwise we lose some information).\nBy transitivity from 4. and 6. we obtain:\n\\[ssn \\rightarrow insurance\\_expiration\\_date\\]\nTherefore, \\(\\mathcal{G}\\) is obtained from \\(\\mathcal{F}\\) by removing 5.\n  Exercise\nExercise 2.2 Given \\(\\mathcal{G}\\), identify the candidate keys in the table Patient.    Solution\nFrom the functional dependencies in \\(\\mathcal{F}\\), it’s easy to see that that the only column that implies all the others is ssn. Therefore, {ssn} is the only candidate key in this table.\n  Exercise\nExercise 2.3 Specify the normal form of the table Patient. Justify your answer.    Solution\n It is immediate to verify that the table is 1NF.\n The table is 2NF because there is only one candidate key, which is composed of only one column.\n The table is not in 3NF. Indeed, there is a functional dependency between two non-prime columns:\n  \\[insurance\\_number \\rightarrow insurance\\_expiration\\_date\\]\n  Exercise\nExercise 2.4 How would you obtain two or more tables in BCNF from the table Patient?    Solution\nWe need to split the data relative to the patient from the data relative to the insurance. Therefore, we propose the following two tables:\n Patient (ssn, first_name, last_name, phone_number, insurance_number) Insurance (insurance_number, insurance_expiration_date)  Note that the column insurance_number in table Patient is foreign key to the column insurance_number in table Insurance.\n   3 Candidate keys and normal forms (2) Let \\(R\\) be a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\(A, B \\rightarrow C\\) \\(C \\rightarrow A\\) \\(C \\rightarrow B\\) \\(C \\rightarrow D\\) \\(D \\rightarrow E\\)  Exercise\nExercise 3.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nFirst, let’s try sets composed of only one column: \\(\\{A\\}\\), \\(\\{B\\}\\), \\(\\{C\\}\\), \\(\\{D\\}\\) and \\(\\{E\\}\\).\nWe have the following: (\\(\\{X\\}^+_{\\mathcal{F}}\\) indicates the set of all columns implied by \\(X\\)).\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D, E\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  Therefore, \\(\\{C\\}\\) is a candidate key because it implies all the other columns.\nFrom the functional dependency 1., we obtain that \\(\\{A, B\\}\\) implies \\(C\\); therefore, by transitivity they imply all the other columns.\nIn conclusion, we have two candidate keys: \\(\\{C\\}\\) and \\(\\{A, B\\}\\).\n  Exercise\nExercise 3.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is in 2NF. In fact, all non-prime columns depend entirely on both candidate keys.\n \\(R\\) is not in 3NF. In fact, the functional dependency \\(D \\rightarrow E\\) is between two non-prime columns.\n     4 Candidate keys and normal forms (3) Let \\(R\\) be the a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n \\(A \\rightarrow C\\) \\(A, B \\rightarrow D\\) \\(A, B \\rightarrow E\\)  Exercise\nExercise 4.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nThe following hold:\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A, C\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{C\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  It is clear that the only possible candidate key could be {A, B}; let’s verify it by computing the closure of {A, B} under \\(\\mathcal{F}\\):\n \\(\\{A, B\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)  Indeed, {A, B} is the candidate key.\n  Exercise\nExercise 4.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is not in 2NF. In fact, the non-prime column \\(C\\) is functionally dependent on only one part of the candidate key.\n \\(R\\) is not in 3NF, because it doesn’t fulfill the first condition, that is being in 2NF.\n     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd4b11df4fa072aaae4db3251396b08d","permalink":"/courses/databases/tutorials/normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/normalization/","section":"courses","summary":"Description of the normalization tutorial.","tags":null,"title":"Normalization","type":"docs"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\n(\\#exr:unnamed-chunk-1)  * Run the script ``avg_temperatures_first.py`` by using ``temperatures_86400.csv`` as an input. To this extent, use the following command: `` spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv ``  You should find the output of the program under the folder ``hdfs://sar01:9000/cpupsmia1/your_username/temperatures_86400.out`` * What's the execution time? * In the output of Spark on the command line you should see a line that mentions something along the following line: `` INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s `` * Run the same script by using ``temperatures_2880.csv`` as an input. * What is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer. * Execute the same script by using ``temperatures_86.csv`` as an input. * What is the execution time? How would you justify it, knowing that the files ``temperatures_2880.csv`` and ``temperatures_86.csv`` have a similar size (11 MB the former, 9 MB the latter)? \\EndKnitrBlock{exercise} ::: ## Second implementation Copy the file ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing the following command: `` cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py ``  ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Based on the observations made in the previous exercise, write an improved implementation of the function ``avg_temperature``. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  * Run the script ``avg_temperatures_second.py`` by using ``temperatures_86.csv`` as an input. * What's the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference. * Run the same script by using ``temperatures_10.csv`` (3 GB!) as an input. Do you think that the program takes too long? Why? \\EndKnitrBlock{exercise} ::: # Average and standard deviation We use the same files as in the first question. Our objective is to write a Spark program that produces triples $(y, t_{\\mu}, t_{\\sigma})$, where $y$, $t_{\\mu}$ and $t_{\\sigma}$ are the year, the average temperature in the year and the standard deviation respectively. We can express the standard deviation of $n$ values $x_1 \\ldots x_n$ with the following formula: $$ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} $$ Copy the file ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing the following command: `` cp ~vialle/DCE-Spark/template_temperatures.py ./avg_stddev_temp.py `` ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  * Complete the definition of the function ``avg_temperature`` in file ``avg_stddev_temp.py``. * Run the script by using ``temperatures_86400.csv`` and ``temperatures_2880.csv`` as input files (small files). * Run the script by using ``temperatures_86.csv`` and ``temperatures_10.csv`` as input files (large files). \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67f3bc33b5d1da6975469e8a8b62caae","permalink":"/courses/bdia/tutorials/spark-programming-dce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/spark-programming-dce/","section":"courses","summary":"Description of the tutorial on Spark programming on a cluster","tags":null,"title":"Introduction to Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\n(\\#exr:unnamed-chunk-1)  * Run the script ``avg_temperatures_first.py`` by using ``temperatures_86400.csv`` as an input. To this extent, use the following command: `` spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv ``  You should find the output of the program under the folder ``hdfs://sar01:9000/cpupsmia1/your_username/temperatures_86400.out`` * What's the execution time? * In the output of Spark on the command line you should see a line that mentions something along the following line: `` INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s `` * Run the same script by using ``temperatures_2880.csv`` as an input. * What is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer. * Execute the same script by using ``temperatures_86.csv`` as an input. * What is the execution time? How would you justify it, knowing that the files ``temperatures_2880.csv`` and ``temperatures_86.csv`` have a similar size (11 MB the former, 9 MB the latter)? \\EndKnitrBlock{exercise} ::: ## Second implementation Copy the file ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing the following command: `` cp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py ``  ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-2)  Based on the observations made in the previous exercise, write an improved implementation of the function ``avg_temperature``. \\EndKnitrBlock{exercise} ::: ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-3)  * Run the script ``avg_temperatures_second.py`` by using ``temperatures_86.csv`` as an input. * What's the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference. * Run the same script by using ``temperatures_10.csv`` (3 GB!) as an input. Do you think that the program takes too long? Why? \\EndKnitrBlock{exercise} ::: # Average and standard deviation We use the same files as in the first question. Our objective is to write a Spark program that produces triples $(y, t_{\\mu}, t_{\\sigma})$, where $y$, $t_{\\mu}$ and $t_{\\sigma}$ are the year, the average temperature in the year and the standard deviation respectively. We can express the standard deviation of $n$ values $x_1 \\ldots x_n$ with the following formula: $$ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} $$ Copy the file ``~vialle/DCE-Spark/template_temperatures.py`` to your home directory by typing the following command: `` cp ~vialle/DCE-Spark/template_temperatures.py ./avg_stddev_temp.py `` ::: {.infobox .exercisebox data-latex=\"{exercisebox}\"} **Exercise** \\BeginKnitrBlock{exercise}(\\#exr:unnamed-chunk-4)  * Complete the definition of the function ``avg_temperature`` in file ``avg_stddev_temp.py``. * Run the script by using ``temperatures_86400.csv`` and ``temperatures_2880.csv`` as input files (small files). * Run the script by using ``temperatures_86.csv`` and ``temperatures_10.csv`` as input files (large files). \\EndKnitrBlock{exercise} ::: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1fa903f48b4529667aa717abb4af5343","permalink":"/courses/plp/tutorials/spark-programming-dce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/spark-programming-dce/","section":"courses","summary":"Description of the tutorial on Spark programming on a cluster","tags":null,"title":"Introduction to Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" First, download DB Browser for SQLite at this link.\n1 Obtain the data We consider the database of DVD rental store containing data on films, actors, customers and the transactions of the store.\nYou can obtain the data at this link.\nThe following figure shows the physical schema of the database.\n Figure 1.1: The physical schema of the database  You should now open the database with DB Browser for SQLite. To this extent, open DB Browser for SQLite, click on Open database and select the downloaded file.\nForeign key alert\nBy default, SQLite doesn’t check foreign key constraints. Open the preferences of DB Browser for SQLite and make sure the checkbox Foreign keys in the tab Database is checked.\n  2 Foreign key constraints Exercise\nExercise 2.1  Try to delete the film with film_id=1 from the table Film. What happens?\n   Solution\nThe query is not allowed, because there are rows in other tables that reference that movie.\n  Exercise\nExercise 2.2  Write a query to get film_id, actor_id and title from the table film joined with film_actor for the film “ACE GOLDFINGER”.\n   Solution\n SELECT f.film_id, fa.film_id, fa.actor_id, f.title FROM film f JOIN film_actor fa ON f.film_id = fa.film_id WHERE f.title=\"ACE GOLDFINGER\"    Exercise\nExercise 2.3  Modify the identifier of the film ACE GOLDFINGER in table Film. Write the query of the previous exercise. What happens?\n   Solution\nThe identifier of the film is changed in all referencing tables. This is because the foreign key constraint is defined with the option ON UPDATE CASCADE.\n   3 Queries Exercise\nExercise 3.1  Write the following queries in SQL:\nQ1. Return the first and last names of all the actors.\nQ2. Return the title and the language of each film.\nQ3. Return the first and the last name of the manager of the store with code 2.\nQ4. Return the first and last names of all the actors who performed in the movie ‘ANGELS LIFE’.\nQ5. Return the number of films where each actor performed. Sort the results in descending order.\nQ6. Return the film categories that contain between 25 and 55 films.\nQ7. Return the first and family name of the customers who have rented more than five family movies.\n   Solution\nQ1.  SELECT first_name,last_name FROM actor  Q2.  SELECT f.title, l.name FROM film AS f JOIN language AS l ON f.language_id = l.language_id;  Q3.  SELECT first_name, last_name FROM store JOIN staff ON manager_staff_id = staff_id WHERE store.store_id=2;  Q4.  SELECT first_name,last_name FROM actor a JOIN film_actor fa ON a.actor_id=fa.actor_id JOIN film f ON fa.Film_ID=f.Film_ID WHERE f.title=\"ANGELS LIFE\"  Q5.  SELECT first_name, last_name, count(*) as nbFilms FROM actor a JOIN film_actor fa on a.actor_id=fa.actor_id GROUP BY a.actor_id, a.first_name, a.last_name ORDER BY nbFilms DESC  Alternative solution:  SELECT t.actor_id, t.nb_films FROM (SELECT actor_id, count(*) as nb_films FROM film_actor GROUP BY actor_id ORDER BY nb_films DESC) t  Q6.  SELECT c.name FROM category c JOIN film_category fc ON c.category_id = fc.category_id GROUP BY c.category_id HAVING count(*) BETWEEN 25 AND 55  Alternative solution:  SELECT name FROM category WHERE category_id IN (SELECT category_id FROM film_category fc GROUP BY category_id HAVING COUNT(*) BETWEEN 25 AND 55)  Q7.  SELECT c.customer_id, c.first_name, c.last_name, COUNT(*) AS nbFilms FROM customer c JOIN rental r ON c.customer_id=r.customer_id JOIN inventory i ON r.inventory_id = i.inventory_id JOIN film f ON i.Film_ID = f.Film_ID JOIN film_category fc on f.Film_ID = fc.Film_ID JOIN category c ON fc.category_id = c.category_id WHERE c.name = \"Family\" GROUP BY r.customer_id HAVING nbFilms  5;  Alternative solution:  SELECT first_name, last_name FROM customer WHERE customer_id IN ( SELECT r.customer_id FROM rental r JOIN inventory i ON r.inventory_id = i.inventory_id JOIN film f ON i.Film_ID = f.Film_ID JOIN film_category fc on f.Film_ID = fc.Film_ID JOIN category c ON fc.category_id = c.category_id WHERE c.name = \"Family\" GROUP BY r.customer_id HAVING count(*)  5)     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ceb065adca61756fa19abe17b6525115","permalink":"/courses/databases/tutorials/sqlite/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sqlite/","section":"courses","summary":"Description of the SQLite tutorial.","tags":null,"title":"Learning SQL queries","type":"docs"}]