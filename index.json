[{"authors":["admin"],"categories":null,"content":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Assistant professor at CentraleSupélec's computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).","tags":null,"title":"Gianluca Quercini","type":"authors"},{"authors":null,"categories":null,"content":" Overview Nowadays, the marketing strategies of most companies are based on the analysis of massive and heterogeneous data that need a considerable amount of computational power. Instead of purchasing new hardware and software infrastructures, companies often resort to the computational and storage power offered by cloud computing platforms over the Internet.\nThe objective of this course is to present the fundamental principles of distributed computing that are at the heart of cloud computing. The course will cover the principles of virtualization and containerization and the methods and tools used for distributed processing (MapReduce and Spark).\n Prerequisites The main prerequisite is the course Information systems and programming:\n Python programming.\n Basic networking notions.\n Basic data management notions.\n  Previous experience with working the command-line terminal and Linux is desired but not essential.\nA beginner introduction to Linux is available here.\n Teaching staff  Gianluca Quercini\n Francesca Bugiotti\n Marc-Antoine Weisser\n Idir Ait Sadoune\n   Required software  Docker. See the installation guide.   Course summary Introduction  Context and applications. Service models (SaaS, PaaS, IaaS). Architectural design. Public cloud platforms. Cloud economic model.  Virtualization  Virtualization principles. Containerization (Docker). Orchestrators (Docker Swarm and Kubernetes).  Cloud programming and software environments  Parallel computing, programming paradigms. Hadoop MapReduce. Apache Spark.    Exam  The Lab assignment 1 will be graded (\\(g_1\\)).\n A mini project will conclude the course (\\(g_2\\)).\n  The final grade will be the average of \\(g_1\\) and \\(g_2\\).\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e76a719efd772c3d82d45e1d0c32b856","permalink":"/courses/cloud-computing/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/","section":"courses","summary":"Presentation of the course Distributed and Cloud Computing","tags":null,"title":"Cloud Computing","type":"docs"},{"authors":null,"categories":null,"content":" Overview All individuals who own a personal computer need to store data on a daily basis. The operating system provides the file system as a way to organize and access files easily, masking the complexities of physical data storage. Although the file system is a simple and efficient data management system, many applications need more sophisticated features, which a file system does not provide, such as data integrity, indexes and fine-grained access control mechanisms.\nThis is where databases come into play.\nThe objective of this course is to introduce the basic notions on which all modern database management systems are grounded. The course is divided into two parts. The first part focuses on relational databases and presents the notions of integrity constraints, data consistency, transactions and normalization, both in single-server and distributed environments. The second part will introduce the four families of NoSQL databases, with a special focus on document (MongoDB) and graph databases (Neo4j).\n Prerequisites Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Teaching staff  Gianluca Quercini   Microsoft Teams group Students are required to join the course team on Microsoft Teams.\n Download and install Microsoft Teams on your computer.\n Log in with your CentraleSupelec credentials.\n Join the team by using the following code: z0ixc50.\n  Each lecture will be broadcast live on Microsoft Teams for all students that are not on campus due to Covid-19 restrictions.\n Required software  DB Browser for SQLite. Tool to manage SQLite databases. Click here to download it and install it.\n MongoDB. A document database management system. Click here to learn how to install it and how to run it.\n Neo4j. A graph database management system. Click here to download it and install it. You’ll be requested to fill in a form.\n   Course summary 1. An introduction to database systems and data modeling.\n Database systems: definitions and motivations. Data models. Introduction to the relational data model. Database design with the Entity-Relationship model (ER).  2. Normalization theory.\n Normalization: definitions and motivations. Notion of functional dependency. 1st, 2nd and 3rd normal forms. Further normal forms: Boyce-Codd, 4th and 5th normal forms. Normalization in real life.  3. Relational database management systems (DBMS).\n Types of relational DBMS: client-server vs. embedded. An embedded relational DBM: SQLite. Interacting with a relational DBMS: the SQL language.  4. Advanced database concepts.\n Overview of a database management system. Transactions: definitions and motivations. Transaction management and processing. Query processing. Indexing.  5. Distributed databases.\n Distributed databases: definitions and motivations. Data replication and sharding. Data consistency in distributed databases. The CAP theorem.  6. NoSQL database systems.\n NoSQL database systems: definitions and motivations. Overview of NoSQL databases systems. Document-oriented databases: MongoDB. Graph databases: Neo4j.   Exam The evaluation is based on:\nA project (40% of the final grade). A written examination (60% of the final grade).  The project is a teamwork assignment where the students use the notions learned in classroom to design and create a database for a travel reservation system. Each team will be composed of 3 students. The students will start the project during the last three hours of the course (Tuesday 24 November 2020, 9:00 AM - 12 PM).\nThe written examination lasts 3 hours and consists of two parts:\n A multiple choice questionnaire (1 hour). A series of exercises on relational/NoSQL database modeling and querying (2 hours).   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c50fbd0dd8e5954b528c8988ad1693b6","permalink":"/courses/databases/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/overview/","section":"courses","summary":"Presentation of the course Introduction to Databases","tags":null,"title":"Introduction to Databases","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB and Neo4j as case studies).\n Prerequisites  Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\n Good knowledge of relational database management systems.\n   Teaching staff  Gianluca Quercini   Course summary 1. Introduction and MapReduce programming.\n Basic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce.  2. Hadoop and its ecosystem: HDFS.\n In-depth description of the Hadoop Distributed File System (HDFS).  3. Introduction to Apache Spark.\n Apache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions.  4. Advanced Apache Spark.\n SparkSQL, Spark streaming, machine learning, graph analysis.  5. Distributed databases and NoSQL.\n Data distribution (replication, sharding, the CAP theorem).\n Overview of NoSQL databases.\n  6. Document oriented databases: MongoDB.\n Presentation of MongoDB.  7. Graph-oriented database systems: Neo4j.\n Presentation of Neo4j.   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"aa867f88f796b12dfa5207b07de94be4","permalink":"/courses/plp/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Big data algorithms, techniques and platforms","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Getting started with Docker\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Wednesday 6 May, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n Tutorial 2 Title: MapReduce - Hadoop\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Thursday 14 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d41cadecab8bb09746ea9ea01e2a42d0","permalink":"/courses/cloud-computing/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: Data modeling\nDate and time: Tuesday 15 September 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 2 Title: Normalization\nDate and time: Tuesday 29 September 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 3 Title: SQL queries\nDate and time: Tuesday 6 October 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 4 Title: SQLite in action\nDate and time: Tuesday 13 October 2020, 10:30 AM - 12 PM\nLink: Click here\n Tutorial 5 Title: Document-based databases: MongoDB.\nDate and time: Tuesday 3 November 2020, 10 AM - 12 AM\nLink: Click here\n Tutorial 6 Title: Graph databases: Neo4j.\nDate and time: Tuesday 10 November 2020, 10 AM - 12 AM\nLink: available soon.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40c351607858b41143809fa1fb45464e","permalink":"/courses/databases/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nDate and time: Monday 4 January 2021, 3.30 PM - 5 PM\nLink: Click here\n Tutorial 2 Title: MapReduce programming in Python\nDate and time: Wednesday 6 January 2021, 10:15 AM - 11:45 AM\nLink: Click here\n Tutorial 3 Title: Introduction to Spark programming\nDate and time: Thursday 7 January 2021, 10:15 AM - 11:45 AM\nLink: Click here\n Tutorial 4 Title: Spark programming using a Spark cluster.\nDate and time: Thursday 7 January 2021, 1:45 PM - 5 PM\nLink: Click here\n Lab assignment 1 Title: Advanced Spark programming\nDate and time: Monday 11 January 2021, 10:15 AM - 11.45 AM\nLink: Click here\n Lab assignment 2 Title: MongoDB\nDate and time: Thursday 21 January 2021, 10:15 AM - 11.45 AM\nLink: Click here\n Tutorial 5 Title: Neo4j\nDate and time: Thursday 28 January 2021, 10:15 AM - 11:45 AM\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89eb8f3c35dc5558cefc06fa0d79071d","permalink":"/courses/plp/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":null,"categories":null,"content":"  1 Windows 1.1 Docker Desktop for Windows 1.2 Docker Toolbox for Windows  2 MacOS 2.1 Docker Desktop for Mac. 2.2 Docker Toolbox for Mac  3 Linux 4 Alternative options 4.1 Docker playground 4.2 Docker in an already prepared virtual machine 4.3 Docker in a Ubuntu Linux virtual machine  5 Verify your installation 6 Interacting with Docker 6.1 Graphical interface  7 Troubleshooting   This document contains information on how to install Docker on your computer.\nAlthough you can access an online Docker environment without installing anything on your computer (see Section 4.1), you should consider this option only if you really cannot install Docker.\nThe installation procedure depends on the operating system that your computer runs.\n1 Windows The installation procedure depends on the Windows version running on your computer.\n1.1 Docker Desktop for Windows If your computer runs Windows 10 64 bits (Pro, Enterprise, or Education, build 15063 or later), you can install Docker Desktop for Windows (recommended).\n Show me more\nHardware prerequisites\n 64 bit processor.\n 4GB system RAM.\n BIOS-level hardware virtualization support must be enabled in the BIOS settings. For more information, see Virtualization.\n  VirtualBox users\nDocker for Windows uses Hyper-V as a virtual machine to run containers. Unfortunately, Hyper-V and VirtualBox are not compatible; when Hyper-V is enabled, VirtualBox will stop working.\nHowever:\n The existing VirtualBox images will not be removed.\n When you want to use VirtualBox, you can turn Hyper-V off.\n  Cannot/don’t want to install Docker Desktop for for Windows\nIf your computer doesn’t meet the hardware requirements, or you don’t want to install Docker Desktop for Windows because you don’t want to mess up your VirtualBox installation (although you shouldn’t really worry about the latter), you have two options:\n Install Docker Toolbox for Windows (Section 1.2).\n See the alternative options (Section 4).\n  Installation procedure\n Download Docker Desktop for Windows.\n Follow the installation instructions. You might need to restart the system to enable Hyper-V.\n Verify your installation (see Section 5).\n    1.2 Docker Toolbox for Windows If your computer runs Windows 7 or higher, and doesn’t meet the hardware requirements for Docker for Windows, you can install Docker Toolbox for Windows.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     2 MacOS The installation procedure depends on the version of MacOS running on your computer.\n2.1 Docker Desktop for Mac. If your computer runs MacOS 10.13 or higher, you can install Docker Desktop for Mac (recommended).\n Show me more\nHardware requirements\n Your computer hardware must be a 2010 or a newer model. Verify that your computer is compatible with Docker Desktop for Mac:  Open a terminal. Run the following command: sysctl kern.hv_support. If the output of the command is kern.hv_support: 1 your computer is compatible.  At least 4GB of RAM.  VirtualBox users\nIf you have a version of VirtualBox older than 4.3.30, you should consider upgrading it, as it would not be compatible with Docker Desktop.\nCannot install Docker Desktop for Mac\nIf your computer doesn’t meet the hardware requirements, you have two options:\n Install Docker Toolbox for Mac (Section 2.2).\n See the alternative options (Section 4).\n  Installation instructions\n Download Docker Desktop for Mac.\n Follow the installation instructions.\n Verify your installation (see Section 5).\n    2.2 Docker Toolbox for Mac If your computer runs MacOs 10.8 or higher, and doesn’t meet the hardware requirements for Docker Desktop for Mac, you can install Docker Toolbox for Mac.\n Show me more\nPlease refer to these installation instructions.\nCannot install Docker Toolbox\n See the alternative options (Section 4).     3 Linux You can install Docker on the following Linux distributions:\n CentOS (installation instructions).\n Debian (installation instructions).\n Fedora (installation instructions).\n Ubuntu (installation instructions).\n  Make sure to read the post-installation steps for Linux and to take the necessary steps to be able to run Docker as a non-root user.\n 4 Alternative options If you’re unable to install Docker on your computer, you have two options left: using the Docker playground or installing Docker in a Linux virtual machine.\n4.1 Docker playground The Docker playground is an online Docker environment that you can play with for free.\n The advantage is that you don’t have anything to install on your computer.\n The disadvantage is that you might be unable to open a session depending on the number of active sessions.\n  In order to connect to the playground, you need to create an account on DockerHub.\n 4.2 Docker in an already prepared virtual machine We provide an Alpine Linux virtual machine with Docker already installed (size: 743MB). The virtual machine will give you a simple command-line interface where you can type the Docker commands.\nDownload the virtual machine and import it into VirtualBox, as shown in the following video (Safari users: watch on YouTube for fullscreen mode; all users: select HD quality for a better experience).\n The username and password to log into the virtual machine are both root. In the video, you’ll be directed to create a folder in your computer called docker_files. There, you’ll place all files that you’ll need to play with Docker. Don’t hesitate to create subdirectories to organize your files (e.g., td-1, final-project). You’ll be able to access this folder from the virtual machine from the folder /mnt/docker_files. This way, you can manipulate your files by using the file system manager of your computer and you’ll just use the terminal of the virtual machine to type the Docker commands.\n 4.3 Docker in a Ubuntu Linux virtual machine  Install VirtualBox on your computer.\n Download the ISO image of Ubuntu Desktop.\n Open VirtualBox and select New to install a new operating system.\n Choose Linux as the operating system type and Ubuntu (64-bit) as the version.\n Set the memory size (1024 MB or higher).\n Create a virtual hard disk with the recommended size.\n Select VDI (VirtualBox Disk Image) as the hard disk type.\n Select the option Fixed size and create the new virtual machine.\n Double-click on the new virtual machine and, when prompted, select the ISO image of Ubuntu.\n Follow the instructions to install Ubuntu.\n When Ubuntu is finally installed, follow the instructions in Section 3 to install Docker in Ubuntu.\n    5 Verify your installation Open a terminal and type the following command:\ndocker run hello-world If everything is OK, you should see the output in the following figure.\n 6 Interacting with Docker In this course, we’ll learn how to interact with the Docker engine by using the command-line terminal. This option might seem a bit tedious (nobody likes to remember textual commands), but it offers a great flexibility.\nThis is the option that we recommend and for which we’ll provide a full support throughout the course.\n6.1 Graphical interface If you really want to use a graphical interface, you might want to look at Portainer, which is itself run as a Docker container.\n Linux or MacOS users\nOpen a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer   Windows users Open a terminal and copy and paste the following commands:\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart always -v \\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine -v portainer_data:C:\\data portainer/portainer   When the container is up and running, the interface is available at the following URL: http://localhost:9000.\n Choose a password and create the user admin.\n Select Local to manage the Docker environment installed on your computer and click on Connect.\n Click on the endpoint Local (figure below) to access the dashboard.\n   The menu on the left of the dashboard allows you to manage the different components of your Docker environment (e.g., containers, images, volumes and networks).  A user guide of Portainer is very much out of the scope of this course. However, the interface is rather intuitive and you should easily find out how to create, run, stop and remove containers, build images and create volumes and networks.\n  7 Troubleshooting In this section we’ll document the installation issues that you might experience.\nDon’t hesitate to contact us to report your installation problems.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1009f24f18a9853858a69a79adeb4f0c","permalink":"/courses/cloud-computing/overview/installing-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/overview/installing-docker/","section":"courses","summary":"Docker installation instructions","tags":null,"title":"Installing Docker","type":"docs"},{"authors":[],"categories":[],"content":"","date":1581344688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581344688,"objectID":"4e459ff727a9e6a0c795e8d8f3d69232","permalink":"/project/data-for-you/","publishdate":"2020-02-10T15:24:48+01:00","relpermalink":"/project/data-for-you/","section":"project","summary":"","tags":[],"title":"Data for You","type":"project"},{"authors":["Armita Khajeh Nassiri","Nathalie Pernelle","Fatiha Saı̈s","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"77dcf9b15a66ddb64a7a8426a803dd21","permalink":"/publication/nassiri-2020/","publishdate":"2020-11-05T15:51:37.231059Z","relpermalink":"/publication/nassiri-2020/","section":"publication","summary":"","tags":null,"title":"Generating Referring Expressions from RDF Knowledge Graphs for Data Linking","type":"publication"},{"authors":["Fatiha Saïs","Joana E. Gonzales Malaverri","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"24e4602ce4c2a42bac26f3019386ecbf","permalink":"/publication/sais-2020/","publishdate":"2020-04-08T14:26:24.211279Z","relpermalink":"/publication/sais-2020/","section":"publication","summary":"","tags":null,"title":"MOMENT: Temporal Meta-Fact Generation and Propagation in Knowledge Graphs","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Coriane Nana Jipmo","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3685ef29403450add392048ea8b1e19d","permalink":"/publication/seghouani-2019/","publishdate":"2020-02-10T15:32:10.718712Z","relpermalink":"/publication/seghouani-2019/","section":"publication","summary":"","tags":null,"title":"Determining the interests of social media users: two approaches","type":"publication"},{"authors":["Suela Isaj","Nacéra Bennacer Seghouani","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f0a2e1e20345b7ff2748dd0ff24c29fd","permalink":"/publication/isaj-2019/","publishdate":"2020-02-10T15:37:10.960652Z","relpermalink":"/publication/isaj-2019/","section":"publication","summary":"","tags":null,"title":"Profile Reconciliation Through Dynamic Activities Across Social Networks","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed97c59bbeebf6f67506ea911cd4d01b","permalink":"/publication/seghouani-2018/","publishdate":"2020-02-10T15:32:10.719498Z","relpermalink":"/publication/seghouani-2018/","section":"publication","summary":"","tags":null,"title":"A frequent named entities-based approach for interpreting reputation in Twitter","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9cdf7741acd87116627b5dd0bc25a27c","permalink":"/publication/seghouani-2018-a/","publishdate":"2020-02-10T15:33:43.343039Z","relpermalink":"/publication/seghouani-2018-a/","section":"publication","summary":"","tags":null,"title":"Élimination des liens inter-langues erronés dans Wikipédia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"edcf86d1dd80092db4864af2aa551e98","permalink":"/publication/bennacer-2017-a/","publishdate":"2020-02-10T15:32:10.723551Z","relpermalink":"/publication/bennacer-2017-a/","section":"publication","summary":"","tags":null,"title":"Eliminating Incorrect Cross-Language Links in Wikipedia","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"048dafc42b622ff1cee4c115e5409243","permalink":"/publication/jipmo-2017/","publishdate":"2020-02-10T15:32:10.722099Z","relpermalink":"/publication/jipmo-2017/","section":"publication","summary":"","tags":null,"title":"Frisk: A multilingual approach to find twitteR InterestS via wiKipedia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1e3d10b83c3c7b1807d422d7d251cca3","permalink":"/publication/bennacer-2017/","publishdate":"2020-02-10T15:32:10.7231Z","relpermalink":"/publication/bennacer-2017/","section":"publication","summary":"","tags":null,"title":"Interpreting reputation through frequent named entities in twitter","type":"publication"},{"authors":["Gianluca Quercini","Nacéra Bennacer","Mohammad Ghufran","Coriane Nana Jipmo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c02251b35ab73929914bbe9b772e5248","permalink":"/publication/quercini-2017/","publishdate":"2020-02-10T15:32:10.720054Z","relpermalink":"/publication/quercini-2017/","section":"publication","summary":"","tags":null,"title":"Liaison: reconciliation of individuals profiles across social networks","type":"publication"},{"authors":["Mohammad Ghufran","Nacéra Bennacer","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c1928a9ee227111e1b27ba31fdbb6a86","permalink":"/publication/ghufran-2017/","publishdate":"2020-02-10T15:32:10.722652Z","relpermalink":"/publication/ghufran-2017/","section":"publication","summary":"","tags":null,"title":"Wikipedia-based extraction of key information from resumes","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"49508dab832a1726fe7da77e8604a7f8","permalink":"/publication/jipmo-2016/","publishdate":"2020-02-10T15:32:10.724083Z","relpermalink":"/publication/jipmo-2016/","section":"publication","summary":"","tags":null,"title":"Catégorisation et Désambiguı̈sation des Intérêts des Individus dans le Web Social.","type":"publication"},{"authors":["Nacéra Bennacer","Mia Johnson Vioulès","Maximiliano Ariel López","Gianluca Quercini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ce3927111f9fc9f8ef041600fa6e6126","permalink":"/publication/bennacer-2015/","publishdate":"2020-02-10T15:32:10.725565Z","relpermalink":"/publication/bennacer-2015/","section":"publication","summary":"","tags":null,"title":"A multilingual approach to discover cross-language links in Wikipedia","type":"publication"},{"authors":["Mohammad Ghufran","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"482b66b1b2e201edf7dfa2685afa010a","permalink":"/publication/ghufran-2015/","publishdate":"2020-02-10T15:32:10.724645Z","relpermalink":"/publication/ghufran-2015/","section":"publication","summary":"","tags":null,"title":"Toponym disambiguation in online social network profiles","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"2bce9aaaae3b0248796b29334c06054f","permalink":"/publication/bennacer-2014/","publishdate":"2020-02-10T15:32:10.726341Z","relpermalink":"/publication/bennacer-2014/","section":"publication","summary":"","tags":null,"title":"Matching user profiles across social networks","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ff35f567785c6703d81172b874c7531b","permalink":"/publication/bennacer-2014-a/","publishdate":"2020-02-10T15:32:10.727368Z","relpermalink":"/publication/bennacer-2014-a/","section":"publication","summary":"","tags":null,"title":"Réconciliation des profils dans les réseaux sociaux.","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"41c8a22b08bd7e057a05ad86a744cac5","permalink":"/publication/quercini-2014/","publishdate":"2020-02-10T15:32:10.728416Z","relpermalink":"/publication/quercini-2014/","section":"publication","summary":"","tags":null,"title":"Uncovering the spatial relatedness in Wikipedia","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"9086beba183fe687c8f4c462cd3153ca","permalink":"/publication/quercini-2013/","publishdate":"2020-02-10T15:32:10.72918Z","relpermalink":"/publication/quercini-2013/","section":"publication","summary":"","tags":null,"title":"Entity discovery and annotation in tables","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cc5f1500c80ecfe28e62ab5e9b608e81","permalink":"/publication/quercini-2012-a/","publishdate":"2020-02-10T15:32:10.737022Z","relpermalink":"/publication/quercini-2012-a/","section":"publication","summary":"","tags":null,"title":"Des données tabulaires à RDF: l’extraction de données de Google Fusion Tables","type":"publication"},{"authors":["Massimo Ancona","Betty Bronzini","Davide Conte","Gianluca Quercini"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"62840715b27d999325c6799de42c8994","permalink":"/publication/ancona-2012/","publishdate":"2020-02-10T15:32:10.717657Z","relpermalink":"/publication/ancona-2012/","section":"publication","summary":"","tags":null,"title":"Developing Attention-Aware and Context-Aware User Interfaces on Handheld Devices","type":"publication"},{"authors":["Antonio Penta","Gianluca Quercini","Chantal Reynaud","Nigel Shadbolt"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"fa3cbdff0934af7e1ad34c9d906d46b4","permalink":"/publication/penta-2012/","publishdate":"2020-02-10T15:32:10.72995Z","relpermalink":"/publication/penta-2012/","section":"publication","summary":"","tags":null,"title":"Discovering Cross-language Links in Wikipedia through Semantic Relatedness.","type":"publication"},{"authors":["Gianluca Quercini","Jochen Setz","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1eb03ccdfc068b0af2eee6b98febab4f","permalink":"/publication/quercini-2012/","publishdate":"2020-02-10T15:32:10.736267Z","relpermalink":"/publication/quercini-2012/","section":"publication","summary":"","tags":null,"title":"Facetted Browsing on Extracted Fusion Tables Data for Digital Cities.","type":"publication"},{"authors":["Jochen Setz","Gianluca Quercini","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"152a779726400c4afb268da398df2025","permalink":"/publication/setz-2012/","publishdate":"2020-02-10T15:32:10.730492Z","relpermalink":"/publication/setz-2012/","section":"publication","summary":"","tags":null,"title":"Facetted search on extracted fusion tables data for digital cities","type":"publication"},{"authors":["Laura Papaleo","Gianluca Quercini","Viviana Mascardi","Massimo Ancona","A Traverso","Henry de Lumley"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2951f449fe1a9a559bd320ee66d7a4df","permalink":"/publication/papaleo-2011/","publishdate":"2020-02-10T15:32:10.7311Z","relpermalink":"/publication/papaleo-2011/","section":"publication","summary":"","tags":null,"title":"Agents and Ontologies for Understanding and Preserving the Rock Art of Mount Bego.","type":"publication"},{"authors":["Gianluca Quercini","Massimo Ancona"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"712870e00c1980526965a476c61c6a9f","permalink":"/publication/quercini-2010/","publishdate":"2020-02-10T15:32:10.731853Z","relpermalink":"/publication/quercini-2010/","section":"publication","summary":"","tags":null,"title":"Confluent drawing algorithms using rectangular dualization","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet","Jagan Sankaranarayanan","Michael D Lieberman"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"c8ae118f82887db05d951390d2ae7695","permalink":"/publication/quercini-2010-a/","publishdate":"2020-02-10T15:33:43.347923Z","relpermalink":"/publication/quercini-2010-a/","section":"publication","summary":"","tags":null,"title":"Determining the spatial reader scopes of news sources using local lexicons","type":"publication"},{"authors":["Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Anton Bogdanovych","H De Lumley","Laura Papaleo","Simeon Simoff","Antonella Traverso"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"6456e6f4265092c5466fcdd21d7bb4eb","permalink":"/publication/ancona-2010/","publishdate":"2020-02-10T15:32:10.73288Z","relpermalink":"/publication/ancona-2010/","section":"publication","summary":"","tags":null,"title":"Virtual institutions for preserving and simulating the culture of Mount Bego's ancient people","type":"publication"},{"authors":["Anton Bogdanovych","Laura Papaleo","Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Simeon Simoff","Alex Cohen","Antonella Traverso"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"c71e7154cbc2dfbe25039ed19758c530","permalink":"/publication/bogdanovych-2009/","publishdate":"2020-02-10T15:32:10.737689Z","relpermalink":"/publication/bogdanovych-2009/","section":"publication","summary":"","tags":null,"title":"Integrating agents and virtual institutions for sharing cultural heritage on the Web","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e93eb6b5f5c5cf19230762a04c9abc05","permalink":"/publication/ancona-2009/","publishdate":"2020-02-10T15:32:10.720588Z","relpermalink":"/publication/ancona-2009/","section":"publication","summary":"","tags":null,"title":"Text Entry in PDAs with WtX","type":"publication"},{"authors":["Massimo Ancona","Davide Conte","Donatella Pian","Sonia Pini","Gianluca Quercini","Antonella Traverso"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"e650a879a57f2a6c286aa2d365cb9a7d","permalink":"/publication/ancona-2008/","publishdate":"2020-02-10T15:32:10.721064Z","relpermalink":"/publication/ancona-2008/","section":"publication","summary":"","tags":null,"title":"Wireless networks in archaeology and cultural heritage","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini","Luca Dominici"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5da2e1599d08bf6b66c9260251f7f87d","permalink":"/publication/ancona-2007-a/","publishdate":"2020-02-10T15:32:10.734718Z","relpermalink":"/publication/ancona-2007-a/","section":"publication","summary":"","tags":null,"title":"An Improved Text Entry Tool for PDAs","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0bb01482186a36901efd9bc59287ade5","permalink":"/publication/ancona-2007-b/","publishdate":"2020-02-10T15:33:43.342472Z","relpermalink":"/publication/ancona-2007-b/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5de935c6dca2f9dc45d3fcc539d7a23e","permalink":"/publication/ancona-2007/","publishdate":"2020-02-10T15:32:10.721486Z","relpermalink":"/publication/ancona-2007/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["Massimo Ancona","Marco Cappello","Marco Casamassima","Walter Cazzola","Davide Conte","Massimiliano Pittore","Gianluca Quercini","Naomi Scagliola","Matteo Villa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"956e4fbb19a8e3db3caf66b9ca3ce215","permalink":"/publication/ancona-2006-a/","publishdate":"2020-02-10T15:33:43.349611Z","relpermalink":"/publication/ancona-2006-a/","section":"publication","summary":"","tags":null,"title":"Mobile vision and cultural heritage: the agamemnon project","type":"publication"},{"authors":["Massimo Ancona","Walter Cazzola","Sara Drago","Gianluca Quercini"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"152d79fb2becb0b67888c71837db1203","permalink":"/publication/ancona-2006/","publishdate":"2020-02-10T15:32:10.733963Z","relpermalink":"/publication/ancona-2006/","section":"publication","summary":"","tags":null,"title":"Visualizing and managing network topologies via rectangular dualization","type":"publication"},{"authors":["M Ancona","S Locati","M Mancini","A Romagnoli","G Quercini"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"a5f6251a4b15ec8ddae9685ad605d62a","permalink":"/publication/ancona-2005/","publishdate":"2020-02-10T15:32:10.735534Z","relpermalink":"/publication/ancona-2005/","section":"publication","summary":"","tags":null,"title":"Comfortable textual data entry for PocketPC: the WTX system","type":"publication"},{"authors":null,"categories":null,"content":"  The Lab assignment 1 will be graded (\\(g_1\\)).\n A mini project will conclude the course (\\(g_2\\)).\n  The final grade will be the average of \\(g_1\\) and \\(g_2\\).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"924a1a1e7867769d4fc29cef00146e05","permalink":"/courses/cloud-computing/exam/exam-presentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/exam/exam-presentation/","section":"courses","summary":"Overview of the exam.","tags":null,"title":"Grading","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The objective of this project is to assess your ability of building and sharing images in Docker. The project consists of five steps:\nChoose an application. Package your application into one or more Docker images. Ship your image(s) to the DockerHub registry. Write a documentation. Submit your project.  In the following sections, you’ll find the details of each step.\nIf you have questions, doubts, need of help, don’t hesitate to contact me anytime in the MS Teams space.\n Choose an application In this step, you’ll need to select an application of your choice. Examples include:\n An application that you developed in the past in another course (does the phrase coding weeks ring a bell?).\n An application that you develop from scratch specifically for this project. That’s a bit ambitious, but if you feel like challenging yourself, why not?\n An application developed by a friend of yours.\n An application that you found on the internet. A good place to look at is GitHub.\n  The application may be written in any programming language.\n Package your application Your application must be packaged as a Docker image, or a set of Docker images, if the application is composed of many independent services.\nRecommendations  Create a directory on your computer named after your application (e.g., sample-app).\n If your application is monolithic (i.e., it’s not composed of many services), put everything into this directory (including the Dockerfile).\n If your application consists of many independent services, create a subdirectory for each service, containing everything needed to run that service (as well as the Dockerfile).\n  In case your application is composed of many independent services, you might also want to create a docker-compose file with the instructions to run the application (you’d greatly simplify my life!). However, writing a docker-compose is not mandatory.\n  Ship your image(s). Upload your image(s) to the DockerHub registry. In the following instructions I show the Docker commands to upload an image named test-image to my DockerHub registry (my username is quercinigia). Of course, when you’ll type these Docker commands yourselves, you’ll have to use your username and your image’s name.\nFor your convenience, in my last lecture I showed a full demo of the procedure, that you can watch in in this video, starting at minute 37.\n If you haven’t done so yet, create an account on DockerHub.\n From the command-line terminal, I log into my DockerHub account. One way to do so is the following:\n I created a file docker_pwd.txt containing my password.\n In the terminal, I place myself in the same directory as the file docker_pwd.txt and I type the following command:\n   cat docker_pwd.txt | docker login --username quercinigia --password-stdin  I tag the image to upload with my DockerHub username, as follows:  docker image tag test_image quercinigia/test_image:1.0  Finally, I push the image to the DockerHub registry:  docker image push quercinigia/test_image:1.0  Write a documentation The documentation must be concise (max 4 pages). It must include:\n Your name and family name. Short description of the application that you chose. The description of the Dockerfile, specifying the meaning of each instruction. The instructions that I have to follow to pull your image(s) from the DockerHub registry and run them.  The last point is particularly important as I have to verify that everything works as expected.\n Submit your project You’ll submit your project on Edunao (click on this link).\nDeadline. June 6, 2020, 23:59.\nDepending on the application that you chose, two cases are possible:\nYour application consists of only one Docker image. In this case, you may just upload the documentation in PDF.\n You application consists of many images and you have created a docker-compose file, or you need to send additional files. In this case, you can upload an archive file (accepted formats include .zip .7z .bdoc .cdoc .ddoc .gtar .gz .gzip .hqx .rar .sit .tar .tgz), containing the documentation in PDF as well as the other files.\n  In any case, do not submit the source code of your application.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c1fa1ef2c9d9118abb69e237496d95f","permalink":"/courses/cloud-computing/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Project","type":"docs"},{"authors":null,"categories":null,"content":" Lab assignment 1 Title: MapReduce - Spark\nSupervisors: Gianluca Quercini, Francesca Bugiotti, Marc-Antoine Weisser, Idir Ait Sadoune (CentraleSupélec)\nDate and time: Tuesday 19 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nRemark: This lab assignment will be graded\nLink: Click here\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6e75e11c1ee6ba55693a9622583beaf9","permalink":"/courses/cloud-computing/labs/cc-labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/labs/cc-labs/","section":"courses","summary":"Presentation of the lab assignments of the course.","tags":null,"title":"Lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Cloud Computing Introduction\nLecturer: Wilfried Kirschenmann (ANEO)\nDate and time: Friday 24 April 2020, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 2 Title: Virtualization and Containerization\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Thursday 30 April, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n Lecture 3 Title: Introduction to MapReduce\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Thursday 7 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao (Part 1, Part 2, Part 3)\n Lecture 4 Title: Introduction to Hadoop and Spark\nLecturer: Francesca Bugiotti (CentraleSupélec)\nDate and time: Friday 15 May, 1:45 PM - 5 PM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao (Part 1, Part 2, Part 3, Part 4, Part 5).\n Lecture 5 Title: Multi-service applications\nLecturer: Gianluca Quercini (CentraleSupélec)\nDate and time: Wednesday 20 May, 8:30 AM - 11:45 AM\nRoom: Remotely on Microsoft Teams\nSlides: Available on Edunao.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f5a608bccc37bec35254441f7fcae9a","permalink":"/courses/cloud-computing/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Class material  Lecture slides, available here.\n An introduction to Docker.\n A Docker cheat sheet, with a summary of the most important Docker commands.\n An introduction to Linux, useful to understand how Docker works and how to interact with Docker.\n   Books  Schenker, Gabriel. Learn Docker - Fundamentals of Docker 18.x. Packt Publishing,. Print.\n Surianarayanan, C., \u0026amp; Chelliah, P. R. (2019). Essentials of Cloud Computing: A Holistic Perspective. Springer Nature.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d57f199c44c3345bf5285f7abd8f4c9","permalink":"/courses/cloud-computing/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\n Figure 1: Cluster architecture (image credit: Stéphane Vialle)  In order to use the Spark cluster, you’ll need to go through the following fours steps:\nChoose a username on Edunao. The password will be communicated by the teacher during the tutorial.\n Open a SSH connection to the machine phome.metz.supelec.fr from your local machine.\n Open a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\n On the machine slurm1 allocate resources for the job from a named reservation.\n  Steps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\n Connect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\n In the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\n In the connection panel, set Enable TCP keepalives and set 30s between keepalives.\n Click on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\n A command-line terminal should pop up, prompting you to enter your username and password.\n   Connect to slurm1 In the command-line terminal type the following command:\nssh slurm1\n Allocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\n Use the cluster only in the evening in weekdays or during the weekends.\n In order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks1 -N 1 --ntasks-per-node=4 --pty bash\n    Source file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuasi1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!).  or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!).  The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\n Windows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\n  Creating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuasi1_X, where X is between 1 and 28.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuasi1/cpuasi1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuasi1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuasi1/\n Preliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\n Copy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\n If you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\n Open the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\n Locate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\n Similarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuasi1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\n Run the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\n When the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\nAs usual, remember to replace cpuasi1_X with your username.\n In order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out/*   Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs rm -r hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c22db20918c1b3dd3408cafa3febaca8","permalink":"/courses/plp/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this project is to assess your knowledge of the main notions presented in classroom.\nThe project must be implemented by students working in groups.\nCreating the groups\nClick here to create your group.\n This project consists in designing a relational database for the given application context, importing the data of a given dataset into a SQLite database and querying the data in SQL.\nThe deadline to submit the work is Friday December 11th.\nSubmit your project\nCreate a .zip archive with the required files and click here to submit it.\n Here is what you have to submit:\n A report in PDF containing the answers to all the questions and exercises, except the SQL queries (max 10 pages).\n A file .db containing the SQLite database with all the data.\n A file .sql with all the SQL queries.\n   Application context We intend to manage the data of a travel reservation system with clients all over the world. Upon registration, customers are automatically given a numeric identifier and they are asked to indicate their first and family names, their gender, date of birth, a phone number, an email address and their country of residence.\nAny customer can book a trip that includes the reservation of one or more flights and, possibly, one or more hotels.\nExample\nAlice wants to fly from Paris, France to New York City (NYC), USA and she intends to stay in NYC for 10 days. Her trip includes two flights: an outbound flight from Paris to NYC and an inbound flight from NYC to Paris; and an hotel in NYC.\n A flight is operated by an airline company, of which the system keeps its name (e.g., British Airways), the country where the airline is incorporated and, when available, its IATA code (e.g., BA, a two-letter code identifying the airline), its ICAO code (e.g., BAW, a three-letter code identifying the airline) and alternate name or alias (e.g., British).\nA flight connects two airports, each having a name (e.g., London Heathrow Airport), and, possibly, a IATA (e.g., LHR) and ICAO code (e.g., EGLL); an airport serves a specific location (e.g., London, UK) and its precise position is given by its geographic coordinates (latitude and longitude).\nA flight connecting two airports at specific departure and arrival times is identified by a flight number. Two flights operated by two different airline companies cannot have the same flight number, but the same flight number can denote two flights operated by the same airline company on different days.\nExample\nEmirates flight EK074 leaves Paris, France at 10 AM and arrives at Dubai, UAE at 7:40 PM (regardless of the departure day).\n For each flight booked by a customer, the system keeps the seat number, the travel class (e.g., economy or business), the price and the date of the flight. Usually, airlines include details on the type of aircraft they plan to use on their flight schedules; these details include the name of the aircraft (e.g., Boeing 787-8) and, when available, the IATA code (e.g., 788, a unique three-letter identifier for the aircraft) and the ICAO code (e.g., B788, a unique four-letter identifier for the aircraft).\nThe system maintains a list of hotels, with their names, addresses and an average review score, which is a real number denoting the average grade assigned to the hotel by its customers. Customers can write a review for an hotel; in which case the system stores the text of the review, the date and its author. When a customer books an hotel, the system keeps the price paid, the check-in and check-out dates and whether the breakfast is included.\n The dataset You can download the dataset by clicking here. The dataset consists of 7 CSV files: aircrafts.csv, airlines.csv, airports.csv, hotels.csv, customers.csv, hotel_bookings.csv, flight_bookings.csv. The separator character in each file is the tab character (‘’).\nNotice\nTake some time to look at the content of the files to understand the structure that your tables will have.\n  Creation of a relational database You’ll now proceed to the definition of a relational database for our travel reservation system. First, you need to define the conceptual schema and then you’ll define the tables that compose the database.\nThe conceptual schema Before defining the logical schema of the database, answer the following questions:\nCan you use the name of the hotel as a primary key? Justify your answer.\n Can you use the flight number as a primary key to identify a flight? Justify your answer and, in case of a negative answer, propose a solution.\n Knowing that it is unlikely that two reviews have the same textual content, would you use it as a primary key? Justify your answer.\n Knowing that the IATA code uniquely identifies an airport, would you choose it as a primary key for the entity Airport? Justify your answer.\n  Exercise\nExercise 1  Propose an Entity-Relationship diagram describing the conceptual model of a relational database for the given application context.\n Specify all the attributes for each entity and relation.\n For each entity, underline the attributes composing the primary key.\n For each relation, clearly indicate the minimum and maximum cardinality.     Normalization Exercise\nExercise 2  Translate the logical schema into a collection of tables. For each table:\n Indicate its name and the names of the columns (but not their types).\n Underline the columns that are part of the primary key.\n Indicate the entity in the ER diagram to which the table corresponds.\n  To make sure you choose the right data types for the columns, you can also check the values in the dataset.\n  Which normal form are your tables in?\nExercise\nExercise 3  For each table:\n Indicate a minimal set of functional dependencies.\n Indicate the normal form that the table satisfies. Justify your answer.    Normalize your tables.\nExercise\nExercise 4  Normalize each table up to the Boyce-Codd Normal Form (BCNF).\n   The physical schema You can now define the physical schema of your database.\nExercise\nExercise 5  Write the SQL code to create the tables. For each table:\n Indicate the primary key.\n Indicate the foreign keys.\n Indicate NOT NULL and UNIQUE constraints, if needed.     Database creation and data import In order to create a database in SQLite, use DB Browser for SQLite:\n Open the program and click on New Database.\n The program will ask you to specify a name for the database file and the folder where you want to store it.\n The program will display a pop-up window where you can create the tables. Alternatively, you can directly execute the SQL code that you wrote before to create your tables.\n  Exercise\nExercise 6  Create a SQLite database with the tables that you defined in the previous exercise.\n  In order to import the data into a table, you can select File \\(\\rightarrow\\) Import \\(\\rightarrow\\) Table from CSV file. The given CSV file might not correspond exactly to the tables that you created, so you’ll need to edit them a bit in order to fit the schema of your tables.\n Running queries Write the following queries in SQL.\nNotice\nYou’ll write your queries in a file queries.sql that you’ll submit with your project.\nOnly write the queries without any comment\nPlease separate each query in the file by a blank line.\nExample:\nSELECT * FROM Airport SELECT * FROM Hotel   Exercise\nExercise 7 \nGet the average ticket price on Air France flights.\n Count the number of customers by country.\n Select the names of the airports in Paris, France.\n Select the name of the cities (city and country name) with the most airports.\n Select the name of the airline companies that use an Airbus A300.\n Select the identifier, first and family names of all customers who flew to Sydney, Australia.\n Select the identifier, name, city and country of the busiest airport (the one with more outbound and inbound flights).\n Select the average price in the economy class.\n Select the average price in the business class.\n Select the name, city and country of the destination airport of french customers.\n Select the destination cities (specify city and country name) preferred by women.\n Select the destination cities (specify city and country name) preferred by men.\n Count the number of customers by country flying to Paris.\n Count the number of hotels by city.\n Determine the amount of money spent by Tatiana REZE in flights.\n     Indexes Exercise\nExercise 8 \nWrite a query to get all the information of a customer with a given family name. Run the query multiple times and note the average running time of the query.\n Create an index on the column containing the family name of a customer.\n Rerun the same query multiple times and note the average running times of the query.\n Do you observe any difference? Can you explain what is going on here?      ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85d2d631269d7307bfde5c4b0669c155","permalink":"/courses/databases/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":" Introduction Slides: Available here.\n Lecture 1 Title: An introduction to database systems and data modeling.\nDate and time: Tuesday 15 September 2020, 9:00 AM - 12 PM.\nSlides: Available here.\n Lecture 2 Title: Normalization theory.\nDate and time: Tuesday 29 September 2020, 9:00 AM - 12 PM.\nSlides: Available here.\n Lecture 3 Title: Relational database management systems.\nDate and time: Tuesday 6 October 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n Lecture 4 Title: Advanced relational database concepts.\nDate and time: Tuesday 13 October 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n Lecture 5 Title: Distributed databases and NoSQL.\nDate and time: Tuesday 20 October 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n Lecture 6 Title: Document-oriented databases: MongoDB.\nDate and time: Tuesday 3 November 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n Lecture 7 Title: Graph databases: Neo4j.\nDate and time: Tuesday 10 November 2020, 9:00 AM - 12 AM.\nSlides: Available here.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f967524fae94aa5965fe18163acab25","permalink":"/courses/databases/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Date, Christopher John. An introduction to database systems. Pearson Education India, 2004.\n Hoffer, Jeffrey A., Venkataraman Ramesh, and Heikki Topi. Modern database management. Pearson, 2016\n Garcia-Molina, Hector, Jeffrey D. Ullman and Jennifer Widom. Database systems: the complete book. Pearson Education India, 2008.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Robinson, Ian, Jim Webber, and Emil Eifrem. Graph databases. \u0026quot; O’Reilly Media, Inc.\u0026quot;, 2013.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"074436047baef381bdf6909058220908","permalink":"/courses/databases/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL\nExample. Building an image from a Dockerfile in the current directory: docker build .\n The command assumes that a file named Dockerfile is in the current directory.  Common options:\n  Tag the image   Name of the Dockerfile       docker build -t my-image:latest .   docker build -f my-dockerfile .       Manage images   List all images   List images (no intermediate)   Remove an image     docker image ls -a   docker image ls   docker image rm my-image     Remove dangling images   Remove unused images   Show the history of an image     docker image prune   docker image prune -a   docker history my-image     Dockerfile In a Dockerfile the following main keywords are used:\n  FROM base-image   FROM scratch   RUN cmd     Specifies the base image   No base image used   Runs a command     COPY src dst   ADD src dst   WORKDIR dir     Copy source file to destination   Copy source file (including URL and TAR) to destination   Sets the working directory     ENTRYPOINT cmd   CMD params   EXPOSE port     Command to execute when container is run   Parameters of the entrypoint command   Exposes a container port     Volumes   Create a volume   Remove a volume   Remove unused volumes     docker volume create my-volume   docker volume rm my-volume   docker volume prune     List volumes         docker volume ls         Networks   Create a network   Remove a network   Remove unused networks     docker network create my-network   docker network rm my-network   docker network prune     List all the networks   Inspect a network   Connect a container to a network     docker network ls   docker network inspect my-network   docker network connect my-network my-container     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f5dc854a8f25cad521a576f1c221397","permalink":"/courses/cloud-computing/references/docker-cheat-sheet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/references/docker-cheat-sheet/","section":"courses","summary":"Run containers Docker command: docker run [options] image-name [command] [arg]\nExample: Running a container from the image alpine.\n  docker run image-name   docker run image-name command   docker run image-name command arg     docker run alpine   docker run alpine ls   docker run alpine ping 192.168.3.1    Common options:\n  Remove the container when it exits   Give the container a name   Allocate a terminal for the container     docker run --rm alpine   docker run --name toto alpine   docker run -it alpine     Mount data-volume at /data**   Container port –\u0026gt; random host port   Host port 8080 –\u0026gt; container port 80     docker run -v data-volume:/data alpine   docker run --P alpine   docker run -p 8080:80 alpine     Attach container to network         docker run --network mynet alpine         Manage containers   List all containers   List running containers   Stop a container     docker container ls -a   docker container ls   docker stop my-container     Remove a container   Remove all stopped containers   Start a container     docker container rm my-container   docker container prune   docker start my-container     Start a container (I/O)   Inspect changes in a container   Create image from container     docker start -ai my-container   docker diff my-container   docker commit my-container new-image     Build images Docker command: docker build [OPTIONS] PATH | URL","tags":null,"title":"Docker Cheat Sheet","type":"docs"},{"authors":null,"categories":null,"content":"  1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server  2 Instructions for Windows users   In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites  Install XCode. You’ll find it for free in the Mac App Store.\n Install Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n  /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot;  Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page.   1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\n  Tap the official MongoDB Homebrew tap with the following command:  brew tap mongodb/brew  Type the following command to install MongoDB:  brew install mongodb-community@4.4  1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4  1.4 Launch MongoDB compass  Open MongoDB Compass.\n After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.   1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n   2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\n  Go to the MongoDB download page and download the installer (.msi file).\n Double-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\n Once the installation procedure is over, the MongoDB server is automatically started.\n MongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.  If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"064b36eb5a5681ac5f2d8d45e19e9c24","permalink":"/courses/databases/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":"  1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server  2 Instructions for Windows users   In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites  Install XCode. You’ll find it for free in the Mac App Store.\n Install Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n  /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot;  Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page.   1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\n  Tap the official MongoDB Homebrew tap with the following command:  brew tap mongodb/brew  Type the following command to install MongoDB:  brew install mongodb-community@4.4  1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4  1.4 Launch MongoDB compass  Open MongoDB Compass.\n After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.   1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n   2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\n  Go to the MongoDB download page and download the installer (.msi file).\n Double-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\n Once the installation procedure is over, the MongoDB server is automatically started.\n MongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\n In the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\n  mongodb://localhost:27017  If everything goes well, you should see a list of databases.  If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bee18108026333e44d7372a340146c8","permalink":"/courses/plp/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" Introduction In this lab assignment you’ll run queries in MongoDB.\nAssignment submission\nThis lab assignment will be evaluated.\nIn order to finalize the submission of the assignment, submit a report in PDF by using this link.\nThe report must not be longer than 4 pages. In the report:\n Indicate your first and family name.\n Describe briefly the MongoDB query language and the aggregation framework.\n Write the queries but not the results of the queries.\n  The submission deadline is Thursday, January 28, 2021 8:00 AM.\n  Setting up the work environment.  Start the MongoDB server. Refer to the documentation to find out how to start the server depending on your operating system.\n Launch MongoDB compass. You should have installed it with the MongoDB server.\n From within MongoDB compass, connect to the local MongoDB server.\n   Creating the database  Download the data by clicking here.\n In MongoDB Compass, click on the button “Create database”.\n Give the database a name (e.g., cinema) and create a collection named movies.\n Click on the button Add data and select Import file.\n Select JSON as a file format and select the file movies.json.\n Click on “Import”; 88 documents should be imported into the database.\n  Troubleshooting\nIn case you experience problems while importing the JSON file, you might want to try to import the file movies.csv.\n  Querying the data with find At the bottom of the MongoDB window, you should find a link to open the MongoDB shell (MongoSH Beta).\nAfter opening the shell, write the following command to switch to database cinema:\nuse cinema To verify that you’re actually connected to the database cinema, type the following query:\ndb.movies.findOne() Exercise\nExercise 1  By using the MongoDB shell, execute the following queries.\nQ1. The release year of the movie “Le parrain III”.\nQ2. The title of the movies released between 1980 and 1990.\nQ3. Same query as b. with the titles must be sorted by alphabetical order.\nQ4. The titles of the french movies.\nQ5. The title of the “crime” or “drama” movies.\nQ6. The names and birth dates of the directors of french movies.\nQ7. The title of the movies in which Sofia Coppola played.\nQ8. The title and the genre of the movies of which Hitchcock is director.\n   Querying the data with the aggregation framework Exercise\nExercise 2  By using the MongoDB shell, execute the following queries by using aggregation pipelines.\nQ1. The number of movies by country. Show by decresing number.\nQ2. The name of the actor in the role “Mary Corleone” in the movie “Le parrain III”.\nQ3. The number of actors by movie. Sort by decreasing number.\nQ4. The average number of actors in a film.\n   Join in MongoDB In the database cinema, create a new collection called movies_boffice. Import the documents in file moviesBoxOffice.json into this collection.\nExercise\nExercise 3  By using the operator $lookup on the collections movies and movies_boffice, find the box office of the movie “Le parrain III”.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b82eb6b804bb0bc2e6736e5b5eac3d54","permalink":"/courses/plp/tutorials/mongodb-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/mongodb-tutorial/","section":"courses","summary":"Description of the MongoDB tutorial.","tags":null,"title":"MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nDate and time: Monday 4 January 2021, 1:45 PAM - 5 PM.\nSlides: Available on Edunao.\n Lecture 2 Title: Hadoop and its ecosystem: HDFS.\nDate and time: Wednesday 6 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\n Lecture 3 Title: Introduction to Apache Spark.\nDate and time: Thursday 7 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\nSpark programming demo: Available in Google Colab.\n Lecture 4 Title: Advanced Apache Spark.\nDate and time: Monday 11 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\nSpark SQL demo: Available in Google Colab.\n Lecture 5 Title: Distributed and NoSQL databases\nDate and time: Thursday 14 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\n Lecture 6 Title: Document-oriented database systems: MongoDB.\nDate and time: Thursday 21 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\n Lecture 7 Title: Graph-oriented database systems: Neo4j.\nDate and time: Thursday 28 January 2021, 8:30 AM - 11:45 AM.\nSlides: Available on Edunao.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2df8cc01c3dc820657e60bdae93d56cd","permalink":"/courses/plp/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":"  Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\n Mehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\n Karau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\n Giamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\n Bradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\n Scifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c41cb45dee07917944a15794098e978b","permalink":"/courses/plp/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this Spark lab assignment is to write Spark programs and run them on a cluster. Refer to this documentation to learn how to connect and interact with the cluster.\nAssignment submission\nThis lab assignment will be evaluated.\nIn order to finalize the submission of the assignment, complete this form available on Edunao.\nThe submission deadline is Tuesday, March 9, 2021 8:00 AM.\n  Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nWe use the following input files available in folder hdfs://sar01:9000/data/sn/:\n sn_tiny.csv. Small social network, that you can use to test your implementation.\n sn_10k_100k.csv. Social network with \\(10^4\\) individuals and \\(10^5\\) links.\n sn_100k_100k.csv. Social network with \\(10^5\\) individuals and \\(10^5\\) links.\n sn_1k_100k.csv. Social network with \\(10^3\\) individuals and \\(10^5\\) links.\n sn_1m_1m.csv. Social network with \\(10^6\\) individuals and \\(10^6\\) links.\n  Exercise\nExercise 1  Write an implementation in Spark. Test your implementation on file sn_tiny.csv.\n  Exercise\nExercise 2  Run your implementation on the other files and write down the execution times. Comment on the execution times considering the file sizes, the number of nodes and links and the number of pairs \\(((A, B), X)\\) generated by the algorithm.\n  Exercise\nExercise 3  Execute your implementation on the file sn_1m_1m.csv by varying the number of cores used by the Spark executors. You can specify the total number of cores with the option --total-executor-cores of the command spark-submit (you can also refer to the Spark documentation).\n What is the impact of the number of cores on the execution time? Make a graph and comment.    Exercise\nExercise 4 \nBy using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of a node in a given graph.\n Compute the minimum, maximum and average degree on all the given input files.\n Do these values confirm or invalidate the considerations that you made on the execution times of the algorithm in the first exercise? Justify your answer.\n     Creating an inverted index In folder hdfs://sar01:9000/data/bbc/ you’ll find a collection of 50 articles obtained from the BBC website (2004-2005) organized into five subfolders: business, entertainment, politics, sport and technology.\nWe want to create an inverted index, which associates each word with the list of the files in which the word occurs. More specifically, for each word, the inverted index will have a list of the names of the files (path relative to the folder /data/bbc) that contain the word.\nThe inverted index:\n must not contain the same word twice;\n must not contain any stopwords (the list of stopwords is provided in the hdfs://sar01:9000/data/stopwords.txt file);\n  Moreover:\n Words in the inverted index must only contain letters.\n Words in the inverted index must be lowercase.\n  Exercise\nExercise 5  Write a Spark program to create an inverted index and execute it on the input folder. You can use the template available at ~vialle/DCE-Spark/template_inverted_index.py.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dc442fca81a29582331cefd6a42cbd11","permalink":"/courses/plp/tutorials/spark-lab-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/spark-lab-assignment/","section":"courses","summary":"Description of the lab advanced Spark programming.","tags":null,"title":"Advanced Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model.  Prerequisites:\n Having attended Lecture 1.  1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1  Give the conceptual schema of the database with an ER diagram.\n   Solution\n  Exercise\nExercise 1.2  Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\n   Solution\nThe collection of tables is the following:\n UserAccount (nickname, first_name, last_name, city, country) Post (post_id, content, date, time, lat, long, nickname) EmailAddress (email_address, nickname) Relationship (nickname_src, nickname_dst, type, date) Tag (post_id, nickname)  The foreign keys are the following:\nPost(nickname) → UserAccount(nickname).\nEmailAddress(nickname) → EmailAddress(nickname).\nRelationship(nickname_src) → UserAccount(nickname).\nRelationship(nickname_dst) → UserAccount(nickname).\nTag(post_id) → Post(post_id).\nTag(nickname) → UserAccount(nickname).\n    2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\n Figure 2.1: The conceptual schema of the bank database  Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer.    Solution\nSince no two banks have the same code_bank or name, either property can be chosen as the primary key of the entity Bank. Both can be considered as valid candidate keys.\n  Exercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer.    Solution\nThe answer is no. While there aren’t any banks that have the same value for {code_bank, name}, two subsets ({code_bank} and {name}) are candidate keys.\n  Exercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises.    Solution\n  Exercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.    Solution\nThe collection of tables is the following:\n Bank (code_bank, name, address) Branch (branch_id, address, code_bank) Account (acct_nbr, acct_type, balance, branch_id, ssn) Loan (loan_nbr, loan_type, amount, branch_id, ssn) Customer (ssn, first_name, last_name, telephone, address)  The foreign keys are the following:\nBranch(code_bank) → Bank(code_bank).\nAccount(branch_id) → Branch(branch_id).\nAccount(ssn) → Customer(ssn).\nLoan(branch_id) → Branch(branch_id).\nLoan(ssn) → Customer(ssn).\n    3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, colour, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\n A salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys.  When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram.    Solution\n  Exercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys.     Solution\nThe collection of tables is the following:\n Car (serial_number, make, model, colour, is_new, owner_id) Customer (cust_id, cust_first_name, cust_last_name, cust_phone) Invoice (invoice_number, date, price, car_serial_number, sp_id) Salesperson (sp_id, sp_first_name, sp_last_name) Mechanic (mec_id, mec_first_name, mec_last_name) Ticket (ticket_number, date_open, date_return, car_serial_number) Repair (ticket_number, mec_id)  The foreign keys are the following:\nCar(owner_id) → Customer(cust_id).\nInvoice(car_serial_number) → Car(serial_number).\nInvoice(sp_id) → Salesperson(sp_id).\nTicket(car_serial_number) → Car(serial_number).\nRepair(ticket_number) → Ticket(ticket_number).\nRepair(mec_id) → Mechanic(mec_id).\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85c2215a0f57e47f0e3f4cc4fba8d167","permalink":"/courses/databases/tutorials/data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/data-modeling/","section":"courses","summary":"Description of the data modeling tutorial.","tags":null,"title":"Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to run containers. How to define and build images. How to create and use volumes. How to define and use networks.  Prerequisites:\n Having installed Docker on your computer. See the installation guide. Being familiar with the notions of containers, images, volumes and networks in Docker. See the Docker primer for an introduction. Being familiar with the basic notions of Linux. Don’t hesitate to look at the Docker cheat sheet to verify the syntax of the Docker commands.  Terminology\n You’ll use the terminal to run Docker commands. Referring to the Docker architecture, the terminal is the client that communicates with the Docker daemon.\n Docker runs containers on your computer. We’ll refer to your computer as the host, the containers being the guests.\n   A containerized application is an application running in a container.    1 Running containers The command used to run a container is docker run followed by four parameters:\ndocker run [options] image-name [command] [arg] The four parameters are:\n options. List of options. image-name. The fully qualified name of the image used to run the container. command. The command to be executed in the container. arg. The arguments taken by the command executed in the container.  Only the parameter image-name is mandatory. The fully qualified name of an image is specified as a sequence of four fields, formatted as follows:\nregistry_url/user/name:tag where:\n registry_url (optional). The URL of the registry that provides the image. If its value is not specified, the image will be looked up for in the DockerHub registry. user (optional). The identifier of the user or organization that created the image. The default value is library. name (mandatory). The name of the image. tag (optional). It specifies the image version. If its value is not specified, the tag latest is used, pointing to the latest image version.  Exercise\nExercise 1.1  For each of the following images, specify the registry name, the user, the name and the tag.\nregistry.redhat.io/rhel8/mysql-80\n alpine:3.11\n  alpine      Solution\nRegistry: registry.redhat.io, user: rhel8, name: mysql-80, tag: latest\n Registry: DockerHub, user: library, name: alpine, tag: 3.11\n Registry: DockerHub, user: library, name: alpine, tag: latest    Exercise\nExercise 1.2 What’s the difference between the following image names?\nalpine:latest\n registry.hub.docker.com/library/alpine\n  alpine      Solution\nThere’s no difference. They all point to the same image, that is the latest version of alpine in the DockerHub registry.\n  We now learn how to use the command docker run and some of its options. In the following exercises, we’ll run containers from the image named alpine that is available on the DockerHub registry. This image provides a lightweight distribution (i.e., it doesn’t contain many features) of Linux.\nExercise\nExercise 1.3 You want to run the container from the latest version of the image alpine. Which command would you write in the terminal?    Solution\nThe goal of this exercise is to start playing with the docker run command. Since the question doesn’t say anything about the options, nor does it mention the command to run inside the container, we’d type:\ndocker run alpine   Exercise\nExercise 1.4 Execute the command that you proposed in the previous exercise, observe the output in the terminal and explain the actions taken by Docker to run the container.    Solution\nThe output obtained from executing the command should look like as follows:\nUnable to find image \u0026#39;alpine:latest\u0026#39; locally latest: Pulling from library/alpine aad63a933944: Pull complete Digest: sha256:b276d875eeed9c7d3f1cfa7edb06b22ed22b14219a7d67c52c56612330348239 Status: Downloaded newer image for alpine:latest Here’s what happens under the hood:\nDocker looks for an image named alpine:latest in the host computer and cannot find it.\n Docker pulls (i.e., downloads) the image from the DockerHub registry.\n    Okay but where’s the result of running the container?\nFirst of all, let’s see if the container is still running in the first place. In order to list all containers still running on the host, type the following command:\ndocker container ls  Your container shouldn’t appear in the output, because it’s not running. In order to see all containers, including those that are not running, type the following command:\ndocker container ls -a Exercise\nExercise 1.5 What information is displayed for each container?    Solution\n The identifier of the container.\n The name of the image used to run the container (it should be alpine for your container).\n The command executed within the container (it should be /bin/sh for your container).\n When the container has been created.\n The container current status (it should be exited (0) x seconds ago for your container).\n The network ports used by the container (we’ll study them later).\n The name of the container. If you don’t specify any when you run the container (as is our case), Docker generates a random name by concatenating an adjective and a famous scientist’s name (e.g., agitated_newton).\n    Exercise\nExercise 1.6 By looking at the command executed within the container (/bin/sh), can you tell why the container stopped without giving any output?    Solution\nThe command is /bin/sh; the container runs a Linux terminal. But since we didn’t specify what to do with that terminal (we didn’t run any Linux command, nor we tried to access the terminal), the container stopped.\n  We’re now going to do something useful with the image alpine. But first, we start with some good practices that you should adopt while playing with images and containers.\n1.1 Good practices Name your containers. Although Docker assigns a default name to a new container, it’s usually a good practice to give a container a name of your choice to make it easily distinguishable. You can do it by using the option --name. Try the following:  docker run --name my-alpine alpine As before, the container stops immediately. If you list all your containers by typing again:\ndocker container ls -a you should see a container named my-alpine.\nRemove automatically a container if you use it once. Unless you want to reuse your container later, you can ask Docker to automatically remove it when it stops by using the option --rm. This will prevent unused containers from taking up too much disk space.  Try the following:\ndocker run --rm --name container-to-remove alpine If you list all the containers you should see that there is no container named container-to-remove.\nRemove unused containers. Stopped containers that have been run without using the option --rm are still stored in the host. If you want to remove a specific container (e.g., my-alpine), use the following command:  docker container rm my-alpine If you want to remove all stopped containers, use the following command:\ndocker container prune Remove unused images. Images can take up a lot of disk space. As a result, you should remember to remove those that you don’t intend to use any longer. The commands to remove a specific image and prune unused ones are docker image rm and docker image prune -a respectively.   1.2 Pass a command to the containerized application Remember that the template of docker run is the following:\ndocker run [options] image-name [command] [arg] The optional parameter command refers to a command that you can pass the containerized application, possibly with some arguments (parameter arg).\nLet’s see an example. As we saw before, when we run a container from the image alpine, a Linux terminal /bin/sh is launched.\nNotice\nThe Linux terminal /bin/sh is run within the container. Henceforth, we’ll use the following terms:\n Host terminal. The terminal that you use to interact with the operating system of your computer.   Guest terminal. The terminal that is run within the container.    By using the optional parameter command, we can run a command in the guest terminal.\nExercise\nExercise 1.7  Run a container from the image alpine and execute the Linux command ls that lists the content of the current directory.\n Where are the listed files stored? In the host or in the container?      Solution\ndocker run --rm --name ls-test alpine ls  The command ls is run in the guest terminal, therefore what we see in the output is a list of files stored in the container.    Notice\nIn Exercise 1.7 the command ls is executed in the guest terminal, but its output is redirected to the host terminal.\nIn other words, when we run the container, we don’t interact directly with the guest terminal; we just send a command and the output is redirected to the host terminal.\n Now let’s see how to execute a command in the guest terminal that also requires an argument.\nExercise\nExercise 1.8 By using the Linux utility ping, check whether the Web site www.centralesupelec.fr is reachable.     Solution\ndocker run --rm --name ping-test alpine ping www.centralesupelec.fr In order to interrupt ping just type the key combination that you’s use to interrupt any other command in your terminal. (typically Ctrl-C on Windows and Cmd-C in MacOs).\n   1.3 Interacting with a container An application running in a container might need to interact with the user. For instance, the Linux command rev reverses whatever the user types on the keyboard. In order to interact with a container, you should use the option -it of docker run.\nExercise\nExercise 1.9 Run a container from the image alpine to execute the Linux command rev and interact with it. You can stop interacting with rev by typing Ctrl+C at any time.    Solution\ndocker run --rm --name rev -it alpine rev After typing the command, type a word on your keyboard (e.g., deeps), press Return and you should see the same word reversed (e.g., speed).\nThe option -t opens a guest terminal (so we can see its output); the option -i allows you to write directly into the guest terminal.\nIn order to stop using the guest terminal, you’ll need to press Ctrl+D (both in Windows and MacOs).\n  Now run the following command:\ndocker run --name my-alpine -it alpine Note: we didn’t use the option --rm (the container will not be removed when we stop it, we’re going to use it again). Moreover, we didn’t specify any command to run in the guest terminal.\nExercise\nExercise 1.10 What do you obtain?    Solution\nWhen we run a container from the image alpine, the command /bin/sh is executed within the container. Since we specified the option -it, what we obtain is an access to the Linux terminal running in the container.\n   1.4 Starting and stopping containers. docker run is a shorthand for two Docker commands, namely docker create, that creates a container from an image, and docker start, that starts the container after its creation.\nSuppose now that you want to download a Web page by using Linux Alpine. You can use the Linux command wget followed by the URL of the page that you want to download.\nExercise\nExercise 1.11 By using the guest terminal in the container my-alpine, download this Web page.\n Where will the Web page be saved? The host computer or the container?      Solution\nJust type in my-alpine guest terminal the following command:\nwget https://www.centralesupelec.fr/fr/presentation  The Web page will be saved in the current directory of the container. You can verify that the file is there by typing ls in the guest terminal.    In my-alpine guest terminal type exit. This closes the guest terminal and, as a result, stops the container.\nNOTICE\nStopping the container will not erase any of the files stored in the container. Removing the container will.\n If you want to start the container my-alpine again, you can use the following command:\ndocker container start -ai my-alpine This will open the guest terminal of the container again; type ls to verify that the Web page that you downloaded before is still there.\n 1.5 A simple use case Suppose that you need to download all the figures of this Web page. The Linux utility wget comes in handy. However, you don’t have Linux and you’d like to avoid the hassle of installing it on your computer, or in a virtual machine, just for this task.\nA great alternative is to run Linux in a Docker container. Unfortunately, the Alpine distribution that we’ve been playing with doesn’t provide an implementation of wget with all the options that we need.\nWe turn to another Linux distribution, Ubuntu, for which DockerHub has several images.\nExercise\nExercise 1.12 Run a container with Ubuntu 19.10 and open a guest terminal. Call the container dl-figures, and avoid the option --rm, we’ll use this container later.    Solution\ndocker run --name dl-figures -it ubuntu:19.10 If you look at the DockerHub registry Web page describing Ubuntu, you’ll see that the version 19.10 has many tags, including 19.10, eoan-20200313, eoan, rolling. You can use any of these tags to download the image. Another way to write the previous command is:\ndocker run --name dl-figures -it ubuntu:eoan   From now on, we’ll be interacting with the guest Ubuntu terminal. If you type the command wget, you’ll get an error (bash: wget: command not found).\nNotice\nThe image Ubuntu doesn’t include all the commands that you’d find in a full-blown Ubuntu distribution; the reason is to keep the size of the image small, a necessary constraint given that images are transferred over the Internet.\n Luckily, there’s a way to install wget in our Ubuntu distribution. Ubuntu provides a powerful command-line package manager called Advanced Package Tool (APT). First, you need to run the following command:\napt-get update which fetches the available packages from a list of sources available in file /etc/apt/sources.list.\nThen, you can install wget by running the following command:\napt-get install -y wget In order to obtain all the figures from a Web page, type the following command:\nwget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation You should see in the current directory a new folder named my-figures containing the downloaded figures; verify it by typing ls my-figures.\nBefore terminating, don’t forget to read your fortune cookie. In the shell, run the following command:\napt-get install -y fortune and then:\n/usr/games/fortune -s When you’re done, you can simply type the command exit to quit the guest terminal and stop the container.\nNotice\nYou might wonder how you can transfer the downloaded figures from the container to the host computer. We’ll see that later when we introduce the notion of volumes.\n   2 Creating Images A Docker image can be thought of as a template to create and run a container. An image is a file that contains a layered filesystem with each layer being immutable; this means that the files that belong to a layer cannot be modified or deleted, nor can files be added to a layer.\nWhen a container is created from an image, it will be composed of all the image read-only layers and, on top of them, a writable layer (termed the container layer), where all the new files created in the container will be written. For example, the figures that we downloaded in the container dl-figures were stored in the writable layer of that container.\n2.1 Interactive image creation When we run the container dl-figures in Section 1.5, we modified the container to install the command wget. You can see the modifications by typing the following command:\ndocker diff dl-figures The output consists of a list of files tagged with the letter A, C or D, indicating respectively that the file has been added (A), changed (C) or deleted (D). In this list you’ll find the downloaded figures, as well as other files that have been added or modified or deleted when we installed wget.\nExercise\nExercise 2.1 If layers, except the top one, are immutable, how can files that belong to the lower layers be modified or deleted?    Solution\nAll files marked with A are new and therefore are added to the writable layer of the container.\nAs for the existing files, they live in the immutable layers of the image, and therefore cannot be touched directly. Instead, they are copied from the bottom layers to the writable layer where they are modified. This strategy is called copy-on-write.\nThe structure of layers generates a layered filesystem in the image; if different copies of the same file exist in different layers, the copy in the uppermost layer overwrites the others.\n  We can create a new image from the container dl-figures, one that provides a Ubuntu distribution with the command wget already installed, with the following command:\ndocker commit dl-figures ubuntu-with-wget The command creates a new image called ubuntu-with-wget.\nExercise\nExercise 2.2 Run a container from the image ubuntu-with-wget and verify that the command wget is actually installed.    Solution\nJust type the following command:\ndocker run --rm -it ubuntu-with-wget In the guest terminal type wget: you should see the following output:\nwget: missing URL Usage: wget [OPTION]... [URL]... Try `wget --help` for more options.    2.2 Dockerfiles The interactive creation of an image is a manual, and therefore inefficient, process. The most common way to create an image is to use a Dockerfile, a text file that contains all the instructions necessary to build the image. The advantage of the Dockerfile is that it can be interpreted by the Docker engine, which makes the creation of images an automated and repeatable task.\nInspired by the previous example, suppose that we want to create a containerized application to download figures from a Web page. As a template for this application, we need to build a new image, that we’ll call fig-downloader.\nThe Dockerfile containing the instructions to build the image fig-downloader is as follows:\nFROM ubuntu:eoan RUN apt-get update RUN apt-get install -y wget RUN mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] Here’s the explanation:\nWe use the image ubuntu:eoan as the base image. This corresponds to the instruction FROM ubuntu:eoan.\n We install the utility wget in the base image. This corresponds to the instructions RUN apt-get update and RUN apt-get install -y wget.\n We create a directory my-figures under the root directory of the image. This corresponds to the instruction RUN mkdir -p /my-figures.\n We set the newly created directory /my-figures as the working directory of the image. This corresponds to the instruction WORKDIR /my-figures.\n We specify the command to be executed when a container is run from this image. This corresponds to the instruction ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;]. This instruction means: execute wget with the options -nd, -r, -A; the last option takes a list of file extensions (jpg,jpeg,bmp,png,gif) as its argument.\n Remember that the utility wget takes the URL of the Web page as an argument. The URL will be specified when we run the container from the image fig-downloader. Optionally, we can specify a default argument by using the keyword CMD. The meaning of the instruction CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] is: if we don’t give any URL when we run the container, the figures will be downloaded from https://www.centralesupelec.fr/fr/presentation.\n  Exercise\nExercise 2.3 What’s the relation between the Dockerfile lines and the image layers?    Solution\nEach line corresponds to a new layer. The first line corresponds to the bottom layer; the last line to the top layer.\n  Exercise\nExercise 2.4 Could you identify a problem in this Dockerfile? Modify the Dockerfile accordingly.    Solution\nWhen creating an image, we should keep the number of layers relatively small; in fact, the more the layers, the bigger the image will be. Here we create three separate layers with three RUN commands; we can simply merge the three layers. The resulting Dockerfile will be:\nFROM ubuntu:eoan RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y wget \u0026amp;\u0026amp; \\ mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;]    2.3 Building an image We’re now going to build an image from a Dockerfile.\nCreate a directory named fig-downloader in your computer with a file named Dockerfile inside.\n In the Dockerfile write the set of instructions that you proposed in Exercise 2.4.\n In the terminal, set the working directory to fig-downloader.\n Build an image called fig-downloader by executing the following command:\n  docker build -t fig-downloader . The . at the end of the command means that the Docker engine will look for a file named Dockerfile in the working directory.\nExercise\nExercise 2.5 Once the image is built, type the command docker image ls -a. What are the images with repository and tag \u0026lt;none\u0026gt;? Why are there three of such images?     Solution\nThese are the intermediate images. Once a layer is compiled, an intermediate image is created that contains that layer and all the layers underneath. In other words, the intermediate image corresponding to the layer \\(i\\) contains all files up to the layer \\(i\\), including layers 1 through \\(i-1\\).\nThe intermediate layers are used by the build cache, of which we’ll see an example later.\nAlthough there are five layers in the new image, there are only three intermediate images because:\n the base image is ubuntu:eoan. the image corresponding to the top layer is the final image fig-downloader.    Good to know\nIf you give the Dockerfile a different name (say, Dockerfile-fig-downloader), the command to build the image will be:\ndocker build -t fig-downloader -f Dockerfile-fig-downloader . The option -f is used to specify the name of the Dockerfile.\n Let’s dive deeper into the anatomy of an image.\nExercise\nExercise 2.6 Run the following command:\ndocker history fig-downloader\nand analyze the layers of the new image.\n Why do some layers have an ID, while others are marked as missing?   Can you find the identifiers of the intermediate images?      Solution\nThe layers with an ID correspond to the layers of the new image, including the top layer and the base image. The layers marked as missing are those that compose the base image. Those layers are not stored in\nyour computer, simply because they belong to an image that hasn’t been built on your computer and you downloaded from the DockerHub registry.\nBy looking at the output of docker image ls -a and the output of this command, we see that the layers between the base image and the top layer have the same identifiers as the intermediate images.\n  Exercise\nExercise 2.7 Run the following command:\ndocker run --name dl-1 fig-downloader\nWhat does it do? Where are the downloaded pictures?    Solution\nWe downloaded the figures of the page https://www.centralesupelec.fr/fr/presentation. The downloaded pictures are in the folder /my-figures of the container dl-1. For now, don’t worry about accessing them.\n  Exercise\nExercise 2.8 Run the following command:\ndocker run --name dl-2 fig-downloader https://www.centralesupelec.fr/fr/nos-campus What does it do? Where are the downloaded pictures?    Solution\nWe downloaded the figures of the page https://www.centralesupelec.fr/fr/nos-campus. We basically overwrote the URL specified by the CMD keyword with a new one. The downloaded pictures are in the folder /my-figures of the container dl-2. For now, don’t worry about accessing these figures.\n   2.4 Containerized Python application Download this archive file and unzip it into your working directory. In this archive you’ll find:\n A Dockerfile. A Python script main.py that asks the user to enter the URL and the language of a Web page, and prints the 10 most frequent words occurring in that page. A file requirements.txt with the list of the Python packages needed to run the given script.  The content of the Dockerfile is as follows:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./main.py ./requirements.txt /app/ RUN pip install -r requirements.txt ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.9 Describe what this Dockerfile does.    Solution\n Takes python:3.7-slim as the base image. Creates a new folder app in the image under the root directory. Changes the working directory to /app. Copies the files main.py and requirements.txt from the local computer to the directory /app in the image. Runs the command pip install to install the Python libraries specified in the file requirements.txt. Executes the command python main.py.    Exercise\nExercise 2.10 Build an image called wordfreq from this Dockerfile.    Solution\ndocker build -t wordfreq .   Exercise\nExercise 2.11 Without changing the Dockerfile, rebuild the same image. What do you notice?    Solution\nThe build is very fast. Since we didn’t change the Dockerfile, the image is rebuilt by using the image layers created previously. This is clearly indicated by the phrase using cache written at each layer. Using the already stored layers is called build cache.\n  Exercise\nExercise 2.12 What happens if you modify a line in the Python script and you rebuild the image? Is the build cache still used?    Solution\nAdd any instruction at the end of main.py, such as:\nprint(\u0026quot;I added this line\u0026quot;) then rebuild the image. The three bottom layers are not affected by the modification, therefore they benefit from the build cache. Layer 4 is the first affected by the modification. This layer, and those above, need therefore to be rebuilt.\n  Exercise\nExercise 2.13 Considering how the build cache is used in Docker, can you tell what’s wrong with this Dockerfile? Modify the Dockerfile accordingly and rebuild the image.    Solution\nEach time we modify main.py and we rebuild the image, the layer 4 and 5 are recreated, meaning that all the Python packages are downloaded and installed. Depending on the size and number of the packages, this might take some while. A better way to structure the Dockerfile is to install the packages before copying the Python script to the image. Here is how we should modify the Dockerfile:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./requirements.txt /app/ RUN pip install -r requirements.txt COPY ./main.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;]   Exercise\nExercise 2.14 Modify main.py by adding a new line of code and rebuild the image. What changed?    Solution\nThe Python packages are not reinstalled, as a result rebuilding the image\ntakes much less time than before.\n    3 Data Volumes In Exercise 2.7 you’ve been asked to run a container named dl-1 to download some figures from a Web page. The figures were downloaded into the directory /my-figures of the container. But we left a question unanswered.\nHow do we transfer those figures from the container to the host computer?\nOne way to go about that is to run the following command in the host terminal:\ndocker cp dl-1:/my-figures . This will copy the directory /my-figures from the container dl-1 to the host computer working directory. You can verify it by yourself.\nExercise\nExercise 3.1 Can you tell why this solution is less than ideal?    Solution\nAfter running the container we need to do an additional action to copy the figures from the container to the host computer.\n The container is created and run only to download some figures. We’d like to remove it automatically (with the option --rm) when its execution is over. However, if we do so, the pictures will be lost before we can copy them to the host computer.\n    3.1 Using a host volume A better solution is to mount (i.e., attach) a directory of the host computer at the container’s directory /my-figures when we run it. Let’s see how it works.\nStep 1. Create a directory named figs-volume in your working directory.\nStep 2. Type and execute the following command:\ndocker run --rm -v $(pwd)/figs-volume:/my-figures fig-downloader This command runs a container from the image fig-downloader.\n With the option -v we specify that we want to mount the directory $(pwd)/figs-volume ($(pwd) indicates the host working directory) at the directory figs-volume in the container;\n The option --rm indicates that we want the container to be removed when its execution is over.\n  Step 3. Verify that the pictures are in the folder figs-volume.\nIn this example, we’ve used the directory figs-volume as a volume (essentially, an external storage area) of the container; when the container is destroyed, the volume remains with all its data.\n 3.2 Docker volumes In the example that we’ve just described, we’ve used a host directory as a volume. This is useful when we, or an application running on the host, need to access the files produced by a container. In all the other cases, a container should use a Docker volume, which is managed directly by the Docker engine.\nLet’s create a new Docker volume called data-volume:\ndocker volume create data-volume Good to know (advanced notion)\nWhere the data will be actually stored?\nYou can inspect the new volume by typing the following command:\ndocker volume inspect data-volume A mount point is indicated; that’s the folder where the data will be actually stored. If your computer runs Linux, that folder will be available on the host; if your computer runs Windows or MacOS, you’ll not find that folder on your computer. Instead, it will be available in the virtual machine that Docker use on MacOS and Windows.\nDo you want to see the directory? (Instructions for MacOS)\nOne way to look into the hidden VM is to run the following containerized application:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1 This application will open a guest terminal into the VM. You can then use the commands cd and ls to browse to the directory indicated as the mount path of the new volume.\n 3.2.1 Sharing data A Docker volume can be used to share data between containers.\nExercise\nExercise 3.2 Run a container from the image ubuntu:eoan, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume at the container’s directory /data.      Solution\ndocker run --rm -it -v data-volume:/data ubuntu:eoan    Exercise\nExercise 3.3 Type a command in the guest Linux terminal to create a file test-file.txt in the directory /data. Verify that the file is created.    Solution\nThe following command:\necho \u0026quot;This is a new file\u0026quot; \u0026gt; /data/test-file.txt creates a file test-file.txt with the line of text “This is a new file”.\nIn order to verify that the file is created, we can type the following command:\nls /data/test-file.txt To see the content of the file, we can type:\ncat /data/test-file.txt   Exercise\nExercise 3.4 Run a container from the image alpine:latest, specifying the options to:\n Remove the container once its execution is over.\n Interact with the guest Linux terminal in the container.\n   Mount the volume data-volume to the directory /my-data of the container.      Solution\ndocker container run --rm -it -v data-volume:/my-data alpine   Exercise\nExercise 3.5 Verify that you can read the file test-file.txt. Which folder would you look in?    Solution\nWe need to look in the folder /my-data because this is where we mounted data-volume.\ncat /my-data/test-file.txt   In the guest terminals of both containers type exit. This will terminate and destroy (since we used the option --rm) the containers.\nExercise\nExercise 3.6 Will the file test-file.txt be removed? Why?    Solution\nNo. The file that we created before has been saved in the volume data-volume. Volumes are a way to persist data beyond the life span of a container.\n     4 Single-Host Networking In order to let containers communicate and, therefore, co-operate, Docker defines a simple networking model known as the container network model\nExercise\nExercise 4.1 Describe the output of the following command:\ndocker network ls\n   Solution\nThe command lists all the networks created by Docker on your computer. For each network, the values of four attributes are shown:\n The identifier. The name. The driver used by the network. The scope of the network (local or global). A local scope means that the network connects containers running on the same host, as opposed to a global scope that means that containers on different hosts can communicate.  Depending on the containers that you used in the past, you might see different networks. However, three networks are worth noting:\n The network named bridge, that uses the driver bridge and a local scope. By default, any new container is attached to this network. The network named host, that uses the driver host and a local scope. It’s used when we want a container to directly use the network interface of the host. It’s important to remember that this network should only be used when analyzing the host’s network traffic. In the other cases, using this network exposes the container to all sorts of security risks. The network named none, that uses the driver null and a local scope. Attaching a container to this network means that the container isn’t connected to any network, and therefore it’s completely isolated.    Exercise\nExercise 4.2 The following command:\ndocker network inspect bridge\noutputs the configuration of the network bridge. By looking at this configuration, can you tell what IP addresses will be given to the containers attached to this network? What’s the IP address of the router of this network?    Solution\nThe information is specified in the field named IPAM, more specifically:\n Subnet indicates the range of IP addresses used by the network. The value of this field should be 172.17.0.0/16; the addresses range from 172.17.0.1 to 172.17.255.255.\n Gateway indicates the IP address of the router of the network. The value should be 172.17.0.1\n    4.1 Creating networks By default, any new container is attached to the network named bridge. As a result, all new containers will be able to communicate over this network. This is not a good idea. If a hacker can compromise any of these containers, s/he might be able to attack the other containers as well. As a rule of thumb, we should attach two containers to the same network only on a need-to-communicate basis.\nExercise 4.3 What if a container doesn’t need to use the network at all? Try to run a container from the image alpine disconnected from all networks and verify that you cannot ping the URL www.google.com.\nLook at the Docker cheat sheet to learn how to attach a container to a network.    Solution\nWe should attach the container to the network none. As an example, we run the following command:\ndocker run --rm -it --network none alpine /bin/sh Then we try to ping www.google.com as follows:\nping www.google.com We should obtain the following message:\nbad address \u0026#39;www.google.com\u0026#39; Type the command exit to quit the container.\nInstead, if we run Linux Alpine without specifying the network (meaning that the container will be attached to the network bridge):\ndocker run --rm -it alpine /bin/sh and we try to ping www.google.com, we should get an answer. In some cases, the command ping would just hang and show no output; this is usually fixed by restarting Docker.\n  In order to create a new network, you can use the following command:\ndocker network create network_name Exercise\nExercise 4.4 Create two networks named buckingham and rochefort that use the driver bridge. By using the docker network inspect command, look at the IP addresses of the new networks and write them down.    Solution\nJust run the following commands:\ndocker network create buckingham docker network create rochefort The IP addresses for the network buckingham are 172.18.0.0/16 (addresses from 172.18.0.1 to 172.18.255.255); The IP addresses for the network rochefort are: 172.19.0.0/16 (assuming that you create buckingham before rochefort).\nThe IP addresses may be different on your machines.\n  Exercise\nExercise 4.5 Create three containers athos, porthos and aramis and attach them to the two networks buckingham and rochefort as displayed in this figure. The three containers will open a Linux Alpine shell. You’ll need to launch the commands in three separate tabs of your terminal window.\n What will the IP addresses of the three containers be in the two networks? Remember that porthos is attached to two networks, therefore it’ll have two network interfaces (endpoints) and, as a result, two IP addresses.   Verify your answers by inspecting the two networks (use the command docker network inspect).      Solution\nHere are the commands to run athos and aramis while connecting them to buckingham and rochefort respectively.\ndocker run --rm -it --name athos --network buckingham alpine /bin/sh docker run --rm -it --name aramis --network rochefort alpine /bin/sh Here’s the command to run porthos and attach it to buckingham:\ndocker run --rm -it --name porthos --network buckingham alpine /bin/sh The following command attaches porthos to the second network rochefort:\ndocker network connect rochefort porthos As for the IP addresses, each network has IP addresses in the range 172.x.0.0/16, where x is 18 in the network buckingham and 19 in the network rochefort. The address 172.x.0.1 is reserved for the router. Therefore, the containers will be assigned IP addresses from 172.x.0.2. In this solution, we created athos, aramis and portos in this order. Therefore, the IP addresses will be:\n In network buckingham:  athos: 172.18.0.2 porthos: 172.18.0.3  In network rochefort:  aramis: 172.19.0.2 porthos: 172.19.0.3   You can actually verify this configuration by inspecting the two networks with the following commands:\ndocker network inspect buckingham docker network inspect rochefort The IP addresses might be different on your machines.\n   4.2 Communication between containers Let’s see if and when the three containers can communicate.\nExercise\nExercise 4.6 Which containers are able to communicate? Justify your answer.    Solution\nThe only containers that cannot communicate are athos and aramis, because they’re not connected to the same network.\n  Exercise\nExercise 4.7 Try to ping porthos from athos by using its IP address.\n Which IP address of porthos would you use?      Solution\nWe need to use the IP address assigned to the endpoint linking porthos to the network buckingham, to which athos is connected. In our case, this is 172.18.0.3.\n  Exercise\nExercise 4.8 Try to ping porthos from athos by using its name. Do you succeed? Are you surprised?    Solution\nWe succeed. Indeed, the network buckingham provides a DNS server, that can translate names into IP addresses.\n  You can now exit the three containers.\n 4.3 A containerized chat room We developed a simple chat room in Python that you can download here.\nParticipants use a client program to connect to the chat room; the chat room is managed by a server application that receives the client connections and forwards the messages between the users. The archive contains the following files:\n client.py. Implementation of the chat room client. server.py. Implementation of the chat room server. utils.py. Library with utility functions used in both client.py and server.py.  Exercise\nExercise 4.9 By using Dockerfiles, create two images chat-client and chat-server that will be used to run the client and the server in Docker.    Solution\nThe Dockerfile for the client (let’s call it Dockerfile-client) is as follows.\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./client.py ./utils.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;client.py\u0026quot;] We build the image with the following command:\ndocker build -t chat-client -f Dockerfile-client . The Dockerfile for the server (let’s call it Dockerfile-server) is as follows.\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./server.py ./utils.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;server.py\u0026quot;] We build the image with the following command:\ndocker build -t chat-server -f Dockerfile-server . Good to know\nThe first three layers in both images are identical. Therefore, when building the second image chat-server the Docker engine reuses the cached layers created for the first image. This is indicated in the output of the docker build command with the phrase using cache.\n   We’ll now run both containers. Since they need to communicate, they need to be attached to the same network (e.g., buckingham).\nExercise\nExercise 4.10 Run a container from the image server-chat. Set the options to:\n Automatically remove the container once its execution is over.\n Give the container a name (e.g., server-chat).\n The server will print messages on the screen. In order to see them, you must use the option -t.\n  Also, keep in mind that server.py takes an argument that is the port number where the server will listen to incoming connections. Choose a random port number in the interval [49152-65535].\nWhat is the IP address of the server?    Solution\nWe execute the following command:\ndocker container run --rm -t --name chat-server --network buckingham chat-server 64903 In my case, the IP address is 172.18.0.2 and port number is 64903\n  Exercise\nExercise 4.11  Run a container from the image client-chat. Set the options to:\n Automatically remove the container once its execution is over.\n Give the container a name (e.g., client-chat).\n Since you’ll use the client to write messages in the chat room, remember to set the option -it.\n  The client takes two arguments: the host where the server is running and the port which the server is listening to.    Solution\ndocker run --rm -it --name chat-client --network buckingham chat-client 172.18.0.2 64903 Instead of the server host IP address, we can use the server container name (the network buckingham has a DNS server).\nAs a result, we can run the client as follows:\ndocker run --rm -it --name chat-client --network buckingham chat-client chat-server 64903   Once the client is started, you’ll be prompted to enter your name. Then you can start writing messages.\nNotice\n You can type #quit at any moment to exit the chat room (client-side).\n Type Ctrl-C to stop the server.\n   Now, suppose that one of your classmates wants to join the chat room, but s/he’s on another computer.\nExercise\nExercise 4.12 Do you think your classmate can connect to the containerized server running in your machine? Justify your answer.    Solution\nNo, s/he can’t. The two containers can communicate only if they’re connected to the same network.\n  What we need to do here is to expose our server to the outside world. The server runs in a container \\(c\\) that, in turns, runs on the host machine \\(h\\). The server listens to port \\(p_c\\) that is opened inside the container. We need to map port \\(p_c\\) to a port \\(p_h\\) in the host computer. This way, the classmate client will connect to the server by specifying the IP address of the host \\(h\\) (not \\(c\\)) and \\(p_h\\) as the port number.\nExercise\nExercise 4.13 Stop both the server and the client.\nRun the server by specifying the option to map port \\(p_c\\) (e.g., 64903) to port \\(p_h\\) (e.g., 8080). As before, attach the server to the network buckingham.    Solution\nWe need to use the option -p. The command to run the container is:\ndocker run --rm -it --name chat-server --network buckingham -p 8080:64903 chat-server 64903   Notice\nAssuming that the name of the server container is chat-server, run the following command:\ndocker container port chat-server The output should look like as follows:\n64903/tcp -\u0026gt; 0.0.0.0:8080 This means that the port 64903 (\\(p_c\\)) in the container is mapped to the port 8080 (\\(p_h\\)) of the host computer. When a remote client wants to connect to the server, it’ll use port 8080. The IP address 0.0.0.0 means that a client can connect to the server by using any of its IP addresses.\n If you’re on the same local network as one of your classmate, do Exercise 4.14\n If none of your classmates is on your local network, do Exercise 4.15\n  Exercise\nExercise 4.14 Ask your classmate to connect to your server. For this, you’ll need to tell your classmate the IP address of your machine and the port number \\(p_h\\).    Solution\nAssuming that the IP address of your machine is 192.168.1.8, the command will be the following:\ndocker run --rm -it --name chat-client chat-client 192.168.1.8 8080   Exercise\nExercise 4.15 Since none of your classmates is on your local network (COVID-19 be damned!), you’ll need to simulate a distant connection to the server running on your machine.\nSimply run an instance of the client without attaching it to the network buckingham. The IP address of the server will be the IP address of your machine; the port will be \\(p_h\\).\n   Solution\nAssuming that the IP address of your machine is 192.168.1.8, the command will be the following:\ndocker run --rm -it --name chat-client chat-client 192.168.1.8 8080     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a74fda64fd4d3f15083e937e46612453","permalink":"/courses/cloud-computing/tutorials/tutorial-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-docker/","section":"courses","summary":"Text of the lab assignment on Docker","tags":null,"title":"Getting started with Docker","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1  Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\n   Solution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).    Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2  What is the maximum number of measurements in a year?\n   Solution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\n  Exercise\nExercise 1.3  Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\n   Solution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\n  Exercise\nExercise 1.4  Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n   Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n \\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\).     2 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 2.1  Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\n   Solution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\n  Exercise\nExercise 2.2  Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year.    Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n \\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\)     3 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 3.1  Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n   Solution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n \\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\).  We note that the reduce function is the identity.\n   4 Creating an inverted index We have a collection of \\(n\\) documents in a directory and we want to create an inverted index, one that associates each word to the list of the files the word occurs in. More precisely, for each word, the inverted index will have a list of the names of the documents that contain the word.\nExercise\nExercise 4.1  Propose a MapReduce implementation to create an inverted index over a collection of documents.\n   Solution\nThe input to the map will be a key-value pair, where the key is the name of a file \\(f\\) and the value is the content \\(C\\) of the file.\nmap: \\((f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]\\)\nreduce: \\((w, L) \\rightarrow (w, L)\\)\nwhere \\(L\\) is the list of the files containing the word \\(w\\).\nWe note that the reduce function is the identity.\nNote also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22d83831482b2a6a4aa5d6546c3dd9cd","permalink":"/courses/plp/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Setting up the work environment.  Start the MongoDB server. Refer to the documentation to find out how to start the server depending on your operating system.\n Launch MongoDB compass. You should have installed it with the MongoDB server.\n From within MongoDB compass, connect to the local MongoDB server.\n   2 Creating the database  Download the data by clicking here.\n In MongoDB Compass, click on the button “Create database”.\n Give the database a name (e.g., cinema) and create a collection named movies.\n Click on the button Add data and select Import file.\n Select JSON as a file format and select the file movies.json.\n Click on “Import”; 88 documents should be imported into the database.\n   3 Querying the data with find At the bottom of the MongoDB window, you should find a link to open the MongoDB shell (MongoSH Beta).\nAfter opening the shell, write the following command to switch to database cinema:\nuse cinema To verify that you’re actually connected to the database cinema, type the following query:\ndb.movies.findOne() Exercise\nExercise 3.1  By using the MongoDB shell, execute the following queries.\nQ1. The release year of the movie “Le parrain III”.\nQ2. The title of the movies released between 1980 and 1990.\nQ3. Same query as b. with the titles must be sorted by alphabetical order.\nQ4. The titles of the french movies.\nQ5. The title of the “crime” or “drama” movies.\nQ6. The names and birth dates of the directors of french movies.\nQ7. The title of the movies in which Sofia Coppola played.\nQ8. The title and the genre of the movies of which Hitchcock is director.\n   4 Querying the data with the aggregation framework Exercise\nExercise 4.1  By using the MongoDB shell, execute the following queries by using aggregation pipelines.\nQ1. The number of movies by country. Show by decresing number.\nQ2. The name of the actor in the role “Mary Corleone” in the movie “Le parrain III”.\nQ3. The number of actors by movie. Sort by decreasing number.\nQ4. The average number of actors in a film.\n   5 Join in MongoDB In the database cinema, create a new collection called movies_boffice. Import the documents in file moviesBoxOffice.json into this collection.\nExercise\nExercise 5.1  By using the operator $lookup on the collections movies and movies_boffice, find the box office of the movie “Le parrain III”.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd6c6f9b7307916c0396c4d4a26dccdb","permalink":"/courses/databases/tutorials/mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/mongodb/","section":"courses","summary":"Description of the MongoDB tutorial.","tags":null,"title":"MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" 1 Setting up the work environment.  Download Neo4j Desktop and install it on your computer.\n Create a new project by clicking on the button “New” that you’ll find on the top left side of the window.\n Click on “Add Database”, then “Create a Local Database”.\n Give the database a name (e.g., MovieLens) and set a password that you can easily remember; then click on “Create”. Choose version 4.1.1 for the database.\n Click on “Start” and wait for the database to become active.\n Click on the button “Open”. The Neo4j Browser will pop up.\n  In the next section, you’ll have to type a sequence of commands to import the data. You’ll write the commands in the text field on top of the Neo4j Browser (where you find the prompt neo4j$).\n1.1 Import the data. The dataset consists of data obtained from MovieLens, a recommender system whose users give movies a rate between 1 and 5, based on whether they dislike or love them. MovieLens uses the rates to recommend movies that its users might like. The dataset is modeled as a directed graph and consists of 100,004 rates on 9,125 movies across 671 users between January 9th, 1995 and October 16, 2016. The dataset also contains the names of the directors and the actors of each movie.\nImport the nodes corresponding to the movies (label Movie) by using the following command (it took 31 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv' as row MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)}) RETURN count(m)  Create an index on the property movie_id of the nodes with label Movie with the following command:   create index movie_idx for (m:Movie) on (m.movie_id)  Import the nodes corresponding to the actors (label Actor) by using the following command (it took 62 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv' as row MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name}) RETURN count(a)  Create an index on the property actor_id of the nodes with label Actor with the following command:   create index actor_idx for (a:Actor) on (a.actor_id)  Import the nodes corresponding to the directors (label Director) by using the following command (it took 4 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv' as row MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name}) RETURN count(d)  Create an index on the property director_id of the nodes with label Director with the following command:   create index director_idx for (d:Director) on (d.director_id)  Import the nodes corresponding to the genres (label Genre) by using the following command (it took 197 ms on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv' as row MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name}) RETURN count(g)  Create an index on the property genre_id of the nodes with label Genre with the following command:   create index genre_idx for (g:Genre) on (g.genre_id)  Import the nodes corresponding to the users (label User) by using the following command (it took 347 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv' as row MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname}) RETURN count(u)  Create an index on the property user_id of the nodes with label User with the following command:   create index user_idx for (u:User) on (u.user_id)  Import the links of type ACTED_IN between actors and movies with the following command (it took 2.5 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (a:Actor {actor_id: toInteger(row.actor_id)}) MERGE (a)-[r:ACTED_IN]-(m) RETURN count(r)  Import the links of type DIRECTED between directors and movies with the following command (it took 688 ms on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (d:Director {director_id: toInteger(row.director_id)}) MERGE (d)-[r:DIRECTED]-(m) RETURN count(r)  Import the links of type HAS_GENRE between movies and genres with the following command (it took 1 second on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (g:Genre {genre_id: toInteger(row.genre_id)}) MERGE (m)-[r:HAS_GENRE]-(g) RETURN count(r)  Import the links of type RATED between users and movies with the following command (it took 5.9 seconds on my computer):   :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (u:User {user_id: toInteger(row.user_id)}) MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-(m) RETURN count(r)    2 Exploratory queries If you looked at the commands used to import the data, you might already have an idea as to the structure of the graph. You can get a glimpse on the node labels, the relationship types and the property keys by clicking on the button circled in the following figure:\nExercise Exercise 2.1  Write and execute the following query:\n MATCH (m:Movie {title_en:\"Toy Story\"}) RETURN m;  What do you obtain? What are the properties associated to a node with label Movie? Click once on the node to display its properties.\n  Exercise Exercise 2.2  Double-click on the node displayed as the result of the previous query. Analyze the neighbouring nodes (their labels and properties) and the incident links (direction, type and properties). You can move around the node by dragging it in the window.\n   3 Queries Exercise Exercise 3.1  Write and execute the following queries:\nQ1. The genres of the movies in the database.\nQ2. The number of movies in the database.\nQ3. The title of the movies released in 2015.\nQ4. The number of directors by movie. Sort in decreasing order.\nQ5. The names of the directors and the title of the movies that they directed and in which they also played.\nQ6. The genres of the movies in which Tom Hanks played.\nQ7. The title and the rate of all the movies that the user with identifier 3 rated. Sort by rate in decreasing order.   3.1 Query chaining Cypher allows the specification of complex queries composed of several queries that are concatenated with the clause WITH. We are now going to see an example to obtain the titles of the movies that have been rated by at least 100 users.\nAt a first glance, the following query looks like a good solution:\n MATCH (n:Movie)= 100 RETURN n.title_en LIMIT 5;  However, executing this query returns the following error:\n Invalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41)) \"MATCH (n:Movie)= 100\"  Similarly to SQL, we cannot use aggregating functions in the clause WHERE.\nA correct formulation of the query requires the use of the clause WITH to concatenate two queries: the first will count the number of rates for each movie:\n MATCH (n:Movie)The second will take in the output of the first and will filter all the movies where nb_rates \u0026lt; 100. In order to chain the two queries, we’ll replace the RETURN clause in the first query with a WITH clause, as follows:\n MATCH (n:Movie)= 100 RETURN n.title_en  Exercise Exercise 3.2  Write and execute a query to obtain the five movies that obtained the best average rate among the movies that have been rated by at least 100 users.\n    4 Movie recommendation We are now going to see how Neo4j can be effectively used in a real application by implementing queries that form the basis of a simple movie recommendation system. This system is based on the notion of collaborative filtering.\nThis consists in recommending a user \\(u\\) some films that s/he hasn’t rated yet and other users with similar preferences have loved. In our context, we say that a user loves a movie if s/he rated the movie at least 3.\nThis concept is explained in the following figure.\nThe user \\(u\\) loves 6 movies, 3 of which are also loved by the user \\(v\\) (the black nodes); it is reasonable to think that \\(u\\) may also love the two movies that \\(v\\) loved and \\(u\\) hasn’t rated yet.\nThe principle of collaborative filtering is based on the computation of a similarity score between two users. Several similarity scores are possible in this context; here, we are going to use the Jaccard coefficient. Let \\(L(u)\\) and \\(L(v)\\) be the sets of movies that \\(u\\) and \\(v\\) love respectively; the similarity score \\(J(u,v)\\) between \\(u\\) and \\(v\\) is given by:\n\\[ J(u, v) = \\frac{|L(u) \\cap L(v)|}{|L(u) \\cup L(v)|} \\]\nIn order to recommend movies to a target user \\(v\\), the recommender system computes the similarity score between \\(v\\) and all the other users of the system and proposes to \\(v\\) the movies that s/he hasn’t rated yet and that the \\(k\\) most similar users loved.\nWe are now going to incrementally write a query to recommend some movies to the target user 3. The first step consists in determining the value \\(|L(v)|\\).\nExercise Exercise 4.1  Write and execute the query to obtain the number of movies that the user 3 loved. This query must return the target user and the number of movies that s/he loves.\n  Next, we are going to determine the value \\(|L(u)|\\), for all users \\(u\\) except \\(v\\).\nExercise Exercise 4.2  Write and execute the query to obtain the number of movies that each user \\(u\\) loves, except the target user 3. This query must return each user \\(u\\) and the number of movies that s/he loves.\n  We put the two queries together with the clause WITH.\nExercise Exercise 4.3  Compose the two previous queries with the clause WITH. This query must return the target user 3, the number of movies that s/he loves, the other users \\(u\\) and the number of movies that they love.\n  Now, we need to determine the value \\(L(u)\\cup L(v)\\), for each user \\(u\\), and compute the similarity score with the Jaccard coefficient.\nExercise Exercise 4.4  Append (by using WITH) to the query written in the previous exercise a query that obtains the number of movies that any user \\(u\\) loved and that the target user 3 loved too, and computes the similarity score between the target user 3 and \\(u\\). This query must return the five most similar users to the target user and the similarity scores.\n Hint Multiply the numerator of the equation by 1.0, otherwise Cypher will compute an integer division.    The last step consists in recommending some movies to the target user. From the previous query, take the identifier of the user \\(w\\) with the highest similarity to the target user. You are going to use this identifier directly in the new query.\nExercise Exercise 4.5  Write and execute the query to obtain the list of the movies that the user \\(w\\) loved and that the target user hasn’t rated yet. Sort this list by decreasing rate.\n Hint\n First, write a query to obtain the list of the movies that the target user rated. In the MATCH clause, use the variable \\(m\\) to indicate a movie that the target user rated. Conclude the query with:   RETURN collect(m.title_en) AS movies  The function collect creates a list called movies.\n Replace RETURN with WITH in the previous query and add a second query to select the titles of the movies \\(m\\) that the user \\(w\\) loved and the target user did not rate. In order to exclude the films that the target user did not rate, use the following predicate:   none(x in movies where x=m.title_en)  in the WHERE clause.\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0acfabbff71d33e6b9759e499c4634ed","permalink":"/courses/plp/tutorials/neo4j-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/neo4j-tutorial/","section":"courses","summary":"Description of the Neo4j tutorial.","tags":null,"title":"Neo4j","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to obtain a non-redundant set of functional dependencies. How to determine the candidate keys of a table given its functional dependencies. How to determine the normal form of a table.  Prerequisites:\n Having attended Lecture 2.  1 Non-redundant functional dependencies Consider the following set \\(\\mathcal{F}\\) of functional dependencies:\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B, C\\)\n \\(A \\rightarrow B\\)\n  Exercise\nExercise 1.1  Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).\n   Solution\nFirst, we rewrite the FDs such that each has a singleton right side.\n \\(A, B \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n  The first has two attributes in the determinant, therefore we need to check whether we can eliminate one of the two columns.\nWe have \\(A \\rightarrow B\\). By augmentation we obtain:\n\\[A \\rightarrow A, B\\]\nWe have \\(A, B \\rightarrow C\\). By transitivity we obtain:\n\\[A \\rightarrow C\\]\nTherefore, the column \\(B\\) is not useful and we can drop it.\nThe set \\(\\mathcal{G}\\) consists of the following FDs:\n \\(A \\rightarrow C\\)\n \\(D \\rightarrow B\\)\n \\(D \\rightarrow C\\)\n \\(A \\rightarrow B\\)\n     2 Candidate keys and normal forms (1) We consider the following table:\nPatient (ssn, first_name, last_name, phone_number, insurance_number, insurance_expiration_date) where the following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\[ \\begin{align} ssn \\rightarrow first\\_name, \u0026amp; last\\_name, phone\\_number, insurance\\_number, \\\\ \u0026amp; insurance\\_expiration\\_date \\end{align} \\]\n\\[ insurance\\_number \\rightarrow insurance\\_expiration\\_date \\]\nExercise\nExercise 2.1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).    Solution\nFirst, we need to rewrite the FDs such that each has a singleton right side.\n\\(ssn \\rightarrow first\\_name\\) \\(ssn \\rightarrow last\\_name\\) \\(ssn \\rightarrow phone\\_number\\) \\(ssn \\rightarrow insurance\\_number\\) \\(ssn \\rightarrow insurance\\_expiration\\_date\\) \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\)  The determinant of each FD is composed of only one column, therefore it is already irreducible. It is easy to see that all FDs with ssn as a determinant must be kept (otherwise we lose some information).\nBy transitivity from 4. and 6. we obtain:\n\\[ssn \\rightarrow insurance\\_expiration\\_date\\]\nTherefore, \\(\\mathcal{G}\\) is obtained from \\(\\mathcal{F}\\) by removing 5.\n  Exercise\nExercise 2.2 Given \\(\\mathcal{G}\\), identify the candidate keys in the table Patient.    Solution\nFrom the functional dependencies in \\(\\mathcal{F}\\), it’s easy to see that that the only column that implies all the others is ssn. Therefore, {ssn} is the only candidate key in this table.\n  Exercise\nExercise 2.3 Specify the normal form of the table Patient. Justify your answer.    Solution\n It is immediate to verify that the table is 1NF.\n The table is 2NF because there is only one candidate key, which is composed of only one column.\n The table is not in 3NF. Indeed, there is a functional dependency between two non-prime columns \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\).\n    Exercise\nExercise 2.4 How would you obtain two or more tables in BCNF from the table Patient?    Solution\nWe need to split the data relative to the patient from the data relative to the insurance. Therefore, we propose the following two tables:\n Patient (ssn, first_name, last_name, phone_number, insurance_number) Insurance (insurance_number, insurance_expiration_date)  Note that the column insurance_number in table Patient is foreign key to the column insurance_number in table Insurance.\n   3 Candidate keys and normal forms (2) Let \\(R\\) be a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\(A, B \\rightarrow C\\) \\(C \\rightarrow A\\) \\(C \\rightarrow B\\) \\(C \\rightarrow D\\) \\(D \\rightarrow E\\)  Exercise\nExercise 3.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nFirst, let’s try sets composed of only one column: \\(\\{A\\}\\), \\(\\{B\\}\\), \\(\\{C\\}\\), \\(\\{D\\}\\) and \\(\\{E\\}\\).\nWe have the following:\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D, E\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  Therefore, \\(\\{C\\}\\) is a candidate key because it implies all the other columns.\nFrom the functional dependency 1., we obtain that \\(\\{A, B\\}\\) imply \\(C\\); therefore, by transitivity they imply all the other columns.\nIn conclusion, we have two candidate keys: \\(\\{C\\}\\) and \\(\\{A, B\\}\\).\n  Exercise\nExercise 3.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is in 2NF. In fact, all non-prime columns depend entirely on both candidate keys.\n \\(R\\) is not in 3NF. In fact, the functional dependency \\(D \\rightarrow E\\) is between two non-prime columns.\n     4 Candidate keys and normal forms (3) Let \\(R\\) be the a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n \\(A \\rightarrow C\\) \\(A, B \\rightarrow D\\) \\(A, B \\rightarrow E\\)  Exercise\nExercise 4.1  Specify the candidate keys of the table \\(R\\).\n   Solution\nThe following hold:\n \\(\\{A\\}^+_{\\mathcal{F}} = \\{A, C\\}\\)\n \\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n \\(\\{C\\}^+_{\\mathcal{F}} = \\{C\\}\\)\n \\(\\{D\\}^+_{\\mathcal{F}} = \\{D\\}\\)\n \\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\n  It is clear that the only possible candidate key could be {A, B}; let’s verify it by computing the closure of {A, B} under \\(\\mathcal{F}\\):\n \\(\\{A, B\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)  Indeed, {A, B} is the candidate key.\n  Exercise\nExercise 4.2  We assume that \\(R\\) is in 1NF.\n Is table \\(R\\) in 2NF? Justify your answer.\n Is table \\(R\\) in 3NF? Justify your answer.     Solution\n \\(R\\) is not in 2NF. In fact, the non-prime column \\(C\\) is functionally dependent on only one part of the candidate key.\n \\(R\\) is not in 3NF, because it doesn’t fulfill the first condition, that is being in 2NF.\n     5 Candidate keys and normal forms (4) Consider the following table Student:\nStudent (stud_id, stud_ssn, course) The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n \\(stud\\_id \\rightarrow stud\\_ssn\\)\n \\(stud\\_ssn \\rightarrow stud\\_id\\)\n  Exercise\nExercise 5.1  Specify the candidate keys of the table Student.\n   Solution\nIf we consider each column independently we have:\n \\(\\{stud\\_id\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)\n \\(\\{stud\\_ssn\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)\n  No single column implies all the other columns. Therefore, the candidate key must be composed of at least 2 columns.\nIt is also easy to verify that:\n \\(\\{stud\\_id, stud\\_ssn\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn\\}\\)  We are left with two possibilities:\n \\(\\{stud\\_id, course\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn, course\\}\\)\n \\(\\{stud\\_ssn, course\\}^+_{\\mathcal{F}} = \\{stud\\_id, stud\\_ssn, course\\}\\)\n  In conclusion, there are two candidate keys: \\(\\{stud\\_id, course\\}\\) and \\(\\{stud\\_ssn, course\\}\\)\n  Exercise\nExercise 5.2  We assume that Student is in 1NF.\n Is table Student in 2NF? Justify your answer.\n Is table Student in 3NF? Justify your answer.\n Is table Student in BCNF? Justify your answer.     Solution\n Student is in 2NF. There are no non-prime columns.\n Student is in 3NF. There are no non-prime columns.\n Student is not in BCNF. In both functional dependencies the determinant is not a superkey.\n     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd4b11df4fa072aaae4db3251396b08d","permalink":"/courses/databases/tutorials/normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/normalization/","section":"courses","summary":"Description of the normalization tutorial.","tags":null,"title":"Normalization","type":"docs"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\nComputing averages We consider a collection of CSV files containing temperature measurements in the following format:\nyear,month,day,hours,minutes,seconds,temperature\nyou can find the files under the directory hdfs://sar01:9000/data/temperatures/\nHere are the details for each file:\n File temperatures_86400.csv contains one measurement per day in the years 1980 - 2018. File temperatures_2880.csv contains one measurement every 2880 seconds in the years 1980 - 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds for the years 1980 - 2018.  We intend to implement a Spark algorithm to generate pairs \\((y, t_{avg})\\), where \\(y\\) is the year and \\(t_{avg}\\) is the average temperature in the year.\nFirst implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py\nOpen the file avg_temperatures_first.py and write the following function:\ndef avg_temperature(theTextFile): temperatures = theTextFile \\ .map(lambda line: line.split(\u0026quot;,\u0026quot;)) \\ .map(lambda term: (term[0], [float(term[6])])) \\ .reduceByKey(lambda x, y: x+y) \\ .mapValues(lambda lv: sum(lv)/len(lv)) return temperatures  In the same file, locate the two variables input_path and output-path. and write the following code:\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/cpupsmia1/your_username/\u0026quot; Always remember to replace your_username with your username! Don’t forget the / at the end of the file paths!.\nExercise\nExercise 1 \n Run the script avg_temperatures_first.py by using temperatures_86400.csv as an input. To this extent, use the following command:\nspark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv  You should find the output of the program under the folder hdfs://sar01:9000/cpupsmia1/your_username/temperatures_86400.out\n What’s the execution time?  In the output of Spark on the command line you should see a line that mentions something along the following line:\nINFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s\n  Run the same script by using temperatures_2880.csv as an input.\n What is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer.\n Execute the same script by using temperatures_86.csv as an input.\n What is the execution time? How would you justify it, knowing that the files temperatures_2880.csv and temperatures_86.csv have a similar size (11 MB the former, 9 MB the latter)?     Second implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py\n Exercise\nExercise 2  Based on the observations made in the previous exercise, write an improved implementation of the function avg_temperature.\n  Exercise\nExercise 3 \n Run the script avg_temperatures_second.py by using temperatures_86.csv as an input.\n What’s the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference.\n Run the same script by using temperatures_10.csv (3 GB!) as an input. Do you think that the program takes too long? Why?\n      Average and standard deviation We use the same files as in the first question. Our objective is to write a Spark program that produces triples \\((y, t_{\\mu}, t_{\\sigma})\\), where \\(y\\), \\(t_{\\mu}\\) and \\(t_{\\sigma}\\) are the year, the average temperature in the year and the standard deviation respectively.\nWe can express the standard deviation of \\(n\\) values \\(x_1 \\ldots x_n\\) with the following formula:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nCopy the file ~vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_stddev_temp.py\nExercise\nExercise 4 \n Complete the definition of the function avg_temperature in file avg_stddev_temp.py.\n Run the script by using temperatures_86400.csv and temperatures_2880.csv as input files (small files).\n Run the script by using temperatures_86.csv and temperatures_10.csv as input files (large files).\n     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1fa903f48b4529667aa717abb4af5343","permalink":"/courses/plp/tutorials/spark-programming-dce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/plp/tutorials/spark-programming-dce/","section":"courses","summary":"Description of the tutorial on Spark programming on a cluster","tags":null,"title":"Introduction to Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\n How to write SQL queries.  Prerequisites:\n Having attended Lecture 3.  1 Database of a social network platform We consider the following collection of relational tables:\nUserAccount (nickname, first_name, last_name, city, country)\nPost (post_id, content, date, time, lat, long, nickname)\nEmailAddress (email_address, nickname)\nRelationship (nickname_src, nickname_dst, type, date)\nTag (post_id, nickname)\nExercise\nExercise 1.1  Write the following queries in SQL:\nQ1. Count the number of posts by nickname. Sort by number of posts in descending order.\nQ2. Get the identifier and the content of all the posts of the user with nickname nick_BLANCO.\nQ3. Get the number of user accounts by country. Order by number of countries in descending order.\nQ4. Get the identifier of the posts where the user with nickname nick_ANIS is tagged.\nQ5. Get the content of all the posts where the user with nickname nick_ANIS is tagged.\nQ6. Get the email addresses of the user with nickname nick_BLANCO.\nQ7. Get the nicknames of the friends of the user with nickname nick_BLANCO.\nQ8. Get the the first and last names of the friends of the user with nickname nick_BLANCO.\nQ9. For each user, get the number of followers by country. Rank the countries by the number of followers in descending order.\nQ10. Get the first and family name of the users tagged by the user with nickname nick_CAMERON.\nQ11. Get the nickname, first and last name of the common friends of the users with nickname nick_CAMERON and nick_ANIS.\nQ12. Rank the users by the number of their followers. The user with the most followers must appear at the top of the list.\n   Solution\nQ1.  SELECT nickname, count(*) as nbPosts FROM Post GROUP BY nickname ORDER BY nbPosts DESC;  Q2.  SELECT post_id,content FROM Post WHERE nickname = 'nick_BLANCO';  Q3.  SELECT country, COUNT(*) c FROM UserAccount GROUP BY country ORDER BY c DESC;  Q4.  SELECT post_id FROM Tag t WHERE t.nickname='nick_ANIS';  Q5.  SELECT content FROM Tag t JOIN Post p ON t.post_id=p.post_id WHERE t.nickname='nick_ANIS';  Q6.  SELECT email_address FROM EmailAddress WHERE u.nickname = 'nick_BLANCO';  Q7.  SELECT nickname_dst FROM Relationship WHERE nickname_src = 'nick_BLANCO' and type = 'friendship';  Q8.  SELECT u1.first_name, u1.last_name FROM Relationship r JOIN UserAccount u ON u.nickname = r.nickname_dst WHERE r.nickname_src = 'nick_BLANCO' and t.type = 'friendship';  Q9.  SELECT nickname_dst, country, count(*) as nbFollowers FROM UserAccount u JOIN Relationship r ON u.nickname = r.nickname_src WHERE type='follower' GROUP BY nickname_dst, country ORDER BY nickname_dst, country, nbFollowers DESC  Q10.  SELECT first_name, last_name FROM UserAccount WHERE nickname in ( SELECT distinct u.nickname FROM Post p JOIN Tag t ON p.post_id=t.post_id JOIN UserAccount u ON u.nickname=t.nickname WHERE p.nickname='nick_CAMERON' )  Q11.  SELECT nickname, first_name, last_name FROM UserAccount WHERE nickname IN ( SELECT nickname_dst as nickname FROM Relationship WHERE nickname_src='nick_CAMERON' AND type='friendship' AND nickname_dst IN ( SELECT nickname_dst as nickname FROM Relationship WHERE nickname_src='nick_ANIS' AND type='friendship') )  Q12.  SELECT u.nickname, u.first_name, u.last_name, nbFollowers FROM UserAccount u JOIN (SELECT nickname_dst as nickname, count(*) as nbFollowers FROM Relationship r WHERE r.type='follower' GROUP BY r.nickname_dst) temp ON u.nickname = temp.nickname ORDER BY nbFollowers DESC     2 Database of a banking system We consider the following collection of relational tables:\nBank (code_bank, name, street_number, street_name, postal_code)\nBranch (branch_id, street_number, street_name, postal_code, code_bank)\nAccount (acct_nbr, acct_type, balance, branch_id, ssn)\nLoan (loan_nbr, loan_type, amount, branch_id, ssn)\nCustomer (ssn, first_name, last_name, telephone, street_number, street_name, postal_code)\nAddress (postal_code, city, country)\nExercise\nExercise 2.1  Write the following queries in SQL:\nQ1. Get the name, street name, street number and postal code of all the banks in the database.\nQ2. Get the city and country of all banks in the database.\nQ3. Count the number of accounts by customer.\nQ4. Get the amount of money owned by the customer.\nQ5. Return the average amount of money loaned by any bank for a mortgage.\nQ6. Get the amount of money loaned by each bank.\nQ7. Get the social security number, the first and last name of all customers of the bank with name “Bank of America”.\nQ8. Get all the data (including the full address) on the customers that have a checking account with a negative balance.\nQ9. Get the SSN of all customers that have no loan at the “Bank of America”.\nQ10. Get the name of the bank that has the most branches.\nQ11. Get the SSN of all customers having an account in more than one bank.\n   Solution\nQ1.  SELECT name_bank, street_number, street_name, postal_code FROM Bank  Q2.  SELECT name_bank, city, country FROM Bank ba JOIN Address a ON ba.postal_code = a.postal_code  Q3.  SELECT ssn, count(*) FROM Account GROUP BY ssn  Q4.  SELECT SUM(balance) FROM Account a JOIN Customer c ON a.ssn = c.ssn WHERE c.first_name = \"John\" AND c.last_name = \"Smith\"  Q5.  SELECT AVG(amount) FROM Loan WHERE loan_type = \"mortgage\"  Q6.  SELECT ba.name_bank, sum(amount) FROM Loan l JOIN Branch br ON l.branch_id = br.branch_id JOIN Bank ba ON br.code_bank = ba.code_bank GROUP BY ba.code_bank  Q7.  SELECT distinct c.ssn, c.first_name, c.last_name FROM Account a JOIN Customer c ON a.ssn = c.ssn JOIN Branch br ON a.branch_id = br.branch_id JOIN Bank ba ON ba.code_bank = br.code_bank WHERE ba.name_bank = \"Bank of America\"  Q8.  SELECT c.*, addr.city, addr.country FROM Customer c JOIN Account acc ON c.ssn = acc.ssn JOIN Address addr ON c.postal_code = addr.postal_code WHERE acc.balance Q9.  SELECT ssn FROM Customer WHERE ssn NOT IN ( SELECT c.ssn FROM Customer c JOIN Loan l ON c.ssn = l.ssn JOIN Branch br ON l.branch_id=br.branch_id JOIN Bank ba ON br.code_bank=ba.code_bank WHERE ba.name_bank='Bank of America' )  Q10.  SELECT name_bank FROM Bank WHERE code_bank IN ( SELECT code_bank FROM Branch GROUP BY code_bank HAVING COUNT(*) = (SELECT MAX(nb_Branches) FROM (SELECT count(*) as nb_Branches FROM Branch GROUP BY code_bank) ) )  Q11.  SELECT ssn FROM (SELECT distinct a.ssn as ssn, br.code_bank as code_bank FROM Account a JOIN Branch br ON a.branch_id = br.branch_id) GROUP BY ssn HAVING COUNT(*)  1     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82410733cad4d5a26e04b28fe7e8b2ec","permalink":"/courses/databases/tutorials/sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sql/","section":"courses","summary":"Description of the SQL tutorial.","tags":null,"title":"SQL queries","type":"docs"},{"authors":null,"categories":null,"content":" 1 Obtain the data We consider the database of DVD rental store containing data on films, actors, customers and the transactions of the store.\nYou can obtain the data at this link.\nThe following figure shows the physical schema of the database.\n Figure 1.1: The physical schema of the database  You should now open the database with DB Browser for SQLite. To this extent, open DB Browser for SQLite, click on Open database and select the downloaded file.\nForeign key alert\nBy default, SQLite doesn’t check foreign key constraints. Open the preferences of DB Browser for SQLite and make sure the checkbox Foreign keys in the tab Database is checked.\n  2 Foreign key constraints Exercise\nExercise 2.1  Try to delete the film with film_id=1 from the table Film. What happens?\n   Solution\nThe query is not allowed, because there are rows in other tables that reference that movie.\n  Exercise\nExercise 2.2  Write a query to get film_id, actor_id and title from the table film joined with film_actor for the film “ACE GOLDFINGER”.\n   Solution\n SELECT f.film_id, fa.film_id, fa.actor_id, f.title FROM film f JOIN film_actor fa ON f.film_id = fa.film_id WHERE f.title=\"ACE GOLDFINGER\"    Exercise\nExercise 2.3  Modify the identifier of the film ACE GOLDFINGER in table Film. Write the query of the previous exercise. What happens?\n   Solution\nThe identifier of the film is changed in all referencing tables. This is because the foreign key constraint is defined ON UPDATE CASCADE.\n   3 Queries Exercise\nExercise 3.1  Write the following queries in SQL:\nQ1. Return the first and last names of all the actors.\nQ2. Return the title and the language of each film.\nQ3. Return the first and the last name of the manager of the store with code 2.\nQ4. Return the first and last names of all the actors who performed in the movie ‘ANGELS LIFE’.\nQ5. Return the number of films where each actor performed. Sort the results in descending order.\nQ6. Return the film categories that contain between 25 and 55 films.\nQ7. Return the first and family name of the customers who have rented more than five family movies.\n   Solution\nQ1.  SELECT first_name,last_name FROM actor  Q2.  SELECT f.title, l.name FROM film AS f JOIN language AS l ON f.language_id = l.language_id;  Q3.  SELECT first_name, last_name FROM store JOIN staff ON manager_staff_id = staff_id WHERE store.store_id=2;  Q4.  SELECT first_name,last_name FROM actor a JOIN film_actor fa ON a.actor_id=fa.actor_id JOIN film f ON fa.Film_ID=f.Film_ID WHERE f.title=\"ANGELS LIFE\"  Q5.  SELECT first_name, last_name, count(*) as nbFilms FROM actor a JOIN film_actor fa on a.actor_id=fa.actor_id GROUP BY a.actor_id, a.first_name, a.last_name ORDER BY nbFilms DESC  Alternative solution:  SELECT t.actor_id, t.nb_films FROM (SELECT actor_id, count(*) as nb_films FROM film_actor GROUP BY actor_id ORDER BY nb_films DESC) t  Q6.  SELECT c.name FROM category c JOIN film_category fc ON c.category_id = fc.category_id GROUP BY c.category_id HAVING count(*) BETWEEN 25 AND 55  Alternative solution:  SELECT name FROM category WHERE category_id IN (SELECT category_id FROM film_category fc GROUP BY category_id HAVING COUNT(*) BETWEEN 25 AND 55)  Q7.  SELECT c.customer_id, c.first_name, c.last_name, COUNT(*) AS nbFilms FROM customer c JOIN rental r ON c.customer_id=r.customer_id JOIN inventory i ON r.inventory_id = i.inventory_id JOIN film f ON i.Film_ID = f.Film_ID JOIN film_category fc on f.Film_ID = fc.Film_ID JOIN category c ON fc.category_id = c.category_id WHERE c.name = \"Family\" GROUP BY r.customer_id HAVING nbFilms  5;  Alternative solution:  SELECT first_name, last_name FROM customer WHERE customer_id IN ( SELECT r.customer_id FROM rental r JOIN inventory i ON r.inventory_id = i.inventory_id JOIN film f ON i.Film_ID = f.Film_ID JOIN film_category fc on f.Film_ID = fc.Film_ID JOIN category c ON fc.category_id = c.category_id WHERE c.name = \"Family\" GROUP BY r.customer_id HAVING count(*)  5)     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ceb065adca61756fa19abe17b6525115","permalink":"/courses/databases/tutorials/sqlite/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sqlite/","section":"courses","summary":"Description of the SQLite tutorial.","tags":null,"title":"SQLite in action","type":"docs"}]