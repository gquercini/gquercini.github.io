[{"authors":["admin"],"categories":null,"content":"Assistant professor at CentraleSupélec\u0026rsquo;s computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Assistant professor at CentraleSupélec\u0026rsquo;s computer science department.\nMember of the LaHDAK team at the Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) (from the merge of LRI and LIMSI).","tags":null,"title":"Gianluca Quercini","type":"authors"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB as a case study).\nPrerequisites Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\nGood knowledge of relational database management systems.\nTeaching staff Gianluca Quercini Course summary 1. Introduction and MapReduce programming.\nBasic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce. 2. Hadoop and its ecosystem: HDFS.\nIn-depth description of the Hadoop Distributed File System (HDFS). 3. Introduction to Apache Spark.\nApache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions. 4. SparkSQL\n4. Spark streaming\n6. Distributed databases and NoSQL.\nData distribution (replication, sharding, the CAP theorem).\nOverview of NoSQL databases.\n7. Document oriented databases: MongoDB.\nPresentation of MongoDB. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cdc33fae1808ae750f7ab3a0b643dad8","permalink":"/courses/bdia_old/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Big data algorithms, techniques and platforms","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course aims to introduce the main technologies to deal with the many challenges posed by Big Data.\nBig Data is a term used to describe a collection of data that is huge in volume and yet grows exponentially over time. In short, this data is so voluminous and complex that none of the traditional data management tools are capable of storing or processing it efficiently.\nIn the first part, this course introduces the existing technologies that make it possible to efficiently process large volumes of data, namely Hadoop MapReduce and Apache Spark.\nIn the second part, we will study the solutions that allow to store and query these volumes of data; we will focus on a variety of NoSQL databases (using MongoDB as a case study).\nPrerequisites Basic understanding of how computer systems work: processor, memory, disk operations and functions of the operating system.\nGood knowledge of relational database management systems.\nTeaching staff Gianluca Quercini Course summary 1. Introduction and MapReduce programming.\nBasic notions and motivations of Big Data. Overview of Hadoop. Introduction to MapReduce. 2. Hadoop and its ecosystem: HDFS.\nIn-depth description of the Hadoop Distributed File System (HDFS). 3. Introduction to Apache Spark.\nApache Spark, its architecture and functionalities. Resilient Distributed Datasets: transformations and actions. 4. Spark Structured APIs and Structured Streaming\nSparkSQL, Spark streaming. 5. Distributed databases and NoSQL.\nData distribution (replication, sharding, the CAP theorem).\nOverview of NoSQL databases.\n6. Document oriented databases: MongoDB.\nPresentation of MongoDB. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"143a84392a4b8526c86e86f4828e51f0","permalink":"/courses/big-data-marseille/overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/","section":"courses","summary":"Presentation of the course Big data algorithms, techniques and platforms","tags":null,"title":"Big data","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nLink: Click here\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eb94b968125a21115c8a8a36c9e0846a","permalink":"/courses/bdia_old/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":null,"categories":null,"content":" Tutorial 1 Title: MapReduce programming\nDate and time: Monday 3 May 2021\nLink: Click here\nTutorial 2 Title: Introduction to Spark RDD programming\nDate and time: Wednesday 5 May 2021\nLink: Click here\nTutorial 3 Title: Introduction to DataFrames and SparkSQL\nDate and time: Monday 10 May 2021\nLinks:\nDataFrames\nDataFrames + SQL\nLab assignment 1 Title: Apache Spark programming\nDate and time: Wednesday 12 May 2021\nLink: Click here\nTutorial 4 Title: Apache Structured Streaming\nDate and time: Wednesday 12 May 2021\nLink: Click here\nLab assignment 2 Title: MongoDB\nDate and time: Wednesday 19 May 2021\nLink: Click here\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f85579f8755ff2ff8b6533862868e6d2","permalink":"/courses/big-data-marseille/tutorials/cc-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/cc-tutorials/","section":"courses","summary":"Presentation of the tutorials of the course.","tags":null,"title":"Tutorials and lab assignments","type":"docs"},{"authors":[],"categories":[],"content":"","date":1581344688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581344688,"objectID":"4e459ff727a9e6a0c795e8d8f3d69232","permalink":"/project/data-for-you/","publishdate":"2020-02-10T15:24:48+01:00","relpermalink":"/project/data-for-you/","section":"project","summary":"","tags":[],"title":"Data for You","type":"project"},{"authors":["Armita Khajeh Nassiri","Nathalie Pernelle","Fatiha Saı̈s","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"77dcf9b15a66ddb64a7a8426a803dd21","permalink":"/publication/nassiri-2020/","publishdate":"2020-11-05T15:51:37.231059Z","relpermalink":"/publication/nassiri-2020/","section":"publication","summary":"","tags":null,"title":"Generating Referring Expressions from RDF Knowledge Graphs for Data Linking","type":"publication"},{"authors":["Fatiha Saïs","Joana E. Gonzales Malaverri","Gianluca Quercini"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"24e4602ce4c2a42bac26f3019386ecbf","permalink":"/publication/sais-2020/","publishdate":"2020-04-08T14:26:24.211279Z","relpermalink":"/publication/sais-2020/","section":"publication","summary":"","tags":null,"title":"MOMENT: Temporal Meta-Fact Generation and Propagation in Knowledge Graphs","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Coriane Nana Jipmo","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3685ef29403450add392048ea8b1e19d","permalink":"/publication/seghouani-2019/","publishdate":"2020-02-10T15:32:10.718712Z","relpermalink":"/publication/seghouani-2019/","section":"publication","summary":"","tags":null,"title":"Determining the interests of social media users: two approaches","type":"publication"},{"authors":["Suela Isaj","Nacéra Bennacer Seghouani","Gianluca Quercini"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f0a2e1e20345b7ff2748dd0ff24c29fd","permalink":"/publication/isaj-2019/","publishdate":"2020-02-10T15:37:10.960652Z","relpermalink":"/publication/isaj-2019/","section":"publication","summary":"","tags":null,"title":"Profile Reconciliation Through Dynamic Activities Across Social Networks","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed97c59bbeebf6f67506ea911cd4d01b","permalink":"/publication/seghouani-2018/","publishdate":"2020-02-10T15:32:10.719498Z","relpermalink":"/publication/seghouani-2018/","section":"publication","summary":"","tags":null,"title":"A frequent named entities-based approach for interpreting reputation in Twitter","type":"publication"},{"authors":["Nacéra Bennacer Seghouani","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9cdf7741acd87116627b5dd0bc25a27c","permalink":"/publication/seghouani-2018-a/","publishdate":"2020-02-10T15:33:43.343039Z","relpermalink":"/publication/seghouani-2018-a/","section":"publication","summary":"","tags":null,"title":"Élimination des liens inter-langues erronés dans Wikipédia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Jorge Galicia","Mariana Patricio","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"edcf86d1dd80092db4864af2aa551e98","permalink":"/publication/bennacer-2017-a/","publishdate":"2020-02-10T15:32:10.723551Z","relpermalink":"/publication/bennacer-2017-a/","section":"publication","summary":"","tags":null,"title":"Eliminating Incorrect Cross-Language Links in Wikipedia","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"048dafc42b622ff1cee4c115e5409243","permalink":"/publication/jipmo-2017/","publishdate":"2020-02-10T15:32:10.722099Z","relpermalink":"/publication/jipmo-2017/","section":"publication","summary":"","tags":null,"title":"Frisk: A multilingual approach to find twitteR InterestS via wiKipedia","type":"publication"},{"authors":["Nacéra Bennacer","Francesca Bugiotti","Moditha Hewasinghage","Suela Isaj","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1e3d10b83c3c7b1807d422d7d251cca3","permalink":"/publication/bennacer-2017/","publishdate":"2020-02-10T15:32:10.7231Z","relpermalink":"/publication/bennacer-2017/","section":"publication","summary":"","tags":null,"title":"Interpreting reputation through frequent named entities in twitter","type":"publication"},{"authors":["Gianluca Quercini","Nacéra Bennacer","Mohammad Ghufran","Coriane Nana Jipmo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c02251b35ab73929914bbe9b772e5248","permalink":"/publication/quercini-2017/","publishdate":"2020-02-10T15:32:10.720054Z","relpermalink":"/publication/quercini-2017/","section":"publication","summary":"","tags":null,"title":"Liaison: reconciliation of individuals profiles across social networks","type":"publication"},{"authors":["Mohammad Ghufran","Nacéra Bennacer","Gianluca Quercini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c1928a9ee227111e1b27ba31fdbb6a86","permalink":"/publication/ghufran-2017/","publishdate":"2020-02-10T15:32:10.722652Z","relpermalink":"/publication/ghufran-2017/","section":"publication","summary":"","tags":null,"title":"Wikipedia-based extraction of key information from resumes","type":"publication"},{"authors":["Coriane Nana Jipmo","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"49508dab832a1726fe7da77e8604a7f8","permalink":"/publication/jipmo-2016/","publishdate":"2020-02-10T15:32:10.724083Z","relpermalink":"/publication/jipmo-2016/","section":"publication","summary":"","tags":null,"title":"Catégorisation et Désambiguı̈sation des Intérêts des Individus dans le Web Social.","type":"publication"},{"authors":["Nacéra Bennacer","Mia Johnson Vioulès","Maximiliano Ariel López","Gianluca Quercini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ce3927111f9fc9f8ef041600fa6e6126","permalink":"/publication/bennacer-2015/","publishdate":"2020-02-10T15:32:10.725565Z","relpermalink":"/publication/bennacer-2015/","section":"publication","summary":"","tags":null,"title":"A multilingual approach to discover cross-language links in Wikipedia","type":"publication"},{"authors":["Mohammad Ghufran","Gianluca Quercini","Nacéra Bennacer"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"482b66b1b2e201edf7dfa2685afa010a","permalink":"/publication/ghufran-2015/","publishdate":"2020-02-10T15:32:10.724645Z","relpermalink":"/publication/ghufran-2015/","section":"publication","summary":"","tags":null,"title":"Toponym disambiguation in online social network profiles","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"2bce9aaaae3b0248796b29334c06054f","permalink":"/publication/bennacer-2014/","publishdate":"2020-02-10T15:32:10.726341Z","relpermalink":"/publication/bennacer-2014/","section":"publication","summary":"","tags":null,"title":"Matching user profiles across social networks","type":"publication"},{"authors":["Nacéra Bennacer","Coriane Nana Jipmo","Antonio Penta","Gianluca Quercini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ff35f567785c6703d81172b874c7531b","permalink":"/publication/bennacer-2014-a/","publishdate":"2020-02-10T15:32:10.727368Z","relpermalink":"/publication/bennacer-2014-a/","section":"publication","summary":"","tags":null,"title":"Réconciliation des profils dans les réseaux sociaux.","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"41c8a22b08bd7e057a05ad86a744cac5","permalink":"/publication/quercini-2014/","publishdate":"2020-02-10T15:32:10.728416Z","relpermalink":"/publication/quercini-2014/","section":"publication","summary":"","tags":null,"title":"Uncovering the spatial relatedness in Wikipedia","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"9086beba183fe687c8f4c462cd3153ca","permalink":"/publication/quercini-2013/","publishdate":"2020-02-10T15:32:10.72918Z","relpermalink":"/publication/quercini-2013/","section":"publication","summary":"","tags":null,"title":"Entity discovery and annotation in tables","type":"publication"},{"authors":["Gianluca Quercini","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cc5f1500c80ecfe28e62ab5e9b608e81","permalink":"/publication/quercini-2012-a/","publishdate":"2020-02-10T15:32:10.737022Z","relpermalink":"/publication/quercini-2012-a/","section":"publication","summary":"","tags":null,"title":"Des données tabulaires à RDF: l’extraction de données de Google Fusion Tables","type":"publication"},{"authors":["Massimo Ancona","Betty Bronzini","Davide Conte","Gianluca Quercini"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"62840715b27d999325c6799de42c8994","permalink":"/publication/ancona-2012/","publishdate":"2020-02-10T15:32:10.717657Z","relpermalink":"/publication/ancona-2012/","section":"publication","summary":"","tags":null,"title":"Developing Attention-Aware and Context-Aware User Interfaces on Handheld Devices","type":"publication"},{"authors":["Antonio Penta","Gianluca Quercini","Chantal Reynaud","Nigel Shadbolt"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"fa3cbdff0934af7e1ad34c9d906d46b4","permalink":"/publication/penta-2012/","publishdate":"2020-02-10T15:32:10.72995Z","relpermalink":"/publication/penta-2012/","section":"publication","summary":"","tags":null,"title":"Discovering Cross-language Links in Wikipedia through Semantic Relatedness.","type":"publication"},{"authors":["Gianluca Quercini","Jochen Setz","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1eb03ccdfc068b0af2eee6b98febab4f","permalink":"/publication/quercini-2012/","publishdate":"2020-02-10T15:32:10.736267Z","relpermalink":"/publication/quercini-2012/","section":"publication","summary":"","tags":null,"title":"Facetted Browsing on Extracted Fusion Tables Data for Digital Cities.","type":"publication"},{"authors":["Jochen Setz","Gianluca Quercini","Daniel Sonntag","Chantal Reynaud"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"152a779726400c4afb268da398df2025","permalink":"/publication/setz-2012/","publishdate":"2020-02-10T15:32:10.730492Z","relpermalink":"/publication/setz-2012/","section":"publication","summary":"","tags":null,"title":"Facetted search on extracted fusion tables data for digital cities","type":"publication"},{"authors":["Laura Papaleo","Gianluca Quercini","Viviana Mascardi","Massimo Ancona","A Traverso","Henry de Lumley"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2951f449fe1a9a559bd320ee66d7a4df","permalink":"/publication/papaleo-2011/","publishdate":"2020-02-10T15:32:10.7311Z","relpermalink":"/publication/papaleo-2011/","section":"publication","summary":"","tags":null,"title":"Agents and Ontologies for Understanding and Preserving the Rock Art of Mount Bego.","type":"publication"},{"authors":["Gianluca Quercini","Massimo Ancona"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"712870e00c1980526965a476c61c6a9f","permalink":"/publication/quercini-2010/","publishdate":"2020-02-10T15:32:10.731853Z","relpermalink":"/publication/quercini-2010/","section":"publication","summary":"","tags":null,"title":"Confluent drawing algorithms using rectangular dualization","type":"publication"},{"authors":["Gianluca Quercini","Hanan Samet","Jagan Sankaranarayanan","Michael D Lieberman"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"c8ae118f82887db05d951390d2ae7695","permalink":"/publication/quercini-2010-a/","publishdate":"2020-02-10T15:33:43.347923Z","relpermalink":"/publication/quercini-2010-a/","section":"publication","summary":"","tags":null,"title":"Determining the spatial reader scopes of news sources using local lexicons","type":"publication"},{"authors":["Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Anton Bogdanovych","H De Lumley","Laura Papaleo","Simeon Simoff","Antonella Traverso"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"6456e6f4265092c5466fcdd21d7bb4eb","permalink":"/publication/ancona-2010/","publishdate":"2020-02-10T15:32:10.73288Z","relpermalink":"/publication/ancona-2010/","section":"publication","summary":"","tags":null,"title":"Virtual institutions for preserving and simulating the culture of Mount Bego's ancient people","type":"publication"},{"authors":["Anton Bogdanovych","Laura Papaleo","Massimo Ancona","Viviana Mascardi","Gianluca Quercini","Simeon Simoff","Alex Cohen","Antonella Traverso"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"c71e7154cbc2dfbe25039ed19758c530","permalink":"/publication/bogdanovych-2009/","publishdate":"2020-02-10T15:32:10.737689Z","relpermalink":"/publication/bogdanovych-2009/","section":"publication","summary":"","tags":null,"title":"Integrating agents and virtual institutions for sharing cultural heritage on the Web","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e93eb6b5f5c5cf19230762a04c9abc05","permalink":"/publication/ancona-2009/","publishdate":"2020-02-10T15:32:10.720588Z","relpermalink":"/publication/ancona-2009/","section":"publication","summary":"","tags":null,"title":"Text Entry in PDAs with WtX","type":"publication"},{"authors":["Massimo Ancona","Davide Conte","Donatella Pian","Sonia Pini","Gianluca Quercini","Antonella Traverso"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"e650a879a57f2a6c286aa2d365cb9a7d","permalink":"/publication/ancona-2008/","publishdate":"2020-02-10T15:32:10.721064Z","relpermalink":"/publication/ancona-2008/","section":"publication","summary":"","tags":null,"title":"Wireless networks in archaeology and cultural heritage","type":"publication"},{"authors":["Massimo Ancona","Gianluca Quercini","Luca Dominici"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5da2e1599d08bf6b66c9260251f7f87d","permalink":"/publication/ancona-2007-a/","publishdate":"2020-02-10T15:32:10.734718Z","relpermalink":"/publication/ancona-2007-a/","section":"publication","summary":"","tags":null,"title":"An Improved Text Entry Tool for PDAs","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"0bb01482186a36901efd9bc59287ade5","permalink":"/publication/ancona-2007-b/","publishdate":"2020-02-10T15:33:43.342472Z","relpermalink":"/publication/ancona-2007-b/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["M Ancona","S Drago","G Quercini","A Bogdanovych"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"5de935c6dca2f9dc45d3fcc539d7a23e","permalink":"/publication/ancona-2007/","publishdate":"2020-02-10T15:32:10.721486Z","relpermalink":"/publication/ancona-2007/","section":"publication","summary":"","tags":null,"title":"Rectangular Dualization of Biconnected Planar Graphs in Linear Time and Related Applications","type":"publication"},{"authors":["Massimo Ancona","Marco Cappello","Marco Casamassima","Walter Cazzola","Davide Conte","Massimiliano Pittore","Gianluca Quercini","Naomi Scagliola","Matteo Villa"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"956e4fbb19a8e3db3caf66b9ca3ce215","permalink":"/publication/ancona-2006-a/","publishdate":"2020-02-10T15:33:43.349611Z","relpermalink":"/publication/ancona-2006-a/","section":"publication","summary":"","tags":null,"title":"Mobile vision and cultural heritage: the agamemnon project","type":"publication"},{"authors":["Massimo Ancona","Walter Cazzola","Sara Drago","Gianluca Quercini"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"152d79fb2becb0b67888c71837db1203","permalink":"/publication/ancona-2006/","publishdate":"2020-02-10T15:32:10.733963Z","relpermalink":"/publication/ancona-2006/","section":"publication","summary":"","tags":null,"title":"Visualizing and managing network topologies via rectangular dualization","type":"publication"},{"authors":["M Ancona","S Locati","M Mancini","A Romagnoli","G Quercini"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"a5f6251a4b15ec8ddae9685ad605d62a","permalink":"/publication/ancona-2005/","publishdate":"2020-02-10T15:32:10.735534Z","relpermalink":"/publication/ancona-2005/","section":"publication","summary":"","tags":null,"title":"Comfortable textual data entry for PocketPC: the WTX system","type":"publication"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\nAssignment submission\nThis lab assignment will be evaluated.\nYou need to submit a .zip file containing the following files:\nSource code of the programs that you write.\nA PDF document with the answer to the questions that you find in this document.\nPlease send me the zip file by email.\nThe submission deadline is Thursday, May 20, 2021 8:00 AM.\n1 Computing averages We consider a collection of CSV files containing temperature measurements in the following format:\nyear,month,day,hours,minutes,seconds,temperature\nyou can find the files under the directory hdfs://sar01:9000/data/temperatures/\nHere are the details for each file:\nFile temperatures_86400.csv contains one measurement per day in the years 1980 - 2018. File temperatures_2880.csv contains one measurement every 2880 seconds in the years 1980 - 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds for the years 1980 - 2018. We intend to implement a Spark algorithm to generate pairs \\((y, t_{avg})\\), where \\(y\\) is the year and \\(t_{avg}\\) is the average temperature in the year.\n1.1 First implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_first.py Open the file avg_temperatures_first.py and write the following function:\ndef avg_temperature(theTextFile): temperatures = theTextFile \\ .map(lambda line: line.split(\u0026quot;,\u0026quot;)) \\ .map(lambda term: (term[0], [float(term[6])])) \\ .reduceByKey(lambda x, y: x+y) \\ .mapValues(lambda lv: sum(lv)/len(lv)) return temperatures In the same file, locate the two variables input_path and output-path. and write the following code:\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/cpuecm1/cpuecm1_XX/\u0026quot; Don’t forget the / at the end of the file paths and to replace XX with the number at the end of your username.\nExercise\nExercise 1.1 Run the script avg_temperatures_first.py by using temperatures_86400.csv as an input. To this extent, use the following command: spark-submit --master spark://sar01:7077 avg_temperatures_first.py temperatures_86400.csv You should find the output of the program under the folder\nhdfs://sar01:9000/cpuecm1/cpuecm1_XX/temperatures_86400.out\nWhat’s the execution time? In the output of Spark on the command line you should see a line that mentions something along the following line: INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s Run the same script by using temperatures_2880.csv as an input.\nWhat is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer.\nExecute the same script by using temperatures_86.csv as an input.\nWhat is the execution time? How would you justify it, knowing that the files temperatures_2880.csv and temperatures_86.csv have a similar size (11 MB the former, 9 MB the latter)?\n```\n1.2 Second implementation Copy the file ~vialle/DCE-Spark/template_temperatures.py to your working directory by typing the following command:\ncp ~vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_second.py Exercise\nExercise 1.2 Based on the observations made in the previous exercise, write an improved implementation of the function avg_temperature.\nExercise\nExercise 1.3 Run the script avg_temperatures_second.py by using temperatures_86.csv as an input.\nWhat’s the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference.\nRun the same script by using temperatures_10.csv (3 GB!) as an input. Do you think that the program takes too long? Why?\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nWe use the following input files available in folder hdfs://sar01:9000/data/sn/:\nsn_tiny.csv. Small social network, that you can use to test your implementation.\nsn_10k_100k.csv. Social network with \\(10^4\\) individuals and \\(10^5\\) links.\nsn_100k_100k.csv. Social network with \\(10^5\\) individuals and \\(10^5\\) links.\nsn_1k_100k.csv. Social network with \\(10^3\\) individuals and \\(10^5\\) links.\nsn_1m_1m.csv. Social network with \\(10^6\\) individuals and \\(10^6\\) links.\nExercise\nExercise 2.1 Write an implementation in Spark. Test your implementation on file sn_tiny.csv.\nExercise\nExercise 2.2 Run your implementation on the other files and write down the execution times. Comment on the execution times considering the file sizes, the number of nodes and links and the number of pairs \\(((A, B), X)\\) generated by the algorithm.\nExercise\nExercise 2.3 By using a MapReduce-style algorithm, write a Spark program to compute the minimum, maximum and average degree of a node in a given graph.\nCompute the minimum, maximum and average degree on all the given input files.\nDo these values confirm or invalidate the considerations that you made on the execution times of the algorithm in the first exercise? Justify your answer.\n3 Creating an inverted index In folder hdfs://sar01:9000/data/bbc/ you’ll find a collection of 50 articles obtained from the BBC website (2004-2005) organized into five subfolders: business, entertainment, politics, sport and technology.\nWe want to create an inverted index, which associates each word with the list of the files in which the word occurs. More specifically, for each word, the inverted index will have a list of the names of the files (path relative to the folder /data/bbc) that contain the word.\nThe inverted index:\nmust not contain the same word twice;\nmust not contain any stopwords (the list of stopwords is provided in the hdfs://sar01:9000/data/stopwords.txt file);\nMoreover:\nWords in the inverted index must only contain letters.\nWords in the inverted index must be lowercase.\nExercise\nExercise 3.1 Write a Spark program to create an inverted index and execute it on the input folder. You can use the template available at ~vialle/DCE-Spark/template_inverted_index.py.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76158ebb08d6aa7e40a693d348db00e7","permalink":"/courses/big-data-marseille/tutorials/spark-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/spark-assignment/","section":"courses","summary":"Lab assignment Spark RDD.","tags":null,"title":"Apache Spark — Programming with RDD","type":"docs"},{"authors":null,"categories":null,"content":" 1 Number of partitions in a RDD We consider a set of CSV files that contain temperature measurements over several years. Each line has the following content: year,month,day,hour,minute,second,temperature.\nThese files are stored at the following location: hdfs://sar01:9000/data/temperatures/.\nHere is the detail for each file:\nFile temperatures_86400.csv contains one measurement per day between 1980 and 2018. File temperatures_2880.csv contains one measurement every 2880 seconds between 1980 and 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds between 1980 - 2018. Get the file avg_temperatures_rdd.py by typing the following command:\ncp /usr/users/cpu-prof/cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .\nThis file contains an efficient implementation of the function that computes the average yearly temperature, as we have seen in a previous tutorial.\n1.1 Performances and number of partitions Line 70. Replace sarXX with sar01.\nLine 80. Replace sarXX with sar01. Replace YOUR_DIRECTORY with bdiaspark23/bdiaspark23_XX/ (where XX corresponds to your username number).\nObserve the instructions at line 98 and line 104. They get and show the number of partitions of the input RDD text_file and the output RDD temperatures respectively.\nExercise\nExercise 1.1 Complete Table 1. Run the program on file temperatures_10.csv. Write down the execution time and the number of partitions of both RDDs text_file and temperatures.\nReminder. The command to run the program is as follows:\nspark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv\nFile Execution time (sec) Number of partitions (text_file) Number of partitions (temperatures) temperatures_86400.csv 3.12 2 2 temperatures_2880.csv 3.67 2 2 temperatures_86.csv 4.59 2 2 temperatures_10.csv Table 1. Execution time and partition numbers with RDD. 1.2 Analysis of Spark’s operation Exercise\nExercise 1.2 Could you understand how Spark determines the number of partitions of the RDD text_file by looking at the size of the input files? HINT. If you divide the size of the file temperatures_10.csv by the number of partitions of the RDD text_file, which value do you obtain? What does this value represent?\nList the files that are stored in the output folder temperatures_10.rdd.out under your HDFS folder. What do you notice? Is there any relation with respect to the number of partitions?\nGood to know\nIn order to list the content of the folder in HDFS, you can use the following command:\nhdfs dfs -ls hdfs://sar01:9000/bdiaspark23/bdiaspark23_XX/temperatures_10.rdd.out\n2 Using the DataFrame API to compute the average temperatures You’re now going to implement a Spark program to compute the average temperatures by using the Spark DataFrame API.\nGo through the following steps:\nCopy the code template avg_temperatures_df.py to your home folder by executing the following command: cp /usr/users/cpu-prof/cpu_quercini/spark-sql-templates/avg_temperatures_df.py .\nThe result of the computation will be stored in the folder temperatures_*.df.out under your HDFS folder bdiaspark23/bdiaspark23_XX. Exercise\nExercise 2.1 Line 78. Replace sarXX with sar01.\nLine 89. Replace sarXX with sar01.\nLine 106. Complete the instruction to read from the input CSV file. Please note that the input CSV files do not have headers. Don’t use schema inference, just specify your schema manually. As a reminder, the columns are: year, month, day, hour, minute, second, temperature.\nLine 55. Complete the definition of function avg_temperature_df.\nExecute your code on all the input CSV files and complete Table 2. File Execution time RDD\n(sec) Execution time DataFrame\n(sec) temperatures_86400.csv 3.12 temperatures_2880.csv 3.67 temperatures_86.csv 4.59 temperatures_10.csv Exercise 1.1 Table 2. RDDs vs. DataFrames. Exercise\nExercise 2.2 Compare the execution times with the ones obtained with the implementation using the RDDs. What do you observe? How do you explain the differences?\n2.1 Caching a DataFrame You’re now going to discover the advantages of caching a DataFrame.\nUncomment the last two lines in file avg_temperatures_df.py\nRemove the file temperatures_10.df.out by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_XX/temperatures_10.df.out\nExercise\nExercise 2.3 Execute the code on file temperatures_10.csv. What is the execution time of each action? Can you explain in detail what is going on here?\nRemove files temperatures_10.df.out and temperatures_10.df.out.bis. Exercise\nExercise 2.4 Cache the DataFrame df_avg and execute the code again on file temperatures_10.csv.\nWhere should you add the cache instruction?\nWhat is the execution time of each action?\nCan you explain in detail what is going on here?\n3 Computing averages with SQL You’re now going to implement the computation of the yearly average temperatures by using SQL on Spark DataFrames.\n3.1 Using a view A first option to query a DataFrame with SQL is to create a view.\nCopy the file avg_temperatures_sql_view.py to your home folder by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .\nComplete the code in lines 90, 101, 118.\nImplement the function avg_temperature_sql (line 57).\nExercise\nExercise 3.1 Execute the code on all CSV files and complete Table 3.\nWhat can you tell about the the running times ? Do you find significant differences between using SQL on a view and DataFrame functions? File Execution time RDD\n(sec) Execution time DataFrame\n(sec) Execution time SQL view\n(sec) temperatures_86400.csv 3.12 Exercise 2.1 temperatures_2880.csv 3.67 Exercise 2.1 temperatures_86.csv 4.59 Exercise 2.1 temperatures_10.csv Exercise 1.1 Exercise 2.1 Table 3. RDDs vs DataFrames with views. 3.2 Using a table A second option to query a DataFrame with SQL is to create a table.\nCopy the file avg_temperatures_sql_table.py to your home directory by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .\nComplete lines 50, 112, 123 and 140.\nImplement the function avg_temperature_sql (line 76).\nExercise\nExercise 3.2 Execute the code on all files.\nComplete Table 4.\nCompare the execution times with the ones that you obtained. Discuss the results. File Execution time RDD\n(sec) Execution time DataFrame\n(sec) Execution time SQL view\n(sec) Execution time SQL table\n(sec) temperatures_86400.csv 3.12 Exercise 2.1 Exercise 3.1 temperatures_2880.csv 3.67 Exercise 2.1 Exercise 3.1 temperatures_86.csv 4.59 Exercise 2.1 Exercise 3.1 temperatures_10.csv Exercise 1.1 Exercise 2.1 Exercise 3.1 Table 4. RDDs vs DataFrames with views and tables. Remove the file temperatures_10.sql.table.out\nExecute the code again on file temperatures_10.csv.\nExercise\nExercise 3.3 What is the execution time that you obtain now?\n4 Using the DataFrame API on large files We now consider the files stored under hdfs://sar01:9000/data/sales/.\nThese files contain tabular data related to the sale of products in a chain of stores. We consider two tables: store_sales and customer. In the first table we find information about each sale, such as the identifier of the product sold, the identifier of the buyer, the quantity of purchased product and the price paid by the customer. For this table, we have 4 files, which only differ in size:\nstore_sales_.100.dat: contains 9.5 GiB of data.\nstore_sales_.200.dat: contains 19 GiB of data.\nstore_sales_.400.dat: contains 38 GiB of data.\nstore_sales_.800.dat: contains 77 GiB of data.\nIn table customer we find data about customers, such as first and last names and birth dates. We only have one file for this table:\ncustomer_10000.dat: contains 8.3 GiB of data. We want to test the performances of the DataFrame API on the following queries (WARNING. you must write a code that uses DataFrame functions, not SQL!):\nQuery Q1: returns the number of clients. This corresponds to the following SQL query: SELECT count(*) FROM customer Query Q2: returns the price of the most expensive product. This corresponds to the following SQL query:\nSELECT max(ss_list_price) FROM store_sales Query Q3: returns the amount of money spent by each client. This corresponds to the following SQL query:\nSELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales GROUP BY ss_customer_sk Query Q4: Query Q3 + sort the result so that the client that spent the most money appears on the top. This corresponds to the following SQL query:\nSELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales GROUP BY ss_customer_sk ORDER BY amountSpent DESC Query Q5: Query Q4 + join with the table customer to get the first and last name of the customers. This corresponds to the following SQL query:\nSELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk GROUP BY ss_customer_sk ORDER BY amountSpent DESC 4.1 Development of the code using the DataFrame API. Copy the file dataframe_api_benchmark.py to your home directory by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py . Modify the code by following the instructions in the file.\nExecute the code on file store_sales_100.dat (the smallest one) to test that your code is bug-free.\nOnce you’re sure that your code is correct, uncomment lines 82 and 83. This will cache the two DataFrames.\nExecute the code on all files store-sales_*.dat\nGood to know\nEach query is executed 5 times to have a correct estimate of the execution time. You’ll see that the execution times fluctuate on the first iterations and they stabilize in the later iterations. When you write down the execution times, only consider the execution times obtained at the last iteration.\nExercise\nExercise 4.1 Complete Table 5 and write down the execution time of each query for each file.\nWhy the execution time of the queries Q1 and Q2 is large at the iteration 0?\nDo you think that the difference between the execution times of the queries is reasonable?\nDo you think that the augmentation of the execution times is reasonable given the size of the input files?\nFile / query Read\n(sec) Query Q1\n(sec) Query Q2\n(sec) Query Q3\n(sec) Query Q4\n(sec) Query Q5\n(sec) store_sales_1_4.100.dat 17.24 0.94 1.05 1.17 2.16 5.22 store_sales_1_4.200.dat store_sales_1_4.400.dat store_sales_1_4.800.dat Table 5. Execution times of the queries on the sales dataset. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5f8a9e6d0e9ea85d7fa3b7ca4dfa4431","permalink":"/courses/bdia/tutorials/spark-sql-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/spark-sql-assignment/","section":"courses","summary":"Description of the assignment on Spark SQL","tags":null,"title":"Spark DataFrames and Spark SQL","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nSlides: Available on Edunao.\nLecture 2 Title: Hadoop and storage: HDFS.\nSlides: Available on Edunao.\nLecture 3 Title: Introduction to Apache Spark.\nSlides: Available on Edunao.\nLecture 4 Title: Spark SQL.\nLecture 5 Title: Spark Streaming\nLecture 6 Title: Distributed and NoSQL databases\nLecture 7 Title: Document-oriented database systems: MongoDB.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b2794a92549814a9265f32990f791e8c","permalink":"/courses/bdia_old/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\nFigure 1: Cluster architecture (image credit: Stéphane Vialle) In order to use the Spark cluster, you’ll need to go through the following fours steps:\nChoose a username on Edunao. The password will be communicated by the teacher during the tutorial.\nOpen a SSH connection to the machine phome.metz.supelec.fr from your local machine.\nOpen a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\nOn the machine slurm1 allocate resources for the job from a named reservation.\nSteps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\nConnect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\nIn the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\nIn the connection panel, set Enable TCP keepalives and set 30s between keepalives.\nClick on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\nA command-line terminal should pop up, prompting you to enter your username and password.\nConnect to slurm1 In the command-line terminal type the following command:\nssh slurm1\nAllocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\nUse the cluster only in the evening in weekdays or during the weekends.\nIn order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks1 -N 1 --ntasks-per-node=4 --pty bash\nSource file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuasi1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!). or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!). The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\nWindows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\nCreating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuasi1_X, where X is between 1 and 28.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuasi1/cpuasi1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuasi1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuasi1/\nPreliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\nCopy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\nIf you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\nOpen the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\nLocate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\nSimilarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuasi1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\nRun the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\nWhen the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\nAs usual, remember to replace cpuasi1_X with your username.\nIn order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out/* Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/cpuasi1/cpuasi1_X/sherlock.out\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"376356a15004133d6391a89312925a1a","permalink":"/courses/bdia_old/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" The architecture of the cluster at the CentraleSupélec campus in Metz is shown in the following figure.\nFigure 1: Cluster architecture (image credit: Stéphane Vialle) In order to use the Spark cluster, you’ll need to go through the following fours steps:\nUse your username in the spreadsheet communicated by the teacher before the tutorial. The password will be communicated by the teacher during the tutorial.\nOpen a SSH connection to the machine phome.metz.supelec.fr from your local machine.\nOpen a SSH connection to the machine slurm1 from the machine phome.metz.supelec.fr.\nOn the machine slurm1 allocate resources for the job from a named reservation.\nSteps 2, 3 and 4 are detailed in the following subsections.\nOpening a SSH connection\nIf your operating system is either Linux or MacOS, the command ssh, necessary to open a SSH connection to a computer, is likely to be already available.\nIf your operating system is Windows, you’re not likely to have a command ssh readily available. In that case, you’ll need to install a SSH client. A good one is PuTTY, that you can download here.\nConnect to phome If your operating system is Linux or MacOS, open a command-line terminal and type the following command (replace your-username with the username that you chose).\nssh phome.metz.supelec.fr -l your-username\nAfter executing the command, you’ll be prompted to enter the password.\nIf your operating system is Windows:\nLaunch PuTTY.\nIn the session panel, specify phome.metz.supelec.fr as the host name. Select ssh (port 22) as connection type.\nIn the connection panel, set Enable TCP keepalives and set 30s between keepalives.\nClick on the button Open and click on the button Yes if you receive a warning informing you the key of the destination server is not cached yet.\nA command-line terminal should pop up, prompting you to enter your username and password.\nConnect to slurm1 In the command-line terminal type the following command:\nssh slurm1\nAllocate resources for the job Once you’re connected to slurm1, you can allocate resources for the job by typing the following command (in place of resa-code you will type a code that will be communicated by the teacher during the tutorial.)\nsrun --reservation=resa-code -N 1 --ntasks-per-node=4 --pty bash\nRead carefully\nIf you want to access the cluster after the tutorial, remember to:\nUse the cluster only in the evening in weekdays or during the weekends.\nIn order to allocate the resources, use the following command instead of the previous one:\nsrun -p ks2 -N 1 --ntasks-per-node=4 --pty bash\nSource file edition The Python source files that you’ll be editing in this tutorial are stored in the remote machines under your home directory /usr/users/cpuecm1/your-username. In order to edit them, you have two options:\nUse a remote text editor, such as vim or nano (Linux users, I’m talking to you!). or,\nDownload the file to your local machine, edit it with your usual code editor and upload it to the remote machine (Windows and MacOS users, I’m talking to you!). The first option should only be chosen by users who are already familiar with command-line editors.\nAs for the other users, keep reading this section.\nMacOS users In order to download a file (say, test.txt) from the home directory of a remote machine in the cluster, you can type the following command on your local machine:\nscp your-username@phome.metz.supelec.fr:~/test.txt .\nThis will copy the file test.txt to your working directory on your local machine.\nOnce you’re done editing test.txt on your local machine, you can upload the file to the remote machine by typing the following command on your local machine:\nscp wc.txt your-username@phome.metz.supelec.fr:~\nIt’s really that easy!\nWindows users Windows users can benefit from a graphical client called WinSCP, that you can download here. Install it, connect to the remote machine and you’ll be able to download/upload files from/to the remote machine by a simple drag-and-drop!\nCreating your working directory in HDFS In this section, you’ll be walked through the procedure to create a directory in HDFS that you’ll use as your working directory in the lab sessions.\nYou user account is: cpuecm1_X, where X is between 1 and 10.\nIn order to create your working directory in HDFS, type the following command in the terminal:\nhdfs dfs -mkdir hdfs://sar01:9000/cpuecm1/cpuecm1_X\nYou can verify that the directory is there by listing the content of the folder hdfs://sar01:9000/cpuecm1/ with the following command:\nhdfs dfs -ls hdfs://sar01:9000/cpuecm1/\nPreliminary exercise The datasets that you’ll be using in this tutorial are available under the folder hdfs://sar01:9000/data/ stored in HDFS. In order to see the content of the directory you can type the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nIn order to get some familiarity with the commands necessary to run Spark programs in the cluster, let’s look at an already implemented example.\nCopy the file ~vialle/DCE-Spark/template_wc.py to your home directory by typing the following command:\ncp ~vialle/DCE-Spark/template_wc.py ./wc.py\nIf you type the command ls, you should see a file named wc.py in your home directory. This file contains the Python code to count the number of occurrences of words in a text file.\nOpen the file wc.py by either using a text editor on the remote machine or by downloading it on your local machine, as explained in the section above.\nLocate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the specified file.\nSimilarly, locate the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction (replace cpuecm1_X with your username!):\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\nRun the Python script wc.py with the following command:\nspark-submit --master spark://sar01:7077 wc.py\nWhen the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command:\nhdfs dfs -ls -h hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\nAs usual, remember to replace cpuecm1_X with your username.\nIn order to see the result, run the following command:\nhdfs dfs -cat hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out/* Output files\nIf you rerun the script by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/cpuecm1/cpuecm1_X/sherlock.out\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56cde29729b5f8b5acea77062bfe0c80","permalink":"/courses/big-data-marseille/overview/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"Connecting to the cluster","type":"docs"},{"authors":null,"categories":null,"content":" The Data Centre d’Enseignement (DCE) is a pool of computing resources that have been financed by the Eurométropole de Metz, Region Grand Est, CentraleSupélec and its foundation, and the Conseil Départemental de Moselle.\nThis tutorial is a quick guide to:\nLearn how to connect to the DCE.\nLearn basic commands to manipulate files stored in HDFS.\nLearn how to run a Spark program on the DCE.\nMore information are available on the DCE official website.\n1 Overview The architecture of the DCE is shown in figure 1.1.\nFigure 1.1: Cluster architecture (image credit: DCE documentation) In this tutorial we only use the CPU nodes. These are divided in two groups: the cluster Sarah and the cluster Kyle.\nTo connect to the DCE you need:\nA valid username and a password. These are provided by your lab supervisor before the first lab session.\nVisual Studio Code with the extension Remote Development installed.\n2 Connection Watch this video to learn how to connect to the DCE with Visual Studio Code.\nSome of the steps shown in the video are explained below:\nWhen you connect to the DCE for the first time, or if you need to re-initialise the connection, you’ll be prompted to enter a SSH command. Type the following (replace your_username with the username that you received from your lab supervisor): ssh your_username@chome.metz.supelec.fr\nAfter executing the command, you’ll be prompted to enter your password.\nOnce you’re connected to chome, open your home folder on chome in Visual Studio Code, as shown in the video.\n3 Allocating resources with slurm You need to allocate computing ressources to run any jobs on the DCE. The command to do so depends on whether you have a reservation for the resources or not.\n3.1 With a reservation The resources are reserved by your lab supervisor before any lab session. Any reservation is given a code.\nGood to know\nThe code is only valid for a single lab session. When the lab session is over, the reservation code might not work any longer.\nThe command to type to allocate the reserved resources is the following (replace code with the reservation code given by your lab supervisor ).\nsrun -N 1 -c 2 --reservation [code] -t 04:00:00 --pty bash\nAfter running the command, you should be connected to one of the Kyle machines.\n3.2 Without a reservation If you don’t have a reservation code, run the following command:\nsrun -p cpu_inter -t 02:00:00 -N 1 --cpus-per-task=2 --pty bash\nGood to know\nIf you don’t have a reservation, please use the DCE only between 6PM-8AM in weekdays. During the weekend, you can allocate resources on the DCE all day.\n4 Running a Spark program The datasets are usually available under directory /data/ stored in HDFS.\nType the following command to look at the content of the directory:\nhdfs dfs -ls -h hdfs://sar01:9000/data\nThe objective is to run a Spark program that counts the number of occurrences of each word in file /data/sherlock.txt.\nCopy the file ~cpu_vialle/DCE-Spark/template_wc.py to your home directory by typing the following command: cp ~cpu_vialle/DCE-Spark/template_wc.py ./wc.py\nType the command ls to verify that the file wc.py is in your working directory. This file contains the Python code of the program. The file should also appear in the Explorer window in Visual Studio Code; if not, click on the Explorer refresh button.\nOpen the file wc.py in Visual Studio Code.\nLocate the following instruction:\ntext_file = sc.textFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following: text_file = sc.textFile(\u0026quot;hdfs://sar01:9000/data/sherlock.txt\u0026quot;)\nThis will create an RDD named text_file with the content of the file.\nSimilarly, locate the following instruction: counts.saveAsTextFile(\u0026quot;hdfs://...\u0026quot;)\nand replace it with the following instruction:\ncounts.saveAsTextFile(\u0026quot;hdfs://sar01:9000/hpda/hpda_X/sherlock.out\u0026quot;)\nThis will create an output directory sherlock.out that will contain the files with the output of the program.\nRun the Python program wc.py with the following command: spark-submit --master spark://sar01:7077 wc.py\nWhen the execution is over, the output will be available under the directory sherlock.out. To verify it, run the following command: hdfs dfs -ls -h hdfs://sar01:9000/hpda/hpda_X/sherlock.out\nAs usual, remember to replace hpda_X with your username.\nIn order to see the result, run the following command: hdfs dfs -cat hdfs://sar01:9000/hpda/hpda_X/sherlock.out/*\nOutput files\nIf you rerun the program by specifying an output file that already exists, you’d get an error. If you really want to overwrite the output file, you first need to remove it explicitly by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/hpda/hpda_X/sherlock.out\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b5cd35c3125b247bbe48e43dcbc2ac8d","permalink":"/courses/bigdata/documentation/cluster-connection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/documentation/cluster-connection/","section":"courses","summary":"How to connect to the cluster","tags":null,"title":"DCE tutorial","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this project is to assess your knowledge of the main notions presented in classroom.\nThe project must be implemented by students working in groups.\nThis project consists in designing a relational database for the given application context, importing the data of a given dataset into a PostgreSQL database and querying the data in SQL.\nApplication context We intend to manage the data of a travel reservation system with clients all over the world. Upon registration, customers are automatically given a numeric identifier and they are asked to indicate their first and family names, their gender, date of birth, a phone number, an email address and their country of residence.\nAny customer can book a trip that includes the reservation of one or more flights and, possibly, one or more hotels.\nExample\nAlice wants to fly from Paris, France to New York City (NYC), USA and she intends to stay in NYC for 10 days. Her trip includes two flights: an outbound flight from Paris to NYC and an inbound flight from NYC to Paris; and an hotel in NYC.\nA flight is operated by an airline company, of which the system keeps its name (e.g., British Airways), the country where the airline is incorporated and, when available, its IATA code (e.g., BA, a two-letter code identifying the airline), its ICAO code (e.g., BAW, a three-letter code identifying the airline) and alternate name or alias (e.g., British).\nA flight connects two airports, each having a name (e.g., London Heathrow Airport), and, possibly, a IATA (e.g., LHR) and ICAO code (e.g., EGLL); an airport serves a specific location (e.g., London, UK) and its precise position is given by its geographic coordinates (latitude and longitude).\nA flight connecting two airports at specific departure and arrival times is identified by a flight number. Two flights operated by two different airline companies cannot have the same flight number, but the same flight number can denote two flights operated by the same airline company on different days.\nExample\nEmirates flight EK074 leaves Paris, France at 10 AM and arrives at Dubai, UAE at 7:40 PM (regardless of the departure day).\nFor each flight booked by a customer, the system keeps the seat number, the travel class (e.g., economy or business), the price and the date of the flight. Usually, airlines include details on the type of aircraft they plan to use on their flight schedules; these details include the name of the aircraft (e.g., Boeing 787-8) and, when available, the IATA code (e.g., 788, a unique three-letter identifier for the aircraft) and the ICAO code (e.g., B788, a unique four-letter identifier for the aircraft).\nThe system maintains a list of hotels, with their names, addresses and an average review score, which is a real number denoting the average grade assigned to the hotel by its customers. Customers can write a review for an hotel; in which case the system stores the text of the review, the date and its author. When a customer books an hotel, the system keeps the price paid, the check-in and check-out dates and whether the breakfast is included.\nDesign of a relational database You’ll now proceed to the definition of a relational database for our travel reservation system. First, you need to define the conceptual schema and then you’ll define the tables that compose the database.\nThe conceptual schema Before defining the logical schema of the database, answer the following questions:\nCan you use the name of the hotel as a primary key? Justify your answer.\nCan you use the flight number as a primary key to identify a flight? Justify your answer and, in case of a negative answer, propose a solution.\nKnowing that it is unlikely that two reviews have the same textual content, would you use it as a primary key? Justify your answer.\nKnowing that the IATA code uniquely identifies an airport, would you choose it as a primary key for the entity Airport? Justify your answer.\nExercise\nExercise 1 Propose an Entity-Relationship diagram describing the conceptual model of a relational database for the given application context.\nSpecify all the attributes for each entity and relation.\nFor each entity, underline the attributes composing the primary key.\nFor each relation, clearly indicate the minimum and maximum cardinality. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85d2d631269d7307bfde5c4b0669c155","permalink":"/courses/databases/exam/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/exam/project/","section":"courses","summary":"Project","tags":null,"title":"Project","type":"docs"},{"authors":null,"categories":null,"content":" The goal of this assignment is to deploy a containerized application to Google Kubernetes Engine (GKE).\nThe lab consists of the following activities:\nGet a Web application from GitHub.\nComplete the Dockerfile of the front-end.\nDeploy and test of the application to your local Kubernetes cluster.\nDeploy the application to the Google Kubernetes Engine (GKE).\nWrite a report describing the application and the deployment procedure. You’ll find detailed information about the report in the last section.\nSubmission information\nYou have a submit a single zip file containing:\nThe report in PDF format.\nThe Dockerfile of the front-end.\nSubmission deadline: March, 22nd, 2024, 11:59 PM.\n1 Get the application You can download the application here.\nIf you have git installed on your computer you can simply open a terminal and type the following command:\ngit clone https://github.com/gquercini/tripmeal-cloud.git\n2 Dockerfile of the front-end The application folder has the same structure as we have seen in the previous labs. You should be able to understand the role of each file and folder.\nExercise\nExercise 2.1 Locate the Dockerfile of the front-end and complete it.\nYou need a Python 3.7 environment to run the application.\n3 Local deployment You need to deploy and test the application TripMeal on a local Kubernetes cluster (either Docker Desktop or Minikube).\nExercise\nExercise 3.1 Build the images of all services of the application.\nNow, specify the names of the images in the file tripmeal.yml.\nExercise\nExercise 3.2 Open the file tripmeal.yml and add the names of the created Docker images to the appropriate fields.\nExercise\nExercise 3.3 Execute the application by typing the following command: kubectl apply -f tripmeal.yml\nWait a minute and then check whether the state of the pods with the following command: kubectl get all\nIf any of the pods is not ready yet, wait a moment and then type the previous command again. If one or more pods are in an error state Check the logs with the following commands: kubectl logs NAME-OF-POD --previous\nIn case you get Exec format error, add to your Dockerfile the following instruction: RUN chmod -x path_to_file_app\nwhere path_to_file_app is the path to the file app.py inside the image. Rebuild the image.\nAfter fixing the error, before executing the application again, run the following command to delete all previously created resources: kubectl delete -f tripmeal.yml\nIf all pods are in running state\nOpen a web browser window.\nType the public IP and port of your application.\nConfirm that the application works correctly.\nAfter confirming that the application works, you can shut down the application.\nkubectl delete -f tripmeal.yml\n4 Deploy to GKE In this section, you’re going to deploy your application to GKE.\nHere are the main steps:\nMake the code available to a public repository so that it can be easily imported into your cloud environment.\nOpen the Google Cloud Shell to access your cloud environment.\nDeploy the application.\nThe next subsections will describe these steps in greater detail.\n4.1 Upload the code to GitHub The easiest way to import your application into your cloud environment is to make it available to a Git repository, such as GitHub or any other Git service for which you already have an account. If necessary, the teacher will guide you in this step.\n4.2 Open the Google Cloud Shell Open the Google Cloud Console. Make sure you log in with your school account.\nYou’ll see in this page an overview of the project created to let you deploy the application to GKE. In particular, you should take note of:\nthe project number (124930651186).\nthe project ID (gq-cloud-computing).\nYou’ll need this information later when you’ll set up the deployment.\nIn the top-right corner of the window, you should see a small icon with a prompt (\u0026gt;). When you click on it, a Google Cloud Shell will open at the bottom of the screen. The shell will be automatically connected to your project environment.\n4.3 Deployment You’ll now have to type few commands in the Google Cloud Shell to deploy the application to Kubernetes.\n4.3.1 Create a repository Create a repository where you’ll be storing the Docker images of your application. The repository is stored in the Artifact Registry, the Google container registry.\nIt is convenient to create an environment variable PROJECT_ID with the following command: export PROJECT_ID=gq-cloud-computing\nVerify that the value of the environment variable is correct: echo $PROJECT_ID\nCreate an environment variable PROJECT_NUMBER whose value is 124930651186.\nCreate an environment variable REPO_NAME whose value is a name of your choice for the container repository.\nCreate an environment variable APP_NAME whose value is a name of your choice for the application.\nCreate an environment variable APP_VERSION whose value is the version of the app. You can choose 1.0 or latest, or any other value of your choice.\nCreate your repository with the following command:\ngcloud artifacts repositories create ${REPO_NAME} \\ --repository-format=docker \\ --location=europe-west2 \\ --description=\u0026quot;Docker repository\u0026quot; 4.3.2 Building the Docker images Download the application from your Git repository with the command git clone.\nBuild and tag the Docker images of your application.\nNames of the images\nYou have to give your images names that are consistent with the names of images stored in the container repository.\nIn the case of your repository, the name of an image should be as follows:\neurope-west2-docker.pkg.dev/${PROJECT_ID}/{$REPO_NAME}/{$APP_NAME}:{$APP_VERSION}\nWarning\nNote: If prompted, authorize Cloud Shell to make Google Cloud API calls.\nVerify that the images have been created with the following command: docker images Add IAM policy bindings (basically, some authorization configurations) to your service account: gcloud artifacts repositories add-iam-policy-binding ${REPO_NAME}$ \\ --location=europe-west2 \\ --member=serviceAccount:{$PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\ --role=\u0026quot;roles/artifactregistry.reader\u0026quot; 4.3.3 Pushing the images to the Artifact registry You must upload the images to a registry so that your GKE cluster can download and run the images. Here we use the repository that we created above.\nConfigure the docker command to authenticate to the Artifact Registry. gcloud auth configure-docker europe-west2-docker.pkg.dev Push the images that you created with the command docker push. Make sure that you use the correct image names, as discussed above. 4.3.4 Creating a GKE cluster You need now to create a Kubernetes cluster using the Google Kubernetes Engine. When you create a cluster, you’ll also specify the option to create the network over which the cluster nodes communicate.\nDefine a new environment variable CLUSTER_NAME whose value is a name of your choice for the cluster.\nDefine a new environment variable NET_NAME whose value is a name of your choice for the network.\nSet your compute engine region.\ngcloud config set compute/region europe-west2 Create the cluster with the following command: gcloud container clusters create-auto ${CLUSTER_NAME} \\ --create-subnetwork name=${NET_NAME} \\ --no-enable-master-authorized-networks \\ --enable-private-nodes Warning\nThe creation of the cluster might take several minutes.\n4.3.5 Deploy the application You’re now ready to deploy your application!\nIn your GitHub repository, modify the file tripmeal.yml to change the names of the images. Use the names of the images that you pushed to the the artifact registry.\nEnsure that you are connected to your GKE cluster with the following command:\ngcloud container clusters get-credentials $CLUSTER_NAME --region europe-west2\nType the following command to deploy your application: kubectl apply -f tripmeal.yml\nWait 30 seconds and then type the following command: kubectl get all\nYou should get some information about the state of the deployment. Keep typing this command until the state of all pods is READY.\nGet the public IP address where your application is available and the port number and type them in your browser to confirm that you can access and use the application. IMPORTANT: SHUT DOWN THE APPLICATION AND REMOVE THE RESOURCES\nShut down the application by typing the following command: kubectl delete -f tripmeal.yml\nDelete the Kubernetes cluster by typing the following command: `` gcloud container clusters delete $CLUSTER_NAME –region europe-west2\nDelete ALL the images from the container registry with the following command: gcloud artifacts docker images delete \\ NAM_OF_IMAGE \\ --delete-tags --quiet\n5 Report In the report you need to include the following elements.\nA high-level description of the application. What is the application intended for?\nThe architecture of the application. How are files organized? How many services is the application composed of? What is the meaning of each file?\nThe technologies used in the application. Which programming languages are used for each service? Which libraries and databases?\nIn which file can you get the information about the port number of the front end?\nLook at the file tripmeal.yml. Can you explain the content of this file? Can you explain the meaning of each object created in this file? In particular explain: what is a service and a deployment? What is a stateful set? Why is a deployment used and for which service? Why is a stateful set used and for which service?\nDon’t hesitate to add figures to your report, should you need them to better explain the different notions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cdf8f266cd4c1772053e1bdbe4a8db56","permalink":"/courses/cloudalbert/tutorials/cloud-deploy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloudalbert/tutorials/cloud-deploy/","section":"courses","summary":"GKE","tags":null,"title":"Deploying to GKE","type":"docs"},{"authors":null,"categories":null,"content":" Source: Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach\nDans ce TP, nous allons jeter un coup d’œil rapide au protocole DHCP. Rappelons que le DHCP est largement utilisé dans les réseaux locaux câblés et sans fil des entreprises, des universités et des particuliers pour attribuer dynamiquement des adresses IP aux hôtes, ainsi que pour configurer d’autres informations relatives au réseau.\n1 Utilisation de DHCP pour configurer une interface réseau Dans cette section, nous forçons notre ordinateur à reconfigurer l’une de ses interfaces réseau à l’aide de DHCP. La manière pour ce faire dépend du système d’exploitation utilisé.\nSi vous travaillez sous Windows\nOuvrez une fenêtre de PowerShell et entrez la commande suivante : ipconfig /release Cette commande permet à votre PC d’abandonner son adresse IP.\nDémarrez la capture Wireshark.\nDans PowerShell, entrez la commande suivante :\nipconfig /renew Le protocole DHCP demandera et recevra alors une adresse IP et d’autres informations d’un serveur DHCP.\nAprès avoir attendu quelques secondes, arrêtez la capture Wireshark. Si vous travaillez sous macOS\nDémarrez la capture Wireshark.\nDans les paramètres réseau sélectionnez “Avancé” puis “Renouveler le bail dhcp”. Le protocole DHCP demandera et recevra une adresse IP et d’autres informations du serveur DHCP.\nAprès avoir attendu quelques secondes, arrêtez la capture Wireshark.\nSi vous travaillez sous Linux\nDans une fenêtre de terminal, entrez les commandes suivantes : sudo ip addr flush en0 sudo dhclient -r Cette commande supprimera l’adresse IP existante de l’interface et libérera tous les baux d’adresses DHCP existants. en0 (dans cet exemple) est l’interface sur laquelle vous souhaitez capturer des paquets à l’aide de Wireshark. Vous pouvez facilement trouver la liste des noms d’interface dans Wireshark en choisissant Capture -\u0026gt; Options.\nDémarrez Wireshark, en capturant des paquets dans l’interface que vous avez déconfigurée à l’étape 1.\nDans la fenêtre de terminal, entrez la commande suivante :\nsudo dhclient en0 où, comme ci-dessus, en0 est l’interface sur laquelle vous capturez actuellement des paquets. Le protocole DHCP demandera et recevra alors une adresse IP et d’autres informations du serveur DHCP.\nAprès avoir attendu quelques secondes, arrêtez la capture Wireshark. Après avoir arrêté la capture Wireshark à l’étape 4, vous devriez jeter un coup d’œil dans votre fenêtre Wireshark pour vous assurer que vous avez bien capturé les paquets que nous recherchons.\nEntrez dhcp dans le champ du filtre d’affichage pour n’afficher que les paquets DHCP.\n2 Questions sur DHCP Commençons par examiner le message DHCP Discover.\nLocalisez le datagramme IP contenant le premier message Discover dans votre trace.\nExercise\nExercise 2.1 Ce message DHCP Discover est-il envoyé en utilisant UDP ou TCP comme protocole de transport sous-jacent ?\nExercise\nExercise 2.2 Quelle est l’adresse IP source utilisée dans le datagramme IP contenant le message Discover ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez pourquoi.\nExercise\nExercise 2.3 Quelle est l’adresse IP de destination utilisée dans le datagramme contenant le message Discover ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.\nExercise\nExercise 2.4 Quelle est la valeur du champ ID de transaction de ce message DHCP Discover ?\nExercise\nExercise 2.5 Examinez maintenant le champ des options du message DHCP Discover. Quelles sont les informations (en plus d’une adresse IP) que le client suggère ou demande de recevoir du serveur DHCP dans le cadre de cette transaction DHCP ?\nPour connaître la signification des options, vous pouvez consulter cette page.\nExaminons maintenant le message DHCP Offer.\nLocalisez dans votre trace le datagramme IP contenant le message DHCP Offer qui a été envoyé par un serveur DHCP en réponse au message DHCP Discover que vous avez étudié dans les questions précédentes.\nExercise\nExercise 2.6 Comment savez-vous que ce message Offer est envoyé en réponse au message DHCP Discover que vous avez étudié dans les questions précédentes ?\nExercise\nExercise 2.7 Quelle est l’adresse IP source utilisée dans le datagramme IP contenant le message Offer ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.\nExercise\nExercise 2.8 Quelle est l’adresse IP de destination utilisée dans le datagramme contenant le message d’offre ? Cette adresse présente-t-elle des particularités ? Expliquez.\nRegardez attentivement votre trace. La réponse à cette question peut différer de ce que vous avez vu en cours.\nSi vous voulez vraiment approfondir cette question, consultez la RFC 2131, page 24.\nExercise\nExercise 2.9 Examinez maintenant le champ des options dans le message d’offre DHCP. Quelles sont les informations que le serveur DHCP fournit au client DHCP dans le message d’offre DHCP ?\nIl semblerait qu’une fois le message d’offre DHCP reçu, le client dispose de toutes les informations dont il a besoin pour continuer.\nCependant, le client peut avoir reçu des offres de plusieurs serveurs DHCP et une deuxième phase est donc nécessaire, avec deux autres messages obligatoires - le message DHCP Request du client au serveur et le message DHCP ACK du serveur au client. Mais au moins, le client sait qu’il existe au moins un serveur DHCP !\nExaminons le message DHCP Request, en nous rappelant que, bien que nous ayons déjà vu un message Discover dans notre trace, ce n’est pas toujours le cas lorsqu’un message DHCP Request est envoyé.\nLocalisez le datagramme IP contenant le premier message DHCP Request dans votre trace, et répondez aux questions suivantes.\nExercise\nExercise 2.10 Quel est le numéro de port source UDP dans le datagramme IP contenant le premier message DHCP Request de votre trace ?\nQuel est le numéro de port de destination UDP utilisé ?\nExercise\nExercise 2.11 Quelle est l’adresse IP source dans le datagramme IP contenant ce message de requête ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.\nExercise\nExercise 2.12 Quelle est l’adresse IP de destination utilisée dans le datagramme contenant ce message ? Cette adresse présente-t-elle des particularités ? Expliquez.\nExercise\nExercise 2.13 Quelle est la valeur du champ ID de transaction de ce message DHCP Request ? Correspond-elle aux ID de transaction des messages Discover et Offer précédents ?\nExercise\nExercise 2.14 Examinez maintenant le champ des options dans le message DHCP Discover et regardez attentivement la “Parameter Request List”. La RFC 2131, page 24 indique que le client peut informer le serveur des paramètres de configuration qui l’intéressent en incluant l’option “parameter request list”. La partie données de cette option énumère explicitement les options demandées. Quelles différences voyez-vous entre les entrées de l’option “parameter request list” dans ce message Request et la même option de liste dans le message Discover précédent ?\nIdentifiez le datagramme IP contenant le premier message DHCP ACK et répondez aux questions suivantes.\nExercise\nExercise 2.15 Quelle est l’adresse IP source du datagramme IP contenant ce message ACK ? Cette adresse a-t-elle quelque chose de particulier ? Expliquez.\nExercise\nExercise 2.16 Quelle est l’adresse IP de destination utilisée dans le datagramme contenant ce message ACK. Cette adresse a-t-elle quelque chose de particulier ? Expliquez.\nExercise\nExercise 2.17 Quel est le nom de la proprieté du message DHCP ACK qui contient l’adresse IP assignée au client ?\nExercise\nExercise 2.18 Pour combien de temps (le “bail”) le serveur DHPC a-t-il attribué cette adresse IP au client ?\nExercise\nExercise 2.19 Quelle est l’adresse IP (renvoyée par le serveur DHCP au client DHCP dans ce message DHCP ACK) de la passerelle par défaut ?\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e31e33239178082fc3ab07f88a5a892c","permalink":"/courses/network/labs/dhcp-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/network/labs/dhcp-lab/","section":"courses","summary":"dhcp-usecase","tags":null,"title":"Etude de cas: DHCP","type":"docs"},{"authors":null,"categories":null,"content":" The Sakila management wants us to migrate their database to Cassandra, they need to compare how Cassandra would compare against MongoDB. To this extent, your first task is to think of a data model for the new database.\nWhen designing your data model, you’ll two follow two basic rules:\nYour data model should distribute the data evenly across the nodes of the cluster.\nFor each frequent query, the number of partition to read should be as minimum as possible.\nExercise\nExercise 1 Analyze the Sakila business domain and propose a series of queries that the Sakila administration might want to run against the database.\nExercise\nExercise 2 Given the queries that you proposed in the first exercise, draw a workflow of the application that the Sakila management will use to query the data.\nExercise\nExercise 3 Based on the application workflow, draw the Chebotko diagram (the logical model) of the Sakila database.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1bbacc0f95fe09b4371ba7cb4c295c7c","permalink":"/courses/gadexed/tutorials/cassandra-data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/cassandra-data-modeling/","section":"courses","summary":"Data modeling in Cassandra","tags":null,"title":"Data modeling in Cassandra","type":"docs"},{"authors":null,"categories":null,"content":" Setting up the environment Download Cassandra from this address.\nMove the downloaded archive file to a folder of your choice (other than downloads).\nExtract the archive. This will create a folder, containing all the files with the Cassandra server.\nOpen a terminal and go the the Cassandra folder.\nRun the Cassandra server with the following command ./bin/cassandra -f.\nOnce the initialization is complete, open a new terminal and go again to the Cassandra folder.\nOpen the CQL shell, by typing the following command: ./bin/cqlsh.\nOnce you enter the CQL shell, you can start interacting with the Cassandra server to create a new database.\nImport the data Download the data here\nExtract the downloaded archive. This will create a new folder containing the data. Let’s refer to this folder as DATA_FOLDER.\nGo back to the CQL shell and type the following command source 'DATAFOLDER/sakila.cql' (replace DATAFOLDER with the full path to the data folder). This command will create the sakila keyspace (database).\nIn the CQL shell, type the command use sakila;\nWe have 11 tables in this database. You can see the definition with the command describe sakila.\nAs the last step, we need to import the data. No luck here, we need to import the data table by table with the following commands:\nCOPY sakila.actors_by_film FROM \u0026#39;DATAFOLDER/actors_by_film.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.categories_by_film FROM \u0026#39;DATAFOLDER/categories_by_film.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.customers FROM \u0026#39;DATAFOLDER/customers.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.film FROM \u0026#39;DATAFOLDER/film.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.inventory FROM \u0026#39;DATAFOLDER/inventory.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.rentals_by_customer FROM \u0026#39;DATAFOLDER/rentals_by_customer.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.rentals_by_film FROM \u0026#39;DATAFOLDER/rentals_by_film.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.rentals_by_staff FROM \u0026#39;DATAFOLDER/rentals_by_staff.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.rentals FROM \u0026#39;DATAFOLDER/rentals.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.staff FROM \u0026#39;DATAFOLDER/staff.csv\u0026#39; WITH HEADER = TRUE ; COPY sakila.store FROM \u0026#39;DATAFOLDER/store.csv\u0026#39; WITH HEADER = TRUE ; If during the import you get the error ‘too many open files’ close the shell with exit and start it again.\nQueries The CQL language used by Cassandra is highly similar to SQL, but there are some key differences. Here we analyze few of them.\nExercise\nExercise 1 Consider the table rentals_by_customer and try some queries that use (not all in the same query):\nThe WHERE condition.\nAggregating functions and the GROUP BY clause.\nThe SELECT JSON statement.\nFeel free to refer to the CQL documentation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"46ffb4259d4f7cb38feadc0b3a2de999","permalink":"/courses/gadexed/tutorials/cassandra-queries/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/cassandra-queries/","section":"courses","summary":"Queries in Cassandra","tags":null,"title":"Queries in Cassandra","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you will learn basic modeling strategies for a data warehouse.\nUse case scenario We intend to set up a data warehouse for the Sakila database that we used in Tutorial 3.\nWe suppose that a DVD rental store chain (let’s call it Sakila) maintains several operational database systems at their different stores, and the management intends to have a single point of truth, in order to check business trends. Hence, the idea of setting up a data warehouse.\nIn order to model a schema for the Sakila data warehouse, we need to identify dimensions and facts. To this extent, we first identify the business questions that the Sakila management wants to be answered.\nExercise\nExercise 1 List some of the business questions that the Sakila management would like to answer.\nSolution\nDifferent sets of questions are possible. Here are some examples.\nWhich store generates the most revenue? Which customers have rented the most in the past year? (maybe to award fidelity points). Which customers have rented the least in the past year? (maybe to offer some discounts to encourage the customer to rent again). Which staffer has processed the most of the rentals in the past year? (staffer of the year). Which staffer has processed the least of the rentals in the past year? (maybe a layoff in sight?). Which month has the highest revenue? By country, by store? Which country generates the least revenue? (is there any concurrent out there that might be soon a threat in other countries too?) Which categories of films are most popular? We recall here the conceptual model of the Sakila database.\nFigure 1: The conceptual schema of the Sakila database Exercise\nExercise 2 Based on the business questions identified in the first exercise, can you tell what the fact and the dimension tables are?\nRecall that the fact table should contain measures and the dimension tables should contain the context of the facts. The dimension table should answer the following questions about a fact: who, what, when, where.\nSolution\nFrom the questions that we identified in the previous exercise, it is clear that we intend to use the data warehouse to answer questions about the rentals.\nTherefore, our fact table will be fact_rental.\nAs for the dimensions:\nQuestion 1. suggests the use of the dimension store (where).\nQuestions 2., 3. suggest the use of the dimension customer (who).\nQuestions 4., 5. suggest the use of the dimension staff (who).\nQuestion 8. suggests the use of the dimension film (what).\nAll questions suggests the use of a time dimension (when).\nThe time dimension is virtually always present in a data warehouse.\nDimensional modeling Now that we identified the fact and the dimension tables, we incrementally draw the star diagram for the Sakila data warehouse.\nExercise\nExercise 3 Draw a first sketch of the star diagram. Do not specify any attributes in the tables. Solution\nExercise\nExercise 4 Identify the primary key of the fact table.\nSolution\nThe primary key of the fact table is composed of all the attributes that refer to the dimensions. So, we write an attribute for each dimension:\nstaff_pk, foreign key to the dimension dim_staff.\nfilm_pk, foreign key to the dimension dim_film.\nperiod_pk, foreign key to the dimension dim_period.\nstore_pk, foreign key to the dimension dim_store.\ncustomer_pk, foreign key to the dimension dim_customer.\nHere is the new schema.\nExercise\nExercise 5 For each dimension table, there is a corresponding table in the operational database.\nDiscuss which attributes you would add to the dimension tables.\nIn particular, consider the following points:\nAre you going to add to a dimension table the\nprimary key of the corresponding operational table?\nAre you going to add to a dimension table attributes that are not part of the corresponding table?\nSolution\nEach dimension table already has a primary key. However, this is the surrogate key that identifies each entity in the dimension table. The business key that is in the corresponding operational table is still a valuable attribute to add. So, for example, in the table dim_film we’ll add the attribute film_id that is the primary key in the operational table film.\nIn each dimension table, we typically add all the attributes that are in the corresponding operational table. However, we also add attributes that are in the linked tables, if they’re necessary to our analysis. For example, in the operational database the category of a film is\nin a separate table film_category (as a result of the normalization process). If we decide to integrate these dimensions and keep a normalized schema, we obtain a snowflake schema.\nHowever, in a data warehouse we tend to denormalize the dimension tables, to avoid to incur the cost of joining tables.\nOne last remark on the attribute film_categories in table dim_film. The value of this attribute is a list. One might want to avoid this by using a one-hot encoding. In other words, if the set of all possible categories is small, we can have one boolean attribute for each category; a True value would indicate that the film is in the corresponding category.\nExercise\nExercise 6 In our schema, the fact table is factless: it only contains a primary key with no measurements.\nWhich measurements would you introduce?\nSolution\nIt all depends on the queries that we intend to ask. In the queries that we identified at the beginning of the tutorial, we were interested in the revenue generated by the rentals. Also, we might want to keep track of the number of rentals of a given film by a given customer in the given period, and the number of returns.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e0f51988875bad75f93f80cf12979fe4","permalink":"/courses/gadexed/tutorials/data-warehouse/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/data-warehouse/","section":"courses","summary":"Data modeling for data warehousing","tags":null,"title":"Data warehousing","type":"docs"},{"authors":null,"categories":null,"content":" Use case scenario The Sakila database is serving an increasing number of queries from staff and customers around the world. A single monolithic database is not sufficient anymore to serve all the requests and the company is thinking of distributing the database across several servers (horizontal scalability). However, a relational database does not handle horizontal scalability very well, due to the fact that the data is scattered across numerous tables, as are result of the normalization process. Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to MongoDB.\nFor the migration to happen, it is necessary to conceive a suitable data model. From the first discussions with the Sakila management, you quickly understand that one of the main use of the database is to manage (add, update and read) rental information.\nAnalysis of the existing model The existing data model is recalled in the following figure.\nFigure 1: The conceptual schema of the Sakila database Exercise\nExercise 1 Determine the cardinality (one-to-one, one-to-many, many-to-many) of each of the relationships in which the entity Rental is involved.\nN.B. Don’t hesitate to look at the attributes of each entity in the existing PostgreSQL database.\nSolution\nThe entity Rental has a relationship with the following tables;\nInventory. A rental refers to a single inventory (essentially, a copy of a DVD). An inventory might be part of several rentals. This is a one-to-many relationship.\nStaff. A rental is taken care of by a single staff member, A staff member might take care of several rentals. This is a one-to-many relationship.\nCustomer. A rental is made by a single customer. A customer might make several rentals. This is a one-to-many relationship.\nPayment. A payment is relative to a single rental. A rental is associated with a single payment. This is a one-to-one relationship.\nExercise\nExercise 2 Look at the tables Rental and Customer in the PostgreSQL Sakila database.\nEstimate the size of a row in bytes in both tables.\nThe following considerations will help you in the task.\nThe storage size of a numeric data type is clearly indicated in the PostgreSQL documentation.\nThe storage size of date/time types is clearly indicated in the PostgreSQL documentation\nThe columns with type text hold UTF-8 characters. We assume that each character is 1 byte long (although some characters might need more than 1 byte).\nThe columns with type boolean needs 1 byte storage.\nWe assume that an email address is 25 characters long on average.\nWe assume that a last name is 7 characters long on average.\nWe assume that a first name is 6 characters long on average.\nIn both tables we ignore the columns last_update and create_date.\nIn table Customer we ignore the column active. Solution\nFor the table Rental we have:\nColumn rental_id, integer, 4 bytes.\nColumn rental_date, timestamp with time zone, 8 bytes.\nColumn inventory_id, integer, 4 bytes.\nColumn customer_id, smallint, 2 bytes.\nColumn return_date, timestamp with time zone, 8 bytes.\nColumn staff_id, smallint, 2 bytes.\nIn total, a row in table Rental needs 28 bytes of storage.\nFor the table Customer we have:\nColumn customer_id, integer, 4 bytes.\nColumn store_id, smallint, 2 bytes.\nColumn first_name, text, 6 bytes (average).\nColumn last_name, text, 7 bytes (average).\nColumn email, text, 25 bytes (average).\nColumn address_id, smallint, 2 bytes.\nColumn activebool, boolean, 1 byte.\nIn total, a row in table Customer needs 47 bytes of storage.\nConsiderations for the new model We need to take some decisions as to the new data model. The considerations that we made in the previous exercises will lead us to the right decisions.\nExercise\nExercise 3 How would you model in MongoDB the entities Rental and Payment, given the cardinalities that you identified in the previous section?\nSolution\nThe relationship between the two given entities is one-to-one. Therefore, we can use a denormalized schema in MongoDB.\nThat is, we can create one single collection to store the information about the two entities.\nWe have two options:\nWe create a collection Payment, where each document contains the attributes of a payment and an embedded document with the details of the rental the payment refers to.\nWe create a collection Rental, where each document contains the attributes of a rental and an embedded document with the details of the payment for the rental.\nAlthough both options are perfectly valid, we prefer the second one, as rentals are our first-class citizens in our database.\nWe can also consider the attributes of a payment as attributes of a rental, without creating an embedded document for the payment.\nExercise\nExercise 4 Suppose that we create a collection Customer and we embed in each customer document the list of rentals for that customer.\nHow many rentals can we store at most for a given customer, knowing that the size of a document cannot exceed 16 MB?\nSolution\nWe’ve seen before that each rental needs 28 bytes of space. To make the computation easier, we round this size up to 32 bytes (a power of two). Considering that 16 MB = \\(16 \\times 2^{20}\\), the maximum number of rentals that we can store for a given customer is given by:\n\\[ \\frac{16 \\times 2^{20}}{32} = 2^{-1} \\times 2^{20} = 2^{19} \\]\nThis gives around 600,000 rentals.\nExercise\nExercise 5 In our current database we have:\n599 customers.\n16044 rentals.\nOn average, each customer has around 27 rentals.\nCompute the size in byte of the two following collections:\nCollection Customer, where each document contains the information about a customer and an embedded list with the information on all the rentals made by the customer.\nCollection Rental, where each document contains the information about a rental and an embedded document with the information on the customer that made the rental. Solution\nWe previously found out that for each customer we need 47 bytes and for each rental we need 28 bytes.\nA document that holds the data about a customer and the list of all the rentals of the customer will need \\(47 + 27 \\times 28 = 830\\) bytes.\nHence, the total size of the Customer collection is \\(803 \\times 599 = 480977\\) bytes, that is 470 KB.\nA document that holds the data about a rental and its customer needs $ 28 + 47 = 75 $ bytes.\nHence, the total size of the Rental collection is \\(75 \\times 16044 = 1203300\\) bytes, that is 1,1 MB.\nExercise\nExercise 6 Discuss advantages and disadvantages of the two following options:\nA collection Customer with a document for each customer holding the list of all the rentals of the customer.\nA collection Rental with a document for each rental holding the data on the relative customer. Solution\nAdvantages of solution 1.:\nThere is no redundancy. In fact, a rental is relative to at most one customer, therefore the data on a rental is not duplicated across different documents.\nAs a result, the size of the collection is smaller than the collection in solution 2.\nFor each customer, we retrieve the information on all his/her rentals with only one read operation\nDisadvantages of solution 1.:\nThere is a limit (albeit an acceptable one) on the number of rentals that we can store for each customer.\nWe lose a “rental-centric” view of our data. As a result, if any other document in another collection (e.g., staff) refers to a rental, all the information about a rental must be denormalized in that document.\nAdvantages of solution 2.\nA “rental-centric” view of our data. Aggregating information from different rentals does not require digging rentals out of several lists.\nThe size of each document is small and will never exceed the 16 MB limit.\nAs a result, reading a document from the collection takes less time and memory than reading a document from the collection in solution 1.\nDisadvantages of solution 2.\nThere is a lot of redundancy. The information about a customer are replicated each time the customer makes a rental.\nAs a result, the size of the collection is much higher than in solution 1.\nIt seems that one of the two solutions has a higher storage demands than the other, and therefore the odds seems to be stacked against that solution.\nHowever….\nWe still have to consider two more entities that are linked to the rentals: staff and inventory.\nExercise\nExercise 7 Discuss how you can fit staff and inventory in each of the solutions presented in the previous exercise. Discuss advantages and disadvantages of each option that you present.\nSolution\nSolution 1\nWe need to somehow link staff and inventory to the relative rental. We have three options:\nWe embed staff and inventory into each rental document, which, let’s recall it, is already embedded in an array. This creates redundancy, as a staff member or an inventory item can appear in more than one rental.\nWe create three separate collections (Customer, Staff and Inventory) and in each we embed an array with the list of the relative rentals. The problem is that the data on the rentals are now replicated three times. This solution is particularly bad, because rentals are frequently written. When we create a rental, we need to write three documents; when a customer returns an item, we need to update the return date in three documents.\nWe create four collections (Customer, Staff, Inventory and Rentals); for each customer, staff and inventory we keep a list of rentals, each item being the identifier that refers to the appropriate rental. We fall back to the normalized schema of the PostgreSQL database. Then, it isn’t clear how this normalized schema will help horizontally scale the database.\nSolution 2\nWe have only one collection Rental; in each document, we have an attribute customer, whose value is an embedded document with all the information about a customer, an attribute staff, whose value is an embedded document with all information about a staff member, and an attribute inventory, whose value is an embedded document with all information about an inventory item.\nThis solution has higher storage requirements than the options presented in solution 1, but it has the clear advantage of being a denormalized schema, where we control every facet of a rental (customer, staff, inventory). Moreover, when we create a rental, we only write one document ; when we update the return date of a rental, we only update one document.\nFrom the previous exercise, we have a clearer idea as to the best solution to our case. We take a closer look at the storage requirements of the adopted solution. Consider that:\nThe size in byte of a document storing the information of a staff member is around 84000 bytes (we also store a profile picture).\nThe size in byte of a document storing the information of an inventory item is 16 bytes.\nExercise\nExercise 8 If we denote by \\(N_{rental}\\) the number of rentals, what is the size in bytes of the whole database for the adopted solution?\nSolution\nLet’s recall that the size in bytes of a document storing the information on a customer is 47 bytes.\nThe size of the only collection Rental (hence, of the whole database) is:\n\\[ N_{rentals} \\times (47 + 84000 + 16) = N_{rentals} \\times 84063 \\]\nWith 10,000 rentals, the size of the database is 800 MB. With 100,000 rentals, the size of the database is 7 GB. With 1,000,000 rentals, the size of the database is 78 GB.\nAlthough the size that we determined in the previous exercise, may not sound impressive, we still have to store other information (films, actors….). If we could save a bit of space, we would be happy.\nExercise\nExercise 9 Discuss how you could save some space in the adopted solution.\nHINT. Do you really need to denormalize all data?\nSolution\nWhile modeling data for a MongoDB database, the choice between a normalized and a denormalized schema does not need to be a black and white one.\nWe can have a denormalized schema, where we embed in the same documents information that are queried together, while storing in a separate collection the information that are rarely queried.\nFor instance, the profile picture of a staff member might not be an important piece of information while we’re trying to analyze which staff members are the more productive ones.\nSo, we might add another collection Staff where each document only contains attributes that are not in embedded documents in the collection Rental.\nExercise\nExercise 10 Propose a solution for all the entities involved and estimate the saving in terms of storage requirements.\nSolution\nHere is a solution with three different collections. The information on inventory are completely denormalized into the collection Rental. For customers and staff members we only keep the necessary information and we normalize the information that are less likely to be queried while analyzing the rentals.\nLet’s try to estimate the savings in terms of storage. In a document of the collection Rental, we have the following attributes:\nrental_id: 4 bytes.\nrental_date and return_date: 16 bytes.\ninventory_id: 4 bytes\nfilm_id and store_id: 4 bytes\ncustomer_id: 4 bytes\ncustomer first and last names: 13 bytes.\ncountry: 9 bytes (on average a country name has 9 characters).\nstaff_id: 4 bytes\nstaff first and last name: 13 bytes.\nIn total a document in our collection Rental will have 71 bytes (a huge improvement wrt the 84000 bytes of our first solution).\nOf course, the size of a document in the Staff collection will be around 84000 bytes (we still store the profile picture). However, the number of staff members \\(N_{staff}\\) is much lower than the number of rentals \\(N_{rentals}\\).\nIt is easy to verify that the size of a document of the collection Customer is 34 bytes (same information as before except first and last name). Again, the number of customers \\(N_{customers}\\) is much lower than \\(N_{rentals}\\).\nWe assume that each customer has on average 30 rentals and for 100 customers we have 1 staff member. We have that:\n$N_{customer} = N_{rental}/30 $\n$N_{staff} = N_{customer}/100 = N_{rental}/3000 $\nSo, the size of the database will be given by:\n\\[ 71\\times N_{rentals} + 84000 \\times N_{rentals}/3000 + 34 \\times N_{rentals}/30 = 100 \\times N_{rental} \\]\nIf we want to compare against the first solution:\nWith 10,000 rentals, the size of the database is 976 KB (instead of 800 MB). With 100,000 rentals, the size of the database is 9,5 MB (instead of 7 GB). With 1,000,000 rentals, the size of the database is 95 MB (instead of 78 GB). As you can see, a big improvement! And we have a denormalized schema that lets us take advantage of the horizontal scalability of MongoDB.\nThe new model In this section we intend to obtain a complete model of the Sakila database.\nExercise\nExercise 11 Consider the model that we obtained at the end of the previous section. Which data can you further denormalize?\nSolution\nCollection Staff\nThe attribute address_id refers to a full address. We can fully denormalize this information into the documents of the collection.\nThe attribute store_id refers to the store where the staff member works. The Store table in the original PostgreSQL database does not have too many columns. Therefore, it might be reasonable to fully denormalize these data. However, information on the stores are also linked to customers and to inventory items. If we need to update, say, the manager of a store, we would need to update three different documents. Hence, we prefer to create a collection Store and keep in the documents of the collection Staff only the city and country of a store (and its identifier of course).\nAttribute Inventory in collection Customer\nSame considerations for the attribute store_id.\nThe attribute film_id is the reference to the film the inventory item is relative to. The table Film in the original PostgreSQL database contains 14 columns. This high number of attributes advises us against a full denormalization, considering that a film can be relative to multiple inventory items. We might only keep the film title and the film categories. The rest of the attributes are kept in documents of a separate collection Film.\nCollection Customer\nSame considerations for the attributes store_id and address_id.\nExercise\nExercise 12 Complete the diagram obtained in the previous exercise so as to obtain a full data model for the Sakila database.\nSolution\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"04a283ce7d4305dbfa85000556910018","permalink":"/courses/gadexed/tutorials/mongodb-data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/mongodb-data-modeling/","section":"courses","summary":"Data modeling in MongoDB","tags":null,"title":"Data modeling in MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn to write basic and advanced queries in MongoDB.\nGet the data Download the this archive file and extract it. You’ll find a file for each collection to import into the database.\nOpen MongoDB Compass, create a new database named sakila and create the different collections while importing the data.\nBasic queries Exercise\nExercise 1 Write the following queries in MongoDB:\nReturn all the information on all customers.\nReturn the email of all customers.\nReturn the email of the customers of Canadian stores.\nReturn the identifier of all rentals made by customers from Iran, where the amount paid is strictly greater than 10 dollars.\nReturn the first and last names of the actors who played a role in film 213.\nSolution\ndb.customer.find() db.customer.find({}, {email:1}) db.customer.find({\"store.country\": \"Canada\"}, {email:1}); db.rental.find({\"customer.country\": \"Iran\", amount: {$gt: 10}}, {rental_id: 1, _id:0}); db.film.find({film_id: 213}, {\"actors.first_name\":1, \"actors.last_name\": 1}).sort({\"actors.last_name\": -1}); Operations on arrays Exercise\nExercise 2 Write the following queries in MongoDB:\nReturn the identifier of the films that have “Behind the Scenes” as special features.\nReturn the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.\nReturn the identifier of all the films where BURT POSEY played a role.\nReturn the identifier of the film that has exactly 15 actors.\nSolution\ndb.film.find({special_features : {$elemMatch: {$eq: \"Behind the Scenes\"}}}, {film_id: 1, _id:0}); db.film.find({special_features : {$all: [\"Commentaries\", \"Deleted Scenes\"]}}, {film_id: 1, _id:0}); db.film.find({\"actors.first_name\": \"BURT\", \"actors.last_name\": \"POSEY\"}, {film_id: 1, _id:0}); db.film.find({actors: {$size : 15}}, {film_id: 1, _id:0}); Aggregation framework Exercise\nExercise 3 Write the following queries in MongoDB using the aggregation framework:\nReturn the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)\nCount the total amount of payments across all rentals.\nReturn the number of actors of each film.\nSort the films by the number of actors (decreasing order).\nReturn the average number of actors for each film.\nReturn the identifier of the customer who made the most of rentals.\nReturn the first and last name of the customer who made the most of rentals.\nReturn the country where the customers have rented the most of the films in the category “Music”.\nSolution\ndb.rental.aggregate({$match: {\"customer.first_name\": \"TOMMY\", \"customer.last_name\": \"COLLAZO\"}}, {$project: {\"inventory.film.title\": 1, _id:0}}) One can also express this query with the function find() db.rental.find({\"customer.first_name\": \"TOMMY\", \"customer.last_name\": \"COLLAZO\"}, {\"inventory.film.title\": 1, _id:0}); db.rental.aggregate({$group: {\"_id\": null, total_amount: {$sum: \"$amount\"}}}) db.film.aggregate({$project: {nb_actors: {$size: \"$actors\"}}}) If we don’t put the match condition, we get an error because for some films the field actors is not defined.\ndb.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, {$project: {film_id: 1, \"nb_actors\": {$size: \"$actors\"}}}, {$sort: {nb_actors: -1}}) db.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, {$project: {film_id: 1, \"nb_actors\": {$size: \"$actors\"}}}, {$group: {_id: null, avg_actors: {$avg: \"$nb_actors\"}}}) db.rental.aggregate({$group: {_id: \"$customer.customer_id\", count: {$sum: 1}}}, {$sort: {count: -1}}) db.rental.aggregate({$group: {_id: {cust_id: \"$customer.customer_id\", cust_first_name: \"$customer.first_name\", cust_last_name: \"$customer.last_name\"}, count: {$sum: 1}}}, {$sort: {count: -1}}, {$limit :1}) db.rental.aggregate({\\(match: {\u0026quot;inventory.film.categories.category\u0026quot;: \u0026quot;Music\u0026quot;}}, {\\)group: {_id: \u0026quot;\\(customer.country\u0026quot;, count: {\\)sum: 1}}}, {\\(sort:{count: -1}}, {\\)limit: 1})\nJoin Operations Exercise\nExercise 4 Write the following queries in MongoDB using the aggregation framework:\nReturn the language of the film with title “ACE GOLDFINGER”.\nReturn all the information about the staff member who took care of rental 2.\nSolution\ndb.rental.aggregate({$match: {\"inventory.film.title\": \"ACE GOLDFINGER\"}}, {$lookup: {from: \"film\", localField: \"inventory.film.film_id\", foreignField:\"film_id\", as:\"film\"}}, {$project: {\"film.language\": 1}}, {$limit : 1}) db.rental.aggregate({$match: {rental_id: 2}}, {$lookup: {from: \"staff\", localField: \"staff.staff_id\", foreignField:\"staff_id\", as:\"staff_member\"}}, {$project: {\"staff_member\": 1}}) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a613c6a1ca8c120630f0e794fdd055c3","permalink":"/courses/gadexed/tutorials/mongodb-queries/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/mongodb-queries/","section":"courses","summary":"Queries in MongoDB","tags":null,"title":"Queries in MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" Setting up the work environment. Download Neo4j Desktop and install it on your computer.\nCreate a new project by clicking on the button “New” that you’ll find on the top left side of the window.\nClick on “Add Database”, then “Create a Local Database”.\nGive the database a name (e.g., MovieLens) and set a password that you can easily remember; then click on “Create”. Choose version 4.4.8 for the database.\nClick on “Start” and wait for the database to become active.\nClick on the button “Open”. The Neo4j Browser will pop up.\nIn the next section, you’ll have to type a sequence of commands to import the data. You’ll write the commands in the text field on top of the Neo4j Browser (where you find the prompt neo4j$).\nImport the data. The dataset consists of data obtained from MovieLens, a recommender system whose users give movies a rate between 1 and 5, based on whether they dislike or love them. MovieLens uses the rates to recommend movies that its users might like. The dataset is modeled as a directed graph and consists of 100,004 rates on 9,125 movies across 671 users between January 9th, 1995 and October 16, 2016. The dataset also contains the names of the directors and the actors of each movie.\nImport the nodes corresponding to the movies (label Movie) by using the following command (it took 31 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies.csv' as row MERGE (m:Movie {movie_id: toInteger(row.movie_id), title_en:row.movie_title_en, title_fr:row.movie_title_fr, year: toInteger(row.movie_year)}) RETURN count(m) Create an index on the property movie_id of the nodes with label Movie with the following command: create index movie_idx for (m:Movie) on (m.movie_id) Import the nodes corresponding to the actors (label Actor) by using the following command (it took 62 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/actors.csv' as row MERGE (a:Actor {actor_id: toInteger(row.actor_id), name:row.actor_name}) RETURN count(a) Create an index on the property actor_id of the nodes with label Actor with the following command: create index actor_idx for (a:Actor) on (a.actor_id) Import the nodes corresponding to the directors (label Director) by using the following command (it took 4 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/directors.csv' as row MERGE (d:Director {director_id: toInteger(row.director_id), name:row.director_name}) RETURN count(d) Create an index on the property director_id of the nodes with label Director with the following command: create index director_idx for (d:Director) on (d.director_id) Import the nodes corresponding to the genres (label Genre) by using the following command (it took 197 ms on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/genres.csv' as row MERGE (g:Genre {genre_id: toInteger(row.genre_id), name:row.genre_name}) RETURN count(g) Create an index on the property genre_id of the nodes with label Genre with the following command: create index genre_idx for (g:Genre) on (g.genre_id) Import the nodes corresponding to the users (label User) by using the following command (it took 347 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/users.csv' as row MERGE (u:User {user_id: toInteger(row.user_id), name:row.user_nickname}) RETURN count(u) Create an index on the property user_id of the nodes with label User with the following command: create index user_idx for (u:User) on (u.user_id) Import the links of type ACTED_IN between actors and movies with the following command (it took 2.5 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_actors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (a:Actor {actor_id: toInteger(row.actor_id)}) MERGE (a)-[r:ACTED_IN]-\u003e(m) RETURN count(r) Import the links of type DIRECTED between directors and movies with the following command (it took 688 ms on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_directors.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (d:Director {director_id: toInteger(row.director_id)}) MERGE (d)-[r:DIRECTED]-\u003e(m) RETURN count(r) Import the links of type HAS_GENRE between movies and genres with the following command (it took 1 second on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/movies_genres.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (g:Genre {genre_id: toInteger(row.genre_id)}) MERGE (m)-[r:HAS_GENRE]-\u003e(g) RETURN count(r) Import the links of type RATED between users and movies with the following command (it took 5.9 seconds on my computer): :auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM 'https://gquercini.github.io/courses/plp/tutorials/neo4j/user_rates.csv' as row MATCH (m:Movie {movie_id: toInteger(row.movie_id)}) MATCH (u:User {user_id: toInteger(row.user_id)}) MERGE (u)-[r:RATED {rate:toFloat(row.rate)}]-\u003e(m) RETURN count(r) Exploratory queries If you looked at the commands used to import the data, you might already have an idea as to the structure of the graph. You can get a glimpse on the node labels, the relationship types and the property keys by clicking on the button circled in the following figure:\nExercise Exercise 1 Write and execute the following query:\nMATCH (m:Movie {title_en:\"Toy Story\"}) RETURN m; What do you obtain? What are the properties associated to a node with label Movie? Click once on the node to display its properties.\nSolution\nThe requested node is displayed in the Neo4j Browser. By clicking on the node, we see that the properties are: movie_id, title_en, title_fr, year.\nExercise Exercise 2 Double-click on the node displayed as the result of the previous query. Analyze the neighbouring nodes (their labels and properties) and the incident links (direction, type and properties). You can move around the node by dragging it in the window.\nSolution\nFrom the interface, we see that the movie Toy Story is rated by 90 users, has 5 genres, 4 actors and 1 director. The following observations can be made:\nEach node with label User has two properties, user_id and name.\nEach node with label Genre has two properties, genre_id and name.\nEach node with label Director has two properties, director_id and name.\nEach node with label Actor has two properties, actor_id and name.\nA relationship of type RATED has a property rate and is directed from a node with label User to a node with label Movie.\nA relationship of type DIRECTED is directed from a node with label Director to a node with label Movie.\nA relationship of type HAS_GENRE is directed from a node with label Movie to a node with label Genre.\nA relationship of type ACTED_IN is directed from a node with label Actor to a node with label Movie.\nQueries Exercise Exercise 3 Write and execute the following queries:\nQ1. The genres of the movies in the database.\nQ2. The number of movies in the database.\nQ3. The title of the movies released in 2015.\nQ4. The number of directors by movie. Sort in decreasing order.\nQ5. The names of the directors and the title of the movies that they directed and in which they also played.\nQ6. The genres of the movies in which Tom Hanks played.\nQ7. The title and the rate of all the movies that the user with identifier 3 rated. Sort by rate in decreasing order. Solution\nQ1. MATCH (g:Genre) RETURN g.name Q2. MATCH (n:Movie) RETURN count(n) Q3. MATCH (n:Movie {year:2015}) RETURN n.title_en; Q4. MATCH (n:Movie)\u003c-[:DIRECTED]-(d:Director) RETURN n.title_en, count(*) AS nb_directors ORDER BY nb_directors DESC Q5. MATCH (a:Actor)-[:ACTED_IN]-\u003e(m:Movie)\u003c-[:DIRECTED]-(d:Director) WHERE a.name=d.name RETURN a.name, m.title_en; Q6. MATCH (:Actor {name:\"Tom Hanks\"})-[:ACTED_IN]-\u003e(:Movie)-[:HAS_GENRE]-\u003e(g:Genre) RETURN DISTINCT (g.name); Q7. MATCH (n:User {user_id:3})-[r:RATED]-\u003e(m:Movie) RETURN m.title_en, r.rate ORDER BY r.rate desc; Query chaining Cypher allows the specification of complex queries composed of several queries that are concatenated with the clause WITH. We are now going to see an example to obtain the titles of the movies that have been rated by at least 100 users.\nAt a first glance, the following query looks like a good solution:\nMATCH (n:Movie)\u003c-[:RATED]-(u:User) WHERE count(u) \u003e= 100 RETURN n.title_en LIMIT 5; However, executing this query returns the following error:\nInvalid use of aggregating function count(...) in this context (line 1, column 42 (offset: 41)) \"MATCH (n:Movie)\u003c-[:RATED]-(u:User) WHERE count(u) \u003e= 100\" Similarly to SQL, we cannot use aggregating functions in the clause WHERE.\nA correct formulation of the query requires the use of the clause WITH to concatenate two queries: the first will count the number of rates for each movie:\nMATCH (n:Movie)\u003c-[:RATED]-(u:User) RETURN n, count(u) as nb_rates The second will take in the output of the first and will filter all the movies where nb_rates \u0026lt; 100. In order to chain the two queries, we’ll replace the RETURN clause in the first query with a WITH clause, as follows:\nMATCH (n:Movie)\u003c-[:RATED]-(u:User) WITH n, count(u) as nb_rates WHERE nb_rates \u003e= 100 RETURN n.title_en Exercise Exercise 4 Write and execute a query to obtain the five movies that obtained the best average rate among the movies that have been rated by at least 100 users.\nSolution\nMATCH (n:Movie)\u003c-[r:RATED]-(u:User) WITH n, avg(r.rate) as avg, count(u) AS nb_rates WHERE nb_rates \u003e= 100 RETURN n.title_en, avg ORDER BY avg DESC; Movie recommendation We are now going to see how Neo4j can be effectively used in a real application by implementing queries that form the basis of a simple movie recommendation system. This system is based on the notion of collaborative filtering.\nThis consists in recommending a user \\(u\\) some films that s/he hasn’t rated yet and other users with similar preferences have loved. In our context, we say that a user loves a movie if s/he rated the movie at least 3.\nThis concept is explained in the following figure.\nThe user \\(u\\) loves 6 movies, 3 of which are also loved by the user \\(v\\) (the black nodes); it is reasonable to think that \\(u\\) may also love the two movies that \\(v\\) loved and \\(u\\) hasn’t rated yet.\nThe principle of collaborative filtering is based on the computation of a similarity score between two users. Several similarity scores are possible in this context; here, we are going to use the Jaccard coefficient. Let \\(L(u)\\) and \\(L(v)\\) be the sets of movies that \\(u\\) and \\(v\\) love respectively; the similarity score \\(J(u,v)\\) between \\(u\\) and \\(v\\) is given by:\n\\[ J(u, v) = \\frac{|L(u) \\cap L(v)|}{|L(u) \\cup L(v)|} \\]\nIn order to recommend movies to a target user \\(v\\), the recommender system computes the similarity score between \\(v\\) and all the other users of the system and proposes to \\(v\\) the movies that s/he hasn’t rated yet and that the \\(k\\) most similar users loved.\nWe are now going to incrementally write a query to recommend some movies to the target user 3. The first step consists in determining the value \\(|L(v)|\\).\nExercise Exercise 5 Write and execute the query to obtain the number of movies that the user 3 loved. This query must return the target user and the number of movies that s/he loves.\nSolution\nMATCH (target:User {user_id:3})-[r:RATED]-\u003e(m:Movie) WHERE r.rate \u003e= 3 RETURN target, count(m) AS lovedByTarget; Next, we are going to determine the value \\(|L(u)|\\), for all users \\(u\\) except \\(v\\).\nExercise Exercise 6 Write and execute the query to obtain the number of movies that each user \\(u\\) loves, except the target user 3. This query must return each user \\(u\\) and the number of movies that s/he loves.\nSolution\nMATCH (other:User)-[r:RATED]-\u003e(m:Movie) WHERE other.user_id \u003c\u003e 3 and r.rate \u003e= 3 RETURN other, count(m) as lovedByOther We put the two queries together with the clause WITH.\nExercise Exercise 7 Compose the two previous queries with the clause WITH. This query must return the target user 3, the number of movies that s/he loves, the other users \\(u\\) and the number of movies that they love.\nSolution\nMATCH (target:User {user_id:3})-[r:RATED]-\u003e(m:Movie) WHERE r.rate \u003e= 3 WITH target, count(m) AS lovedByTarget MATCH (other:User)-[r:RATED]-\u003e(m:Movie) WHERE other \u003c\u003e target and r.rate \u003e= 3 RETURN target, lovedByTarget, other, count(m) as lovedByOther Now, we need to determine the value \\(L(u)\\cup L(v)\\), for each user \\(u\\), and compute the similarity score with the Jaccard coefficient.\nExercise Exercise 8 Append (by using WITH) to the query written in the previous exercise a query that obtains the number of movies that any user \\(u\\) loved and that the target user 3 loved too, and computes the similarity score between the target user 3 and \\(u\\). This query must return the five most similar users to the target user and the similarity scores.\nHint Multiply the numerator of the equation by 1.0, otherwise Cypher will compute an integer division. Solution\nThe following is the query written at the previous exercise, where the RETURN clause has been replaced with WITH.\nMATCH (target:User {user_id:3})-[r:RATED]-\u003e(m:Movie) WHERE r.rate \u003e= 3 WITH target, count(m) AS lovedByTarget MATCH (other:User)-[r:RATED]-\u003e(m:Movie) WHERE other \u003c\u003e target and r.rate \u003e= 3 WITH target, lovedByTarget, other, count(m) as lovedByOther We have to append the following query:\nMATCH (target)-[r:RATED]-\u003e(m:Movie)\u003c-[r1:RATED]-(other) WHERE r.rate \u003e= 3 and r1.rate \u003e= 3 RETURN other, count(m)*1.0 / (lovedByTarget + lovedByOther - count(m)) as sim ORDER BY sim DESC LIMIT 5 The last step consists in recommending some movies to the target user. From the previous query, take the identifier of the user \\(w\\) with the highest similarity to the target user. You are going to use this identifier directly in the new query.\nExercise Exercise 9 Write and execute the query to obtain the list of the movies that the user \\(w\\) loved and that the target user hasn’t rated yet. Sort this list by decreasing rate.\nHint\nFirst, write a query to obtain the list of the movies that the target user rated. In the MATCH clause, use the variable \\(m\\) to indicate a movie that the target user rated. Conclude the query with: RETURN collect(m.title_en) AS movies The function collect creates a list called movies.\nReplace RETURN with WITH in the previous query and add a second query to select the titles of the movies \\(m\\) that the user \\(w\\) loved and the target user did not rate. In order to exclude the films that the target user did not rate, use the following predicate: none(x in movies where x=m.title_en) in the WHERE clause.\nSolution\nThe following is the query written at the previous exercise, where the RETURN clause has been replaced with WITH.\nMATCH (target:User {user_id:3})-[r:RATED]-\u003e(m:Movie) WITH collect(m.title_en) as movies MATCH (u:User {user_id:129})-[r:RATED]-\u003e(m:Movie) WHERE r.rate \u003e=3 AND none(x IN movies WHERE x=m.title_en) RETURN m.title_en ORDER BY r.rate DESC; ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6366e58486d6b6b25b071d1accc8c5c6","permalink":"/courses/gadexed/tutorials/neo4j/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/neo4j/","section":"courses","summary":"Queries in Neo4j","tags":null,"title":"Neo4j","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to obtain a non-redundant set of functional dependencies. How to determine the candidate keys of a table given its functional dependencies. How to determine the normal form of a table. Question 1 We consider a relational table \\(R\\) with four columns \\(A\\), \\(B\\), \\(C\\) and \\(D\\). Let \\(\\mathcal{F}\\) be the following set of functional dependencies:\n\\(AB \\rightarrow C\\)\n\\(D \\rightarrow BC\\)\n\\(A \\rightarrow B\\)\nExercise\nExercise 1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).\nSolution\nWe first need to write the functional dependencies in canonical form (the right-side of each FD must consist of only one column).\nThe FD (2.) can be rewritten using the decomposition axiom.\n\\(AB \\rightarrow C\\)\n\\(D \\rightarrow B\\)\n\\(D \\rightarrow C\\)\n\\(A \\rightarrow B\\)\nNext, we need to make sure that all functional dependencies are left-irreducible.\nAll functional dependencies except the first one is trivially left-irreducible (the determinant consists of only one column). The first has two columns in the determinant, therefore we need to check whether we can eliminate one of the two columns and still preserve an equivalent set of functional dependencies.\nWe apply Armstrong’s axioms to compute the closure of this set of functional dependencies.\nFrom (1.) and (4.), we can apply the pseudotransitivity axiom and we obtain:\n\\[A \\rightarrow B \\wedge AB \\rightarrow C \\implies AA \\rightarrow C \\implies A \\rightarrow C\\]\nSince \\(A \\rightarrow C\\) is in the cover, the column \\(B\\) in FD (1.) is useless and can therefore be omitted.\nThe set \\(\\mathcal{G}\\) consists of the following FDs:\n\\(A \\rightarrow C\\)\n\\(D \\rightarrow B\\)\n\\(D \\rightarrow C\\)\n\\(A \\rightarrow B\\)\nNone of these functional dependencies are redundant.\nQuestion 2 Let \\(R\\) be a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\(AB \\rightarrow C\\) \\(C \\rightarrow A\\) \\(C \\rightarrow B\\) \\(C \\rightarrow D\\) \\(D \\rightarrow E\\) Exercise\nExercise 2 Specify the candidate keys of the table \\(R\\).\nSolution\nA candidate key is a set of columns that imply all the other columns.\nFirst, let’s try sets composed of only one column: \\(\\{A\\}\\), \\(\\{B\\}\\), \\(\\{C\\}\\), \\(\\{D\\}\\) and \\(\\{E\\}\\).\nWe have the following: (\\(\\{X\\}^+_{\\mathcal{F}}\\) indicates the set of all columns implied by \\(X\\)).\n\\(\\{A\\}^+_{\\mathcal{F}} = \\{A\\}\\)\n\\(\\{B\\}^+_{\\mathcal{F}} = \\{B\\}\\)\n\\(\\{C\\}^+_{\\mathcal{F}} = \\{A, B, C, D, E\\}\\)\n\\(\\{D\\}^+_{\\mathcal{F}} = \\{D, E\\}\\)\n\\(\\{E\\}^+_{\\mathcal{F}} = \\{E\\}\\)\nTherefore, \\(\\{C\\}\\) is a candidate key because it implies all the other columns.\nFrom the functional dependency (1), we obtain that \\(AB\\) implies \\(C\\); therefore, by transitivity they imply all the other columns.\nIn conclusion, we have two candidate keys: \\(\\{C\\}\\) and \\(\\{A, B\\}\\).\nExercise\nExercise 3 We assume that \\(R\\) is in 1NF.\nIs table \\(R\\) in 2NF? Justify your answer.\nIs table \\(R\\) in 3NF? Justify your answer.\nIs table \\(R\\) in BCNF? Justify your answer. Solution\n\\(R\\) is in 2NF. In fact, all non-prime columns depend entirely on both candidate keys.\n\\(R\\) is not in 3NF. In fact, the functional dependency \\(D \\rightarrow E\\) is between two non-prime columns.\nAs a result, \\(R\\) is not in BCNF either.\nExercise\nExercise 4 If the table \\(R\\) from the previous exercise is not in BCNF, how would you change the schema so that BCNF is satisfied? For each table, specify the primary key and the foreign keys linking the tables to each other.\nSolution\nThe table is not in 3NF. The offending functional dependency is \\(D \\rightarrow E\\).\nWe need to create a new table \\(R_1\\), where the primary key is the determinant in the offending functional dependency (\\(D\\)). We then move all columns that are dependent on \\(D\\) (only \\(E\\) in our case) from \\(R\\) to \\(R_1\\). Note that \\(D\\) is kept in \\(R\\) so that we can use it to link \\(R\\) with \\(R_1\\).\nIn summary:\n\\[R = \\{A, B, C, D\\}\\]\n\\[R_1 = \\{D, E\\}\\]\nThe primary key of \\(R\\) is \\(\\{C\\}\\) (or, we can also choose \\(\\{A, B\\}\\) if we want). The primary key of \\(R_1\\) is \\(\\{D\\}\\).\nThe column \\(D\\) in \\(R\\) is a foreign key referencing the column \\(D\\) in \\(R_1\\).\nQuestion 3 Let \\(R\\) be a relational table with four columns \\((A, B, C, D)\\). \\(\\{A, B\\}\\) is the only candidate key.\nExercise\nExercise 5 Identify a minimal set of functional dependencies. Justify your answer.\nSolution\nA candidate key implies all the other columns of the table. Therefore we have :\n\\[AB \\rightarrow C\\] \\[AB \\rightarrow D\\]\nBoth functional dependencies are in canonical form (the right side only consist of one column). Trivially, both functional dependencies are left-irreducible (we cannot remove any of the columns in the determinant without losing information). So, this is a minimal set of functional dependencies.\nExercise\nExercise 6 Add \\(B \\rightarrow D\\) to the set of functional dependencies that you identified in the previous exercise. Modify the minimal set of functional dependencies accordingly. Justify your answer.\nSolution\nWe consider the following functional dependencies:\n\\(AB \\rightarrow C\\) \\(AB \\rightarrow D\\) \\(B \\rightarrow D\\) The FD (3) clearly makes FD (2) redundant. By using the augmentation axiom on (3) we obtain:\n\\(AB \\rightarrow AD\\) By applying the decomposition axiom on (4) we obtain:\n\\(AB \\rightarrow A\\)\n\\(AB \\rightarrow D\\)\nWhich shows that (2) is redundant.\nIn summary, we have the following functional dependencies:\n\\[AB\\rightarrow C \\] \\[B\\rightarrow D \\]\nExercise\nExercise 7 We assume that \\(R\\) is in 1NF.\nIs table \\(R\\) in 2NF? Justify your answer.\nIs table \\(R\\) in 3NF? Justify your answer.\nIs table \\(R\\) in BCNF? Justify your answer. Solution\n\\(R\\) is not in 2NF. In fact, the non-prime column \\(C\\) is functionally dependent on only one part of the candidate key.\n\\(R\\) is not in 3NF, let alone BCNF, because it doesn’t fulfill the first condition, that is being in 2NF.\nExercise\nExercise 8 Normalize \\(R\\) to the BCNF form. Justify your choices.\nSolution\nThe functional dependency that gives a partial dependency is: \\(B \\rightarrow D\\). We create a new table \\(R_1\\), where the determinant (\\(B\\)) of the offending functional dependency is the primary key. We move the columns that depend on \\(B\\) (here, only \\(D\\)) from table \\(R\\) to table \\(R_1\\).\nIn summary:\n\\[R = (A, B, C)\\]\n\\[R_1 = (B, D)\\]\nWe can see that \\(R\\) is in 3NF because:\nThe only non-prime column is \\(C\\), therefore there cannot be any dependency between non-prime columns. \\(R\\) is in BCNF because:\nThe only functional dependency \\(AB \\rightarrow C\\) has a key as its determinant. Similarly, for \\(R_1\\).\nQuestion 4 We consider the following table:\nPatient (ssn, first_name, last_name, phone_number, insurance_number, insurance_expiration_date) where the following set \\(\\mathcal{F}\\) of functional dependencies holds:\nssn -\u0026gt; first_name, last_name, phone_number, insurance_number, insurance_expiration_date insurance_number -\u0026gt; insurance_expiration_date Exercise\nExercise 9 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\). Solution\nFirst, we need to rewrite the FDs in canonical form.\n\\(ssn \\rightarrow first\\_name\\) \\(ssn \\rightarrow last\\_name\\) \\(ssn \\rightarrow phone\\_number\\) \\(ssn \\rightarrow insurance\\_number\\) \\(ssn \\rightarrow insurance\\_expiration\\_date\\) \\(insurance\\_number \\rightarrow insurance\\_expiration\\_date\\) The determinant of each FD is composed of only one column, therefore it is already left-irreducible. It is easy to see that all FDs with ssn as a determinant must be kept (otherwise we lose some information).\nBy transitivity from 4. and 6. we obtain:\n\\[ssn \\rightarrow insurance\\_expiration\\_date\\]\nTherefore, \\(\\mathcal{G}\\) is obtained from \\(\\mathcal{F}\\) by removing 5.\nExercise\nExercise 10 Given \\(\\mathcal{G}\\), identify the candidate keys in the table Patient. Solution\nFrom the functional dependencies in \\(\\mathcal{F}\\), it’s easy to see that that the only column that implies all the others is ssn. Therefore, {ssn} is the only candidate key in this table.\nExercise\nExercise 11 Specify the normal form of the table Patient. Justify your answer. Solution\nIt is immediate to verify that the table is 1NF.\nThe table is 2NF because there is only one candidate key, which is composed of only one column. Therefore, there cannot be any partial dependency.\nThe table is not in 3NF. Indeed, there is a functional dependency between two non-prime columns:\n\\[insurance\\_number \\rightarrow insurance\\_expiration\\_date\\]\nExercise\nExercise 12 How would you normalize table Patient to BCNF? Justify your answer. Solution\nThe offending functional dependency is:\n\\[insurance\\_number \\rightarrow insurance\\_expiration\\_date\\]\nWe need to create another table (let’s call it Insurance) that contains the determinant of the offending functional dependency (insurance_number) as its primary key. We need to move the colum (insurance_expiration_date) that depends on insurance_number from table Patient to table Insurance.\nIn summary:\nPatient (ssn, first_name, last_name, phone_number, insurance_number) Insurance (insurance_number, insurance_expiration_date) Note that the column insurance_number in table Patient is foreign key to the column insurance_number in table Insurance.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"35fe9e284966e1fb96db406a664544bb","permalink":"/courses/gadexed/tutorials/normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/normalization/","section":"courses","summary":"Normalization theory exercises","tags":null,"title":"Normalization theory","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model. 1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1 Give the conceptual schema of the database with an ER diagram.\nSolution\nExercise\nExercise 1.2 Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\nSolution\nThe collection of tables is the following:\nUserAccount (nickname, first_name, last_name, city, country) Post (post_id, content, date, time, lat, long, nickname) EmailAddress (email_address, nickname) Relationship (nickname_src, nickname_dst, type, date) Tag (post_id, nickname) The foreign keys are the following:\nPost(nickname) → UserAccount(nickname).\nEmailAddress(nickname) → EmailAddress(nickname).\nRelationship(nickname_src) → UserAccount(nickname).\nRelationship(nickname_dst) → UserAccount(nickname).\nTag(post_id) → Post(post_id).\nTag(nickname) → UserAccount(nickname).\n2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\nFigure 2.1: The conceptual schema of the bank database Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer. Solution\nSince no two banks have the same code_bank or name, either property can be chosen as the primary key of the entity Bank. Both can be considered as valid candidate keys.\nExercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer. Solution\nThe answer is no. While there aren’t any banks that have the same value for {code_bank, name}, two subsets ({code_bank} and {name}) are candidate keys.\nExercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises. Solution\nExercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. Solution\nThe collection of tables is the following:\nBank (code_bank, name, address) Branch (branch_id, address, code_bank) Account (acct_nbr, acct_type, balance, branch_id, ssn) Loan (loan_nbr, loan_type, amount, branch_id, ssn) Customer (ssn, first_name, last_name, telephone, address) The foreign keys are the following:\nBranch(code_bank) → Bank(code_bank).\nAccount(branch_id) → Branch(branch_id).\nAccount(ssn) → Customer(ssn).\nLoan(branch_id) → Branch(branch_id).\nLoan(ssn) → Customer(ssn).\n3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, color, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\nA salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys. When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram. Solution\nExercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. Solution\nThe collection of tables is the following:\nCar (serial_number, make, model, color, is_new) Customer (cust_id, cust_first_name, cust_last_name, cust_phone) Invoice (invoice_number, date, price, car_serial_number, sp_id, cust_id) Salesperson (sp_id, sp_first_name, sp_last_name) Mechanic (mec_id, mec_first_name, mec_last_name) Ticket (ticket_number, date_open, date_return, car_serial_number) Repair (ticket_number, mec_id) The foreign keys are the following:\nInvoice(cust_id) → Customer(cust_id).\nInvoice(car_serial_number) → Car(serial_number).\nInvoice(sp_id) → Salesperson(sp_id).\nTicket(car_serial_number) → Car(serial_number).\nRepair(ticket_number) → Ticket(ticket_number).\nRepair(mec_id) → Mechanic(mec_id).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7b0aae78fcc25e4bcc51ddd1e6472217","permalink":"/courses/gadexed/tutorials/rel-data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/rel-data-modeling/","section":"courses","summary":"Relational data modeling exercises","tags":null,"title":"Relational data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nThe key integrity constraints in a relational database. Basic SQL queries. Advanced SQL queries. Installing PostgreSQL In this tutorial you’ll be using PostgreSQL as a relational database management system (RDBMS).\nInstructions for Windows users\nYou can download the latest version of PostgreSQL (14.3) from this page.\nThe installer contains:\nThe PostgreSQL server. pgAdmin, a graphical administration tool. You’ll find detailed installation instructions on this page.\nInstructions for macOS users\nThe best way to get PostgreSQL is to download and install Postgres.app. You’ll find more details on this page.\nIn addition, you need to install pgAdmin, ad administration tool that lets you interact with your database through a graphical interface.\nStarting PostgreSQL and pgAdmin Instructions for Windows users\nOnce the installation is completed, a PostgreSQL server should be automatically started.\nYou’ll open a connection to the server through the following steps:\nExecute pgAdmin.\nOn the left-side menu, right-click on Servers and select Create server.\nIn the General tab, give the server a name of your choice.\nIn the Connection tab, write localhost as host. Don’t change the values of the other options.\nWhat if I get the Unable to connect to server error?\nYou might want to have a look at the list of the services running on your computer to verify whether the PostgreSQL server is actually running.\nThis page explains you how to open the panel with the list of services. You should locate PostgreSQL in this panel. If the service is not running, you’ll need to manually start it.\nInstructions for MacOS users\nYou’ll open a connection to the server through the following steps:\nExecute pgAdmin.\nOn the left-side menu, right-click on Servers and select Create server.\nIn the General tab, give the server a name of your choice.\nIn the Connection tab, write localhost as host. Don’t change the values of the other options.\nObtain the data We consider the database of a DVD rental store chain containing data on films, actors, customers and the transactions of the store.\nThis database goes by the name Sakila\nand was developed by Mike Hillyer, a former member of the MySQL team.\nFigure 1: The conceptual schema of the Sakila database The tables of the database are documented on this page\nSakila has been ported from MySQL to PostgreSQL under the name pagila.\nIn order to import the data into PostgreSQL, follow the steps below:\nFrom the pgAdmin interface, create a new database (right-click on Databases, select Create database). You can call the new database pagila (or any name you want).\nDownload the dump file pagila-insert-data.sql This file contains all the SQL statements necessary to create the tables of the database and populate them with some data.\nRight-click on the new database, select Query tool.\nClick on the second (from the left) small icon on the top menu of the query tool to open a new file.\nThrough the interface, locate pagila-schema.sql and open it.\nHit on the F5 button or click on the play icon on the top menu of the query tool to execute the script.\nThe previous steps should have created the schema of the database\nand inserted the data.\nYou can see the list of tables by selecting (on the left menu) Schemas, public, Tables (15 tables should appear).\nOpen a new query tool to do the exercises.\nIntegrity constraints Exercise\nExercise 1 Execute the following statement:\ninsert into film_actor values(1, 25) What is this statement supposed to do? What is the reaction of the DBMS?\nSolution\nThis statement should insert a new row in table film_actor, where the value of actor_id is 1 and the value of film_id is 25.\nThe DBMS returns an error because there is already a row with these values and the two columns film_actor, film_id form the primary key.\nExercise\nExercise 2 Write the statement to delete the film with film_id=1 from the table Film.\nExecute the command. What happens?\nSolution\ndelete from film where film_id=1 The statement is rejected because there are rows in other tables that reference the row that we intend to delete. This is the effect of the foreign key constraint.\nExercise\nExercise 3 Look at the definition of the foreign key constraints in table film_actor (right-click on the table, select Constraints, foreign key).\nIs the definition of the foreign key constraint to table film coherent with the behavior observed in the previous exercise?\nNB In order to see the options of a foreign key, click on the edit button on the left of the constraint. Then look at the tab Action.\nSolution\nThe foreign key linking table film_actor to table film is defined with the option RESTRICT on delete. This is coherent with the behavior that we observed in the previous exercise. Referenced rows cannot be deleted if there are still referencing rows.\nExercise\nExercise 4 Execute the following query:\nSELECT f.film_id as film_id_in_table_film, fa.film_id AS film_id_in_table_film_actor, fa.actor_id as actor_id, f.title as title FROM film f JOIN film_actor fa ON f.film_id = fa.film_id WHERE f.title='ACE GOLDFINGER' What does the query? Note down the identifier of the film in both tables film and film_actor. (columns film_id_in_table_film and film_id_in_table_film_actor).\nSolution\nThe query returns the list of all actors in the film titled Ace Goldfinger. We note that the identifier of the film in both tables is identical (2), as it should because the query joins the two tables on the equality of these two values.\nExercise\nExercise 5 Write and execute a statement to set the value 10000 to the identifier of the film ACE GOLDFINGER in table Film.\nAfter the modification, execute the query of the previous exercise. What changed? Explain in details what happened.\nSolution\nThe statement to modify the film_id of the given film is as follows:\nUPDATE film SET film_id=10000 WHERE title=‘ACE GOLDFINGER’ After executing the same query as the previous exercise, we see that the identifier of the film has changed in the table film_actor too. This is expected, because the foreign key constraint between the colum film_id in table film_actor and the column film_id in table film has the option ON UPDATE CASCADE. This means that if we modify the identifier of the film in table film, the modification is propagated to all the referencing columns.\nExercise\nExercise 6 Execute the following statement:\nUPDATE film_actor SET film_id=2 WHERE film_id=10000 What does? What happens? Explain.\nSolution\nThe statement intends to set the identifier of the film titled Ace Goldfinger (in the previous exercise we gave it the identifier 10000) back to its original value. However, we execute the statement on the table film_actor. The action is not allowed, as the identifier 2 does not correspond to any film in table film.\nThe foreign key enforces the referential integrity constraint. A row cannot refer to a non-existing entity in the referenced table.\nBasic queries Exercise\nExercise 7 Write the following SQL queries:\nReturn all the information on all customers.\nReturn the first and last name of all customers.\nReturn the first and last name of all customers of the store with identifier 1. Solution\nselect * from customer select first_name, last_name from customer select first_name, last_name from customer where store_id=1 Sorting and paginating Exercise\nExercise 8 Write the following SQL queries:\nReturn the last and first name of all customers. Sort by last name in ascending order.\nSame as in 1., but only return the first 100 customers.\nReturn the last and first name of all customers of the store with identifier 1. Sort by last name in ascending order and by first name in descending order. Solution\nselect last_name, first_name from customer order by last_name asc select last_name, first_name from customer order by last_name asc limit 100 select first_name, last_name from customer where store_id=1 Aggregating queries Exercise\nExercise 9 Write the following SQL queries:\nCount the number of films in the database (expected result: 1000).\nHow many distinct actor last names are there?\nCompute the total amount of payments across all rentals (expected result: 67416.51).\nCompute the average, minimum and maximum duration of rental across all films (expected result: 4.9850000000000000, 3, 7).\nReturn the number of actors for each film.\nReturn the number of copies of each film in each store (table inventory).\nSame as 6., but only returns the pairs (film, store) if the number of copies is greater than or equal to 3.\nSolution\nselect count(*) from film select count (distinct last_name) from actor select sum(amount) from payment select avg(rental_duration), min(rental_duration), max(rental_duration) from film select film_id, count(*) as nb_actors from film_actor group by film_id select film_id, store_id, count(*) as nb_films from inventory group by film_id, store_id select film_id, store_id, count(*) as nb_films from inventory group by film_id, store_id having count(*) \u003e=3 Join queries Exercise\nExercise 10 Write the following SQL queries:\nReturn the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).\nReturn the first and last name of the actors in the film ACE GOLDFINGER.\nReturn first and last name of each actor and the number of films in which they played a role.\nSame as in 3., but order by number of films in descending order.\nSame as in 4., but only return actors who played a role in at least 10 films.\nReturn the identifier, the first and family name of the customers who have rented between 10 and 20 movies in the category Family. Solution\nselect first_name, last_name from staff join store using(store_id) where store_id=1 select first_name, last_name from film join film_actor using(film_id) join actor using(actor_id) where title='ACE GOLDFINGER' select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id order by nb_films desc select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id having count(*) \u003e= 10 order by nb_films desc select cust.customer_id, first_name, last_name, count(*) as nb_films from customer cust join rental using(customer_id) join inventory using(inventory_id) join film_category using(film_id) join category cat using(category_id) where cat.name='Family' group by customer_id having count(*) between 5 and 10 Miscellaneous queries Exercise\nExercise 11 Write the following SQL queries:\nWhich last names are not repeated in table actor?\nIs a copy of the movie ACADEMY DINOSAUR available for rent from store 1?\nReturn the title and the release year of all films in one of the following categories: Family, Animation, Documentary.\nTip You can use the operator IN Find all customers (id, last name, first name) whose last name starts with the letter L. Tip You can use the operator LIKE Return the total paid by each customer. For each customer, display a single column containing first and last name and another column with the total amount paid. Order the result alphabetically by last name Tip You can use the operator CONCAT Return the total revenue from the rentals across the stores in each country. Order by descending revenue.\nThe first and last name of the actor that played in the most films. If two or more actors are tied, the query must return the names of all of them. Solution\nselect last_name from actor group by last_name having count(*) = 1 select distinct i.inventory_id from film f join inventory i using(film_id) join rental r using(inventory_id) where f.title='ACADEMY DINOSAUR' and i.store_id=1 and r.return_date is not null select distinct f.title, f.release_year from film f join film_category using(film_id) join category cat using(category_id) where cat.name in ('Family', 'Animation', 'Documentary') select customer_id, first_name, last_name from customer where last_name LIKE 'L%' select concat(first_name, ' ', last_name), sum(amount) from customer join payment using (customer_id) group by customer_id order by last_name asc select country, sum(amount) as revenue from payment join rental using (rental_id) join inventory using (inventory_id) join store using (store_id) join address using (address_id) join city using (city_id) join country using (country_id) group by country_id order by revenue desc select actor_id, first_name, last_name, count() from film_actor join actor using(actor_id) group by actor_id having count() = (select max(nb_films) from (select count(*) as nb_films from film_actor group by actor_id) t) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e2ac3e94b52829fb2d2a46f7dd64ebb8","permalink":"/courses/gadexed/tutorials/sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/gadexed/tutorials/sql/","section":"courses","summary":"Exercises on SQL","tags":null,"title":"SQL","type":"docs"},{"authors":null,"categories":null,"content":" Source: Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach\nL’objectif de ce TD est d’explorer le protocole HTTP et avoir un aperçu des principales caractéristiques des protocoles des couches inférieures.\n1 HTTP: requêtes et réponses simples Nous commençons notre exploration du protocole HTTP en téléchargeant un fichier HTML très simple et court et ne contenant pas d’objets intégrés, comme par exemple des images.\nProcédez comme suit :\nDémarrez votre navigateur web.\nDémarrez Wireshark, mais sans commencer encore la capture de paquets.\nSaisissez http dans la fenêtre de spécification du filtre d’affichage, afin que seuls les messages HTTP capturés soient affichés plus tard dans la fenêtre de liste de paquets.\nCommencez la capture de paquets Wireshark.\nCliquez sur l’URL suivant : lien vers la page HTML. Votre navigateur devrait afficher le fichier HTML très simple d’une ligne.\nArrêtez la capture de paquets Wireshark.\nExercise\nExercise 1.1 Votre navigateur utilise-t-il la version 1.0, 1.1 ou 2 du protocole HTTP ? Quelle est la version de HTTP utilisée par le serveur ?\nExercise\nExercise 1.2 Quelles langues (le cas échéant) votre navigateur indique-t-il qu’il peut accepter sur le serveur ?\nExercise\nExercise 1.3 Quelle est l’adresse IP de votre ordinateur ? Quelle est l’adresse IP du serveur gaia.cs.umass.edu ?\nExercise\nExercise 1.4 Quel est le code d’état renvoyé par le serveur à votre navigateur ?\nExercise\nExercise 1.5 Quand le fichier HTML que vous récupérez a-t-il été modifié pour la dernière fois sur le serveur ?\nExercise\nExercise 1.6 Combien d’octets de contenu sont renvoyés à votre navigateur ?\n2 HTTP: requêtes conditionnelles La plupart des navigateurs Web mettent en cache les objets téléchargés (pages HTML, images.. ) et effectuent donc souvent une requête conditionnalle lors de la récupération d’un objet HTTP.\nActivity\nAssurez-vous que le cache de votre navigateur est vide. Cette page vous expliquera comment faire.\nProcédez maintenant comme suit :\nAssurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus.\nDémarrez la capture des paquets dans Wireshark.\nCliquez sur ce lien. Votre navigateur doit afficher un fichier HTML très simple de cinq lignes.\nCliquez simplement sur le bouton “rafraîchir” de votre navigateur.\nArrêtez la capture de paquets Wireshark et saisissez http dans la fenêtre de spécification du filtre d’affichage.\nExercise\nExercise 2.1 Examinez le contenu de la première requête HTTP GET envoyée par votre navigateur au serveur. Voyez-vous une ligne “IF-MODIFIED-SINCE” dans la requête HTTP GET ?\nExercise\nExercise 2.2 Examinez le contenu de la réponse du serveur. Le serveur a-t-il explicitement renvoyé le contenu du fichier ? Comment pouvez-vous le savoir ?\nExercise\nExercise 2.3 Inspectez maintenant le contenu de la deuxième requête HTTP GET envoyée par votre navigateur au serveur. Voyez-vous une ligne “IF-MODIFIED-SINCE :” dans la requête HTTP GET ? Si oui, quelles informations suivent l’en-tête “IF-MODIFIED-SINCE :”?\nExercise\nExercise 2.4 Quel est le code d’état HTTP et la phrase renvoyée par le serveur en réponse à ce deuxième HTTP GET ? Le serveur a-t-il explicitement renvoyé le contenu du fichier ? Expliquez.\n3 Récupération de documents longs Dans nos exemples jusqu’à présent, les documents récupérés étaient des fichiers HTML simples et courts. Voyons maintenant ce qui se passe lorsque nous téléchargeons un long fichier HTML. Procédez comme suit :\nDémarrez votre navigateur web et assurez-vous que le cache de votre navigateur a été vidé, comme nous l’avons vu plus haut.\nDémarrez la capture de paquets Wireshark.\nCliquez sur ce lien. Votre navigateur devrait afficher l’assez longue Déclaration des droits des États-Unis.\nArrêtez la capture de paquets Wireshark.\nDans la fenêtre de listage des paquets, vous devriez voir votre message HTTP GET, suivi de plusieurs segments TCP.\nAssurez-vous que le filtre d’affichage de Wireshark est désactivé pour que les segments TCP soient affichés dans la liste des paquets.\nExercise\nExercise 3.1 Pourquoi obtenez-vous autant de segments TCP ?\nExercise\nExercise 3.2 Combien de messages de requête HTTP GET votre navigateur a-t-il envoyés ? Quel numéro de paquet dans la trace contient le message GET pour la Déclaration des droits ?\nExercise\nExercise 3.3 Quel numéro de paquet dans la trace contient le code d’état et la phrase associés à la réponse à la requête HTTP GET ?\nExercise\nExercise 3.4 Quel est le code d’état et la phrase de la réponse ?\nExercise\nExercise 3.5 Combien de segments TCP contenant des données ont été nécessaires pour transporter la réponse HTTP unique et le texte de la Déclaration des droits ?\n4 Documents HTML avec objets intégrés Maintenant que nous avons vu comment Wireshark affiche le trafic de paquets capturés pour de gros fichiers HTML, nous pouvons examiner ce qui se passe lorsque votre navigateur télécharge un fichier avec des objets intégrés, c’est-à-dire un fichier qui inclut d’autres objets (dans l’exemple ci-dessous, des fichiers images) qui sont stockés sur un ou plusieurs autres serveurs.\nDémarrez votre navigateur web, et assurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus.\nDémarrez la capture de paquets Wireshark.\nCliquez sur ce lien. Votre navigateur doit afficher un court fichier HTML contenant deux images. Ces deux images sont référencées dans le fichier HTML de base.\nEn d’autres termes, les images elles-mêmes ne sont pas contenues dans le fichier HTML, mais les URL des images sont contenues dans le fichier HTML téléchargé. Comme indiqué dans le manuel, votre navigateur devra récupérer ces logos sur les sites web indiqués.\nArrêtez la capture de paquets Wireshark.\nExercise\nExercise 4.1 Combien de requêtes HTTP GET votre navigateur a-t-il envoyées ? À quelles adresses Internet ces requêtes GET ont-elles été envoyées ?\nExercise\nExercise 4.2 Pouvez-vous dire si votre navigateur a téléchargé les deux images en série ou si elles ont été téléchargées en parallèle à partir des deux sites web ? Expliquez.\n5 Authentification HTTP Nous essayons maintenant de visiter un site web protégé par un mot de passe et examinons la séquence de messages HTTP échangés pour un tel site.\nAssurez-vous que le cache de votre navigateur est vidé, comme indiqué ci-dessus, et fermez votre navigateur. Ensuite, redémarrez votre navigateur\nDémarrez la capture de paquets dans Wireshark.\nCliquez sur ce lien.\nSaisissez le nom d’utilisateur (wireshark-students) et le mot de passe (network) demandés dans la fenêtre contextuelle.\nArrêtez la capture de paquets Wireshark.\nExercise\nExercise 5.1 Quelle est la réponse du serveur (code d’état et phrase) au message HTTP GET initial de votre navigateur ?\nExercise\nExercise 5.2 Lorsque votre navigateur envoie le message HTTP GET pour la deuxième fois, quel nouveau champ est inclus dans le message HTTP GET ?\nLe nom d’utilisateur (wireshark-students) et le mot de passe (network) que vous avez saisis sont encodés dans la chaîne de caractères d2lyZXNoYXJrLXN0dWRlbnRzOm5ldHdvcms= qui suit l’en-tête “Authorization : Basic” dans le message HTTP GET du client.\nBien qu’il puisse sembler que votre nom d’utilisateur et votre mot de passe soient cryptés, ils sont simplement encodés dans un format connu sous le nom de format Base64. Le nom d’utilisateur et le mot de passe ne sont pas cryptés !\nD’ailleurs Wireshark décode la chaine dans le champs “Credentials”.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c9bcc332246a0af1ef43424908a8b01","permalink":"/courses/network/labs/http-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/network/labs/http-lab/","section":"courses","summary":"http-usecase","tags":null,"title":"Etude de cas: HTTP","type":"docs"},{"authors":null,"categories":null,"content":" Source: Jim Kurose, Keith Ross: Computer networking: A Top-Down Approach\nDans ce TP, nous allons explorer plusieurs aspects du protocole ICMP (Internet Control Message Protocol) :\nLes messages ICMP générés par le programme Ping ; les messages ICMP générés par le programme Traceroute ; le format et le contenu d’un message ICMP. Pour rappel, le protocole ICMP est utilisé par les hôtes et les routeurs pour se communiquer des informations au niveau de la couche réseau. L’utilisation la plus courante de l’ICMP est le signalement d’erreurs. Par exemple, lors de l’exécution d’une session HTTP, vous avez peut-être rencontré un message d’erreur tel que “Destination network unreachable” (réseau de destination inaccessible). Ce message trouve son origine dans l’ICMP. À un moment donné, un routeur n’a pas été en mesure de trouver un chemin vers l’hôte spécifié dans votre requête HTTP. Ce routeur a créé et envoyé un message ICMP à votre hôte pour lui signaler l’erreur.\n1 ICMP et Ping Commençons notre étude sur ICMP en capturant les paquets générés par le programme Ping. Ping est un outil simple qui permet à n’importe qui (par exemple, un administrateur réseau) de vérifier si un hôte est actif ou non. Le programme Ping de l’hôte source envoie un paquet ICMP à l’adresse IP cible ; si la cible est active, le programme Ping de l’hôte cible répond en renvoyant un paquet ICMP à l’hôte source.\nOuvrez un terminal.\nDémarrez Wireshark et commencez la capture de paquets.\nSaisissez la commande ping www.ust.hk dans le terminal pour solliciter une réponse ICMP du serveur Web de l’Université des sciences et technologies de Hong Kong. Exécutez ensuite le programme Ping en tapant la touche “Entrée”.\nAu bout d’une dizaine de réponses, arrêtez ping avec la combinaison de touches CTRL+C.\nArrêtez la capture de paquets dans Wireshark.\nRegardez la sortie dans le terminal.\nExercise\nExercise 1.1 Que signfie le mot RTT que vous voyez à chaque réponse ICMP dans le terminal ? Vous pouvez rechercher l’information sur Internet.\nSaisissez icmp dans la fenêtre de filtres Wireshark pour n’afficher que les paquets ICMP dans Wireshark.\nExercise\nExercise 1.2 Quelle est l’adresse IP de votre hôte ? Quelle est l’adresse IP de l’hôte de destination ?\nExercise\nExercise 1.3 Quelle est la valeur de TTL utilisée par votre système d’exploitation lors de la préparation d’un datagramme IP ?\nExercise\nExercise 1.4 Pourquoi un paquet ICMP n’a-t-il pas de numéros de port source et de port de destination ?\nExercise\nExercise 1.5 Examinez l’un des paquets de requête ping envoyés par votre hôte. Quels sont les numéros de type et de code ICMP ? Quels sont les autres champs de ce paquet ICMP ? Combien d’octets représentent les champs somme de contrôle (checksum), numéro de séquence (sequence number) et identifiant (identifier) ?\nExercise\nExercise 1.6 Examinez le paquet de réponse ping correspondant. Quels sont les numéros de type et de code ICMP ? Quel est le numéro de séquence ? Est-ce que cette valeur correspond à celle que vous avez vue dans le message ping envoyé par votre ordinateur ?\n2 ICMP et Traceroute Poursuivons notre étude de ICMP en capturant les paquets générés par le programme Traceroute.\nTraceroute peut être utilisé pour déterminer le chemin emprunté par un paquet de la source à la destination.\nTraceroute est implémenté de différentes manières sous Unix/Linux/MacOS et sous Windows. Sous Unix/Linux, la source envoie une série de paquets UDP à la destination cible en utilisant un numéro de port de destination improbable ; sous Windows, la source envoie une série de paquets ICMP à la destination cible. Pour les deux systèmes d’exploitation, le programme envoie le premier paquet avec TTL=1, le deuxième avec TTL=2, et ainsi de suite. Rappelons qu’un routeur décrémente la valeur TTL d’un paquet au fur et à mesure que celui-ci le traverse. Lorsqu’un paquet arrive à un routeur avec TTL=1, le routeur envoie un paquet d’erreur ICMP à la source.\nSi vous travaillez sous Windows\nVous pourrez utiliser Traceroute en ouvrant une fenêtre de PowerShell et saisissant la commande tracert.\nSi vous travaillez sous macOS/Linux\nVous pourrez utiliser Traceroute en ouvrant une fenêtre de terminal et saisissant la commande traceroute.\nCommencez par ouvrir une fenêtre de terminal.\nDémarrez Wireshark et commencez la capture de paquets.\nSaisissez la commande appropriée de Traceroute suivie par www.ust.hk.\nLorsque le programme Traceroute se termine, arrêtez la capture de paquets dans Wireshark. Il est possible que le programme ne termine pas, dans ce cas, arrêtez-le avec CTRL-C.\nDans Wireshark utilisez le filtre icmp ou udp (icmp || udp) pour n’afficher que les paquets de Traceroute.\nBon à savoir\nPour obtenir des informations sur des adresses IP, vous pouvez toujours un moteur de recherche mis à disposition par le RIPE : rendez-vous sur cette page.\nExercise\nExercise 2.1 Quelle est l’adresse IP de votre hôte ? Quelle est l’adresse IP de l’hôte cible ?\nExercise\nExercise 2.2 Pouvez-vous determiner la route géographique suivie par le paquet ? Observez notamment les délais associés aux réponses.\nExercise\nExercise 2.3 Quelle est la valeur du champ Protocol dans l’en-tête IP ?\nExercise\nExercise 2.4 Examinez le paquet ICMP error dans votre capture d’écran. Quels champs/propriétés apparaîssent ?\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"898140def09a3c681f9dd17c0e4f3c01","permalink":"/courses/network/labs/icmp-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/network/labs/icmp-lab/","section":"courses","summary":"icmp-usecase","tags":null,"title":"Etude de cas: ICMP","type":"docs"},{"authors":null,"categories":null,"content":" 1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server 2 Instructions for Windows users In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites Install XCode. You’ll find it for free in the Mac App Store.\nInstall Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n/bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot; Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page. 1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\nTap the official MongoDB Homebrew tap with the following command: brew tap mongodb/brew Type the following command to install MongoDB: brew install mongodb-community@4.4 1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4 1.4 Launch MongoDB compass Open MongoDB Compass.\nAfter few clicks you should land on the “New Connection” page.\nIn the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\nmongodb://localhost:27017 If everything goes well, you should see a list of databases. 1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\nGo to the MongoDB download page and download the installer (.msi file).\nDouble-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\nOnce the installation procedure is over, the MongoDB server is automatically started.\nMongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\nIn the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\nmongodb://localhost:27017 If everything goes well, you should see a list of databases. If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"347e0dc2ec6bf3782e8274155e40e8d0","permalink":"/courses/bdia_old/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" 1 Instructions for MacOS users 1.1 Prerequisites 1.2 Installation 1.3 Launch the MongoDB server 1.4 Launch MongoDB compass 1.5 Stop the MongoDB server 2 Instructions for Windows users In this page you’ll find instructions to install MongoDB on your computer.\n1 Instructions for MacOS users You’ll need to use commands in the Terminal to install MongoDB.\n1.1 Prerequisites Install XCode. You’ll find it for free in the Mac App Store.\nInstall Homebrew, a command-line utility that let’s you install several software applications. Type the following command in the Terminal:\n/bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026quot; Install MongoDB Compass, a MongoDB client that communicates with a MongoDB server and lets you manipulate your MongoDB databases through a graphical interface. You’ll find the installer package at this page. 1.2 Installation You can watch the following video that details how to install MongoDB, start the server and connect to the server through MongoDB Compass.\nThe commands used in the video are detailed below.\nTap the official MongoDB Homebrew tap with the following command: brew tap mongodb/brew Type the following command to install MongoDB: brew install mongodb-community@4.4 1.3 Launch the MongoDB server Type the following command:\nbrew services start mongodb-community@4.4 1.4 Launch MongoDB compass Open MongoDB Compass.\nAfter few clicks you should land on the “New Connection” page.\nIn the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\nmongodb://localhost:27017 If everything goes well, you should see a list of databases. 1.5 Stop the MongoDB server If, for any reason, you want to stop the MongoDB server, type the following command in the Terminal:\nbrew services stop mongodb-community@4.4 NOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n2 Instructions for Windows users You can follow the procedure in the following video. You can read a detailed description of the installation steps below.\nGo to the MongoDB download page and download the installer (.msi file).\nDouble-click the downloaded .msi file and follow the instructions. The procedure will also install MongoDB Compass, a MongoDB client that lets you manage your database through a graphical interface.\nOnce the installation procedure is over, the MongoDB server is automatically started.\nMongoDB Compass should execute automatically too. After few clicks you should land on the “New Connection” page.\nIn the textfield under the message “Paste your connection string (SRV or Standard)”, paste the following URI and click on “Connect”.\nmongodb://localhost:27017 If everything goes well, you should see a list of databases. If, for any reason, you wish to stop the MongoDB server, you can use the Services console and stop the corresponding service, as shown in the video.\nNOTICE\nRemember to start the MongoDB server when you need to access your databases using MongoDB Compass.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ffc657bc33e600d8a2856451c403fd03","permalink":"/courses/big-data-marseille/overview/installation-mongodb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/overview/installation-mongodb/","section":"courses","summary":"How to install MongoDB","tags":null,"title":"How to install MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" Objectives In this lab assignment you will:\nBuild and deploy a multi-service application with Docker Compose.\nDeploy a multi-service application on a local Kubernetes cluster.\nDeploy a multi-service application on a Kubernetes cluster in Microsoft Azure.\nSubmission In order to submit your work, you need to answer all the questions that you find here.\nOnly one member of the group must fill in the questionnaire.\nYou can answer the open questions either in French or in English.\nSome questions require you to upload files.\nYou can pause the test at any moment. Your answers will be stored; you’ll find them when you resume the test.\nAfter answering the last question, you need to click on the button Terminer le test (Finish attempt if you use the interface in English) to submit the questionnaire. The submission is final. After submitting, you cannot change your answers anymore.\nDeadline: 23 May 2024, 23h59\nContext We intend to build and deploy a Web application called TripMeal that let users share their favorite recipes. You can download the application here. The source code has been readapted from this GitHub repository.\nThe downloaded file is an archive. Extracting the archive will create a folder named tripmeal_sujet.\n1 Bulding and deploying with Docker Compose You’re going to build and deploy the application using Docker Compose. In the folder tripmeal_sujet you should see a file named docker-compose.yml.\nExercise\nExercise 1.1 Answer on Edunao. What is the file docker-compose.yml used for?\nSolution\nThe folder contains:\nA file docker-compose.yml that will contain the instructions to build and deploy the application.\nA subdirectory for each service composing the application. Each subdirectory contains the source code of the corresponding service, as well as a Dockerfile with the instructions to build an image for the service.\nLook at the files and folders inside the folder tripmeal_sujet.\nExercise\nExercise 1.2 Answer on Edunao. How many services does the application TripMeal have?\nSolution\nThe application consists of two services:\nweb: It is a web application, developed with a combination of HTML, Python and Flask\ndb: It is a relational database. From the Dockerfile, which is already given, we understand that the DBMS used is MySQL.\nThe Dockerfile in the directory db is already implemented. Open it and read the content to answer the following questions.\nExercise\nExercise 1.3 Answer on Edunao. Which DataBase Management System (DBMS) is used in the TripMeal application?\nSolution\nMySQL. You can see it from the Dockerfile of the db service.\nExercise\nExercise 1.4 Answer on Edunao. What is the version (tag) of the image used for the database management system of TripMeal? Find the documentation of the image and use it to answer this question.\nSolution\nThe documentation is available at here. Since no tag is specified in the Dockerfile, the latest version is selected.\nExercise\nExercise 1.5 Where does Docker get the base image for the database management system from?\nSolution\nFrom the Docker official registry. In fact, in the Dockerfile we only give the name of the image and we don’t specify any reference to another registry. Therefore, by default, Docker looks into its own registry.\nOf course, if the image is already available locally, Docker does not look for it in the registry.\n1.1 Dockerfile in folder web If you open folder web, you’ll see that the Dockerfile is empty.\nActivity\nComplete the Dockerfile in the directory web.\nYou need a Python 3.7 environment to run the application.\nThe application consists of several files and folders. You need to include all of them into a Docker image.\nAdd the following instruction in your Dockerfile:\nRUN chmod -x path_to_file_app\nwhere path_to_file_app is the path to the file app.py inside the image. This will prevent an Exec format error from occurring.\nTime to make sure that we can build an image from this Dockerfile and we can run a container from that image.\nExercise\nExercise 1.6 Build an image from the Dockerfile that you’ve just completed using the docker build command. You can give the image any name you want. For simplicity, I assume that you call it web-test.\nRun a container from this image by typing the following command. The command will only work if you type it from the directory where the file tripmeal.env is stored.\ndocker run --env-file tripmeal.env -p 3000:5000 web-test\nOpen a Web browser and type the following URL: localhost:3000. You should see the interface of the application. If that’s not the case, stop the container, and correct the Dockerfile.\nIf you click on the links New recipe, All recipes, Weekly menu and Login in the application interface, you will get a connection error to the database server. This is normal and will be fixed later.\nIf the image passes the test, upload the Dockerfile to Edunao.\nSolution\nDifferent solutions exist, here is a proposition. It is important that we build a minimal image, so we choose the environment python:3-7-slim.\nFROM python:3.7-slim RUN mkdir -p /app \u0026amp; \\ mkdir -p /app/templates \u0026amp; \\ mkdir -p /app/static RUN /usr/local/bin/python -m pip install --upgrade pip COPY ./static /app/static/ COPY ./templates /app/templates/ COPY requirements.txt /app/ WORKDIR /app RUN pip install -r requirements.txt COPY app.py dbconnect.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;,\u0026quot;app.py\u0026quot;] 1.2 The environment variables When testing the image of the service web, you may have noticed that we passed the file tripmeal.env as an argument to the command docker run. This file contains the definition of environment variables that are used in the application. Open this file and look at the environment variables to answer the following questions.\nExercise\nExercise 1.7 Answer on Edunao. What does the environment variable SERVER_PORT refer to?\nSolution\nSERVER_PORT refers to the port where the the web application will be waiting for incoming requests.\nExercise\nExercise 1.8 Answer on Edunao. Which of the following sentences are true?\nWhen you execute TripMeal, SERVER_PORT will be opened on the network interface of the host computer (your computer).\nWhen you execute TripMeal, SERVER_PORT will be opened on the network interface of a container.\nIn order to access the Web interface of TripMeal, you’ll need to map SERVER_PORT to a port number p; p will be opened on the network interface of the host computer (your computer).\nWhen you execute TripMeal, you’ll be able to access the Web interface by typing the URL localhost:$SERVER_PORT in your browser.\nSolution\nand c. are true. Exercise\nExercise 1.9 Answer on Edunao. When you write the file docker-compose.yml, you’ll need to give the database service a name. Which of the environment variables tells you the name that you must use?\nSolution\nThe variable that tells us the name of the database service is DATABASE_HOST. The web service connects to the database that is found at DATABASE_HOST. In our case, the host is a container. Evidently, Docker Compose uses the name of the service as the host name for the container.\n1.3 Volumes and networks The TripMeal application keeps user information and recipe information in a database. Therefore, you’ll need to define a Docker volume to hold the data.\nExercise\nExercise 1.10 Answer on Edunao. Look at the documentation of the base image of the database management system of the application. Which directory inside the database image will you attach the volume to?\nSolution\nLooking at the documentation of the MySQL image, the path is /var/lib/mysql.\nThe containers composing the TripMeal application need to communicate through a Docker network.\nExercise\nExercise 1.11 Answer on Edunao. If you don’t explicitly create any network, the containers will be able to communicate anyway. How do you explain it?\nNote. You can answer this question after writing the Docker compose file and running the application.\nSolution\nBy default, containers that are not explicitly connected to a network are automatically connected to the network called bridge, which is the default network in Docker. Therefore, the services of the application TripMeal can communicate through this network.\nExercise\nExercise 1.12 Answer on Edunao. Why is not creating a network for the application considered a bad idea, even if the application works?\nSolution\nAll containers created on the host machine will be connected to a single network. As a result, all containers can communicate. If only one container is compromised, the attacker will have a shot at compromising all of them.\n1.4 The compose file You’re finally ready to complete the file docker-compose.yml.\nActivity\nWrite the file docker-compose.yml.\nRemember that you need to pass the environment variables to your application.\nAs a documentation of Docker Compose you can use:\nThe examples that we’ve seen together.\nThe overview presented on the official Docker website.\nYou can also find the full specification of Compose here.\nRun the application and test it. You should be able to see the Web interface of the application, create an account, share a recipe and look at the list of shared recipes.\nIf you experience problems, you might find the command docker-compose logs useful.\nSolution\nversion: \u0026quot;3\u0026quot; services: web: build: web image: quercinigia/trip-meal-web env_file: tripmeal.env networks: - tripmeal-network ports: - \u0026quot;5000:5000\u0026quot; db: build: database image: quercinigia/trip-meal-db env_file: tripmeal.env networks: - tripmeal-network volumes: - db-data:/var/lib/mysql volumes: db-data: networks: tripmeal-network: If you successfully deployed the application, you can continue.\nShut down the application!\nShut down both the application before you move on.\nExercise\nExercise 1.13 Upload your file docker-compose.yml to Edunao.\nExercise\nExercise 1.14 Use Docker Compose to push all the images that you built in this section to your DockerHub registry.\nIn the answer to this exercise write the link to the uploaded images.\nSolution\nWe can use DockerCompose to push the images. The command is:\ndocker-compose push 2 Local Kubernetes cluster We intend to deploy the application TripMeal on a local Kubernetes cluster (either Docker Desktop or Minikube).\nExercise\nExercise 2.1 Answer on Edunao. For each service of the application TripMeal, specify the Kubernetes objects that you need to create and their types. Justify your answers.\nSolution\nFor the service web we need two objects:\nA deployment, which is collection of identical pods, each pod being an instance of the service. Deployments are often used for stateless services, which is the case of the web service.\nA LoadBalancer service. We need it in order to expose the service so that a client can connect to it.\nFor the service db we need two objects:\nA StatefulSet, which is a sort of Deployment used for stateful services.\nA ClusterIP service, used to expose the service internally in the Kubernetes cluster. The database is only used the by service web, it doesn’t need to be accessed from external clients.\nHelp\nIn order to write the specification of each object, you can refer to the examples that we discussed together in the second tutorial. You can also refer to the API documentation on the Kubernetes website.\nActivity\nFor each Kubernetes object:\nConfigure. Write a Yaml configuration file. Remember that you need to pass the environment variables to the application.\nCreate. Create the object in Kubernetes with kubectl.\nAnalyze. Execute the command kubectl get all to confirm that all components run as intended.\nTest. Play with the application to confirm that is works as intended.\nIn case of problems.\nYou can look at the logs of a pod with the following command: kubectl logs NAME-OF-POD --previous\nSolution\nThe configuration of the Deployment associated to the web service is as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 1 selector: matchLabels: app: tripmeal service: web template: metadata: labels: app: tripmeal service: web spec: containers: - image: quercinigia/trip-meal-web:latest name: web ports: - containerPort: 5000 protocol: TCP env: - name: DATABASE_NAME value: \u0026quot;tripmealdb\u0026quot; - name: DATABASE_USER value: \u0026quot;root\u0026quot; - name: MYSQL_ROOT_PASSWORD value: \u0026quot;my-secret-pw\u0026quot; - name: DATABASE_HOST value: \u0026quot;db\u0026quot; - name: DATABASE_PORT value: \u0026quot;3306\u0026quot; - name: TRIPMEAL_KEY value: \u0026quot;my-secret-key\u0026quot; - name: SERVER_PORT value: \u0026quot;5000\u0026quot; When we create the deployment with the command:\nkubectl create -f web-deployment.yml\nthree objects are created in Kubernetes:\nA pod, that represents an instance of the service. Here we specified only one replica, hence we have only one pod.\nA Deployment, the object that we demanded.\nA ReplicaSet, the object that controls the collection of pods that are the instances of the service. The ReplicaSet is managed by the Deployment.\nThe configuration of the LoadBalancer service associated with the web service is as follows:\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 5000 targetPort: 5000 protocol: TCP selector: app: tripmeal service: web When we create the service with the following command:\nkubectl create -f web-service.yml\nA service object is created. The type of this service is LoadBalancer. An external IP address is associated with the service. It is through this IP address that we can connect to the service.\nThe configuration of the StatefulSet associated to the service db is as follows:\napiVersion: apps/v1 kind: StatefulSet metadata: name: db spec: selector: matchLabels: app: tripmeal service: db serviceName: db template: metadata: labels: app: tripmeal service: db spec: containers: - image: quercinigia/trip-meal-db:latest name: db ports: - containerPort: 3306 volumeMounts: - mountPath: /var/lib/mysql name: tripmeal-data env: - name: DATABASE_NAME value: \u0026quot;tripmealdb\u0026quot; - name: DATABASE_USER value: \u0026quot;root\u0026quot; - name: MYSQL_ROOT_PASSWORD value: \u0026quot;my-secret-pw\u0026quot; volumeClaimTemplates: - metadata: name: tripmeal-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi When we create the object with the command:\nkubectl create -f db-stateful-set.yml\ntwo objects are created:\nA pod, that represents one instance of the service.\nA StatefulSet, that manages the pod.\nFinally, the configuration of the ClusterIP service associated with the db service is as follows:\napiVersion: v1 kind: Service metadata: name: db spec: type: ClusterIP ports: - port: 3306 protocol: TCP selector: app: tripmeal service: db When we create the object with the following command:\nkubectl create -f db-service.yml\nA new service appears in the output. The service has no external IP, which is normal given that it is only available internally. Its cluster IP address is used by the service web to connect to the database.\nShut down the application!\nShut down the application by removing all the Kubernetes objects.\nExercise\nExercise 2.2 Create a new file called tripmeal.yml that contains the definition of all the Kubernetes objects of the application, as we have seen in the conclusion of the tutorial 2.\nUpload this file to Edunao.\n3 Kubernetes cluster on Microsoft Azure At this point you should have successfully deployed the application on your local Kubernetes cluster.\nYou’re now going to create a Kubernetes cluster on Microsoft Azure and deploy TripMeal on that cluster.\nWarning\nIf you haven’t activated it yet:\nConnect to this webpage.\nClick on Start free.\nSign in by using your CentraleSupélec credentials.\nFollow the instructions.\nDuring this activity, you’ll need to answer some questions that encourage you to gain a deeper knowledge of the Azure platform and better understand the commands that you type. Feel free to read the documentation on the Azure platform in order to answer the questions.\n3.1 Login to Azure through CLI Warning\nIf you get the message command not found when you type az, it means that you haven’t installed the Azure CLI yet. You’ll find more information on the Edunao course page.\nMake sure you run version 2.25 or higher of the Azure CLI, by typing:\naz --version\nIf you use a VM with Multipass you’ll need to install the Azure CLI with the following command:\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\nIf you use a VM either with Multipass or with VirtualBox you’ll need to re-install kubectl:\nType the following command so as the command kubectl does not refer to minikube kubectl anymore: unalias kubectl\nType the following command to install a new version of kubectl: sudo az aks install-cli\nWhether you’re on Windows, macOS or Linux, you need to open a terminal.\nIn order to log in to your Azure account, type the following command:\naz login\nA Web page will open in your default Web browser, where you can type your username and password (your CentraleSupélec credentials). After authenticating your Azure account, you can go back to the terminal, where you should see some information about your account, such as:\n[ { \u0026quot;cloudName\u0026quot;: \u0026quot;AzureCloud\u0026quot;, \u0026quot;homeTenantId\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;isDefault\u0026quot;: true, \u0026quot;managedByTenants\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;Azure for Students\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;Enabled\u0026quot;, \u0026quot;tenantId\u0026quot;: \u0026quot;\u0026lt;id\u0026gt;\u0026quot;, \u0026quot;user\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;\u0026lt;email-address\u0026gt;\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;user\u0026quot; } } ] 3.2 Deploy and use Azure Container Registry When we run TripMeal on a local Kubernetes cluster, we assumed that our computer could have a direct access to the Internet and, therefore, to the DockerHub registry, where we could pull the images of the TripMeal services. When we run a containerized application in production, we cannot make this assumption as the production servers have often no direct access to the Internet.\nWe need to place the images in a container registry that is in the same context as our production servers. Since we’re going to run the application on Azure, we can use the Azure Container Registry (ACR).\n3.2.1 Registry creation First, we need to create a resource group.\nExercise\nExercise 3.1 Answer on Edunao. What is a resource group in Azure and how is it useful?\nSolution\nIt is a logical container of resources that are managed together, typically because they belong to the same application. Resource groups are useful to manage with a single click the whole set of resources (e.g., the permissions). For instance, when we want to decommission the application, we can remove all the resources with just one command/one click.\nThe command to create a new resource group is the following (replace RES_GRP_NAME with a name of your choice).\naz group create --name RES_GRP_NAME --location francecentral\nWarning\nIf francecentral does not work for your account, the previous command will normally suggest a possible location. Choose one that is near you.\nYou can verify that the resources are correctly created by looking in the Azure portal.\nNext, we need to create a container registry with the following command (replace RES_GRP_NAME with the name of your resource group and REG_NAME with a name of your choice for the registry).\naz acr create --resource-group RES_GRP_NAME --name REG_NAME --sku Basic\nExercise\nExercise 3.2 In the previous command, what does sku refer to? Feel free to look up on the Azure website to find the answer.\nSolution\nSKU stands for Stock Keeping Unit, it represents the different service levels of a product. Here we use a registry container with a Basic service level.\nExercise\nExercise 3.3 Answer on Edunao. Based on the specified sku, find on the Internet how much the container registry will cost (in euros) per day if the container registry is instantiated in the region France Central.\nSolution\nEach sku is associated with a price. Here students need to do some searching. If you look at this webpage we find the price of a basic container registry and the included storage. The idea here is to make students aware of the fact that the options that we select have a cost.\n3.2.2 Registry login After creating the registry, we can log into it with the following command:\naz acr login --name REG_NAME\n3.2.3 Image tagging We’re almost ready to push the images that compose the application TripMeal to the registry. In order to do that, we need to tag (i.e., rename) our images so that their names are preceded by the login server name of the registry.\nIn order to get the name of the login server name (that we denote here as acrloginserver) of your registry, you can type the following command:\naz acr list --resource-group RES_GRP_NAME --query \u0026quot;[].{acrLoginServer:loginServer}\u0026quot; --output table\nYour acrloginserver will be something like xxx.azurecr.io.\nActivity\nTag the images that correspond to the services of the application TripMeal so that their name is similar to: acrloginserver/nameofimage:latest. You can use the command docker tag.\nSolution\nWe can use the docker tag function. For instance, in my case I use the following two commands to rename my two images:\ndocker tag quercinigia/trip-meal-web:latest tripmealacr.azurecr.io/trip-meal-web:latest\ndocker tag quercinigia/trip-meal-db:latest tripmealacr.azurecr.io/trip-meal-db:latest\n3.2.4 Pushing the images We can push the images with the following command (use your image name instead of xxx.azurecr.io/imagename:latest).\ndocker push xxx.azurecr.io/imagename:latest\nMake sure to type this command for each image that you want to push.\nWarning\nIt might take few minutes before the images are completely pushed to the registry.\nFinally, verify that the images are actually in the registry with the following command:\naz acr repository list --name REG_NAME --output table\nAlternatively, you can connect to your Azure portal and visually browse your resources.\n3.3 Deploy a Kubernetes cluster We now deploy a Kubernetes cluster on Azure.\n3.3.1 Cluster creation We create the cluster with the following command (replace CLUSTER_NAME with a name of your choice. As before, RES_GRP_NAME is the name of your resource group and REG_NAME is the name of the container registry).\naz aks create \\ --resource-group RES_GRP_NAME \\ --name CLUSTER_NAME \\ --node-count 2 \\ --generate-ssh-keys \\ --attach-acr REG_NAME Warning\nIf you get an error on your SSH key, then your key is not in the right format.\nHere is how we suggest you to proceed.\nOpen your home directory in Visual Studio.\nLocate the hidden folder .ssh.\nYou should have a file named id_rsa.pub under folder .ssh. Open it.\nCopy the whole content of the file.\nCreate a new file under folder .ssh. Give it a name of your choice (for instance, id_kube_rsa.pub).\nPaste the content into the new file, while making sure that you don’t have any whitespace before the first character.\nSave and close the file.\nType the command az aks create and replace the --generate--ssh-keys option with --ssh-key-value ~/.ssh/id_kube_rsa.pub.\nThe cluster will take a while to start. Time for a coffee! But before, answer the following question!\nExercise\nExercise 3.4 What is the meaning of the option node-count in the previous command?\nSolution\nThe number of nodes in the Kubernetes cluster. Here we specify 2. Note that this refers to the number of worker nodes. In fact, the control plane is managed by Azure. It is still possible though to create a custom advanced configuration to control the number of master and etcd nodes.\n3.3.2 Connect to the cluster We can configure kubectl to connect to the newly created cluster. You need to type the following command:\naz aks get-credentials --resource-group RES_GRP_NAME --name CLUSTER_NAME\nNow, your Kubernetes cluster should be visible locally. To verify it, type the following command:\nkubectl config get-contexts\nHere context refers to the Kubernetes clusters that kubectl has access to. The Kubernetes cluster that you created on Azure should be visible in the output; an asterisk should appear in front of its name, indicating that it is the current context (that is the Kubernetes cluster being currently referenced by kubectl).\nType the following command:\nkubectl get nodes\nYou should see the information about the nodes in the cluster.\n3.4 Deploy the application We’re almost done! The images are in the registry, the Kubernetes cluster is up and running. The only missing piece of the puzzle is our application TripMeal.\nFirst thing to do is to slightly modify the file tripmeal.yml that you created at the end of the previous section.\nActivity\nLook at the names of the images of each service in that file. How must these names change? Modify the file accordingly (no need to upload it on Edunao).\nSolution\nThe existing names refer to the images on the DockerHub registry. We must change them so that they refer to the images on our Azure container registry.\nIt is the moment you’ve been waiting for! Deploy your application by typing the following command:\nkubectl apply -f tripmeal.yml\nLook at Kubernetes objects created after this command:\nkubectl get all\nWait for all the components to be up and running. Get the external IP address of the web service and try to connect to the application, in the same way you did in the previous section.\nIf you can play with the application like you did in your local deployment it means that you reached the conclusion of this assignment! Bravo!\nExercise\nExercise 3.5 Submit a video like that the one that you can see here.\nThe video must clearly show that:\nYou’re connected to your Azure portal (Email address on the top right corner of the portal).\nAll the passages that you see in the sample video: you need to show the public IP address of the application that you deployed, use that address to connect to your application and play with the application to show that it works correctly.\nThe next exercise is optional.\nIf you don’t intend to do it, make sure to read the conclusion of this document to take down the application and remove all resources!!.\nExercise\nExercise 3.6 The title of this exercise is Cerise sur le gâteau (i.e., this is optional!).\nAt this point, you can connect to TripMeal by using an IP address. It would be nice if you could use a URL, like in the real websites. Can you find a way to assign a URL to your web service? Describe your procedure.\nYou need a bit of Googling here….\nSolution\nWe have to modify the definition of the web service in the file tripmeal.yml More precisely, here is the new definition:\napiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/azure-dns-label-name: mytripmeal name: web spec: type: LoadBalancer ports: - port: 5000 targetPort: 5000 protocol: TCP selector: app: tripmeal service: web We added the field metadata.annotations and specify a DNS label. Then we need to restart the service. If everything goes well, the application is reachable at the following address:\nhttp://mytripmeal.francecentral.cloudapp.azure.com:5000/\nThe URL is composed by the DNS label that we specified + the region where we deployed the cluster + cloudapp.azure.com\nConclusion Make sure you follow these instructions:\nTake down your application in Kubernetes by using the following command: kubectl delete -f tripmeal.yml\nChange the context of the kubectl command so that it points back to a local Kubernetes cluster. Type the following command, where CONTEXT_NAME will be docker-desktop or minikube, depending on which local Kubernetes cluster you use. kubectl config use-context CONTEXT_NAME\nDestroy all the resources linked to the application TripMeal on Microsoft Azure, otherwise you’ll get billed even if you don’t use them! You can destroy all the resources by simply deleting the resource group to which they belong. You can do it through the Azure portal or by typing the following command (replace RES_GRP_NAME with the name of the resource group that you intend to remove). az group delete --name RES_GRP_NAME\nYou can check the balance of your Azure credit here.\nYou can stop Docker and Kubernetes if you don’t need it anymore.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8769051e2a9dcc07ef8f1874d00534fd","permalink":"/courses/cloud-computing/tutorials/kube-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/kube-lab/","section":"courses","summary":"Text of the lab assignment on Docker + Kubernetes","tags":null,"title":"Multi-service applications in the Cloud","type":"docs"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e70f3cf26c86687ee06a7652bddb6a36","permalink":"/courses/bdia_old/tutorials/mongodb-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/mongodb-tutorial/","section":"courses","summary":"Description of the MongoDB tutorial.","tags":null,"title":"MongoDB","type":"docs"},{"authors":null,"categories":null,"content":" 1 Use case scenario We consider a relational database that holds the data of a chain of DVD stores; the database name is Sakila.\nThe Sakila database is serving an increasing number of queries from staff and customers around the world. A single monolithic database is not sufficient anymore to serve all the requests and the company is thinking about distributing the database across several servers (horizontal scalability). However, a relational database does not handle horizontal scalability very well, due to the fact that the data is scattered across numerous tables, as the result of the normalization process. Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to MongoDB.\nFor the migration to happen, it is necessary to conceive a suitable data model. From the first discussions with the Sakila management, you quickly understand that one of the main use of the database is to manage (add, update and read) rental information.\n1.1 Description of the relational model The existing data model is recalled in Figure 1.1.\nFigure 1.1: The logical schema of the Sakila database Here is the description of the tables:\nThe table actor lists information for all actors. Columns: actor_id, first_name, last_name.\nThe table address contains address information for customers, staff and stores. Columns: address_id, address, address2, district, city_id, postal_code, phone.\nThe table category lists the categories that can be assigned to a film. Columns: category_id, name.\nThe table city contains a list of cities. Columns: city_id, city, country_id.\nThe table country contains a list of countries. Columns: country_id, country.\nThe table customer contains a list of all customers. Columns: customer_id, store_id, first_name, last_name, email, address_id, active, create_date.\nThe table film is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table. Columns: film_id, title, description, release_year, language_id, original_language_id, rental_duration, rental_rate, length.\nThe table film_actor is used to support a many-to-many relationship between films and actors. Columns: film_id, actor_id.\nThe table film_category is used to support a many-to-many relationship between films and categories. Columns: film_id, category_id.\nThe table inventory contains one row for each copy of a given film in a given store. Columns: inventory_id, film_id, store_id.\nThe table language contains possible languages that films can have for their language and original language values. Columns: language_id, name.\nThe table payment records each payment made by a customer, with information such as the amount and the rental being paid for. Columns: payment_id, customer_id, staff_id, rental_id, amount, payment_date.\nThe table rental contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned. Columns: rental_id, rental_date, inventory_id, customer_id, return_date, staff_id.\nThe table staff lists all staff members. Columns: staff_id, first_name, last_name, address_id, picture, email, store_id, active, username, password.\nThe table store lists all stores in the system. Columns: store_id, manager_staff_id, address_id.\n2 Data types in MongDB A MongoDB document is represented as a JSON record. However, internally MongoDB serializes the JSON record into a BSON record. In practice, a BSON record is a binary representation of a JSON record.\nExercise\nExercise 2.1 Looking at the specification of BSON, can you tell how many bytes do you need to represent: an integer, a date, a string and a boolean?\nExercise\nExercise 2.2 The size of a document in MongoDB is limited to 16 MiB. Can you tell why there is such a limit?\nThe table rental has four integer columns (rental_id, inventory_id, customer_id, staff_id) and 2 dates (rental_date, return_date).\nThe table customer has three integer columns (customer_id, store_id, address_id), three strings (first_name, last_name and email), one boolean value (active) and one date (create_date).\nExercise\nExercise 2.3 Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.\nEstimate the size of a document in both collections.\nWe make the following assumptions:\nOn average, each character needs 1.5 bytes.\nAn email address is 20 characters long on average.\nA last name is 8 characters long on average.\nA first name is 6 characters long on average. 3 Considerations for the new model Denormalization in MongoDB is strongly encouraged to read and write a record relative to an entity in one single operation.\nIn the following exercises, we explore different options and analyze advantages and disadvantages.\nExercise\nExercise 3.1 Suppose that we create a collection Customer, where each document includes information about a customer.\nSuppose that we embed in each document the list of rentals for a customer.\nHow many rentals can we store for a given customer, knowing that the size of a document in MongoDB cannot exceed 16 MiB?\nExercise\nExercise 3.2 Consider the two following options:\nA collection customer, where each document contains the information about a customer and an embedded list with the information on all the rentals made by the customer. We assume that the average number of rentals per customer is 32.\nA collection rental, where each document contains the information about a rental and an embedded document with the information on the customer that made the rental.\nCompute the size in byte of a document in the two collections.\nExercise\nExercise 3.3 Suppose that we have in our database\n512 customers.\n16384 rentals.\nOn average, each customer has around 32 rentals.\nCompute the size in byte of the collections customer and rental described in the previous question.\nExercise\nExercise 3.4 Based on the answers in the previous questions, discuss advantages and disadvantages of the two options: having a collection customer (solution 1) or a collection rental (solution 2).\nExercise\nExercise 3.5 Look again at the model in Figure 1.1. A rental document doesn’t only need to include information on the customer who made the rental, but also:\nThe staff member who took care of the rental.\nThe inventory item being rented.\nThe payment information.\nQuestions.\nDiscuss the different ways we can include this information in the collections customer and rental.\nBased on the discussion, which solution would you retain? A collection customer or a collection rental?\n4 The data model in MongoDB In the last question, we chose the collection that we intend to create.\nExercise\nExercise 4.1 Give the complete schema (name and type of the properties) of a document in the collection that you chose in the previous question.\nIf the value of any property is an embedded document:\nSpecify the schema of that document too.\nIf any property of an embedded document is an identifier referencing another entity, use that identifier (don’t try, for now, to further denormalize the schema).\nLet’s take a closer look at the storage requirements of the adopted solution. Consider that:\nThe size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.\nThe size in bytes of a document storing the information of an inventory item is 12 bytes.\nThe size in bytes of a document storing the information about a payment is 20 bytes.\nExercise\nExercise 4.2 If we denote by \\(N_{rental}\\) the number of rentals, what is the size in bytes of the database for the adopted solution? What do you get if we set \\(N_{rental}\\) to \\(10^4\\), \\(10^5\\) or \\(10^6\\) ?\nAlthough the size that we determined in the previous exercise, may not sound impressive, we still have to store other information (films, actors….). If we could save a bit of space, we would be happy.\nExercise\nExercise 4.3 Discuss how you could save some space in the adopted solution.\nHINT. Do you really need to denormalize all data?\nExercise\nExercise 4.4 Propose a solution for all the entities involved and estimate the savings in terms of storage requirements.\n5 The new model In this section we intend to obtain a complete model of the Sakila database.\nExercise\nExercise 5.1 Consider the model that we obtained at the end of the previous section. Which data can you further denormalize?\nExercise\nExercise 5.2 Complete the diagram obtained in the previous exercise so as to obtain a full data model for the Sakila database.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ba4ba7fbfed0b39fe1599ff51a59496f","permalink":"/courses/databases/tutorials/mongodb-data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/mongodb-data-modeling/","section":"courses","summary":"MongoDB data modeling.","tags":null,"title":"MongoDB data modeling","type":"docs"},{"authors":null,"categories":null,"content":" 1 Setup Download MongoDB Compass at this page.\nWatch this video to learn how to start a MongoDB server and connect via MongoDB Compass.\n2 Basic queries We might want to see a list of common operators in this page.\nExercise\nExercise 2.1 Write the following queries in MongoDB:\nReturn all the information on all customers.\nReturn the email of all customers.\nReturn the email of the customers of Canadian stores.\nReturn the identifier of all rentals made by customers from Iran, where the amount paid is strictly greater than 10 dollars.\nReturn the first and last names of the actors who played a role in film 213.\n3 Operations on arrays Useful array operators are listed here.\nExercise\nExercise 3.1 Write the following queries in MongoDB:\nReturn the identifier of the films that have “Behind the Scenes” as special features.\nReturn the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.\nReturn the identifier of all the films where BURT POSEY played a role.\nReturn the identifier of the film that has exactly 15 actors.\n4 Aggregation framework A useful reference for the aggregation pipeline can be found here here.\nExercise\nExercise 4.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)\nCount the total amount of payments across all rentals.\nReturn the number of actors of each film.\nSort the films by the number of actors (decreasing order).\nReturn the average number of actors for each film.\nReturn the identifier of the customer who made the most of rentals.\nReturn the first and last name of the customer who made the most of rentals.\nReturn the country where the customers have rented the most of the films in the category “Music”.\n5 Join Operations The join operation is explained here.\nExercise\nExercise 5.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the language of the film with title “ACE GOLDFINGER”.\nReturn all the information about the staff member who took care of rental 2.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a234b98e2eb6424436aa4c247c5af3e","permalink":"/courses/databases/tutorials/mongodb-queries/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/mongodb-queries/","section":"courses","summary":"MongoDB data queries.","tags":null,"title":"MongoDB queries","type":"docs"},{"authors":null,"categories":null,"content":" 1 Use case scenario We consider a relational database that holds the data of a chain of DVD stores; the database name is Sakila.\nThe Sakila database is serving an increasing number of queries from staff and customers around the world. A single monolithic database is not sufficient anymore to serve all the requests and the company is thinking of distributing the database across several servers (horizontal scalability). However, a relational database does not handle horizontal scalability very well, due to the fact that the data is scattered across numerous tables, as the result of the normalization process. Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to MongoDB.\nFor the migration to happen, it is necessary to conceive a suitable data model. From the first discussions with the Sakila management, you quickly understand that one of the main use of the database is to manage (add, update and read) rental information.\n1.1 Description of the relational model The existing data model is recalled in Figure 1.1.\nFigure 1.1: The logical schema of the Sakila database Here is the description of the tables:\nThe table actor lists information for all actors. Columns: actor_id, first_name, last_name.\nThe table address contains address information for customers, staff and stores. Columns: address_id, address, address2, district, city_id, postal_code, phone.\nThe table category lists the categories that can be assigned to a film. Columns: category_id, name.\nThe table city contains a list of cities. Columns: city_id, city, country_id.\nThe table country contains a list of countries. Columns: country_id, country.\nThe table customer contains a list of all customers. Columns: customer_id, store_id, first_name, last_name, email, address_id, active, create_date.\nThe table film is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table. Columns: film_id, title, description, release_year, language_id, original_language_id, rental_duration, rental_rate, length.\nThe table film_actor is used to support a many-to-many relationship between films and actors. Columns: film_id, actor_id.\nThe table film_category is used to support a many-to-many relationship between films and categories. Columns: film_id, category_id.\nThe table inventory contains one row for each copy of a given film in a given store. Columns: inventory_id, film_id, store_id.\nThe table language contains possible languages that films can have for their language and original language values. Columns: language_id, name.\nThe table payment records each payment made by a customer, with information such as the amount and the rental being paid for. Columns: payment_id, customer_id, staff_id, rental_id, amount, payment_date.\nThe table rental contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned. Columns: rental_id, rental_date, inventory_id, customer_id, return_date, staff_id.\nThe table staff lists all staff members. Columns: staff_id, first_name, last_name, address_id, picture, email, store_id, active, username, password.\nThe table store lists all stores in the system. Columns: store_id, manager_staff_id, address_id.\n2 Data types in MongDB A MongoDB document is represented as a JSON record. However, internally MongoDB serializes the JSON record into a BSON record. In practice, a BSON record is a binary representation of a JSON record.\nExercise\nExercise 2.1 Looking at the specification of BSON, can you tell how many bytes do you need to represent: an integer, a date, a string and a boolean?\nExercise\nExercise 2.2 The size of a document in MongoDB is limited to 16 MiB. Can you tell why there is such a limit?\nThe table rental has four integer columns (rental_id, inventory_id, customer_id, staff_id) and 2 dates (rental_date, return_date).\nThe table customer has three integer columns (customer_id, store_id, address_id), three strings (first_name, last_name and email), one boolean value (active) and one date (create_date).\nExercise\nExercise 2.3 Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.\nEstimate the size of a document in both collections.\nWe make the following assumptions:\nOn average, each character needs 1.5 bytes.\nAn email address is 20 characters long on average.\nA last name is 8 characters long on average.\nA first name is 6 characters long on average. 3 Considerations for the new model Denormalization in MongoDB is strongly encouraged to read and write a record relative to an entity in one single operation.\nIn the following exercises, we explore different options and analyze advantages and disadvantages.\nExercise\nExercise 3.1 Suppose that we create a collection Customer, where each document includes information about a customer.\nSuppose that we embed in each document the list of rentals for a customer.\nHow many rentals can we store for a given customer, knowing that the size of a document in MongoDB cannot exceed 16 MiB?\nExercise\nExercise 3.2 Consider the two following options:\nA collection customer, where each document contains the information about a customer and an embedded list with the information on all the rentals made by the customer.\nA collection rental, where each document contains the information about a rental and an embedded document with the information on the customer that made the rental.\nCompute the size in byte of a document in the two collections.\nExercise\nExercise 3.3 Suppose that we have in our database\n512 customers.\n16384 rentals.\nOn average, each customer has around 32 rentals.\nCompute the size in byte of the collections customer and rental described in the previous question.\nExercise\nExercise 3.4 Based on the answers in the previous questions, discuss advantages and disadvantages of the two options: having a collection customer (solution 1) or a collection rental (solution 2).\nExercise\nExercise 3.5 Look again at the model in Figure 1.1. A rental document doesn’t only need to include information on the customer who made the rental, but also:\nThe staff member who took care of the rental.\nThe inventory item being rented.\nThe payment information.\nQuestions.\nDiscuss the different ways we can include this information in the collections customer and rental.\nBased on the discussion, which solution would you retain? A collection customer or a collection rental?\n4 The data model in MongoDB In the last question, we chose the collection that we intend to create.\nExercise\nExercise 4.1 Give the complete schema (name and type of the properties) of a document in the collection that you chose in the previous question.\nIf the value of any property is an embedded document:\nSpecify the schema of that document too.\nIf any property of an embedded document is an identifier referencing another entity, use that identifier (don’t try, for now, to further denormalize the schema).\nLet’s take a closer look at the storage requirements of the adopted solution. Consider that:\nThe size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.\nThe size in bytes of a document storing the information of an inventory item is 12 bytes.\nThe size in bytes of a document storing the information about a payment is 20 bytes.\nExercise\nExercise 4.2 If we denote by \\(N_{rental}\\) the number of rentals, what is the size in bytes of the database for the adopted solution? What do you get if we set \\(N_{rental}\\) to \\(10^4\\), \\(10^5\\) or \\(10^6\\) ?\nAlthough the size that we determined in the previous exercise, may not sound impressive, we still have to store other information (films, actors….). If we could save a bit of space, we would be happy.\nExercise\nExercise 4.3 Discuss how you could save some space in the adopted solution.\nHINT. Do you really need to denormalize all data?\nExercise\nExercise 4.4 Propose a solution for all the entities involved and estimate the savings in terms of storage requirements.\n5 The new model In this section we intend to obtain a complete model of the Sakila database.\nExercise\nExercise 5.1 Consider the model that we obtained at the end of the previous section. Which data can you further denormalize?\nExercise\nExercise 5.2 Complete the diagram obtained in the previous exercise so as to obtain a full data model for the Sakila database.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e41f463c031b90d038566a4d7d63d30a","permalink":"/courses/bigdata/tutorials/mongodb-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/mongodb-modeling/","section":"courses","summary":"MongoDB data modeling.","tags":null,"title":"MongoDB modeling","type":"docs"},{"authors":null,"categories":null,"content":" We learn how to write queries in MongoDB on the Sakila database.\n1 Initialization Launch a MongoDB server on your computer.\nOpen MongoDB Compass.\nConnect to the MongoDB server with the following URI: mongodb://localhost:27017.\nDownload the this archive file and extract it. Each file corresponds to a collection.\nIn MongoDB Compass create a new database named sakila.\nCreate the collections customer, film, rental, staff and store.\nImport the data from the downloaded JSON files.\n2 Basic queries We might want to see a list of common operators in this page.\nExercise\nExercise 2.1 Write the following queries in MongoDB:\nReturn all the information on all customers.\nReturn the email of all customers.\nReturn the email of the customers of Canadian stores.\nReturn the identifier of all rentals made by customers from Iran, where the amount paid is strictly greater than 10 dollars.\nReturn the first and last names of the actors who played a role in film 213.\n3 Operations on arrays Useful array operators are listed here.\nExercise\nExercise 3.1 Write the following queries in MongoDB:\nReturn the identifier of the films that have “Behind the Scenes” as special features.\nReturn the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.\nReturn the identifier of all the films where BURT POSEY played a role.\nReturn the identifier of the film that has exactly 15 actors.\n4 Aggregation framework A useful reference for the aggregation pipeline can be found here here.\nExercise\nExercise 4.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)\nCount the total amount of payments across all rentals.\nReturn the number of actors of each film.\nSort the films by the number of actors (decreasing order).\nReturn the average number of actors for each film.\nReturn the identifier of the customer who made the most of rentals.\nReturn the first and last name of the customer who made the most of rentals.\nReturn the country where the customers have rented the most of the films in the category “Music”.\n5 Join Operations The join operation is explained here.\nExercise\nExercise 5.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the language of the film with title “ACE GOLDFINGER”.\nReturn all the information about the staff member who took care of rental 2.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"113427e1416de4e30e98d831986d1a2a","permalink":"/courses/bigdata/tutorials/mongodb-querying/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/mongodb-querying/","section":"courses","summary":"MongoDB queries.","tags":null,"title":"MongoDB queries","type":"docs"},{"authors":null,"categories":null,"content":" Lecture 1 Title: Introduction and MapReduce programming.\nDate and time: Monday 3 May 2021, 1:45 PAM - 5 PM.\nSlides: Available here\nLecture 2 Title: Hadoop and its ecosystem: HDFS.\nDate and time: Wednesday 5 May 2021, 8:30 AM - 10 AM.\nSlides: Available here\nLecture 3 Title: Introduction to Apache Spark.\nDate and time: Wednesday 5 May 2021, 10 AM - 11:30 AM.\nSlides: Available here\nSpark RDD programming demo: Available here\nLecture 4 Title: Apache Spark’s Structured APIs and Structured Streaming.\nDate and time: Monday 10 May 2021, 8:30 AM - 11:30 AM.\nSlides: Available here\nDataFrames: Notebook available here\nSpark SQL: Notebook available here\nLecture 5 Title: Distributed and NoSQL databases\nDate and time: Monday 17 May 2021, 10:30 AM - 11:30 AM / 14 PM - 16 PM\nSlides: Available here\nLecture 6 Title: Document-oriented database systems: MongoDB.\nDate and time: Monday 17 May 2021, 16 PM - 17PM\nNotebook: Available here\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2392a0bcf324168321fd0099e6a6b96","permalink":"/courses/big-data-marseille/lectures/lectures/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/lectures/lectures/","section":"courses","summary":"Presenting the lectures.","tags":null,"title":"Lectures","type":"docs"},{"authors":null,"categories":null,"content":" Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\nMehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\nKarau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\nGiamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\nBradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\nScifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e64f1a69e66e30cbd6db3cab82bc1556","permalink":"/courses/bdia_old/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" Singh, Chanchal, and Manish Kumar. Mastering Hadoop 3: Big data processing at scale to unlock unique business insights. Packt Publishing Ltd, 2019.\nMehrotra, Shrey, and Akash Grade. Apache Spark Quick Start Guide: Quickly learn the art of writing efficient big data applications with Apache Spark. Packt Publishing Ltd, 2019.\nKarau, Holden, et al. Learning spark: lightning-fast big data analysis. O’Reilly Media, Inc., 2015\nGiamas, Alex. Mastering MongoDB 4.x: Expert techniques to run high-volume and fault-tolerant database solutions using MongoDB 4.x. Packt Publishing Ltd, 2019.\nBradshaw, Shannon, Eoin Brazil, and Kristina Chodorow. MongoDB: The Definitive Guide: Powerful and Scalable Data Storage. O’Reilly Media, 2019.\nScifo, Estelle, Hands-on Graph Analytics with Neo4j. Packt Publishing Ltd, 2020\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"63bc28b1c571ba26f8b80edb6882bae4","permalink":"/courses/big-data-marseille/references/cc-references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/references/cc-references/","section":"courses","summary":"Bibliographic references for the course","tags":null,"title":"References","type":"docs"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c802d274954d1b3f3a03319fd538f7ed","permalink":"/courses/bdia_old/tutorials/spark-lab-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/spark-lab-assignment/","section":"courses","summary":"Description of the lab advanced Spark programming.","tags":null,"title":"Advanced Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" 1 Calcul des moyennes Nous considérons une collection de fichiers CSV contenant des mesures de température au format suivant :\nannée, mois, jour, heure, minute, seconde, température\nvous pouvez trouver les fichiers dans le répertoire hdfs://sar01:9000/data/temperatures/\nVoici les détails de chaque fichier :\nLe fichier temperatures_86400.csv contient une mesure par jour pour les années 1980 - 2018. Le fichier temperatures_2880.csv contient une mesure toutes les 2880 secondes pour les années 1980 - 2018. Le fichier temperatures_86.csv contient une mesure toutes les 86 secondes pour la seule année 1980. Le fichier temperatures_10.csv contient une mesure toutes les 10 secondes pour les années 1980 - 2018. 1.1 Première implémentation Copiez le fichier /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py dans votre répertoire personnel en saisissant la commande suivante :\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py\nOuvrez le fichier avg_temperatures_slow.py. Le fichier contient l’implémentation de la fonction avg_temperature_slow qui:\nprend en arguments un RDD, dont chaque élément est une ligne du fichier donné ;\nrenvoie un RDD, où chaque élément est une paire clé-valeur (year, avg_temp).\nDans le même fichier, localisez les deux variables input_path et output-path et écrivez le code suivant :\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/\u0026quot; Remplacez X par le numéro correspondant à votre compte ! N’oubliez pas le / à la fin des chemins d’accès aux fichiers !\nFaites les actions suivantes :\nExécutez le script avg_temperatures_slow.py en utilisantle fichier temperatures_86400.csv comme argument. Pour ce faire, utilisez la commande suivante : spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv\nVous devriez trouver la sortie du programme dans le dossier suivant : hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out\nTapez la commande suivante pour le vérifier :\nhdfs dfs -ls hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out\nSi vous souhaitez accéder au résultat du calcul, vous pouvez exécuter la commande suivante : hdfs dfs -cat hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out/*\nExercice\nExercise 1.1 Dans la sortie que Spark affiche dans le terminal, vous devriez voir une ligne qui ressemble à la phrase suivante :\nINFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s Notez le temps d’exécution que vous avez obtenu.\nExécutez le même script en utilisant le fichier températures_2880.csv comme argument.\nQuel est le temps d’exécution ? Semble-t-il raisonnable par rapport au temps d’exécution que vous avez observé auparavant ? Justifiez votre réponse.\nExécutez le même script en utilisant le fichier températures_86.csv comme argument.\nQuel est le temps d’exécution ? Comment le justifiez-vous, sachant que les fichiers temperatures_2880.csv et temperatures_86.csv ont une taille similaire (11 Mo pour le premier, 9 Mo pour le second) ? 1.2 Deuxième implémentation Nous voulons mettre en œuvre une meilleure version du programme. Vous pouvez rédiger vos idées sur papier avant d’écrire du code.\nLorsque vous êtes prêts, créez une copie du fichier avg_temperatures_slow.py et renommez-la en avg_temperatures_fast.py, avec la commande suivante :\ncp ./avg_temperatures_slow.py ./avg_temperatures_fast.py\nExercice\nExercise 1.2 Ouvrez le fichier et implémentez la fonction avg_temperature_fast.\nNotez que vous devez commenter l’appel à la fonction avg_temperature_slow() et décommenter l’appel à la fonction avg_temperature_fast() à la fin du fichier.\nExercice\nExercise 1.3 Exécutez le script avg_temperatures_fast.py en utilisant le fichier temperatures_86.csv comme argument.\nQuel est le temps d’exécution ? Comparez-le avec le temps d’exécution obtenu dans l’exercice précédent et commentez la différence.\nExécutez le même script en utilisant le fichier températures_10.csv (3 Go !) comme argument. Pensez-vous que le programme prend trop de temps ? Justifiez votre réponse.\n2 Amis communs dans un réseau social Considérons un réseau social décrit par un graphe encodé dans un fichier texte. Chaque ligne du fichier est une liste d’identifiants séparés par des virgules. Par exemple, la ligne \\(A,D,C,B\\) signifie que \\(A\\) est ami avec \\(D\\), \\(C\\) et \\(B\\). Un extrait du fichier ressemble à ce qui suit :\nB,D,A A,D,C,B D,A,C,B C,A,D ... Nous supposons que la relation d’amitié est symétrique : \\((A, B)\\) implique (B, A)$.\nNous voulons obtenir la liste des amis communs pour chaque paire d’individus :\n(B, C), [A, D] (A, D), [B, C] (C, D), [A] (A, C), [D] (B, D), [A] (A, B), [D] Comme contrainte supplémentaire, nous voulons représenter un couple une seule fois et éviter de représenter un couple symétrique. En d’autres termes, si nous produisons \\((A, B)\\), nous ne voulons pas produire \\((B, A)\\).\nNous utilisons les fichiers suivants, disponibles dans le dossier hdfs://sar01:9000/data/social-network/:\nsn_tiny.csv. Petit réseau social, que vous pouvez utiliser pour tester votre implémentation.\nsn_10k_100k.csv. Réseau social avec \\(10^4\\) individus et \\(10^5\\) liens.\nsn_100k_100k.csv. Réseau social avec \\(10^5\\) individus et \\(10^5\\) liens.\nsn_1k_100k.csv. Réseau social avec \\(10^3\\) individus et \\(10^5\\) liens.\nsn_1m_1m.csv. Réseau social avec \\(10^6\\) individus et \\(10^6\\) liens.\n2.1 Implémentation Obtenez un squelette du code à l’aide de la commande suivante :\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_common_friends.py .\nExercice\nExercise 2.1 Écrivez une implémentation qui utilise la fonction groupByKey().\nÉcrivez une implémentation qui utilise la fonction reduceByKey().\nTestez les deux implémentations sur le fichier sn_tiny.csv.\n2.2 Tests et mesures de performance Exercice\nExercise 2.2 Exécutez les deux implémentations sur les autres fichiers.\nRemplissez un tableau dans lequel vous indiquez : le nom et la taille de chaque fichier et les temps d’exécution mesurés des deux implémentations.\n2.3 Degré minimum, maximum et moyen Exercice\nExercise 2.3 Ajoutez une fonction au fichier template_common_friends.py qui renvoie un tuple contenant le degré minimum, maximum et moyen d’un noeud dans le réseau social. Vous devez utiliser des RDDs pour le faire, n’essayez pas d’utiliser la fonction collect() pour ensuite calculer les valeurs sur des listes Python.\nExécutez la fonction pour tous les fichiers donnés.\nComplétez le tableau que vous avez créé dans l’exercice précédent en ajoutant les valeurs suivantes : le nombre minimum, maximum et moyen d’amis pour chaque individu.\n2.4 Analyses de performance Exercice\nExercise 2.4 Nous supposons que chaque nœud a un nombre d’amis égal au nombre moyen d’amis. Calculez (sur papier, pas besoin d’écrire un code pour cela) le nombre de paires intermédiaires \\(((A, B), X)\\) générées par votre code.\nComplétez le tableau en écrivant le nombre de paires intermédiaires pour chaque fichier.\nTracez trois graphiques, où l’axe des \\(y\\) représente les temps d’exécution du programme et l’axe des \\(x\\) représente respectivement le nombre de paires intermédiaires, le degré moyen et la taille du fichier.\nQuels graphiques prédisent le mieux l’évolution du temps de calcul ? ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a2a0366a01effacff26380bed69311ff","permalink":"/courses/bigdata-mds/labs/spark-rdd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata-mds/labs/spark-rdd/","section":"courses","summary":"2. Programmes Spark sur DCE","tags":null,"title":"Programmes Spark sur DCE","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We consider a collection of CSV files containing temperature measurements in the following format:\nyear,month,day,hours,minutes,seconds,temperature\nyou can find the files under the directory hdfs://sar01:9000/data/temperatures/\nHere are the details for each file:\nFile temperatures_86400.csv contains one measurement per day in the years 1980 - 2018. File temperatures_2880.csv contains one measurement every 2880 seconds in the years 1980 - 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds for the years 1980 - 2018. We intend to implement a Spark algorithm to generate pairs \\((y, t_{avg})\\), where \\(y\\) is the year and \\(t_{avg}\\) is the average temperature in the year.\n1.1 First implementation Copy the file /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py\nOpen the file avg_temperatures_slow.py. The file contains the implementation of the function avg_temperature_slow that:\ntakes in an RDD, where each item is a line of the input text file;\nreturns an RDD, where each item is a key-value pair (year, avg_temp).\nIn the same file, locate the two variables input_path and output-path and write the following code:\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/\u0026quot; Replace X with the number corresponding to your account! Don’t forget the / at the end of the file paths!\nExecute the following actions:\nRun the script avg_temperatures_slow.py by using temperatures_86400.csv as an input. To this extent, use the following command: spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv\nYou should find the output of the program under the following folder: hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out\nType the following command to verify it :\nhdfs dfs -ls hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out\nIf you want to read the result of the computation, you can execute the following command: hdfs dfs -cat hdfs://sar01:9000/hpdaspark24/hpdaspark24_X/temperatures_86400.out/*\nExercise\nExercise 1.1 In the output of Spark on the command line you should see a line that reads something similar to the following phrase:\nINFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s Note the execution time that you obtained.\nRun the same script by using temperatures_2880.csv as an input.\nWhat is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer.\nExecute the same script by using temperatures_86.csv as an input.\nWhat is the execution time? How would you justify it, knowing that the files temperatures_2880.csv and temperatures_86.csv have a similar size (11 MB the former, 9 MB the latter)?\n1.2 Second implementation We want to implement a better version of the program. You can draft your ideas on paper before you write any code.\nWhen you’re ready, create a copy of avg_temperatures_slow.py and rename it as avg_temperatures_fast.py, with the following command:\ncp ./avg_temperatures_slow.py ./avg_temperatures_fast.py\nExercise\nExercise 1.2 Open the file and implement the function avg_temperature_fast.\nNOTE. Remember to comment the call to avg_temperature_slow and to uncomment the call to avg_temperature_fast at the end of the file.\nExercise\nExercise 1.3 Run the script avg_temperatures_fast.py by using temperatures_86.csv as an input.\nWhat’s the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference.\nRun the same script by using temperatures_10.csv (3 GB!) as an input. Do you think that the program takes too long? Why?\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nB,A,D A,B,C,D D,A,B,C C,A,D ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(B, C), [A, D] (A, D), [B, C] (C, D), [A] (A, C), [D] (B, D), [A] (A, B), [D] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nWe use the following input files available in folder hdfs://sar01:9000/data/social-network/:\nsn_tiny.csv. Small social network, that you can use to test your implementation.\nsn_10k_100k.csv. Social network with \\(10^4\\) individuals and \\(10^5\\) links.\nsn_100k_100k.csv. Social network with \\(10^5\\) individuals and \\(10^5\\) links.\nsn_1k_100k.csv. Social network with \\(10^3\\) individuals and \\(10^5\\) links.\nsn_1m_1m.csv. Social network with \\(10^6\\) individuals and \\(10^6\\) links.\n2.1 Implementation Get the code template with the following command:\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_common_friends.py .\nExercise\nExercise 2.1 Write an implementation that uses a groupByKey.\nWrite an implementation that uses a reduceByKey.\nTest both implementations on file sn_tiny.csv.\n2.2 Tests and performance measures Exercise\nExercise 2.2 Run both implementations on the other files.\nFill in a table where you indicate: the name and size of each file and the measured running times of both implementations.\n2.3 Minimum, maximum and average degree Exercise\nExercise 2.3 Add a function to file template_common_friends.py that returns a tuple containing the minimum, the maximum and the average degree of a node in the social network. You must use RDDs to do so, don’t try to collect() the content of the RDDs and so compute the values on Python lists.\nExecute the function for all the given input files.\nComplete the table that you created in the previous exercise by adding the minimum, maximum and average number of friends.\n2.4 Performance analysis Exercise\nExercise 2.4 We suppose that each node has a number of friends that is equal to the average number of friends. Compute (with pencil or paper, no need to write a code for that) the number of intermediate pairs \\(((A, B), X)\\) generated by your code.\nComplete the table by writing down the number of intermediate pairs for each file.\nPlot three graphs, where the y-axis has the program running times and the x-axis has: the number of intermediate pairs, the average degree and the file size respectively.\nWhich graphs best predict the evolution of the computational time?\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f86800dc3cc300ad9e5a7b68c8c313ca","permalink":"/courses/bigdata/tutorials/spark-low-level/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/spark-low-level/","section":"courses","summary":"Low-level Spark programming (RDD)","tags":null,"title":"Spark RDD programming","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, you’ll learn how to use the Spark MLlib library to train, test and evaluate machine learning models. We’ll be using a dataset of AirBnb accommodation in the San Francisco area. The dataset is available in HDFS at the following path:\nhdfs://sar01:9000/prof/cpu_quercini/sf-airbnb-clean.parquet\nDataset source\nThe dataset has been obtained from this GitHub repository.\nThe goal of the exercise is to predict the price per night of an apartment given all the features in the dataset.\n1 Obtaining a training and test set In order to build and evaluate a machine learning model, we need to split our dataset into a training and test set.\nActivity\nCopy the file train_test.py to your home folder in the cluster by typing the following command: cp /usr/users/prof/cpu_quercini/mllib/train_test.py . Complete Line 9, by specifying your directory in HDFS, which is as follows (replace X with your account number). hdfs://sar01:9000/itpspark23/itpspark23_X Exercise\nExercise 1.1 Complete Line 23. Write the code to read the input file, which is stored in HDFS as a Parquet file, into the DataFrame airbnb_df.\nFrom Line 26, add the instructions necessary to print the schema of airbnb_df and the first 5 rows. Which option should you use to nicely see the values in the columns?\nExecute the code with spark-submit and verify that the data is correctly loaded.\nWe now split airbnb_df into two separate DataFrames, train_df and test_df, containing the training and test instances respectively.\nExercise\nExercise 1.2 Uncomment Line 32. Write the code to split the dataset into a training and test set. 80% of the instances must be taken as training instances, while the remaining 20% will be used a test instances. Looking at the DataFrame API documentation, which function are you going to use?\nFrom Line 36, write the code to print the number of instances in the training and test set.\nExecute the code with spark-submit. You should have 5780 training instances and 1366 test instances.\nIt is time to save the training and test sets to a HDFS file. This way, we can load them whenever we need them to build a machine learning model for this problem.\nExercise\nExercise 1.3 From Line 47. Write the code to save the training and test sets to two Parquet files in HDFS. The paths to the two files are already available in variables training_set_file and test_set_file defined at Lines 40 and 43 respectively.\n2 Preparing the features In both training and test sets, the features correspond to DataFrame columns. Most of the machine learning algorithms in Spark need to have all features in one single vector. We need to use a transformer.\nGood to know\nTransformers take in a DataFrame and return a new DataFrame that has the same columns as the input DataFrame and additional columns that are specified as an argument to the transformer.\nCopy the file train_test_model to your home folder in the cluster by typing the following command:\ncp /usr/users/prof/cpu_quercini/mllib/train_test_model.py .\nExercise\nExercise 2.1 Implement the function read_training_set at Line 22. The file reads into a Spark DataFrame the training set Parquet file that you generated in the previous section.\nImplement the function read_test_function at Line 39. The file reads into a Spark DataFrame the test set Parquet file that you generated in the previous section.\nModify the variables at Line 105 and 106 so that they point to the two HDFS Parquet files that contain the training and test sets.\nUncomment Lines 109 and 110, so as to print the number of training and test instances.\nExecute the file train_test_model with spark-submit. Check that the number of training and test instances correspond to what you got in the first section.\nIt is now time to learn how to use a transformer to put all the necessary features into a vector. For the time being, we’ll only be using the feature bedrooms to predict the price.\nExercise\nExercise 2.2 Implement the function get_vector at Line 55. The comments in the file describe what the function is supposed to do. You’ll need to use a VectorAssembler object to implement the function.\nUncomment Lines 117 and 118 to call the function get_vector and display the result.\nExecute the file train_test_model with spark-submit. Observe the content of the the DataFrame train_vect. It should have a column named features containing a list with one value (the value of feature bedrooms).\nIt is now time to train a linear regression model on the given training set and the selected features.\nExercise\nExercise 2.3 Implement the function train_linear_regression_model at Line 79. The comments in the file describe what the function is supposed to do. Looking at the documentation, identify the object that you need to use to create a linear regressor and the function that you need to invoke to train the linear regressor.\nUncomment Lines 121, 124, 125, 126 to call the function train_linear_regression_model and display the coefficients learned by the linear regression model.\nExecute the file train_test_model with spark-submit.\nWe can now use the trained model to make some predictions. The model returned by the function that you implemented in the previous exercise is an object of type LinearRegressionModel. Looking at the documentation, we learn that there is a function predict to make a prediction given a single instance. If we want to make a prediction on the whole test set, we should use the function transform. The function takes in a DataFrame with the test set and returns the same DataFrame with an additional column prediction that contains the predicted value.\nExercise\nExercise 2.4 Look at Lines 130 and 131. Which DataFrame do we need to pass the function transform? Is test_df a good choice? Why?\nComplete line 130 and uncomment lines 130, 131, 132.\nExecute the file train_test_model with spark-submit and observe the result. Is the predicted value the one that you expect given that you know the formula of the regression line?\n3 Pipelines In the previous section we learned that a training and test set need to go through the same transformation steps in order to be fed to a machine learning model. When we have few transformations, it is easy to remember the ones that we applied to a training set and apply them on the test set. However, when we apply a series of transformations, the order of which is important, mistakes are around the corner; we may very well apply different sets of transformations to the training and test instances, which leads to erroneous predictions.\nA good practice is to use the Pipeline API. A pipeline is composed of stages. Each stage may be a transformer or an estimator. A transformer is an object on which we call the function transform to obtain a new DataFrame from an input DataFrame. An estimator is an object on which we call the function fit to learn a model on a given DataFrame. The learned model is itself a transformer.\nWhen the function fit is called on a Pipeline, the training set goes through all the transformers and the estimators in the order in which they are declared in the pipeline; the estimator specified in the last stage is trained on the training set. The model returned by applying the function fit on the pipeline is itself a transformer. If we invoke the function transform on that model on the test set, we obtain a DataFrame that contains a column named predictions. Implicitly, all the transformations in the pipeline will be applied to the test set too, before making the predictions.\nCopy the file pipeline_example.py to your home folder in the cluster by typing the following command:\ncp /usr/users/prof/cpu_quercini/mllib/pipeline_example.py .\nExercise\nExercise 3.1 Look at the documentation and write a code from Line 34 to create a pipeline that does the same operations as in file train_test_model.py to train and test a linear regression model on the given training and test sets.\nDon’t forget to specify the paths to the training and test set files on HDFS at Lines 27 and 28. 3.1 From categorical to numerical features Many machine learning models do not handle categorical values: they need all features to be numerical. Linear regression is such an example.\nActivity\nCopy the file onehot_playground.py to your home directory in the cluster by typing the following command: cp /usr/users/prof/cpu_quercini/mllib/onehot_playground.py .\nChange lines 27 and 28 and write the paths to the files with the training and test sets. First, let’s find out which features are categorical in our dataset.\nExercise\nExercise 3.2 Complete the function get_categorical_columns.\nIn order to get an idea as to how to get the categorical columns, execute the code with spark_submit. This execute the instruction at Line 86.\nOnce you’re done with the implementation, execute the file with spark-submit and observe the output.\nOne example of a categorical feature in our dataset is property_type, which takes values such as Apartment, House, Condominium… Each value of a categorical feature is also referred to as a category.\nOne way to turn this feature into a numerical one would be to assign a number to each category (e.g., Apartment corresponds to 1, House to 2….). However, this implicitly introduces an order among the categories: the category House would be worth twice as much as the category Apartment; this would inevitably bias the trained model.\nA commonly used method is one-hot encoding. Let’s find out how it works. Let’s only focus on the feature property_type.\nActivity\nUncomment Lines 40, 41 and 42. These lines instantiate an estimator that is called StringIndexer. This estimator associates numeric indexes to the values of property_type. The indexes will be stored in another column named property_type_index.\nUncomment Line 46. The StringIndexer is applied to the training set to learn how to associate indexes to the values of property_type. The result of the instruction is a new transformer.\nUncomment Line 49. The transformer learned on Line 46 is used to transform the training set into a new DataFrame. This DataFrame contains an additional column called property_type_index.\nExercise\nExercise 3.3 Complete the function count_property_types. Follow the instructions in the file.\nOnce the function is complete, uncomment Line 53 to call the function.\nExecute the file with spark-submit and observe the output. Can you guess how the indexes are assigned to the categories of the feature property_type?\nIt is time to find out how one-hot encoding works.\nActivity\nUncomment Lines 57, 58, 61, 64, 67. These lines instantiate an estimator that is called OneHotEncoder. This estimator uses the indexes in property_type_index and creates a new column property_type_ohe.\nThe estimator is trained on the training set and a new transformer is obtained (line 61).\nThe transformer is used on the training set to transform it into a new DataFrame, where a new column property_type_ohe exists.\nLine 67 prints the selected columns of this transformed training set.\nExercise\nExercise 3.4 Execute the file with spark-submit.\nCan you understand how one-hot encoding works?\nNow, you have all the ingredients to create a full pipeline to train and test a linear regression model by using all features.\nExercise\nExercise 3.5 Create a new file full_pipeline.py where:\nYou select the categorical features.\nSet up a StringIndexer and a OneHotEncoder to apply one-hot encoding to all categorical features.\nObtain the numeric features.\nSet up a VectorAssembler to put into a single vector the one-hot encoded features and the numerical ones.\nSet up a linear regression model that takes in all features and the variable to predict (price).\nMix all these ingredients in a Pipeline.\nUse the Pipeline to train and test the model.\nYou can display the first five rows of the DataFrame to see the predictions.\n4 Evaluating a model In the previous sections we looked at the predictions to get a vague idea of how our estimator performs. In order to quantify the quality of our estimator, we need some evaluation measures.\nExercise\nExercise 4.1 Use a TrainValidationSplit object to do a model selection based on a grid search of parameters.\nWarning. TrainValidationSplit expects the features to be in a column named “features” and the variable to predict in a column named “label”.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14641e858b079492e098480a1233bd74","permalink":"/courses/bigdata/tutorials/mllib/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/mllib/","section":"courses","summary":"MLlib lab.","tags":null,"title":"Spark MLlib","type":"docs"},{"authors":null,"categories":null,"content":" 1 Number of partitions in a RDD We consider a set of CSV files that contain temperature measurements over several years. Each line has the following content: year,month,day,hour,minute,second,temperature.\nThese files are stored at the following location: hdfs://sar01:9000/data/temperatures/.\nHere is the detail for each file:\nFile temperatures_86400.csv contains one measurement per day between 1980 and 2018. File temperatures_2880.csv contains one measurement every 2880 seconds between 1980 and 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds between 1980 - 2018. Get the file avg_temperatures_rdd.py by typing the following command:\ncp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_rdd.py .\nThis file contains an efficient implementation of the function that computes the average yearly temperature, as we have seen in a previous tutorial.\n1.1 About the number of partitions Line 70. Replace sarXX with sar01.\nLine 80. Replace sarXX with sar01. Replace YOUR_DIRECTORY with hpdaspark24/hpdaspark24_XX/ (where XX corresponds to your username number).\nObserve the instructions at line 98 and line 104. They get and show the number of partitions of the input RDD text_file and the output RDD temperatures respectively.\nExercise\nExercise 1.1 Complete Table 1. Run the program on file temperatures_10.csv. Write down the execution time and the number of partitions of both RDDs text_file and temperatures.\nReminder. The command to run the program is as follows:\nspark-submit --master spark://sar01:7077 avg_temperatures_rdd.py temperatures_10.csv\nFile Execution time (sec) Number of partitions (text_file) Number of partitions (temperatures) temperatures_86400.csv 3.12 2 2 temperatures_2880.csv 3.67 2 2 temperatures_86.csv 4.59 2 2 temperatures_10.csv Table 1. Execution time and partition numbers with RDD. 1.2 Analysis of Spark’s operation Exercise\nExercise 1.2 Could you understand how Spark determines the number of partitions of the RDD text_file by looking at the size of the input files? HINT. If you divide the size of the file temperatures_10.csv by the number of partitions of the RDD text_file, which value do you obtain? What does this value represent?\nList the files that are stored in the output folder temperatures_10.rdd.out under your HDFS folder. What do you notice? Is there any relation with respect to the number of partitions?\nGood to know\nIn order to list the content of the folder in HDFS, you can use the following command:\nhdfs dfs -ls hdfs://sar01:9000/hpdaspark24/hpdaspark24_XX/temperatures_10.rdd.out\n2 Using the DataFrame API to compute the average temperatures You’re now going to implement a Spark program to compute the average temperatures by using the Spark DataFrame API.\nGo through the following steps:\nCopy the code template avg_temperatures_df.py to your home folder by executing the following command: cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_df.py .\nThe result of the computation will be stored in the folder temperatures_*.df.out under your HDFS folder hpdaspark24/hpdaspark24_XX. Exercise\nExercise 2.1 Line 78. Replace sarXX with sar01.\nLine 89. Replace sarXX with sar01.\nLine 106. Complete the instruction to read from the input CSV file. Please note that the input CSV files do not have headers. Don’t use schema inference, just specify your schema manually. As a reminder, the columns are: year, month, day, hour, minute, second, temperature.\nLine 55. Complete the definition of function avg_temperature_df.\nExecute your code on all the input CSV files and complete Table 2. File Execution time RDD\n(sec) Execution time DataFrame\n(sec) temperatures_86400.csv 3.12 temperatures_2880.csv 3.67 temperatures_86.csv 4.59 temperatures_10.csv Exercise 1.1 Table 2. RDDs vs. DataFrames. Exercise\nExercise 2.2 Compare the execution times with the ones obtained with the implementation using the RDDs. What do you observe? How do you explain the differences?\n2.1 Caching a DataFrame You’re now going to discover the advantages of caching a DataFrame.\nUncomment the last two lines in file avg_temperatures_df.py\nRemove the file temperatures_10.df.out by typing the following command:\nhdfs dfs -rm -r hdfs://sar01:9000/hpdaspark24/hpdaspark24_XX/temperatures_10.df.out\nExercise\nExercise 2.3 Execute the code on file temperatures_10.csv. What is the execution time of each action? Can you explain in detail what is going on here?\nRemove files temperatures_10.df.out and temperatures_10.df.out.bis. Exercise\nExercise 2.4 Cache the DataFrame df_avg and execute the code again on file temperatures_10.csv.\nWhere should you add the cache instruction?\nWhat is the execution time of each action?\nCan you explain in detail what is going on here?\n3 Computing averages with SQL You’re now going to implement the computation of the yearly average temperatures by using SQL on Spark DataFrames.\n3.1 Using a view A first option to query a DataFrame with SQL is to create a view.\nCopy the file avg_temperatures_sql_view.py to your home folder by typing the following command: cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_view.py .\nComplete the code in lines 90, 101, 118.\nImplement the function avg_temperature_sql (line 57).\nExercise\nExercise 3.1 Execute the code on all CSV files and complete Table 3.\nWhat can you tell about the the running times ? Do you find significant differences between using SQL on a view and DataFrame functions? File Execution time RDD\n(sec) Execution time DataFrame\n(sec) Execution time SQL view\n(sec) temperatures_86400.csv 3.12 Exercise 2.1 temperatures_2880.csv 3.67 Exercise 2.1 temperatures_86.csv 4.59 Exercise 2.1 temperatures_10.csv Exercise 1.1 Exercise 2.1 Table 3. RDDs vs DataFrames with views. 3.2 Using a table A second option to query a DataFrame with SQL is to create a table.\nCopy the file avg_temperatures_sql_table.py to your home directory by typing the following command: cp /usr/users/prof/cpu_quercini/spark-sql-templates/avg_temperatures_sql_table.py .\nComplete lines 50, 112, 123 and 140.\nImplement the function avg_temperature_sql (line 76).\nExercise\nExercise 3.2 Execute the code on all files.\nComplete Table 4.\nCompare the execution times with the ones that you obtained. Discuss the results. File Execution time RDD\n(sec) Execution time DataFrame\n(sec) Execution time SQL view\n(sec) Execution time SQL table\n(sec) temperatures_86400.csv 3.12 Exercise 2.1 Exercise 3.1 temperatures_2880.csv 3.67 Exercise 2.1 Exercise 3.1 temperatures_86.csv 4.59 Exercise 2.1 Exercise 3.1 temperatures_10.csv Exercise 1.1 Exercise 2.1 Exercise 3.1 Table 4. RDDs vs DataFrames with views and tables. Remove the file temperatures_10.sql.table.out\nExecute the code again on file temperatures_10.csv.\nExercise\nExercise 3.3 What is the execution time that you obtain now?\n4 Using the DataFrame API on large files We now consider the files stored under hdfs://sar01:9000/data/sales/.\nThese files contain tabular data related to the sale of products in a chain of stores. We consider two tables: store_sales and customer. In the first table we find information about each sale, such as the identifier of the product sold, the identifier of the buyer, the quantity of purchased product and the price paid by the customer. For this table, we have 4 files, which only differ in size:\nstore_sales_.100.dat: contains 9.5 GiB of data.\nstore_sales_.200.dat: contains 19 GiB of data.\nstore_sales_.400.dat: contains 38 GiB of data.\nstore_sales_.800.dat: contains 77 GiB of data.\nIn table customer we find data about customers, such as first and last names and birth dates. We only have one file for this table:\ncustomer_10000.dat: contains 8.3 GiB of data. We want to test the performances of the DataFrame API on the following queries (WARNING. you must write a code that uses DataFrame functions, not SQL!):\nQuery Q1: returns the number of clients. This corresponds to the following SQL query: SELECT count(*) FROM customer Query Q2: returns the price of the most expensive product. This corresponds to the following SQL query:\nSELECT max(ss_list_price) FROM store_sales Query Q3: returns the amount of money spent by each client. This corresponds to the following SQL query:\nSELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales GROUP BY ss_customer_sk Query Q4: Query Q3 + sort the result so that the client that spent the most money appears on the top. This corresponds to the following SQL query:\nSELECT ss_customer_sk, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales GROUP BY ss_customer_sk ORDER BY amountSpent DESC Query Q5: Query Q4 + join with the table customer to get the first and last name of the customers. This corresponds to the following SQL query:\nSELECT c.c_first_name, c.c_last_name, SUM(ss_net_paid_inc_tax) as amountSpent FROM store_sales s JOIN customer c ON s.ss_customer_sk = c.c_customer_sk GROUP BY ss_customer_sk ORDER BY amountSpent DESC 4.1 Development of the code using the DataFrame API. Copy the file dataframe_api_benchmark.py to your home directory by typing the following command: cp /usr/users/prof/cpu_quercini/spark-sql-templates/dataframe_api_benchmark.py . Modify the code by following the instructions in the file.\nExecute the code on file store_sales_100.dat (the smallest one) to test that your code is bug-free.\nOnce you’re sure that your code is correct, uncomment lines 82 and 83. This will cache the two DataFrames.\nExecute the code on all files store-sales_*.dat\nGood to know\nEach query is executed 5 times to have a correct estimate of the execution time. You’ll see that the execution times fluctuate on the first iterations and they stabilize in the later iterations. When you write down the execution times, only consider the execution times obtained at the last iteration.\nExercise\nExercise 4.1 Complete Table 5 and write down the execution time of each query for each file.\nWhy the execution time of the queries Q1 and Q2 is large at the iteration 0?\nDo you think that the difference between the execution times of the queries is reasonable?\nDo you think that the augmentation of the execution times is reasonable given the size of the input files?\nFile / query Read\n(sec) Query Q1\n(sec) Query Q2\n(sec) Query Q3\n(sec) Query Q4\n(sec) Query Q5\n(sec) store_sales_1_4.100.dat 17.24 0.94 1.05 1.17 2.16 5.22 store_sales_1_4.200.dat store_sales_1_4.400.dat store_sales_1_4.800.dat Table 5. Execution times of the queries on the sales dataset. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e668b7e668636e9711fc8aef29ab9801","permalink":"/courses/bigdata/tutorials/spark-sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/spark-sql/","section":"courses","summary":"Spark Structured API","tags":null,"title":"Spark Structured API","type":"docs"},{"authors":null,"categories":null,"content":" 1 Introduction The goal of this tutorial is to learn how to analyze streams of data with the Spark Structured Streaming API.\nDocumentation\nIn order to answer the questions and do the exercises, you might want to refer to the following documentation:\nThe Structured Streaming programming guide.\nThe Spark SQL API reference.\n2 Warming up Copy the file warmup.py to your home folder by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/warmup.py .\nExercise\nExercise 2.1 Read the code and answer the following questions.\nWhere does this program get its input from?\nWhat object type does the variable lines contain (instruction at Line 28)?\nWhere does this program write its output?\nWhat is the output of this program?\nWhat is the option checkpointLocation intended for?\nWhat does the instruction streamingQuery.awaitTermination()?\nYou can now verify your answers to the previous questions by executing the program.\nActivity\nCreate a checkpoint directory for the first exercise (e.g., checkpoint_dir) under your home directory hdfs://sar01:9000/bdiaspark23/bdiaspark23_X in HDFS. The command to do so is as follows: hdfs dfs -mkdir hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir\nComplete line 21 of file warmup.py. Choose a port number in the interval [49152, 65535].\nComplete line 25 of file warmup.py. Write the path to the checkpoint location that you created on step 1.\nOpen a new terminal window and connect to the same kyle machine to which you’re connected in the first terminal window:\nssh kyleXX\nIn the new terminal window, start a netcat server. Use the following command. Replace [port_number] with the port number that you chose on step 2. The command will hang waiting for some input. This is normal. nc -lk [port_number]\nIn the old terminal window, execute the Spark program with spark-submit. Wait until Spark displays the content of the first micro-batch. In case the program stops for an error, remove all files generated in the checkpoint location with the following command and restart the Spark program. hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*\nIn the terminal window where the netcat server is running, write few lines of text. Look at the terminal where the Spark program is running and observe the output. Stop the program\nWhen you’re done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running.\nRemove all files in the checkpoint directory.\nhdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*\nDon’t stop the netcat server, you’ll need it in the next exercise. 3 Triggers In a Structured Streaming program we can choose a trigger type. A trigger determines when the streaming source is checked for new data.\nBy looking at the Structured Streaming programming guide, answer the following questions.\nExercise\nExercise 3.1 What is the trigger type in the previous program?\nModify the code of the previous program in order to set the Fixed interval micro-batches trigger type. Set an interval of 10 seconds.\nRun the program. Write many lines back to back on the netcat server. How is the behavior of this program different from before?\n4 Checkpoint location and output mode We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.\nExercise\nExercise 4.1 Look again at the Structured Streaming programming guide, and check the options for the output mode.\nWhat is the output mode of the previous program?\nYou’re now going to code a program that counts the number of occurrences of each word in a streaming data source. But first do the actions that you find in the following Activity.\nActivity\nCreate a copy of the file warmup.py and rename it to wordcount.py by typing the following command: cp warmup.py wordcount.py\nRemove all files in the checkpoint location by typing the following command: hdfs dfs -rm -r hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/checkpoint_dir/*\nExercise\nExercise 4.2 Before you code the program, you should first answer the following questions:\nObserve the output of a batch in the previous executions of the Spark programs. What is the type of the output?\nGiven the type of the output, which Spark API are you going to use to code the program?\nWhich operations do you need to execute to count the number of occurrences of each word across batches? For the moment, don’t worry about the specific functions you need to use, just think in abstract terms.\nLook at the Structured Streaming programming guide. Which output mode can’t you use? Why?\nExercise\nExercise 4.3 Add to the file wordcount.py the instructions to count the number of occurrences of each word in the stream. Where are you going to add the new instructions?\nBased on the answer to the previous exercises, set the appropriate output mode. You have two choices. Try one of them.\nMake sure that the netcat server is still running.\nRun the program with spark-submit. In the netcat terminal, write few lines and observe the output of the Spark program.\nStop the Spark program and run it again with no modifications. The first time you get a java.lang.IndexOutOfBoundsException. Don’t lose your hope and run it again.\nWrite few lines in the netcat terminal and observe the output of the Spark program. What can you say about the word counts?\nStop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts?\nRun the program again with a different output mode and observe the result.\n5 Window operations on event time Netcat and checkpoint\nImportant actions to do!\nStop the netcat server now.\nRemove all files from the checkpoint directory.\nWe’re now going to find out how to perform aggregations over a sliding event-time window.\nA given data source generates two words every second for a certain amount of time. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the event time.\nAfter generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we’ll refer to this directory as the source directory.\nAt any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format word,timestamp (no whitespace before nor after the comma).\nActivity\nCreate the source directory under your home directory in HDFS, by typing the following command: hdfs dfs -mkdir hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir\nCopy to your home directory the file tempws_gen.py (the data source) by typing the following command: the data generator that you find at the following path cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/tempws_gen.py .\nCopy to your home directory the file win_events.py by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/win_events.py .\nYou now need to complete the code in file win_events.py.\nExercise\nExercise 5.1 Open the file win_events.py.\nComplete Line 19. Specify the path to the source directory that you created in the previous activity.\nComplete Line 31. Write the query to count the number of occurrences of each word within a 10 second window that slides every 5 seconds. Look at the documentation of the window function to learn how to do it.\nComplete Line 34. Specify the path to the checkpoint directory.\nComplete Line 40. Write the code to write the output of the streaming query to console.\nUse triggers of 5 seconds. Use the output mode update. Don’t forget to specify the location of the checkpoint directory. Execute win_events.py with spark-submit.\nIf you get some errors, stop the program. Correct the errors in the code and then re-execute the program again. If no error arises, the program should hang waiting for some input. Let the program wait and go on to the next exercise. We now test the Spark program.\nActivity\nOpen a new terminal window and type the following command to connect to the same kyle machine where you’re connected in the first terminal window: ssh kyleXX\nExecute the data generator program by typing the following command (remember to replace X with your account number!): python3 tempws_gen.py -s hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir\nAfter launching the data generator, the generated words are shown in the terminalyou should see some output in the terminal window where you launched the Spark program. Wait for the script tempws_gen.py to terminate the data generation.\nIf you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory.\nWe now want to analyze the output of the program.\nActivity\nCreate a new folder on your own computer (not the computers in the cluster) and open it in a new Visual Studio Code window. For the sake of simplicity, let’s call this folder window_analysis.\nThe script tempws_gen.py has generated a file gen_words.csv in your home directory in the cluster. This file contains the list of all words generated with the relative timestamps.\nOpen gen_words.csv and copy the whole content.\nCreate a new file gen_words.csv under window_analysis and paste the content that you copied on the previous step.\nCopy the file timeline_visualization.py into your home folder in the cluster by typing the following command:\ncp /usr/users/cpu-prof/cpu_quercini/structured_streaming/timeline_visualization.py .\nOpen the file timeline_visualization.py and copy all its content.\nCreate a new file timeline_visualization.py under window_analysis and paste the content that you copied on the previous step.\nIn the Visual Studio Code window where you opened window_analysis, open a terminal and run the following command: WARNING. If you’re on macOS, you should type python3 instead of python.\npython timeline_visualization.py -i gen_words.csv -ft [first timestamp] -lt [last timestamp] -it 5 --slide 5\nwhere:\ngen_words.csv is the file that you previously created.\nReplace first timestamp with the timestamp of the left boundary of the first window (look at the output of your Spark program).\nReplace last timestamp with the timestamp of the right boundary of the last window (look at the output of your Spark program).\nThe option -it is used to specify the interval between two consecutive triggers.\nThe option --slide is used to specify the window slide frequency.\nThe plot will show a grid line at each trigger and a blue line at each window boundary.\nExercise\nExercise 5.2 Analyze the output of your Spark program and the timeline of the generated words.\nDescribe how the counts are updated by the Spark program.\n6 Late data and watermarking We’re now going to learn how Structured Streaming handles late data in windowed aggregations.\nRemove generated files\nRemove all the files in the source directory.\nRemove all the files in the checkpoint directory.\nThe data generator tempws_gen.py can generate a stream of words, some of which might be written to the directory tempws with some amount of delay. In other words, there is a gap between the event time (when the word is generated) and the processing time (when the word is written to the directory).\nGood to know\nTo generate data with some delay you can use the following command (remember to replace the X with your account number!):\npython3 tempws_gen.py -s hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/source_dir --delay 0.8\nIn the previous command, the 0.8 indicates that the probability that an event arrives with some delay is 80%. The delay is between 5 and 15 seconds. You can adjust these values by using the appropriate options. To learn more, type the following command:\npython3 tempws_gen.py --help\nSpark uses a mechanism called watermarking to specify the maximum delay that an event can take to be counted. If the difference between the arrival time and the event time is higher than a certain threshold (the watermark), the delayed data will be discarded.\nExercise\nExercise 6.1 Write a Spark program that does the same aggregation as in the previous exercise. Additionally, the program must use watermarking to handle late data. You can look at the programming guide to learn how to do it\nStart the Spark program.\nGenerate some data with delay with the program tempws_gen.py. Once the data generation stops, you can stop the Spark program.\nVisualize the generated words with the visualization tool. Late words have the delay indicated between parentheses.\nObserve the output of the Spark program and describe how the watermarking mechanism works on this example.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f182e031eb58f9594caf6f5a6ac56051","permalink":"/courses/bdia/tutorials/spark-structured-streaming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/spark-structured-streaming/","section":"courses","summary":"Spark structured streaming","tags":null,"title":"Spark structured streaming","type":"docs"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"137dc9cba074d30d32fe28b9317e31f8","permalink":"/courses/bdia_old/tutorials/structured-streaming-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/structured-streaming-lab/","section":"courses","summary":"Description of the lab structured streaming.","tags":null,"title":"Spark structured streaming","type":"docs"},{"authors":null,"categories":null,"content":" 1 Introduction The goal of this tutorial is to learn how to analyze streams of data with the Spark Structured Streaming API.\nDocumentation\nIn order to answer the questions and do the exercises, you might want to refer to the following documentation:\nThe Structured Streaming programming guide.\nThe Spark SQL API reference.\n2 Warming up Copy the file warmup.py to your home folder by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/warmup.py .\nExercise\nExercise 2.1 Read the code and answer the following questions.\nWhere does this program get its input from?\nWhat object type does the variable lines contain (instruction at Line 28)?\nWhere does this program write its output?\nWhat is the output of this program?\nWhat is the option checkpointLocation intended for?\nWhat does the instruction streamingQuery.awaitTermination()?\nSolution\nThe program gets its input from a TCP socket.\nlines is a DataFrame.\nThe program writes the output to the console.\nAny input is copied to the output without any transformation. We didn’t call any function on the DataFrame line.\nThe checkpoint location is a reference to a HDFS folder where the output of a streaming query on a micro-batch is saved.\nThe instruction awaitTermination() waits for the user to stop the Spark program to interrupt the streaming query.\nYou can now verify your answers to the previous questions by executing the program.\nActivity\nCreate a checkpoint directory for the first exercise (e.g., checkpoint_dir) under your home directory hdfs://sar01:9000/itpspark23/itpspark23_X in HDFS. The command to do so is as follows: hdfs dfs -mkdir hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir\nComplete line 21 of file warmup.py. Choose a port number in the interval [49152, 65535].\nComplete line 25 of file warmup.py. Write the path to the checkpoint location that you created on step 1.\nOpen a new terminal window and connect to the same kyle machine to which you’re connected in the first terminal window:\nssh kyleXX\nIn the new terminal window, start a netcat server. Use the following command. Replace [port_number] with the port number that you chose on step 2. The command will hang waiting for some input. This is normal. nc -lk [port_number]\nIn the old terminal window, execute the Spark program with spark-submit. Wait until Spark displays the content of the first micro-batch. In case the program stops for an error, remove all files generated in the checkpoint location with the following command and restart the Spark program. hdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*\nIn the terminal window where the netcat server is running, write few lines of text. Look at the terminal where the Spark program is running and observe the output. Solution\nThe complete code is available in file:\n/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/warmup.py\nStop the program\nWhen you’re done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running.\nRemove all files in the checkpoint directory.\nhdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*\nDon’t stop the netcat server, you’ll need it in the next exercise. 3 Triggers In a Structured Streaming program we can choose a trigger type. A trigger determines when the streaming source is checked for new data.\nBy looking at the Structured Streaming programming guide, answer the following questions.\nExercise\nExercise 3.1 What is the trigger type in the previous program?\nModify the code of the previous program in order to set the Fixed interval micro-batches trigger type. Set an interval of 10 seconds.\nRun the program. Write many lines back to back on the netcat server. How is the behavior of this program different from before?\nSolution\nWe didn’t specify any trigger type, therefore the default one is applied (micro-batches mode).\nThe complete code is available in file:\n/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/warmup-fixed-intervals.py\nEverything that we write within 10 seconds is considered to be part of the same micro-batch, while before each line was a single micro-batch. 4 Checkpoint location and output mode We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.\nExercise\nExercise 4.1 Look again at the Structured Streaming programming guide, and check the options for the output mode.\nWhat is the output mode of the previous program? Solution\nWe didn’t specify any output mode. So the default option is considered. The default output mode is append. Only the new output since the last trigger will be displayed.\nYou’re now going to code a program that counts the number of occurrences of each word in a streaming data source. But first do the actions that you find in the following Activity.\nActivity\nCreate a copy of the file warmup.py and rename it to wordcount.py by typing the following command: cp warmup.py wordcount.py\nRemove all files in the checkpoint location by typing the following command: hdfs dfs -rm -r hdfs://sar01:9000/itpspark23/itpspark23_X/checkpoint_dir/*\nExercise\nExercise 4.2 Before you code the program, you should first answer the following questions:\nObserve the output of a batch in the previous executions of the Spark programs. What is the type of the output?\nGiven the type of the output, which Spark API are you going to use to code the program?\nWhich operations do you need to execute to count the number of occurrences of each word across batches? For the moment, don’t worry about the specific functions you need to use, just think in abstract terms.\nLook at the Structured Streaming programming guide. Which output mode can’t you use? Why? Solution\nThe output of a batch is a DataFrame with one column named value.\nWe need to use the Spark DataFrame API.\nWe need to split the string in the column value into its constituent words. The package pyspark.sql.functions contains a function split that allows us to do so. Then, we need to aggregate by word and count.\nAppend mode (the default one) cannot be used. In fact, we need to use an aggregating function (count), that can update the count of a word seen in a previous batch. This is not supported in append mode.\nExercise\nExercise 4.3 Add to the file wordcount.py the instructions to count the number of occurrences of each word in the stream. Where are you going to add the new instructions?\nBased on the answer to the previous exercises, set the appropriate output mode. You have two choices. Try one of them.\nMake sure that the netcat server is still running.\nRun the program with spark-submit. In the netcat terminal, write few lines and observe the output of the Spark program.\nStop the Spark program and run it again with no modifications. The first time you get a java.lang.IndexOutOfBoundsException. Don’t lose your hope and run it again.\nWrite few lines in the netcat terminal and observe the output of the Spark program. What can you say about the word counts?\nStop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts?\nRun the program again with a different output mode and observe the result.\nSolution\nThe instructions need to be inserted right below the load instruction and before writing the result to the output stream. The code is available in the following file:\n/usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/wordcount.py\nThe output mode that is chosen in the file is complete. The other possible is update.\nOK, running\nAt each micro-batch, the set of all words written up to that point is shown withe corresponding counts.\nThe word counts that we had in the previous execution are still available in the output. They are saved in the checkpoint directory.\nNow, the words counts that we had in the previous executions are not available anymore.\nSince we tried the complete mode, we try the update mode. At each micro-batch, only the word counts that change are displayed.\n5 Window operations on event time Netcat and checkpoint\nImportant actions to do!\nStop the netcat server now.\nRemove all files from the checkpoint directory.\nWe’re now going to find out how to perform aggregations over a sliding event-time window.\nA given data source generates two words every second for a certain amount of time. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the event time.\nAfter generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we’ll refer to this directory as the source directory.\nAt any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format word,timestamp (no whitespace before nor after the comma).\nActivity\nCreate the source directory under your home directory in HDFS, by typing the following command: hdfs dfs -mkdir hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir\nCopy to your home directory the file tempws_gen.py (the data source) by typing the following command: the data generator that you find at the following path cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/tempws_gen.py .\nCopy to your home directory the file win_events.py by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/structured_streaming/win_events.py .\nYou now need to complete the code in file win_events.py.\nExercise\nExercise 5.1 Open the file win_events.py.\nComplete Line 19. Specify the path to the source directory that you created in the previous activity.\nComplete Line 31. Write the query to count the number of occurrences of each word within a 10 second window that slides every 5 seconds. Look at the documentation of the window function to learn how to do it.\nComplete Line 34. Specify the path to the checkpoint directory.\nComplete Line 40. Write the code to write the output of the streaming query to console. Use triggers of 5 seconds. Use the output mode update. Don’t forget to specify the location of the checkpoint directory. Execute win_events.py with spark-submit. If you get some errors, stop the program. Correct the errors in the code and then re-execute the program again. If no error arises, the program should hang waiting for some input. Let the program wait and go on to the next exercise. Solution\nThe complete code is available in the following file in the cluster: /usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/win_events.py\nWe now test the Spark program.\nActivity\nOpen a new terminal window and type the following command to connect to the same kyle machine where you’re connected in the first terminal window: ssh kyleXX\nExecute the data generator program by typing the following command (remember to replace X with your account number!): python3 tempws_gen.py -s hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir\nAfter launching the data generator, the generated words are shown in the terminalyou should see some output in the terminal window where you launched the Spark program. Wait for the script tempws_gen.py to terminate the data generation.\nIf you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory.\nWe now want to analyze the output of the program.\nActivity\nCreate a new folder on your own computer (not the computers in the cluster) and open it in a new Visual Studio Code window. For the sake of simplicity, let’s call this folder window_analysis.\nThe script tempws_gen.py has generated a file gen_words.csv in your home directory in the cluster. This file contains the list of all words generated with the relative timestamps.\nOpen gen_words.csv and copy the whole content.\nCreate a new file gen_words.csv under window_analysis and paste the content that you copied on the previous step.\nCopy the file timeline_visualization.py into your home folder in the cluster by typing the following command:\ncp /usr/users/cpu-prof/cpu_quercini/structured_streaming/timeline_visualization.py .\nOpen the file timeline_visualization.py and copy all its content.\nCreate a new file timeline_visualization.py under window_analysis and paste the content that you copied on the previous step.\nIn the Visual Studio Code window where you opened window_analysis, open a terminal and run the following command: WARNING. If you’re on macOS, you should type python3 instead of python.\npython timeline_visualization.py -i gen_words.csv -ft [first timestamp] -lt [last timestamp] -it 5 --slide 5\nwhere:\ngen_words.csv is the file that you previously created.\nReplace first timestamp with the timestamp of the left boundary of the first window (look at the output of your Spark program).\nReplace last timestamp with the timestamp of the right boundary of the last window (look at the output of your Spark program).\nThe option -it is used to specify the interval between two consecutive triggers.\nThe option --slide is used to specify the window slide frequency.\nThe plot will show a grid line at each trigger and a blue line at each window boundary.\nExercise\nExercise 5.2 Analyze the output of your Spark program and the timeline of the generated words.\nDescribe how the counts are updated by the Spark program. 6 Late data and watermarking We’re now going to learn how Structured Streaming handles late data in windowed aggregations.\nRemove generated files\nRemove all the files in the source directory.\nRemove all the files in the checkpoint directory.\nThe data generator tempws_gen.py can generate a stream of words, some of which might be written to the directory tempws with some amount of delay. In other words, there is a gap between the event time (when the word is generated) and the processing time (when the word is written to the directory).\nGood to know\nTo generate data with some delay you can use the following command (remember to replace the X with your account number!):\npython3 tempws_gen.py -s hdfs://sar01:9000/itpspark23/itpspark23_X/source_dir --delay 0.8\nIn the previous command, the 0.8 indicates that the probability that an event arrives with some delay is 80%. The delay is between 5 and 15 seconds. You can adjust these values by using the appropriate options. To learn more, type the following command:\npython3 tempws_gen.py --help\nSpark uses a mechanism called watermarking to specify the maximum delay that an event can take to be counted. If the difference between the arrival time and the event time is higher than a certain threshold (the watermark), the delayed data will be discarded.\nExercise\nExercise 6.1 Write a Spark program that does the same aggregation as in the previous exercise. Additionally, the program must use watermarking to handle late data. You can look at the programming guide to learn how to do it\nStart the Spark program.\nGenerate some data with delay with the program tempws_gen.py. Once the data generation stops, you can stop the Spark program.\nVisualize the generated words with the visualization tool. Late words have the delay indicated between parentheses.\nObserve the output of the Spark program and describe how the watermarking mechanism works on this example. Solution\nThe complete code is available in the following file in the cluster: /usr/users/cpu-prof/cpu_quercini/structured-streaming-playground/win_event_watermarking.py\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7da488ecedb8e8487aee2accdce9d3d0","permalink":"/courses/bigdata/tutorials/structured-streaming-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/structured-streaming-lab/","section":"courses","summary":"Spark structured streaming","tags":null,"title":"Spark structured streaming","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The goal of this lab assignment is to learn how to analyze streams of data with the Spark Structured Streaming API. Refer to this documentation to learn how to connect and interact with the cluster.\nDocumentation\nIn order to answer the questions and do the exercises, you might want to refer to the following documentation:\nThe Structured Streaming programing guide.\nThe Spark SQL API reference.\n1 Warming up Consider the following program.\nfrom pyspark.sql import SparkSession from pyspark.sql.types import * import pyspark.sql.functions as F port_number = COMPLETE HERE checkpoint_location = COMPLETE HERE spark = (SparkSession.builder.appName(\u0026quot;Structured Streaming - exo1\u0026quot;).getOrCreate()) lines = (spark\\ .readStream.format(\u0026quot;socket\u0026quot;)\\ .option(\u0026quot;host\u0026quot;, \u0026quot;localhost\u0026quot;)\\ .option(\u0026quot;port\u0026quot;, port_number)\\ .load()) streamingQuery = lines.writeStream\\ .option(\u0026quot;checkpointLocation\u0026quot;, checkpoint_location)\\ .format(\u0026quot;console\u0026quot;).start() streamingQuery.awaitTermination() Exercise\nExercise 1.1 Where does this program get its input from?\nWhat object type does the variable lines contain?\nWhere does this program write its output?\nWhat is the output of this program?\nWhat is the option checkpointLocation intended for?\nWhat does the instruction streamingQuery.awaitTermination()?\nSolution\nThe input is a socket, where a program generates some data.\nlines is a dataframe.\nTo the console.\nJust copies the input to the output.\nIt is where the Spark program writes the progress of the streaming query.\nThe instructions means that the program will block on this instruction. The program won’t stop until there is data to process.\nYou can now verify your answers to the previous questions by executing the program.\nActivity\nConnect to the cluster, if you haven’t done so yet. Refer to this documentation.\nAfter running the command srun ..., you should be connected to a machine on the cluster Kyle. Note the name of this machine (you should see it at the terminal prompt).\nCreate a checkpoint directory for the first exercise (e.g., checkpoint_exo1) under your home directory hdfs://sar01:9000/cpuecm1/cpuecm1_X in HDFS.\nCopy and paste the code into a Python file (e.g., exo1.py) that you’ll save into your home directory in the local filesystem of the cluster machine. Change the value of the variable checkpoint_location so that it points to the directory that you created at point 3. Change the value of the variable port_number to any value in the range [49152, 65535]. Open a new terminal window, connect to phome.metz.supelec.fr and then to the same machine that you noted at point 2.\nIn the new terminal, start a netcat server listening on the port number that you selected at point 4. Use the following command:\nnc -lk port_number Run the Python code with the command spark-submit. Wait until Spark does not display any more messages on screen. In case the program stops for an error, read the box “What to do in case of errors” below. In the netcat terminal, write few lines of text. Look at the terminal where the Spark program is running and observe the output. What to do in case of errors\nIf any error arises, before running the spark-submit again it would be better to remove all files from the checkpoint directory.\nStop the program\nWhen you’re done with your experiments, you can stop the Spark program by simply typing CTRL-C in the terminal where Spark is running.\nDon’t stop the netcat server, you’ll need it in the next exercise.\nRemove all files from the checkpoint location.\n2 Triggering policy In a Structured Streaming program we can choose a triggering policy.\nExercise\nExercise 2.1 What is a triggering policy?\nWhat is the triggering policy in the previous program?\nModify the code of the previous program in order to set the Fixed interval micro-batch triggering policy.\nRun the program. How is the behaviour of this program different from before?\nSolution\nIt is a policy that dictates the timing of streaming data processing.\nNo triggering policy is specified, so the default is chosen. In practice, as soon as the previous micro-batch finishes processing, the next one is read in.\nHere is the code. We need to specify a `trigger.\nstreamingQuery = lines.writeStream .trigger(processingTime = \u0026quot;15 seconds\u0026quot;) .format(\u0026quot;console\u0026quot;).start() The new data is checked at the specified interval. It is possible that within the specified interval (15 seconds), we write many lines, so the program will get multiple lines in a micro-batch (unlike before, when the processing is triggered as soon as there is data available). 3 Checkpoint location and output mode We’re now going to see the impact of the checkpoint location and the output modes on a streaming query.\nExercise\nExercise 3.1 What is an output mode and what are the available options?\nWhat is the output mode of the previous program? Solution\nThe output mode tells Spark how the output is presented. There are several options: append (only the new rows added to the Result Table since the last trigger are visible in the output), complete (the whole Result Table is visible after each trigger) and update (only the rows that were updated since the last trigger are visible in the output).\nIn the previous program we didn’t specify any output mode, so the default mode (append) is selected. We’re now going to write a new streaming query.\nExercise\nExercise 3.2 Create a new checkpoint location in HDFS. You may also keep the same directory as before; in this case, make sure you remove all files from that directory.\nWrite a new program that reads a streaming text from a TCP socket and counts the number of occurrences of each word.\nWhich output mode are you going to choose and why?\nRun the program. Write few lines on the netcat server and observe the output.\nStop the program and run it again with no modifications. Write few lines in the netcat terminal and observe the output. What can you say about the word counts?\nStop the program and remove the files in the checkpoint location. Run the program again and write few lines on the netcat terminal. What can you say about the word counts?\nPlay with the different output modes and observe how the output changes.\nSolution\nThe new program is as follows:\nlines = (spark\\ .readStream.format(\u0026quot;socket\u0026quot;)\\ .option(\u0026quot;host\u0026quot;, \u0026quot;localhost\u0026quot;)\\ .option(\u0026quot;port\u0026quot;, port_number)\\ .load()) lines = lines.select(F.explode(F.split(lines.value, \u0026quot; \u0026quot;))\\ .alias(\u0026quot;word\u0026quot;))\\ .groupBy(\u0026quot;word\u0026quot;).count() streamingQuery = lines.writeStream\\ .trigger(processingTime = \u0026quot;15 seconds\u0026quot;)\\ .option(\u0026quot;checkpointLocation\u0026quot;, checkpoint_location)\\ .outputMode(\u0026quot;update\u0026quot;)\\ .format(\u0026quot;console\u0026quot;)\\ .start() streamingQuery.awaitTermination() The append mode doesn’t work, because aggregating function might modify previous lines of the ResultTable. So, the only options left are update and complete. We choose update to just have the values that changed since the last trigger.\n4 Window operations on event time Netcat and checkpoint\nYou can stop the netcat server now.\nRemember to create a new checkpoint location for this exercise. Alternatively, you can also use the same directory as in the previous exercises, but you should remove all its files.\nWe’re now going to find out how to perform aggregations over a sliding event-time window.\nA given data source generates some words for a certain time interval. Each word is accompanied with a timestamp that indicates the exact moment when the word is generated. This timestamp is the event time.\nAfter generating a word, the data source saves the word and its timestamp into a CSV file in a directory on HDFS. For convenience, we’ll refer to this directory as the source directory.\nActivity\nCreate the source directory under your home directory hdfs://sar01:9000/cpuecm1/cpuecm1_X in HDFS.\nAt any given moment, the source will contain zero to many CSV files, where each file only contains exactly one line in the format word,timestamp (no whitespace before nor after the comma).\nExercise\nExercise 4.1 Write a Spark program that:\nReads the stream of data from the source directory.\nCounts the number of occurrences of each word within 10 minute windows that slide every 5 minutes.\nPrint the output counts to the console. Use triggers of 5 seconds. Solution\nThe new program is as follows:\nsource_directory = hdfs://.... words = (spark .readStream.format(\u0026quot;csv\u0026quot;) .schema(\u0026quot;word STRING, timestamp TIMESTAMP\u0026quot;) .load(source_directory)) windowedCount = words.groupBy(F.window(words.timestamp, \u0026quot;10 seconds\u0026quot;, \u0026quot;5 seconds\u0026quot;, startTime=0), words.word).count() windowedQuery = windowedCount.withColumn(\u0026quot;trigger_timestamp\u0026quot;, F.expr(\u0026quot;get_current_timestamp()\u0026quot;)).writeStream\\ .trigger(processingTime=\u0026quot;5 seconds\u0026quot;)\\ .outputMode(\u0026quot;update\u0026quot;)\\ .format(\u0026quot;console\u0026quot;)\\ .option(\u0026quot;truncate\u0026quot;, False)\\ .start() streamingQuery.awaitTermination() We now test the new Spark program.\nData source and timeline visualization\nWe provide two Python programs for this exercise: a data generator and a tool for visualizing words in a timeline. Instructions to get and run these two programs are given in the activity below.\nThe data generator is our data source. It generates two words every second for a certain amount of time. Each word is saved in a separate CSV file in source directory. It also saves the list of all generated words to a summary CSV file.\nThe visualization tool takes as its input the summary CSV file written by the data generator and visualizes the words on a timeline.\nActivity\nCopy to your home directory in the local filesystem the data generator that you find at the following path /usr/users/cpu-prof/cpu_quercini/structured-streaming/tempws_gen.py Start your Spark program. When running the first time, you might get some errors. Correct your code accordingly.\nIn another terminal, run the Python script tempws_gen.py. Use the following command to learn how to run this program:\npython3 tempws_gen.py --help For this exercise, do not introduce any delay (keep the default values of the parameters --delay, --mindelay, --maxdelay).\nAfter launching the data generator, you should see some output in the terminal where you launched the Spark program. Wait for the script tempws_gen.py to terminate the data generation. The output might be a bit overwhelming. Scroll up to identify the results on each micro-batch.\nIf you need to rerun the Spark program and the data generator, make sure you delete all the files in the checkpoint location and the source directory.\nWe now want to analyze the output of the program.\nThe script tempws_gen.py has generated a file gen_words.csv in your home directory. This file contains the list of all words generated with the relative timestamps. Download the file to your computer.\nDownload the visualization tool that you find at the following path:\n/usr/users/cpu-prof/cpu_quercini/structured-streaming/timeline_visualization.py to your computer.\nVisualization tool\nUse the following command to learn how to run the visualization tool:\npython timeline_visualization.py --help The visualization tool displays a vertical blue bar at each trigger. To this purpose, you’ll need to pass the tool the timestamps associated to the first and last trigger and the interval (in seconds) between two consecutive triggers.\nYou can get the timestamps associated to the first and last trigger by analyzing the output of Spark. More specifically, for each micro-batch, Spark outputs the progress details of the streaming query; you’ll need to look at the timestamp associated to the first and last micro-batch.\nExercise\nExercise 4.2 Analyze the output of your Spark program and the timeline of the generated words.\nDescribe how the counts are updated by the Spark program. Solution\nUnlike the previous exercise, here the number of occurrences of each word is counted based on a time window of 10 seconds that slides every 5 seconds. Each word is associated with an event time that is used to compute the number of occurrences. The time window starts from the first trigger, say at 12:00. If a word arrives at 12:07, the count associated to this word are updated in two time windows, 12:00 - 12:10 and 12:05 - 12:15.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"96a6833d49e7a19d1120d1cc206d0e1a","permalink":"/courses/big-data-marseille/tutorials/spark-streaming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/spark-streaming/","section":"courses","summary":"Tutorial Structured Streaming.","tags":null,"title":"Apache Spark — Structured Streaming","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model. 1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1 Give the conceptual schema of the database with an ER diagram.\nExercise\nExercise 1.2 Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\n2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\nFigure 2.1: The conceptual schema of the bank database Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer. Exercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer. Exercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises. Exercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. 3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, colour, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\nA salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys. When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram. Exercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1872079c6f44d0d87b5b42c90f60dfc5","permalink":"/courses/bdalbert/tutorials/data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdalbert/tutorials/data-modeling/","section":"courses","summary":"Data modeling.","tags":null,"title":"Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to create a conceptual schema of a database. How to draw an entity-relationship (ER) diagram. How to translate a conceptual model into a logical model. Prerequisites:\nHaving attended Lecture 1. 1 Database of a social network platform A social network platform wants to design a relational database to store information on its users. For each user, the platform keeps its nickname, that uniquely identifies the user in the platform, first and family name, geographic location (city and country) and email address; the user can register as many email addresses as s/he wishes. Any user can share content on the platform; each post is characterized by its content, date, time and, when available, the geolocation (latitude, longitude). Optionally, users can tag one or more friends in their posts.\nTwo users are linked by a friendship relationship if both agree on befriending each other; a user can also follow another user without necessarily befriending her. For any type of relationship (friendship or follower), the platform registers the date when the relationship is established.\n1.1 Exercises Exercise\nExercise 1.1 Give the conceptual schema of the database with an ER diagram.\nSolution Exercise\nExercise 1.2 Translate the conceptual schema into a logical schema. For each table, underline the primary key and specify the foreign keys.\nSolution The collection of tables is the following:\nUserAccount (nickname, first_name, last_name, city, country) Post (post_id, content, date, time, lat, long, nickname) EmailAddress (email_address, nickname) Relationship (nickname_src, nickname_dst, type, date) Tag (post_id, nickname) The foreign keys are the following:\nPost(nickname) → UserAccount(nickname).\nEmailAddress(nickname) → EmailAddress(nickname).\nRelationship(nickname_src) → UserAccount(nickname).\nRelationship(nickname_dst) → UserAccount(nickname).\nTag(post_id) → Post(post_id).\nTag(nickname) → UserAccount(nickname).\n2 Database of a banking system The following figure shows the ER diagram with the conceptual schema of a banking system database.\nFigure 2.1: The conceptual schema of the bank database Each bank is identified by a unique code and name, and has one or several branches. A branch is responsible for opening accounts and granting loans to customers. Each account is identified by a number (acct_nbr) and is either a checking or savings account (property acct_type). Each customer is identified by its social security number (ssn); a customer can be granted several loans and open as many accounts as s/he wishes.\n2.1 Exercises Exercise\nExercise 2.1 Which primary key would you choose for the entity Bank? Justify your answer. Solution Since no two banks have the same code_bank or name, either property can be chosen as the primary key of the entity Bank. Both can be considered as valid candidate keys.\nExercise\nExercise 2.2 Would you consider {code_bank, name} as a valid candidate key for the entity Bank? Justify your answer. Solution The answer is no. While there aren’t any banks that have the same value for {code_bank, name}, two subsets ({code_bank} and {name}) are candidate keys.\nExercise\nExercise 2.3 Complete the diagram in the figure by adding the cardinalities to the relations. Justify your choices when any ambiguity arises. Solution Exercise\nExercise 2.4 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. Solution The collection of tables is the following:\nBank (code_bank, name, address) Branch (branch_id, address, code_bank) Account (acct_nbr, acct_type, balance, branch_id, ssn) Loan (loan_nbr, loan_type, amount, branch_id, ssn) Customer (ssn, first_name, last_name, telephone, address) The foreign keys are the following:\nBranch(code_bank) → Bank(code_bank).\nAccount(branch_id) → Branch(branch_id).\nAccount(ssn) → Customer(ssn).\nLoan(branch_id) → Branch(branch_id).\nLoan(ssn) → Customer(ssn).\n3 Car dealership database We want to design the database of a car dealership. The dealership sells both new and used cars, and it operates a service facility. The database should keep data about the cars (serial number, make, model, colour, whether it is new or used), the salespeople (first and family name) and the customers (first and family name, phone number, address). Also, the following business rules hold:\nA salesperson may sell many cars, but each car is sold by only one salesperson. A customer may buy many cars, but each car is bought by only one customer. A salesperson writes a single invoice for each car s/he sells. The invoice is identified by a number and indicates the sale date and the price. A customer gets an invoice for each car s/he buys. When a customer takes one or more cars in for repair, one service ticket is written for each car. The ticket is identified by a number and indicates the date on which the car is received from the customer, as well as the date on which the car should be returned to the customer. A car brought in for service can be worked on by many mechanics, and each mechanic may work on many cars.\n3.1 Exercises Exercise\nExercise 3.1 Give the conceptual schema of the database with an ER diagram. Solution Exercise\nExercise 3.2 Translate the conceptual schema into a logical schema. For each table, underline the primary keys and specify the foreign keys. Solution The collection of tables is the following:\nCar (serial_number, make, model, colour, is_new) Customer (cust_id, cust_first_name, cust_last_name, cust_phone) Invoice (invoice_number, date, price, car_serial_number, sp_id, cust_id) Salesperson (sp_id, sp_first_name, sp_last_name) Mechanic (mec_id, mec_first_name, mec_last_name) Ticket (ticket_number, date_open, date_return, car_serial_number) Repair (ticket_number, mec_id) The foreign keys are the following:\nInvoice(cust_id) → Customer(cust_id).\nInvoice(car_serial_number) → Car(serial_number).\nInvoice(sp_id) → Salesperson(sp_id).\nTicket(car_serial_number) → Car(serial_number).\nRepair(ticket_number) → Ticket(ticket_number).\nRepair(mec_id) → Mechanic(mec_id).\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85c2215a0f57e47f0e3f4cc4fba8d167","permalink":"/courses/databases/tutorials/data-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/data-modeling/","section":"courses","summary":"Description of the data modeling tutorial.","tags":null,"title":"Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to run containers. How to define and build images. How to create and use volumes. How to define and use networks. Prerequisites:\nHaving installed Docker on your computer or having imported the Linux virtual machine either with VirtualBox or with Multipass. Related course material:\nSlides of the first lecture. You can find them on Edunao.\nHandbook (poly) of the first lecture. You can find it on Edunao.\nUsing a virtual machine with Multipass? Click here for more info\nMultipass commands\nTo start the virtual machine, type multipass start cloudvm.\nThe folder /home/ubuntu/labs in the virtual machine is mounted (i.e., linked) to the folder on YOUR computer (let’s call it LAB) where you’ll store all your lab material. You specified this folder when you installed the virtual machine.\nIf you don’t remember the path to the folder linked to /home/ubuntu/labs, type the command multipass info cloudvm.\nTo open a terminal into the virtual machine, type multipass shell cloudvm.\nWhen the terminal opens, the current working directory is /home/ubuntu.\nType cd labs to move to the folder where you’ll find your lab material.\nYou’ll use the virtual machine terminal only to type the Docker commands. You can use all the tools installed on your machine to create and manage the files in the directory LAB directly from your computer.\nOnce the lab is over, type exit to leave the virtual machine terminal. This will lead you back to the terminal of your computer.\nIn the terminal of your computer, type multipass stop cloudvm to stop the virtual machine. This will NOT destroy your files! It just stops the virtual machine from needlessly using the resources of your computer.\nTerminology\nYou’ll use the terminal to run Docker commands. The terminal is the client that communicates with the Docker daemon.\nDocker runs containers on your computer. We’ll refer to your computer as the host, the containers being the guests.\nA containerized application is an application running in a container. 1 Running containers We now learn how to use the command docker run and some of its options. In the following exercises, we’ll run containers from the image named alpine that is available on the DockerHub registry. This image provides a lightweight distribution (i.e., it doesn’t contain many features) of Linux.\nExercise\nExercise 1.1 You want to run the container from the latest version of the image alpine. Which command would you write in the terminal? Execute it. Solution\nThe goal of this exercise is to start playing with the docker run command. Since the question doesn’t say anything about the options, nor does it mention the command to run inside the container, we’d type:\ndocker run alpine\nExercise\nExercise 1.2 Is the container still running? Solution\nIn order to list all containers still running on the host, type the following command:\ndocker container ls\nYour container shouldn’t appear in the output, because it’s not running. In order to see all containers, including those that are not running, type the following command:\ndocker container ls -a\nExercise\nExercise 1.3 By looking at the command executed within the container (/bin/sh), can you tell why the container stopped without giving any output? Solution\nThe command is /bin/sh; the container runs a Linux terminal. But since we didn’t specify what to do with that terminal (we didn’t run any Linux command, nor we tried to access the terminal), the container stopped.\nWe’re now going to do something useful with the image alpine. Make sure you read the good practices that you should adopt while playing with images and containers.\nGood practices\nName your containers. Although Docker assigns a default name to a new container, it’s usually a good practice to give a container a name of your choice to make it easily distinguishable. You can do it by using the option --name. Try the following: docker run --name my-alpine alpine\nAs before, the container stops immediately. If you list all your containers by typing again:\ndocker container ls -a\nyou should see a container named my-alpine.\nRemove automatically a container if you use it once. Unless you want to reuse your container later, you can ask Docker to automatically remove it when it stops by using the option --rm. This will prevent unused containers from taking up too much disk space. Try the following:\ndocker run --rm --name container-to-remove alpine\nIf you list all the containers you should see that there is no container named container-to-remove.\nRemove unused containers. Stopped containers that have been run without using the option --rm are still stored in the host. If you want to remove a specific container (e.g., my-alpine), use the following command: docker container rm my-alpine\nIf you want to remove all stopped containers, use the following command:\ndocker container prune\nRemove unused images. Images can take up a lot of disk space. As a result, you should remember to remove those that you don’t intend to use any longer. The commands to remove a specific image and prune unused ones are docker image rm and docker image prune -a respectively. 1.1 Pass a command to the containerized application Remember that the template of docker run is the following:\ndocker run [options] image-name [command] [arg]\nThe optional parameter command refers to a command that you can pass the containerized application, possibly with some arguments (parameter arg).\nLet’s see an example. As we saw before, when we run a container from the image alpine, a Linux terminal /bin/sh is launched.\nNotice\nThe Linux terminal /bin/sh is run within the container. Henceforth, we’ll use the following terms:\nHost terminal. The terminal that you use to interact with the operating system of your computer. Guest terminal. The terminal that is run within the container. By using the optional parameter command, we can run a command in the guest terminal.\nExercise\nExercise 1.4 Run a container from the image alpine and execute the Linux command ls that lists the content of the current directory.\nWhere are the listed files stored? In the host or in the container? Solution\ndocker run --rm --name ls-test alpine ls\nThe command ls is run in the guest terminal, therefore what we see in the output is a list of files stored in the container. Notice\nIn Exercise 1.4 the command ls is executed in the guest terminal, but its output is redirected to the host terminal.\nIn other words, when we run the container, we don’t interact directly with the guest terminal; we just send a command and the output is redirected to the host terminal.\nNow let’s see how to execute a command in the guest terminal that also requires an argument.\nExercise\nExercise 1.5 By using the Linux utility ping, check whether the Web site www.centralesupelec.fr is reachable. Solution\ndocker run --rm --name ping-test alpine ping www.centralesupelec.fr\nIn order to interrupt ping just type the key combination that you’s use to interrupt any other command in your terminal. (typically Ctrl-C on Windows and Cmd-C in MacOs).\nNow run the following command:\ndocker run --name my-alpine -it alpine\nNote: we didn’t use the option --rm (the container will not be removed when we stop it, we’re going to use it again). Moreover, we didn’t specify any command to run in the guest terminal.\nExercise\nExercise 1.6 What do you obtain? Solution\nWhen we run a container from the image alpine, the command /bin/sh is executed within the container. Since we specified the option -it, what we obtain is an access to the Linux terminal running in the container.\n1.2 Starting and stopping containers. docker run is a shorthand for two Docker commands, namely docker create, that creates a container from an image, and docker start, that starts the container after its creation.\nSuppose now that you want to download a Web page by using Linux Alpine. You can use the Linux command wget followed by the URL of the page that you want to download.\nExercise\nExercise 1.7 By using the guest terminal in the container my-alpine, download this Web page.\nWhere will the Web page be saved? The host computer or the container? Solution\nJust type in my-alpine guest terminal the following command:\nwget https://www.centralesupelec.fr/fr/presentation\nThe Web page will be saved in the current directory of the container. You can verify that the file is there by typing ls in the guest terminal. Want to copy the donwloaded file from the container to your computer?\nTo copy the file presentation to your working directory, type the following command in the host terminal:\ndocker cp \u0026lt;containerId\u0026gt;:/presentation . In the previous command, replace \u0026lt;containerId\u0026gt; with the identifier of your container. You can obtain the identifier of the container from the output of the command docker container ls -a.\nWhere are the containers and image files stored?\nIf you use MacOS or Windows\nThe files managed by Docker are not stored directly in your computer, but in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).\nTherefore, you need to open a terminal inside that Linux virtual machine by typing the following command:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1 Once the terminal is opened, you can follow the instructions given below for Linux users.\nIf you use Docker on Linux\nAll files managed by Docker are stored under folder /var/lib/docker.\nTo access that folder, you need to be root (i.e., administrator). Type the command sudo su.\nIf you type ls /var/lib/docker you can look at the folders stored under this directory. You’ll see that there are folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).\nTo locate the files of a specific container, you first need to get the container identifier by typing docker container ls -a.\nType the command docker inspect \u0026lt;container-id\u0026gt; (replace \u0026lt;container-id\u0026gt; with the identifier of the container that you intend to inspect).\nLocate the field UpperDir. The value of this field is the path to the directory (let’s call it CONTAINER_DIR) that contains the upper layer of the container (the writable layer). It should be a path that ends by /diff.\nIf you type cd CONTAINER_DIR (replace CONTAINER_DIR with the value of the field UpperDir) you can finally see the files stored in your container.\nIn my-alpine guest terminal type exit. This closes the guest terminal and, as a result, stops the container.\nNOTICE\nStopping the container will not erase any of the files stored in the container. Removing the container will.\nIf you want to start the container my-alpine again, you can use the following command:\ndocker container start -ai my-alpine\nThis will open the guest terminal of the container again; type ls to verify that the Web page that you downloaded before is still there.\nHomework (optional)\nSuppose that you need to download all the figures of this Web page. The Linux utility wget comes in handy. However, you don’t have Linux and you’d like to avoid the hassle of installing it on your computer, or in a virtual machine, just for this task.\nA great alternative is to run Linux in a Docker container. Unfortunately, the Alpine distribution that we’ve been playing with doesn’t provide an implementation of wget with all the options that we need.\nWe turn to another Linux distribution, Ubuntu, for which DockerHub has several images.\nExercise\nExercise 1.8 Run a container with Ubuntu (latest) and open a guest terminal. Call the container dl-figures, and avoid the option --rm, we’ll use this container later. Solution\ndocker run --name dl-figures -it ubuntu\nFrom now on, we’ll be interacting with the guest Ubuntu terminal. If you type the command wget, you’ll get an error (bash: wget: command not found).\nNotice\nThe image Ubuntu doesn’t include all the commands that you’d find in a full-blown Ubuntu distribution; the reason is to keep the size of the image small, a necessary constraint given that images are transferred over the Internet.\nLuckily, there’s a way to install wget in our Ubuntu distribution. Ubuntu provides a powerful command-line package manager called Advanced Package Tool (APT). First, you need to run the following command:\napt update\nwhich fetches the available packages from a list of sources available in file /etc/apt/sources.list.\nThen, you can install wget by running the following command:\napt install -y wget\nIn order to obtain all the figures from a Web page, type the following command:\nwget -nd -H -p -P /my-figures -A jpg,jpeg,png,gif -e robots=off -w 0.5 https://www.centralesupelec.fr/fr/presentation You should see in the current directory a new folder named my-figures containing the downloaded figures; verify it by typing ls my-figures.\nBefore terminating, don’t forget to read your fortune cookie. In the shell, run the following command:\napt-get install -y fortune\nand then:\n/usr/games/fortune -s\nWhen you’re done, you can simply type the command exit to quit the guest terminal and stop the container.\n2 Creating Images A Docker image can be thought of as a template to create and run a container. An image is a file that contains a layered filesystem with each layer being immutable; this means that the files that belong to a layer cannot be modified or deleted, nor can files be added to a layer.\nWhen a container is created from an image, it will be composed of all the image read-only layers and, on top of them, a writable layer (termed the container layer), where all the new files created in the container will be written. For example, the Web page that you downloaded in Exercise 1.7 were stored in the writable layer of that container.\n2.1 Dockerfiles The most common way to create an image is to use a Dockerfile, a text file that contains all the instructions necessary to build the image. The advantage of the Dockerfile is that it can be interpreted by the Docker engine, which makes the creation of images an automated and repeatable task.\nSuppose that we want to create a containerized application to download figures from a Web page. As a template for this application, we need to build a new image, that we’ll call fig-downloader.\nThe Dockerfile containing the instructions to build the image fig-downloader is as follows:\nFROM ubuntu RUN apt-get update RUN apt-get install -y wget RUN mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] Here’s the explanation:\nWe use the image ubuntu as the base image. This corresponds to the instruction FROM ubuntu.\nWe install the utility wget in the base image. This corresponds to the instructions RUN apt-get update and RUN apt-get install -y wget.\nWe create a directory my-figures under the root directory of the image. This corresponds to the instruction RUN mkdir -p /my-figures.\nWe set the newly created directory /my-figures as the working directory of the image. This corresponds to the instruction WORKDIR /my-figures.\nWe specify the command to be executed when a container is run from this image. This corresponds to the instruction\nENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;]\nThis instruction means: execute wget with the options -nd, -r, -A; the last option takes a list of file extensions (jpg,jpeg,bmp,png,gif) as its argument.\nRemember that the utility wget takes the URL of the Web page as an argument. The URL will be specified when we run the container from the image fig-downloader. Optionally, we can specify a default argument by using the keyword CMD. The meaning of the instruction: CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;]\nis: if we don’t give any URL when we run the container, the figures will be downloaded from https://www.centralesupelec.fr/fr/presentation.\nExercise\nExercise 2.1 What’s the relation between the Dockerfile lines and the image layers? Solution\nEach line corresponds to a new layer. The first line corresponds to the bottom layer; the last line to the top layer.\nExercise\nExercise 2.2 Could you identify a problem in this Dockerfile? Modify the Dockerfile accordingly. Solution\nWhen creating an image, we should keep the number of layers relatively small; in fact, the more the layers, the bigger the image will be. Here we create three separate layers with three RUN commands; we can simply merge the three layers. The resulting Dockerfile will be:\nFROM ubuntu RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y wget \u0026amp;\u0026amp; \\ mkdir -p /my-figures WORKDIR /my-figures ENTRYPOINT [\u0026quot;wget\u0026quot;, \u0026quot;-nd\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-A\u0026quot;, \u0026quot;jpg,jpeg,bmp,png,gif\u0026quot;] CMD [\u0026quot;https://www.centralesupelec.fr/fr/presentation\u0026quot;] 2.2 Building an image We’re now going to build an image from a Dockerfile.\nCreate a directory named fig-downloader in your computer with a file named Dockerfile inside.\nIn the Dockerfile write the set of instructions that you proposed in Exercise 2.2.\nIn the terminal, set the working directory to fig-downloader.\nBuild an image called fig-downloader by executing the following command:\ndocker build -t fig-downloader .\nThe . at the end of the command means that the Docker engine will look for a file named Dockerfile in the working directory.\nGood to know\nIf you give the Dockerfile a different name (say, Dockerfile-fig-downloader), the command to build the image will be:\ndocker build -t fig-downloader -f Dockerfile-fig-downloader .\nThe option -f is used to specify the name of the Dockerfile.\nIn order to verify that the new image has been created, type the following command:\ndocker images\nExercise\nExercise 2.3 Run the following command:\ndocker run --name dl-1 fig-downloader\nWhat does it do? Where are the downloaded pictures? Solution\nWe downloaded the figures of this page. The downloaded pictures are in the folder /my-figures of the container dl-1.\nExercise\nExercise 2.4 Run the following command:\ndocker run --name dl-2 fig-downloader https://www.centralesupelec.fr/ What does it do? Where are the downloaded pictures? Solution\nWe downloaded the figures of this page. We basically overwrote the URL specified by the CMD keyword with a new one. The downloaded pictures are in the folder /my-figures of the container dl-2.\n2.3 Containerized Python application Download this archive file and unzip it into your working directory. In this archive you’ll find:\nA Dockerfile. A Python script main.py that asks the user to enter the URL and the language of a Web page, and prints the 10 most frequent words occurring in that page. A file requirements.txt with the list of the Python packages needed to run the given script. The content of the Dockerfile is as follows:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./main.py ./requirements.txt /app/ RUN pip install -r requirements.txt ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.5 Describe what this Dockerfile does. Solution\nTakes python:3.7-slim as the base image. Creates a new folder app in the image under the root directory. Changes the working directory to /app. Copies the files main.py and requirements.txt from the local computer to the directory /app in the image. Runs the command pip install to install the Python libraries specified in the file requirements.txt. Executes the command python main.py. Exercise\nExercise 2.6 Build an image called wordfreq from this Dockerfile. Solution\ndocker build -t wordfreq .\nExercise\nExercise 2.7 Without changing the Dockerfile, rebuild the same image. What do you notice? Solution\nThe build is very fast. Since we didn’t change the Dockerfile, the image is rebuilt by using the image layers created previously. This is clearly indicated by the word CACHED written at each layer. Using the already stored layers is called build cache.\nExercise\nExercise 2.8 What happens if you modify a line in the Python script and you rebuild the image? Solution\nAdd any instruction at the end of main.py, such as:\nprint(\u0026quot;Finish!\u0026quot;) then rebuild the image. The three bottom layers are not affected by the modification, therefore they benefit from the build cache. Layer 4 is the first affected by the modification. This layer, and those above, need therefore to be rebuilt.\nExercise\nExercise 2.9 Based on the previous considerations, can you tell what’s wrong with this Dockerfile? Modify the Dockerfile accordingly and rebuild the image. Solution\nEach time we modify main.py and we rebuild the image, the layer 4 and 5 are recreated, meaning that all the Python packages are downloaded and installed. Depending on the size and number of the packages, this might take some while. A better way to structure the Dockerfile is to install the packages before copying the Python script to the image. Here is how we should modify the Dockerfile:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./requirements.txt /app/ RUN pip install -r requirements.txt COPY ./main.py /app/ ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.10 Modify main.py by adding a new line of code and rebuild the image. What changed? Solution\nThe Python packages are not reinstalled, as a result rebuilding the image\ntakes much less time than before.\nPlay with the application by running the following command:\ndocker run --rm -it wordfreq\nThe containerized application will prompt you to insert the URL of a webpage and the language of the page (in English). The output will be the 20 most used words in the webpage.\n3 Data Volumes In Exercise 2.3 you’ve been asked to run a container named dl-1 to download some figures from a Web page. The figures were downloaded into the directory /my-figures of the container. But we left a question unanswered.\nHow do we transfer those figures from the container to the host computer?\nThe solution is to use volumes.\n3.1 Docker volumes A volume can be seen as a virtual storage device attached to a container. All files there are written to a volume survive the containerized application that created them. In other words, when a container is destroyed, all the files created by the application in the container remain in the volume.\nLet’s create a new Docker volume called data-volume:\ndocker volume create data-volume\nGood to know (advanced notion)\nWhere the data will be actually stored?\nYou can inspect the new volume by typing the following command:\ndocker volume inspect data-volume\nA mount point is indicated; that’s the folder where the data will be actually stored. If your computer runs Linux, that folder will be available on the host; if your computer runs Windows or MacOS, you’ll not find that folder on your computer. Instead, it will be available in the virtual machine that Docker use on MacOS and Windows.\nDo you want to see the directory? (Instructions for Windows and MacOS)\nOne way to look into the hidden VM is to run the following containerized application:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1\nThis application will open a guest terminal into the VM. You can then use the commands cd and ls to browse to the directory indicated as the mount path of the new volume.\n3.1.1 Sharing data A Docker volume can be used to share data between containers.\nExercise\nExercise 3.1 Run a container from the image ubuntu, specifying the options to:\nRemove the container once its execution is over.\nInteract with the guest Linux terminal in the container.\nMount the volume data-volume at the container’s directory /data. Solution\ndocker run --rm -it -v data-volume:/data ubuntu\nType the following command in the guest Linux terminal to create a file test-file.txt in the directory /data: echo \u0026quot;This is a new file\u0026quot; \u0026gt; /data/test-file.txt\nPrint to the console the content of the file with the following command: cat /data/test-file.txt\nType exit to leave the guest terminal. Since we’ve specified the option --rm, the container is destroyed. Now we’re going to verify that test-file.txt is still accessible. Exercise\nExercise 3.2 Run a container from the image alpine:latest, specifying the options to:\nRemove the container once its execution is over.\nInteract with the guest Linux terminal in the container.\nMount the volume data-volume to the directory /my-data of the container. Solution\ndocker container run --rm -it -v data-volume:/my-data alpine\nExercise\nExercise 3.3 Verify that you can read the file test-file.txt. Which folder would you look in? Solution\nWe need to look in the folder /my-data because this is where we mounted data-volume.\ncat /my-data/test-file.txt\nType exit to exit the guest terminal and terminate the container.\n4 Single-Host Networking In order to let containers communicate and, therefore, co-operate, Docker defines a simple networking model known as the container network model (figure 4.1).\nIf you use a Linux virtual machine with Multipass In this section, you’ll need to open several terminals in the virtual machine. You can do it easily by using byobu, an advanced window manager already available in your virtual machine.\nJust type byobu to launch the window manager.\nIf you want to open a new terminal, just press F2.\nIf you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4 (to move to next terminal).\nIf you want to close a terminal, just type exit.\nWhen you close all terminals, byobu will stop executing.\nFigure 4.1: The Docker container network model Exercise\nExercise 4.1 Describe the output of the following command:\ndocker network ls\nSolution\nThe command lists all the networks created by Docker on your computer. For each network, the values of four attributes are shown:\nThe identifier. The name. The driver used by the network. The scope of the network (local or global). A local scope means that the network connects containers running on the same host, as opposed to a global scope that means that containers on different hosts can communicate. Depending on the containers that you used in the past, you might see different networks. However, three networks are worth noting:\nThe network named bridge, that uses the driver bridge and a local scope. By default, any new container is attached to this network. The network named host, that uses the driver host and a local scope. It’s used when we want a container to directly use the network interface of the host. It’s important to remember that this network should only be used when analyzing the host’s network traffic. In the other cases, using this network exposes the container to all sorts of security risks. The network named none, that uses the driver null and a local scope. Attaching a container to this network means that the container isn’t connected to any network, and therefore it’s completely isolated. Exercise\nExercise 4.2 The following command:\ndocker network inspect bridge\noutputs the configuration of the network bridge. By looking at this configuration, can you tell what IP addresses will be given to the containers attached to this network? What’s the IP address of the router of this network? Solution\nThe information is specified in the field named IPAM, more specifically:\nSubnet indicates the range of IP addresses used by the network. The value of this field should be 172.17.0.0/16; the addresses range from 172.17.0.1 to 172.17.255.255.\nGateway indicates the IP address of the router of the network. The value should be 172.17.0.1\n4.1 Creating networks By default, any new container is attached to the network named bridge.\nExercise 4.3 Explain why it is not a good practice to attach all our containers to the same network.\nSolution\nAll new containers will be able to communicate over this network. This is not a good idea. If a hacker can compromise any of these containers, s/he might be able to attack the other containers as well. As a rule of thumb, we should attach two containers to the same network only on a need-to-communicate basis.\nIn order to create a new network, you can use the following command:\ndocker network create network_name\nExercise\nExercise 4.4 Create two networks named buckingham and rochefort that use the driver bridge (see figure 4.1). By using the docker network inspect command, look at the IP addresses of the new networks and write them down. Solution\nJust run the following commands:\ndocker network create buckingham\ndocker network create rochefort\nThe IP addresses for the network buckingham are 172.18.0.0/16 (addresses from 172.18.0.1 to 172.18.255.255); The IP addresses for the network rochefort are: 172.19.0.0/16 (assuming that you create buckingham before rochefort).\nThe IP addresses may be different on your machines.\nExercise\nExercise 4.5 Create three containers athos, porthos and aramis and attach them to the two networks buckingham and rochefort as displayed in figure 4.1. The three containers will open a Linux Alpine shell. You’ll need to launch the commands in three separate tabs of your terminal window.\nWhat will the IP addresses of the three containers be in the two networks? Remember that porthos is attached to two networks, therefore it’ll have two network interfaces (endpoints) and, as a result, two IP addresses. Verify your answers by inspecting the two networks (use the command docker network inspect). Solution\nHere are the commands to run athos and aramis while connecting them to buckingham and rochefort respectively.\ndocker run --rm -it --name athos --network buckingham alpine\ndocker run --rm -it --name aramis --network rochefort alpine\nHere’s the command to run porthos and attach it to buckingham:\ndocker run --rm -it --name porthos --network buckingham alpine\nThe following command attaches porthos to the second network rochefort:\ndocker network connect rochefort porthos\nAs for the IP addresses, each network has IP addresses in the range 172.x.0.0/16, where x is 18 in the network buckingham and 19 in the network rochefort. The address 172.x.0.1 is reserved for the router. Therefore, the containers will be assigned IP addresses from 172.x.0.2. In this solution, we created athos, aramis and portos in this order. Therefore, the IP addresses will be:\nIn network buckingham: athos: 172.18.0.2 porthos: 172.18.0.3 In network rochefort: aramis: 172.19.0.2 porthos: 172.19.0.3 You can actually verify this configuration by inspecting the two networks with the following commands:\ndocker network inspect buckingham\ndocker network inspect rochefort\nThe IP addresses might be different on your machines.\n4.2 Communication between containers Let’s see if and when the three containers can communicate.\nExercise\nExercise 4.6 Which containers are able to communicate? Justify your answer. Solution\nThe only containers that cannot communicate are athos and aramis, because they’re not connected to the same network.\nExercise\nExercise 4.7 Try to ping porthos from athos by using its IP address.\nWhich IP address of porthos would you use? Solution\nWe need to use the IP address assigned to the endpoint linking porthos to the network buckingham, to which athos is connected. In our case, this is 172.18.0.3.\nExercise\nExercise 4.8 Try to ping porthos from athos by using its name. Do you succeed? Are you surprised? Solution\nWe succeed. Indeed, the network buckingham provides a DNS server, that can translate names into IP addresses.\nYou can now exit the three containers.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a74fda64fd4d3f15083e937e46612453","permalink":"/courses/cloud-computing/tutorials/tutorial-docker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-docker/","section":"courses","summary":"Text of the lab assignment on Docker","tags":null,"title":"Getting started with Docker","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to run containers. How to define and build images. How to create and use volumes. How to define and use networks. Terminology\nYou’ll use the terminal to run Docker commands. Referring to the Docker architecture, the terminal is the client that communicates with the Docker daemon (the server).\nDocker runs containers on your computer. We’ll refer to your computer as the host, the containers being the guests.\nA containerized application is an application running in a container. 1 Running containers We now learn how to use the command docker run and some of its options. In the following exercises, we’ll run containers from the image named alpine that is available on the DockerHub registry. This image provides a lightweight distribution (i.e., it doesn’t contain many features) of Linux.\nExercise\nExercise 1.1 You want to run the container from the latest version of the image alpine. Which command would you write in the terminal? Execute it. Exercise\nExercise 1.2 Is the container still running? Exercise\nExercise 1.3 By looking at the command executed within the container (/bin/sh), can you tell why the container stopped without giving any output? We’re now going to do something useful with the image alpine. Make sure you read the good practices that you should adopt while playing with images and containers.\nGood practices\nName your containers. Although Docker assigns a default name to a new container, it’s usually a good practice to give a container a name of your choice to make it easily distinguishable. You can do it by using the option --name. Try the following: docker run --name my-alpine alpine\nAs before, the container stops immediately. If you list all your containers by typing again:\ndocker container ls -a\nyou should see a container named my-alpine.\nRemove automatically a container if you use it once. Unless you want to reuse your container later, you can ask Docker to automatically remove it when it stops by using the option --rm. This will prevent unused containers from taking up too much disk space. Try the following:\ndocker run --rm --name container-to-remove alpine\nIf you list all the containers you should see that there is no container named container-to-remove.\nRemove unused containers. Stopped containers that have been run without using the option --rm are still stored in the host. If you want to remove a specific container (e.g., my-alpine), use the following command: docker container rm my-alpine\nIf you want to remove all stopped containers, use the following command:\ndocker container prune\nRemove unused images. Images can take up a lot of disk space. As a result, you should remember to remove those that you don’t intend to use any longer. The commands to remove a specific image and prune unused ones are docker image rm and docker image prune -a respectively. 1.1 Pass a command to the containerized application Remember that the template of docker run is the following:\ndocker run [options] image-name [command] [arg]\nThe optional parameter command refers to a command that you can pass the containerized application, possibly with some arguments (parameter arg).\nLet’s see an example. As we saw before, when we run a container from the image alpine, a Linux terminal /bin/sh is launched.\nNotice\nThe Linux terminal /bin/sh is run within the container. Henceforth, we’ll use the following terms:\nHost terminal. The terminal that you use to interact with the operating system of your computer. Guest terminal. The terminal that is run within the container. By using the optional parameter command, we can run a command in the guest terminal.\nExercise\nExercise 1.4 Run a container from the image alpine and execute the Linux command ls that lists the content of the current directory.\nWhere are the listed files stored? In the host or in the container? Notice\nIn Exercise 1.4 the command ls is executed in the guest terminal, but its output is redirected to the host terminal.\nIn other words, when we run the container, we don’t interact directly with the guest terminal; we just send a command and the output is redirected to the host terminal.\nNow let’s see how to execute a command in the guest terminal that also requires an argument.\nExercise\nExercise 1.5 By using the Linux utility ping, check whether the Web site en.albertschool.com is reachable. Now run the following command:\ndocker run --name my-alpine -it alpine\nNote: we didn’t use the option --rm (the container will not be removed when we stop it, we’re going to use it again). Moreover, we didn’t specify any command to run in the guest terminal.\nExercise\nExercise 1.6 What do you obtain? 1.2 Starting and stopping containers. docker run is a shorthand for two Docker commands, namely docker create, that creates a container from an image, and docker start, that starts the container after its creation.\nSuppose now that you want to download a Web page by using Linux Alpine. You can use the Linux command wget followed by the URL of the page that you want to download.\nExercise\nExercise 1.7 By using the guest terminal in the container my-alpine, download this Web page.\nWhere will the Web page be saved? The host computer or the container? Want to copy the donwloaded file from the container to your computer?\nTo copy the file formulaire to your working directory, type the following command in the host terminal:\ndocker cp \u0026lt;containerId\u0026gt;:/formulaire . In the previous command, replace \u0026lt;containerId\u0026gt; with the identifier of your container. You can obtain the identifier of the container from the output of the command docker container ls.\nWhere are the containers and image files stored?\nIf you use MacOS or Windows\nThe files managed by Docker are not stored directly in your computer, but in the Linux virtual machine installed and operated by Docker Desktop (remember, Docker always need Linux to be executed).\nTherefore, you need to open a terminal inside that Linux virtual machine by typing the following command:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1 Once the terminal is opened, you can follow the instructions given below for Linux users.\nIf you use Docker on Linux\nAll files managed by Docker are stored under folder /var/lib/docker.\nTo access that folder, you need to be root (i.e., administrator). Type the command sudo su.\nIf you type ls /var/lib/docker you can look at the folders stored under this directory. You’ll see that there are folders corresponding to the different objects managed by Docker (containers, images, volumes and networks).\nTo locate the files of a specific container, you first need to get the container identifier by typing docker container ls -a.\nType the command docker inspect \u0026lt;container-id\u0026gt; (replace \u0026lt;container-id\u0026gt; with the identifier of the container that you intend to inspect).\nLocate the field UpperDir. The value of this field is the path to the directory (let’s call it CONTAINER_DIR) that contains the upper layer of the container (the writable layer). It should be a path that ends by /diff.\nIf you type cd CONTAINER_DIR (replace CONTAINER_DIR with the value of the field UpperDir) you can finally see the files stored in your container.\nIn my-alpine guest terminal type exit. This closes the guest terminal and, as a result, stops the container.\nNOTICE\nStopping the container will not erase any of the files stored in the container. Removing the container will.\nIf you want to start the container my-alpine again, you can use the following command:\ndocker container start -ai my-alpine\nThis will open the guest terminal of the container again; type ls to verify that the Web page that you downloaded before is still there.\n2 Creating Images A Docker image can be thought of as a template to create and run a container. An image is a file that contains a layered filesystem with each layer being immutable; this means that the files that belong to a layer cannot be modified or deleted, nor can files be added to a layer.\nWhen a container is created from an image, it will be composed of all the image read-only layers and, on top of them, a writable layer (termed the container layer), where all the new files created in the container will be written. For example, the Web page that you downloaded in Exercise 1.7 were stored in the writable layer of that container.\n2.1 Dockerfiles The most common way to create an image is to use a Dockerfile, a text file that contains all the instructions necessary to build the image. The advantage of the Dockerfile is that it can be interpreted by the Docker engine, which makes the creation of images an automated and repeatable task.\nSuppose that you want a quick way to download all the images from a Web page. You learn that a quick way is to use the almighty Linux commands and you find on the Web a Bash script that does exactly what you want:\n#!/bin/bash wget $1 -O page.html grep -E \u0026quot;(https?:)?//[^/\\s]+/\\S+\\.(jpg|png|gif|svg)\u0026quot; page.html -o | sed \u0026quot;s/(^https?)?\\/\\//https\\:\\/\\//g\u0026quot; -r \u0026gt; urls.txt sed -E \u0026quot;s/\\/thumb//g; s/\\/[0-9]+px-.+\\.(jpg|png)$//g\u0026quot; urls.txt | uniq \u0026gt; urls-new.txt wget -i urls-new.txt -P downloads/ Don’t worry if you don’t understand these instructions. Suffices to say that:\nThe first instruction gets an URL as an argument ($1) and downloads the page pointed by the URL into a file called page.html.\nThe second instruction extracts from page.html all the URLs pointing to images. The URLs are saved to a file called url.txt.\nThe third instruction filters the URLs that point to full-size images. It saves the filtered URLs to a file called urls-new.txt.\nThe fourth instruction downloads the full-size images to a folder named downloads.\nYou now create a folder in your computer named myfigures; you create a new text file that you name dlimages.sh and you save it under folder myfigures; you copy the above Bash script into the new file and you save it.\nBut now you wonder: how can you execute this script if you’re not using Linux?\nThe answer is: you can create a new Docker image (let’s call it dlimages) from the official Ubuntu image!\nThe Dockerfile containing the instructions to build the image dlimages is as follows:\nFROM ubuntu RUN apt-get update RUN apt-get install -y wget RUN mkdir -p /scripts COPY dlimages.sh /scripts/dlimages.sh RUN chmod +x /scripts/dlimages.sh ENTRYPOINT [\u0026quot;/scripts/dlimages.sh\u0026quot;] CMD [\u0026quot;https://en.wikipedia.org/wiki/Cat\u0026quot;] Here’s the explanation:\nWe use the image ubuntu as the base image. This corresponds to the instruction FROM ubuntu.\nWe install the utility wget in the base image. This corresponds to the instructions RUN apt-get update and RUN apt-get install -y wget.\nWe create a directory scripts under the root directory of the image. This corresponds to the instruction RUN mkdir -p /scripts.\nWe copy the file dlimages.sh to the folder scripts in the image. This corresponds to the COPY instruction.\nWe specify that dlimages.sh can be executed. This corresponds to the RUN chmod -x instruction.\nWe specify the command to be executed when a container is run from this image. This corresponds to the instruction ENTRYPOINT. When we run a container from this image, we execute the script dlimages.sh.\nWe specify a default argument to be passed to the script. This corresponds to the instruction CMD.\nExercise\nExercise 2.1 What’s the relation between the Dockerfile lines and the image layers? Exercise\nExercise 2.2 Could you identify a problem in this Dockerfile? Modify the Dockerfile accordingly. 2.2 Building an image We’re now going to build an image from a Dockerfile.\nCreate a new file named Dockerfile under the folder myfigures.\nIn the Dockerfile write the set of instructions that you proposed in Exercise 2.2.\nIn the terminal, set the working directory to myfigures.\nBuild an image called dlimages by executing the following command:\ndocker build -t dlimages .\nThe . at the end of the command means that the Docker engine will look for a file named Dockerfile in the working directory.\nIn order to verify that the new image has been created, type the following command:\ndocker images\nExercise\nExercise 2.3 Run the following command:\ndocker run --name dl1 dlimages\nWhat does it do? Where are the downloaded pictures? Exercise\nExercise 2.4 Run the following command:\ndocker run --name dl2 dlimages https://en.wikipedia.org/wiki/Dog What does it do? Where are the downloaded pictures? 2.3 Containerized Python application Download this archive file and unzip it into your working directory. In this archive you’ll find:\nA Dockerfile. A Python script main.py that asks the user to enter the URL and the language of a Web page, and prints the 10 most frequent words occurring in that page. A file requirements.txt with the list of the Python packages needed to run the given script. The content of the Dockerfile is as follows:\nFROM python:3.7-slim RUN mkdir -p /app WORKDIR /app COPY ./main.py ./requirements.txt /app/ RUN pip install -r requirements.txt ENTRYPOINT [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] Exercise\nExercise 2.5 Describe what this Dockerfile does. Exercise\nExercise 2.6 Build an image called wordfreq from this Dockerfile. Exercise\nExercise 2.7 Without changing the Dockerfile, rebuild the same image. What do you notice? Exercise\nExercise 2.8 What happens if you modify a line in the Python script and you rebuild the image? Exercise\nExercise 2.9 Based on the previous considerations, can you tell what’s wrong with this Dockerfile? Modify the Dockerfile accordingly and rebuild the image. Exercise\nExercise 2.10 Modify main.py by adding a new line of code and rebuild the image. What changed? Play with the application by running the following command:\ndocker run --rm -it wordfreq\nThe containerized application will prompt you to insert the URL of a webpage and the language of the page (in English). The output will be the 20 most used words in the webpage.\n3 Data Volumes In Exercise 2.3 you’ve been asked to run a container named dl1 to download images from a Web page. The figures were downloaded into the directory /downloads of the container. But we left a question unanswered.\nHow do we transfer those figures from the container to the host computer?\nYes, we can use the command docker cp, but there is still a problem left: if we remove the container dl1, we also lose all the images. In other words, any file created within the container has a lifetime that depends on the container itself. This is not a good option in production.\nThe solution is to use volumes to decouple the application from the the data.\n3.1 Docker volumes A volume can be seen as a virtual storage device attached to a container. All files there are written to a volume survive the containerized application that created them. In other words, when a container is destroyed, all the files created by the application in the container remain in the volume.\nLet’s create a new Docker volume called data-volume:\ndocker volume create data-volume\nGood to know (advanced notion)\nWhere the data will be actually stored?\nYou can inspect the new volume by typing the following command:\ndocker volume inspect data-volume\nA mount point is indicated; that’s the folder where the data will be actually stored. If your computer runs Linux, that folder will be available on the host; if your computer runs Windows or MacOS, you’ll not find that folder on your computer. Instead, it will be available in the virtual machine that Docker use on MacOS and Windows.\nDo you want to see the directory? (Instructions for Windows and MacOS)\nOne way to look into the hidden VM is to run the following containerized application:\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1\nThis application will open a guest terminal into the VM. You can then use the commands cd and ls to browse to the directory indicated as the mount path of the new volume.\n3.1.1 Sharing data A Docker volume can be used to share data between containers.\nExercise\nExercise 3.1 Run a container from the image ubuntu, specifying the options to:\nRemove the container once its execution is over.\nInteract with the guest Linux terminal in the container.\nMount the volume data-volume at the container’s directory /data. Feel free to use the Web to find the answer.\nType the following command in the guest Linux terminal to create a file test-file.txt in the directory /data: echo \u0026quot;This is a new file\u0026quot; \u0026gt; /data/test-file.txt\nPrint to the console the content of the file with the following command: cat /data/test-file.txt\nType exit to leave the guest terminal. Since we’ve specified the option --rm, the container is destroyed. Now we’re going to verify that test-file.txt is still accessible. Exercise\nExercise 3.2 Run a container from the image alpine:latest, specifying the options to:\nRemove the container once its execution is over.\nInteract with the guest Linux terminal in the container.\nMount the volume data-volume to the directory /my-data of the container. Exercise\nExercise 3.3 Verify that you can read the file test-file.txt. Which folder would you look in? Type exit to exit the guest terminal and terminate the container.\n4 Single-Host Networking In order to let containers communicate and, therefore, co-operate, Docker defines a simple networking model known as the container network model.\nFigure 4.1: Container network model Exercise\nExercise 4.1 Describe the output of the following command:\ndocker network ls\nExercise\nExercise 4.2 The following command:\ndocker network inspect bridge\noutputs the configuration of the network bridge. By looking at this configuration, can you tell what IP addresses will be given to the containers attached to this network? What’s the IP address of the router of this network? 4.1 Creating networks By default, any new container is attached to the network named bridge.\nExercise 4.3 Explain why it is not a good practice to attach all our containers to the same network.\nIn order to create a new network, you can use the following command:\ndocker network create network_name\nExercise\nExercise 4.4 Create two networks named buckingham and rochefort that use the driver bridge. By using the docker network inspect command, look at the IP addresses of the new networks and write them down. Exercise\nExercise 4.5 Create three containers athos, porthos and aramis and attach them to the two networks buckingham and rochefort as displayed in figure. The three containers will open a Linux Alpine shell. You’ll need to launch the commands in three separate tabs of your terminal window.\nWhat will the IP addresses of the three containers be in the two networks? Remember that porthos is attached to two networks, therefore it’ll have two network interfaces (endpoints) and, as a result, two IP addresses. Verify your answers by inspecting the two networks (use the command docker network inspect). 4.2 Communication between containers Let’s see if and when the three containers can communicate.\nExercise\nExercise 4.6 Which containers are able to communicate? Justify your answer. Exercise\nExercise 4.7 Try to ping porthos from athos by using its IP address.\nWhich IP address of porthos would you use? Exercise\nExercise 4.8 Try to ping porthos from athos by using its name. Do you succeed? Are you surprised? You can now exit the three containers.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db73f7bfd934e918e3c2b4c41d331b1a","permalink":"/courses/cloudalbert/tutorials/docker-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloudalbert/tutorials/docker-tutorial/","section":"courses","summary":"Docker","tags":null,"title":"Introduction to Docker","type":"docs"},{"authors":null,"categories":null,"content":" Warning\nIf you use the Linux VM, you need to change the hardware configuration of the VM before booting it. You can also watch this video. Here are the necessary modifications:\nIncrease the main memory to 4GB.\nSet the number of CPUs to 2.\nWithout these modifications you’re not going to have a good experience with Kubernetes.\nIn this tutorial you’ll learn:\nHow to build and deploy a multi-service application with Docker Compose.\nHow to push an image to DockerHub.\nHow to deploy a multi-service application with Kubernetes.\nWarning\nThis tutorial is adapted from the examples presented in Chapters 14 and 20 of the book G. Schenker, Learn Docker - Fundamentals of Docker 19.x (March 2020).\n1 Docker Compose In this section, you’re going to build and deploy a multi-service application by using Docker Compose.\nDownload this archive file and unzip it into a folder on your own computer. The archive contains all the necessary files to build and run a web application consisting of two services:\nweb. This is the frontend (the part the user interacts with) of the application. It consists of HTML and JavaScript code.\ndb. This is the backend (the part hidden to the user). It is a PostgreSQL (relational) database.\nThe structure of the application is shown in Figure 1.1. The root directory of the application contains two subdirectories, one for each service (database and web).\nFigure 1.1: Structure of the application Good to know\nThe files with extension .conf under the directory database contain configuration parameters of the PostgreSQL database.\nThe file init-db.sql under the directory database contains the SQL queries to populate the database with some data (photos of cats).\nThe directory web contains the HTML and JavaScript code of the web application. The file package.json contains the dependencies to install.\nBoth directories contain a Dockerfile. The one in directory database is as follows:\nFROM postgres COPY init-db.sql /docker-entrypoint-initdb.d/ RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql ENV POSTGRES_USER dockeruser ENV POSTGRES_PASSWORD dockerpass ENV POSTGRES_DB pets The Dockerfile builds on an existing image called postgres, that is documented here.\nExercise\nExercise 1.1 Consider the following line in the Dockerfile:\nCOPY init-db.sql /docker-entrypoint-initdb.d/\nBy looking at the documentation of the image postgres, answer the two following questions:\nWhere is the directory /docker-entrypoint-initdb.d/?\nWhy do we copy the file init-db.sql to this directory?\nSolution\nThe directory already exists in the base image.\nBy looking at the documentation, we learn that we can put any initialization script in this directory. In other words, at the startup the database will execute the SQL queries in file init-db.sql so that the database is populated with new data.\nFrom the documentation, we also learn that the script is not executed if the data directory is not empty. This means that already existing databases will not be touched.\nThe last three lines of the Dockerfile contain a keyword (ENV) that we never came across before.\nExercise\nExercise 1.2 Look again at the documentation of the image postgres and try to explain the meaning of the last three lines of the Dockerfile.\nSolution\nThese lines set the value of three environment variables, useful to pass some parameters to the database. From the documentation we learn that:\nPOSTGRES_USER is the name of the database user. If not set, the default is the user postgres.\nPOSTGRES_PASSWORD is the password associated with the user. This is the only mandatory variable.\nPOSTGRES_DB is the name given to the database. If not specified, the database name would be the same as the username.\nThe Dockerfile of the web application is as follows:\nFROM node:9.6-alpine RUN mkdir /app WORKDIR /app COPY package.json /app/ RUN npm install COPY ./src /app/src EXPOSE 3000 CMD node src/server.js The Dockerfile builds on the image node:9.6-alpine that contains a Node.js environment, a JavaScript-based platform for server-side applications. The instructions in the Dockerfile look like the ones of the examples that we’ve seen in the first tutorial and in the lectures.\nGood to know\nThe command npm install installs all the dependencies specified in the file package.json from the software registry npm.\nThe instruction EXPOSE 3000 informs that the container listens on port 3000 when it is executed from the image.\nFrom the Docker documentation we learn that the “EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published”.\nIn order to actually publish the port, we’ll use the file docker-compose.yml.\n1.1 Describing the application The application showcases a building block of a pet store. In the current version, the application shows a few pictures of cats.\nThe root directory of the application contains a file named docker-compose.yml that contains the declarative configuration of the application. The content of the file is as follows. It is a sequence of key-value pairs.\nversion: \u0026quot;3.6\u0026quot; services: web: build: web image: pet-store-web networks: - backend ports: - 5000:3000 db: build: database image: pet-store-db networks: - backend volumes: - pets-data:/var/lib/postgresql/data networks: backend: volumes: pets-data: There are three main sections:\nservices. Defines the services of the application. Here two services are defined: web and db.\nnetworks. Defines the networks used by the application. Here one network is defined: backend.\nvolumes. Defines the volumes used by the application. Here one volume is defined: pets-data. The volume is attached to the directory /var/lib/postgresql/data (that is in the container).\nExercise\nExercise 1.3 What key informs docker compose where to find the Dockerfile of the two services?\nSolution\nThe value of the key build is the name of the directory that contains the Dockefile. For the service web, this is the directory web; for the service db, it is the directory database. All paths are relative to the position of the file docker-compose.yml itself.\nWhen we’ll build the application from the file docker-compose.yml, two images will be created, one for each service.\nExercise\nExercise 1.4 What will the name of the two images be? What key in the file docker-compose.yml gives you this information?\nSolution\nThe name of the image corresponding to the service web will be pet-store-web.\nThe name of the image corresponding to the service db will be pet-store-db.\nThis information is given by the key image in the file docker-compose.yml.\n1.2 Building the application We now build the application.\nOpen the command-line terminal and by using the command cd position yourself in the root directory pet-store of the application.\nExecute the following command:\ndocker-compose build\nDuring the build you might get a npm warning. Just ignore it.\nWhen the build is complete, verify that the two images corresponding to the two services have been created (which docker command do you need here?).\nSolution\ndocker images\n1.3 Executing the application We now execute the application with the following command:\nIf you use a Linux virtual machine with Multipass In this section, you’ll need to open several terminals in the virtual machine. You can do it easily by using byobu, an advanced window manager already available in your virtual machine.\nJust type byobu to launch the window manager.\nIf you want to open a new terminal, just press F2.\nIf you want to switch from a terminal to another, just press F3 (to move to previous terminal) or F4 (to move to next terminal).\nIf you want to close a terminal, just type exit.\nWhen you close all terminals, byobu will stop executing.\ndocker-compose up\nWarning\nIf your application fails to start because the port number that you chose is already in use, try using another port.\nDon’t use the following ports as they are deemed unsafe by the browsers:\n1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080\nThe execution of the command will print a series of messages on screen. The terminal should hang when the messages stop (the last message should be database system is ready to accept connections).\nTake no action on the screen and open a new terminal or a new tab in the current terminal window.\nExercise\nExercise 1.5 Verify that the network and the volumes associated with the application have been correctly created.\nSolution\nType the commands:\ndocker volume ls\nand\ndocker network ls\nto verify respectively that the volumes and the networks have been created\nExercise\nExercise 1.6 How many containers do you expect to be associated to the application?\nSolution\nEach image corresponds to a service. Here we have two. Each container corresponds to a service instance.\nWe didn’t specify any parameter when we launched the application, so by default one instance of each service is executed.\nSo, we expect to have two containers associated to the application: one container for the service web and one for the service db.\nYou can verify the answer to the previous question by typing the following command:\ndocker-compose ps\nThis command is exactly equivalent to docker container ls, except that it only shows the containers associated with the application that we’ve just executed.\nExercise\nExercise 1.7 In the output of the command docker-compose ps, can you explain the meaning of the following?\n0.0.0.0:5000-\u0026gt;3000/tcp\nSolution\nThe notation 5000-\u0026gt;3000 means that the port 5000 of the host computer is mapped to the port 3000 of the container. More specifically, when a client application connects to the port 5000 of the host computer, the connection is redirected to the port 3000 of the container.\nThe IP address 0.0.0.0 means that we can connect to the container by using any IP address on the host computer.\nExercise\nExercise 1.8 Can you tell which URL you have to type in your web browser to access the application?\nIf you use a Linux virtual machine with Multipass\nIn order to open a Web browser window in the VM, you need to open a new terminal window on your computer and follow these instructions:\nType multipass ls to list all the virtual machines managed by Multipass.\nCopy the first IP address associated to the virtual machine cloudvm.\nType the following command:\nxpra start ssh://ubuntu@ip/ --start=firefox where you’ll replace ip with the IP address that you copied above.\nA Firefox window should appear. Solution\nhttp://localhost:5000 or http://127.0.0.1:5000/.\nIn the network settings of you computer, you can also get the IP address associated by the DHCP server of your\nlocal network and use that address to connect to the application from another device connected to the same network (for instance, your mobile phone).\nWarning\nIf you want to see the cats, you need to append /pet to the URL that you found in the previous question. This is determined in file server.js.\n1.4 Shutting down the application You can shut down the application by typing the following command:\ndocker-compose down\nExercise\nExercise 1.9 Does shutting down the application remove the networks created for the application? What about the volumes?\nSolution\nJust type the command:\ndocker network ls\nand you’ll see that the network backend is not there anymore.\nType the command:\ndocker volume ls\nand you’ll see that the volume pet-store_pets-data is still there.\nBy default volumes are not removed, to avoid the risk of permanently deleting data that might still be useful later. If you wish to remove the volumes when shutting down the application, you can type:\ndocker-compose down -v\n1.5 Scaling a service When we launch the application, we can specify the number of instances of each service. This is useful when we expect our application to be solicited by many users; the workload will be automatically balanced across all the instances of the service.\nLet’s launch our application with 3 instances of the service web:\ndocker-compose up --scale web=3 -d\nExercise\nExercise 1.10 Running the previous command results in an error.\nCan you explain why?\nWhat fix would you propose in file docker-compose.yml? Solution\nWe’re trying to bind three instances to the port 5000 on the host computer. This is not possible.\nIn the section ports of file docker-compose.yml we need to remove 5000 and we only leave 3000. This way, each time we run a container, a random port will be chosen on the host computer to bind it with port 3000 of the container.\nWarning\nYou need to shut down the application before relaunching it again. In fact, even if we got an error, an instance of the db service and an instance of the service web are still running.\nModify the file docker-compose.yml and relaunch the application.\nRun the following command and verify that you actually have three running instances of the service web.\ndocker-compose ps\nTry to connect to the application by using the three port numbers indicated in the output of the previous command. Warning\nDon’t forget to shut down the application before you go on.\n2 Pushing an application Once we have built an application, we might want to share it by pushing it to the DockerHub registry or any other container registry, be it private or public.\n2.1 Creating an account on DockerHub You need to create an account on the DockerHub website in order to perform this activity.\n2.2 Renaming the images In order to push your images to the registry, you need to rename them, so as the new name has the following structure:\nyourusername/image-name:image-tag\nFor instance, since my username is quercinigia, I’ll rename the two images pet-store-web and pet-store-db with the docker tag command as follows:\ndocker tag pet-store-web quercinigia/pet-store-web:1.0\ndocker tag pet-store-db quercinigia/pet-store-db:1.0\nI chosen 1.0 as a tag, but feel free to pick another one.\n2.3 Logging in You need to log in to your DockerHub account.\nInstructions for Docker Desktop (macOS and Windows)\nmacOS users: to see how to log in click here.\nWindows users: to see how to log in click here.\nInstructions for the Linux VM\nLog in to DockerHub by typing the following command in the terminal (replace YOUR-USERNAME with your actual username).\ndocker login -u YOUR-USERNAME\nWarning\nYou might get a warning that your password will be stored unencrypted. There are methods to prevent this from happening. These methods being out of the scope of this course, the interested reader can look them up at this link.\n2.4 Pushing the images Once you’re successfully logged in, you can type the following commands in the terminal to push the two images of your application:\ndocker push YOUR-USERNAME/pet-store-web:1.0\ndocker push YOUR-USERNAME/pet-store-db:1.0\nRemember to replace YOUR-USERNAME with your actual username in the commands above.\nAfter the task is completed, verify that the images appear in your Docker registry.\nGood to know\nHere we manually tagged and uploaded the two images. This method becomes quickly annoying when the application consists of more than two images. Another way to go about this task is:\nSpecify the image names with your username in file docker-compose.yml. For instance, in my docker-compose.yml I would write: services: web: build: web image: quercinigia/pet-store-web:1.0 ... db: build: database image: quercinigia/pet-store-db:1.0 ... Build the application with the following command: docker-compose build\nPush the images of the application with the following command: docker-compose push\nThis way, with just two commands we push to the registry all the images of the application, no matter how many they are.\n3 Introduction to Kubernetes Kubernetes is the most popular orchestrator to date. It is used to manage a multi-service application, usually deployed across multiple hosts in a cluster.\nWhile using Docker Compose, a service corresponds to a Docker image and a service instance to a Docker container.\nAs opposed to that, in Kubernetes the computation unit is a pod, that is a collection of containers. In other words, we don’t reason in terms of containers anymore, but in terms of pods. Of course, a pod can also consist of just one container.\nGood to know\nIn practice, we rarely have to manipulate pods directly in Kubernetes, as there are higher-level objects that manage them. We’ll use these objects in the next sections.\n3.1 Activate Kubernetes Instructions for Docker Desktop (macOS and Windows)\nYou need to follow all the instructions documented on this page\nBefore moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated\nkubectl get nodes\nYou should see a node called docker-desktop whose status is set to READY. If the status is NOT READY, just wait few seconds before typing the command again.\nWarning\nThis solution might not be working for you. In that case, disable Kubernetes in Docker Desktop and install minikube, by following these instructions.\nInstructions for the Linux VM\nIn the Linux VM you’ll find Minikube, a single-node Kubernetes cluster in VirtualBox. Please follow all the instructions:\nStart the cluster, type the following command: minikube start\nVerify that Kubernetes is correctly activated by typing the following command: kubectl get nodes\nYou should see a node called minikube whose status is set to READY. If the status is NOT READY, just wait few seconds before typing the command again.\nOpen a new terminal and type the command: minikube tunnel\nWhen prompted to enter a password, just type ENTER. The command will start to produce some output. Leave that terminal open and go back to the previous terminal.\nTunnel\nThe Minikube tunnel is used to create a route to services deployed with type LoadBalancer. If you don’t activate the tunnel, you won’t be able to use these services in the exercises below.\n3.2 Deploying a pod In order to deploy a pod, we first have to give its specification, basically its name and the containers that compose it with their settings. Similarly to Docker Compose, a pod is specified in a declarative way with a YAML configuration file.\nConsider the following specification.\napiVersion: v1 kind: Pod metadata: name: web-pod labels: app: web-pod spec: containers: - name: web image: nginx:alpine ports: - containerPort: 80 Here is an explanation of the properties in the specification.\napiVersion. Defines the versioned schema of this representation.\nkind. The type of the resource that we intend to create.\nmetadata. The resource metadata. The list of all metadata is specified here.\nspec. The specification of the desired behaviour of the pod. The list of the possible specifications can be found here.\nEssentially, the previous specification defines a pod with a container that is launched from the image nginx:alpine (a Web server) and listens to port 80.\nActivity\nCopy the previous specification to a file named sample-pod.yaml (or any other name of your liking).\nBy using the command cd in the terminal, place yourself in the directory where the file sample-pod.yaml is stored.\nDeploy the pod by typing the following command:\nkubectl create -f sample-pod.yaml\nVerify that the pod is deployed by typing the following command: kubectl get pods\nThe first time you run the last command, you might see that the pod is not ready yet. You need to wait for the image nginx:alpine to be pulled from DockerHub. Wait few seconds and try the command again until the pod is marked as running.\nYou can also get more information on the running pod (e.g., its assigned IP address) by typing the following command:\nkubectl get pod -o wide web-pod\nExercise\nExercise 3.1 Open a web browser and type http://localhost:80.\nWhat do you get? What if you try to use the IP of the pod instead of locahost?\nWhat should we define in order to fix the problem? Solution\nThe connection is refused both when using localhost and the IP address of the pod.\nIn order to connect to the service instance, we need to define a Service object in Kubernetes, that exposes an IP address that client applications can use.\nThe following configuration defines a Service object of type LoadBalancer:\napiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer ports: - port: 8080 targetPort: 80 protocol: TCP name: http selector: app: web-pod Good to know\nA LoadBalancer service is a NodePort service. that offers load balancing capabilities. It is intended to expose an IP address that client applications (external to the Kubernetes cluster) can use to access the service.\nExercise\nExercise 3.2 What is the field selector in the definition of the service?\nSolution\nThe selector is used internally by Kubernetes to identify the pods that are the instances of the service. If you look at the specification of the pod above, the field metadata contains a field labels. One of the labels is app: web-pod ; the selector matches this label.\nExercise\nExercise 3.3 In the specification of the service, what port and targetPort mean?\nSolution\nport specifies the port number where the service will be available.\ntargetPort specifies the port number opened at the pods that are instances of the service.\nActivity\nCopy and paste the service specification into a file named sample-service.yaml (or any other name of your liking).\nBy using the command cd in the terminal, place yourself in the directory where the file sample-service.yaml is stored.\nDeploy the service by typing the following command:\nkubectl create -f sample-service.yaml\nVerify that the service is deployed by typing the following command: kubectl get services\nThis command returns all services running in Kubernetes. For each service, you also get a name. In order to only target the service that you’ve just created, simply type the following command:\nkubectl get svc/nginx-service\nYour service’s name is nginx-service (as specified in file sample-service.yml); svc is only the namespace where all Kubernetes services are put.\nExercise\nExercise 3.4 By looking at the output of the command kubectl get svc/nginx-service, which URL do you need to type in the Web browser in order to access the service?\nSolution\nThe CLUSTER-IP is an IP address that is only visible within the cluster. In other words, only the services running within the cluster can connect to our nginx-service by using this IP address.\nWhat we need to use is the IP address specified under EXTERNAL-IP.\nIf you’re using Docker Desktop\nEXTERNAL-IP is likely to be localhost or 127.0.0.1\nIf you’re using Minikube in the Linux VM\nEXTERNAL-IP will be whatever IP address assigned to the service.\nAs for the port number, under PORT(S) you should see something like 8080:30548 (the second number is likely different, but it should be something in the range [30000-32767]). The port 30548 is opened on each node of the Kubernetes cluster. In practice, when we type http://localhost:8080, the load balancer replaces it with the IP address of a node in the cluster, followed by port number 30548. When the kube-proxy on that node receives this request, it forwards it to an instance of the service, even if that instance runs in another node.\nWarning\nStop the pod and the service before moving on. Here are the commands to do so:\nkubectl delete po/web-pod\nkubectl delete svc/nginx-service\n4 Kubernetes: deploying an application In this section, we’re going to deploy our pet store in Kubernetes. As a reminder, our application consists of two services: web and db.\nIn order to define an application in Kubernetes, we need to use two types of objects for each service of the application:\nA workload resource that gives the specification of the service, such as the pods that make up the service itself (images, network settings) and metadata, such as the desired number of instances.\nA Service object. As we have seen previously, a service object exposes an IP address that allows client applications, both inside and outside the Kubernetes cluster, to connect to the service.\nWhen we define an application in Kubernetes, we rarely, if ever, need to play directly with pods. Instead, we can resort to higher-level objects, called Controllers, for an easier definition of the desired state of the application itself. The type of the controller that we need to use depends on the nature of the service itself: stateless or stateful.\nExercise\nExercise 4.1 Is the service web of our application stateless or stateful?\nIs the service db of our application stateless of stateful? Solution\nStateful service: one that manages permanent data and is bound to that data. Stateless service: one that is not bound to permanent data. The service web is stateless because it does not have to be tightly bound to the data. In other words, the service web uses the data that is stored in the backend database, but it can be scheduled on any node of the cluster independently of that data.\nThe service db is stateful because it directly manages permanent data.\n4.1 The web service: deployment For stateless application services, we can use a Deployment for the deployment of a set of identical pods that are launched across several nodes in the Kubernetes cluster.\nDeployment and ReplicaSet\nDeployments and ReplicaSets have been introduced in Lecture 3.\nWe here define the specification of a Deployment corresponding to the service web.\napiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP Here is the explanation of the specification:\nA Deployment named web is created, as indicated by the field metadata.name.\nThe Deployment creates three replicated pods, as indicated by the field spec.replicas.\nThe Deployment considers that the pods with both labels app: pets and service: web are part of the deployment. This is indicated by the field spec.selector.matchLabels. This field is used to let the Deployment know how to find its pods.\nThe configuration of the pods that are part of the Deployment is given in the field spec.template and its subfields.\nEach pod of the Deployment is given labels app: pets and service: web. This is indicated by the field spec.template.metadata.labels. Note that here we specify exactly the same values as in spec.selector.matchLabels. This field is used to give pods labels so that they can be identified and located in Kubernetes.\nEach pod has exactly one container, named web, run from the image quercinigia/pet-store-web:1.0 stored in the DockerHub. The container listens on port 3000 and uses the TCP protocol. This is specified in the field spec.template.spec.containers.\nActivity\nCopy and paste the previous specification to a new file web-deployment.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-web:1.0 with the one that you pushed to your DockerHub registry.\nUse the command cd in the terminal to position yourself in the directory where the file web-deployment.yaml is stored.\nDeploy this Deployment in Kubernetes by typing the following command:\nkubectl create -f web-deployment.yaml\nExercise\nExercise 4.2 Type the following command:\nkubectl get all\nWhich objects have been created following the creation of the Deployment?\nSolution\nWe can clearly see three types of objects:\nThree pods, that correspond to the three replicas that we specified in the configuration.\nOne deployment. Note that the 3/3 indicates that the deployment has 3 identical pods, of which 3 are running.\nOne ReplicaSet. The Deployment creates a ReplicaSet, that is the actual object that controls the three pods. The Deployment provides some additional services (i.e., updates) on top of a ReplicaSet.\nExercise\nExercise 4.3 Get the name of one of the running pods and kill it by using the following command\nkubectl delete name-of-pod\nIf the command hangs in the terminal, feel free to type Ctrl-C to get back control of the terminal.\nType again following command:\nkubectl get all\nHow many pods do you see? Is it surprising?\nSolution\nIf we type the command right after deleting the pod, it is likely that we’ll see four pods, one that is terminating and another one that is running. Try to type the command until you see three running pods.\nThis is not surprising. If we kill the pod, the orchestrator detects a deviation from the desired state (which is, we want three replicas) and reschedules immediately another pod. This is what we mean by a self-healing system. This is one of the major roles of an orchestrator.\n4.2 The web service: service Now we need to define a Service in order to expose the web service to the public. Here is the definition:\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web Exercise\nExercise 4.4 Describe the specification of this service.\nSolution\nWe create a service named web. This indicated in the field metadata.name.\nThe service has type LoadBalancer. This is indicated in the field spec.type.\nThe service listens on port 8080 and uses the TCP protocol. Each service instance (i.e., pod) listens on port 3000. This is indicated in the field spec.ports.\nThe instances of the service (i.e., the pods) are those with labels app: pets and service: web. This is indicated in the field spec.selector.\nActivity\nCopy and paste the previous specification to a new file web-service.yaml (or any name of your liking).\nUse the command cd in the terminal to position yourself in the directory where the file web-service.yaml is stored.\nCreate the service with the following command:\nkubectl create -f web-service.yaml\nVerify that the service has been created with the following command: kubectl get services\nLocate the external IP (let’s call it EXTERNAL-IP) of the web service and type the URL http://EXTERNAL-IP:8080 in your Web browser. You should see a Web page where the phrase Pet store appears. 4.3 The db service: StatefulSet Kubernetes has defined a special type of ReplicaSet for stateful services that is called StatefulSet.\nLet’s define a StatefulSet to give the specification of the db service.\napiVersion: apps/v1 kind: StatefulSet metadata: name: db spec: selector: matchLabels: app: pets service: db serviceName: db template: metadata: labels: app: pets service: db spec: containers: - image: quercinigia/pet-store-db:1.0 name: db ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: pets-data volumeClaimTemplates: - metadata: name: pets-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi By now you shouldn’t have any problem understanding the meaning of the fields in this specification. The fields are also documented in the official Kubernetes documentation. Two points are worth a comment. First, the specification of a StatefulSet needs an attribute serviceName that indicates the name of the service that is responsible for the network identify of the pods in the set. That attribute is mandatory.\nAnother novelty is the field volumeClaimTemplates. It describes additional constraints on the volumes defined in the specification, in this case the volume named pets-data (where PostgreSQL keeps the data). In particular, two claims are given:\nThe access mode ReadWriteOnce means that the volume can be mounted as read-write by a single node in the Kubernetes cluster. Access modes are documented here.\nWe request at least 100MB of storage for the database.\nActivity\nCopy and paste the previous specification to a new file db-stateful-set.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-db:1.0 with the one that you pushed to your DockerHub registry.\nUse the command cd in the terminal to position yourself in the directory where the file db-stateful-set.yaml is stored.\nDeploy this StatefulSet in Kubernetes by typing the following command:\nkubectl create -f db-stateful-set.yaml\nExercise\nExercise 4.5 Type the command:\nkubectl get all\nWhich objects have been created when you deployed the StatefulSet?\nSolution\nA pod (likely to be called pod/db-0)\nA new StatefulSet that governs the pod.\nNote that the StatefulSet has only one pod, which is normal, since we didn’t specify a higher number of replicas.\nExercise\nExercise 4.6 Open the Web browser and type the following URL:\nIf you’re using Docker Desktop\nhttp://localhost:8080/pet\nIf you’re using Minikube on the Linux VM\nhttp://minikube-ip:8080/pet\nwhere you’ll replace minikube-ip with the external IP address associated with the web service.\nRight after, type the command kubectl get all. What do you observe? Can you explain the reason?\nSolution\nWe cannot access the application. If you type the command kubectl get all quickly, you should show that the three pods that are instances of the service web are in an error state. If you were too slow, you should see that the restart count of each pod has changed. This means that the pods have been restarted because of an error.\nWhat happens here is that we launched the database, but we didn’t create the Service that lets the clients access the database.\nIn the output of the command get kubectl all, look at the names of the pods that are instances of the web service. Take the name of any these pods, and put it in place of NAME-OF-POD in the following command:\nkubectl logs NAME-OF-POD --previous\nExercise\nExercise 4.7 Does the output of the previous command confirm the explanation given in the previous question?\nSolution\nYes, we can clearly read ENOTFOUND db db:5432. This means that Kubernetes cannot resolve the name db to an IP address.\n5 The db service: Service object From the above observations, we understand that we need to define a service to expose the database to the clients.\nExercise\nExercise 5.1 Should the db service be accessible to client applications that are external to the Kubernetes cluster?\nSolution\nNo, this component is the backend of the application. The only client that needs to access the database is the frontend, that is the web service, that runs inside the cluster.\nExercise\nExercise 5.2 Given the answer to the previous question, what should the type of this service be?\nSolution\nClusterIP; this is the type of a service that does not need to be exposed to the outside world.\nExercise\nExercise 5.3 Write the specification of the db service in a file named db-service.yaml (or any other name of your liking).\nCaution. In the file db-stateful-set.yaml the field spec.serviceName indicates the name that the service must have.\nSolution\napiVersion: v1 kind: Service metadata: name: db spec: type: ClusterIP ports: - port: 5432 protocol: TCP selector: app: pets service: db Activity\nDeploy the service by typing the following command: kubectl create -f db-service.yaml\nVerify that the service has been created with the following command: kubectl get all\nVerify that you can reach the application at http://localhost:8080/pet (Minikube users: replace localhost with the external IP address associated to the web service!). It might happen that the database service is not ready yet, and so you’ll get a connection error. Just wait and retry later. 5.1 Shutting down the application After you’re done with the application, you can shut it down with the following commands:\nkubectl delete svc/web\nkubectl delete svc/db\nkubectl delete deploy/web\nkubectl delete statefulset/db\nThe order in which you type these commands doesn’t matter.\nIf you type multiple times the command:\nkubectl get all\nyou should see that the resources progressively disappear.\n5.2 Conclusion In the previous exercises, we deployed an application with two services, for which we had to create four files and type as many commands. For larger applications this gets a bit annoying. We can write all the definitions in a single file (e.g., pets.yaml) where each specification is terminated by - - -.\nHere is an example, where we write the specification of the Service and Deployment associated with the service web.\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web --- apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f5b5b01a686c8fdb871e8a71fba8ba3","permalink":"/courses/cloud-computing/tutorials/tutorial-kube/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloud-computing/tutorials/tutorial-kube/","section":"courses","summary":"Text of the lab assignment on multi-service applications","tags":null,"title":"Multi-service applications","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to build and deploy a multi-service application with Docker Compose.\nHow to push an image to DockerHub.\nHow to deploy a multi-service application with Kubernetes.\nWarning\nThis tutorial is adapted from the examples presented in Chapters 14 and 20 of the book G. Schenker, Learn Docker - Fundamentals of Docker 19.x (March 2020).\n1 Docker Compose In this section, you’re going to build and deploy a multi-service application by using Docker Compose.\nDownload this archive file and unzip it into a folder on your own computer. The archive contains all the necessary files to build and run a web application consisting of two services:\nweb. This is the frontend (the part the user interacts with) of the application. It consists of HTML and JavaScript code.\ndb. This is the backend (the part hidden to the user). It is a PostgreSQL (relational) database.\nThe structure of the application is shown in Figure 1.1. The root directory of the application contains two subdirectories, one for each service (database and web).\nFigure 1.1: Structure of the application Good to know\nThe files with extension .conf under the directory database contain configuration parameters of the PostgreSQL database.\nThe file init-db.sql under the directory database contains the SQL queries to populate the database with some data (photos of cats).\nThe directory web contains the HTML and JavaScript code of the web application. The file package.json contains the dependencies to install.\nBoth directories contain a Dockerfile. The one in directory database is as follows:\nFROM postgres:10.2-alpine COPY init-db.sql /docker-entrypoint-initdb.d/ RUN chown postgres:postgres /docker-entrypoint-initdb.d/*.sql ENV POSTGRES_USER dockeruser ENV POSTGRES_PASSWORD dockerpass ENV POSTGRES_DB pets The Dockerfile builds on an existing image called postgres:10-2-alpine, that is documented here.\nExercise\nExercise 1.1 Consider the following line in the Dockerfile:\nCOPY init-db.sql /docker-entrypoint-initdb.d/\nBy looking at the documentation of the image postgres:10-2-alpine, answer the two following questions:\nWhere is the directory /docker-entrypoint-initdb.d/?\nWhy do we copy the file init-db.sql to this directory?\nSolution\nThe directory already exists in the base image.\nBy looking at the documentation, we learn that we can put any initialization script in this directory. In other words, at the startup the database will execute the SQL queries in file init-db.sql so that the database is populated with new data.\nFrom the documentation, we also learn that the script is not executed if the data directory is not empty. This means that already existing databases will not be touched.\nThe last three lines of the Dockerfile contain a keyword (ENV) that we never came across before.\nExercise\nExercise 1.2 Look again at the documentation of the image postgres:10-2-alpine and try to explain the meaning of the last three lines of the Dockerfile.\nSolution\nThese lines set the value of three environment variables, useful to pass some parameters to the database. From the documentation we learn that:\nPOSTGRES_USER is the name of the database user. If not set, the default is the user postgres.\nPOSTGRES_PASSWORD is the password associated with the user. This is the only mandatory variable.\nPOSTGRES_DB is the name given to the database. If not specified, the database name would be the same as the username.\nThe Dockerfile of the web application is as follows:\nFROM node:9.6-alpine RUN mkdir /app WORKDIR /app COPY package.json /app/ RUN npm install COPY ./src /app/src EXPOSE 3000 CMD node src/server.js The Dockerfile builds on the image node:9.6-alpine that contains a Node.js environment, a JavaScript-based platform for server-side applications. The instructions in the Dockerfile look like the ones of the examples that we’ve seen in the first tutorial and in the lectures.\nGood to know\nThe command npm install installs all the dependencies specified in the file package.json from the software registry npm.\nThe instruction EXPOSE 3000 informs that the container listens on port 3000 when it is executed from the image.\nFrom the Docker documentation we learn that the “EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published”.\nIn order to actually publish the port, we’ll use the file docker-compose.yml.\n1.1 Describing the application The application showcases a building block of a pet store. In the current version, the application shows a few pictures of cats.\nThe root directory of the application contains a file named docker-compose.yml that contains the declarative configuration of the application. The content of the file is as follows. It is a sequence of key-value pairs.\nversion: \u0026quot;3.6\u0026quot; services: web: build: web image: pet-store-web networks: - backend ports: - 5000:3000 db: build: database image: pet-store-db networks: - backend volumes: - pets-data:/var/lib/postgresql/data networks: backend: volumes: pets-data: There are three main sections:\nservices. Defines the services of the application. Here two services are defined: web and db.\nnetworks. Defines the networks used by the application. Here one network is defined: backend.\nvolumes. Defines the volumes used by the application. Here one volume is defined: pets-data. The volume is attached to the directory /var/lib/postgresql/data (that is in the container).\nExercise\nExercise 1.3 What key informs docker compose where to find the Dockerfile of the two services?\nSolution\nThe value of the key build is the name of the directory that contains the Dockefile. For the service web, this is the directory web; for the service db, it is the directory database. All paths are relative to the position of the file docker-compose.yml itself.\nWhen we’ll build the application from the file docker-compose.yml, two images will be created, one for each service.\nExercise\nExercise 1.4 What will the name of the two images be? What key in the file docker-compose.yml gives you this information?\nSolution\nThe name of the image corresponding to the service web will be pet-store-web.\nThe name of the image corresponding to the service db will be pet-store-db.\nThis information is given by the key image in the file docker-compose.yml.\n1.2 Building the application We now build the application.\nOpen the command-line terminal and by using the command cd position yourself in the root directory pet-store of the application.\nExecute the following command:\ndocker-compose build\nDuring the build you might get a npm warning. Just ignore it.\nWhen the build is complete, verify that the two images corresponding to the two services have been created (which docker command do you need here?).\nSolution\ndocker images\n1.3 Executing the application We now execute the application with the following command:\ndocker-compose up\nWarning\nIf your application fails to start because the port number that you chose is already in use, try using another port.\nDon’t use the following ports as they are deemed unsafe by the browsers:\n1, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 37, 42, 43, 53, 69, 77, 79, 87, 95, 101, 102, 103, 104, 109, 110, 111, 113, 115, 117, 119, 123, 135, 137, 139, 143, 161, 179, 389, 427, 465, 512, 513, 514, 515, 526, 530, 531, 532, 540, 548, 554, 556, 563, 587, 601, 636, 993, 995, 1719, 1720, 1723, 2049, 3659, 4045, 5060, 5061, 6000, 6566, 6665, 6666, 6667, 6668, 6669, 6697, 10080\nThe execution of the command will print a series of messages on screen. The terminal should hang when the messages stop (the last message should be database system is ready to accept connections).\nTake no action on the screen and open a new terminal or a new tab in the current terminal window.\nExercise\nExercise 1.5 Verify that the network and the volumes associated with the application have been correctly created.\nSolution\nType the commands:\ndocker volume ls\nand\ndocker network ls\nto verify respectively that the volumes and the networks have been created\nExercise\nExercise 1.6 How many containers do you expect to be associated to the application?\nSolution\nEach image corresponds to a service. Here we have two. Each container corresponds to a service instance.\nWe didn’t specify any parameter when we launched the application, so by default one instance of each service is executed.\nSo, we expect to have two containers associated to the application: one container for the service web and one for the service db.\nYou can verify the answer to the previous question by typing the following command:\ndocker-compose ps\nThis command is exactly equivalent to docker container ls, except that it only shows the containers associated with the application that we’ve just executed.\nExercise\nExercise 1.7 In the output of the command docker-compose ps, can you explain the meaning of the following?\n0.0.0.0:5000-\u0026gt;3000/tcp\nSolution\nThe notation 5000-\u0026gt;3000 means that the port 5000 of the host computer is mapped to the port 3000 of the container. More specifically, when a client application connects to the port 5000 of the host computer, the connection is redirected to the port 3000 of the container.\nThe IP address 0.0.0.0 means that we can connect to the container by using any IP address on the host computer.\nExercise\nExercise 1.8 Can you tell which URL you have to type in your web browser to access the application?\nSolution\nhttp://localhost:5000 or http://127.0.0.1:5000/.\nIn the network settings of you computer, you can also get the IP address associated by the DHCP server of your\nlocal network and use that address to connect to the application from another device connected to the same network (for instance, your mobile phone).\nWarning\nIf you want to see the cats, you need to append /pet to the URL that you found in the previous question. This is determined in file server.js.\n1.4 Shutting down the application You can shut down the application by typing the following command:\ndocker-compose down\nExercise\nExercise 1.9 Does shutting down the application remove the networks created for the application? What about the volumes?\nSolution\nJust type the command:\ndocker network ls\nand you’ll see that the network backend is not there anymore.\nType the command:\ndocker volume ls\nand you’ll see that the volume pet-store_pets-data is still there.\nBy default volumes are not removed, to avoid the risk of permanently deleting data that might still be useful later. If you wish to remove the volumes when shutting down the application, you can type:\ndocker-compose down -v\n1.5 Scaling a service When we launch the application, we can specify the number of instances of each service. This is useful when we expect our application to be solicited by many users; the workload will be automatically balanced across all the instances of the service.\nLet’s launch our application with 3 instances of the service web:\ndocker-compose up --scale web=3 -d\nExercise\nExercise 1.10 Running the previous command results in an error.\nCan you explain why?\nWhat fix would you propose in file docker-compose.yml? Solution\nWe’re trying to bind three instances to the port 5000 on the host computer. This is not possible.\nIn the section ports of file docker-compose.yml we need to remove 5000 and we only leave 3000. This way, each time we run a container, a random port will be chosen on the host computer to bind it with port 3000 of the container.\nWarning\nYou need to shut down the application before relaunching it again. In fact, even if we got an error, an instance of the db service and an instance of the service web are still running.\nModify the file docker-compose.yml and relaunch the application.\nRun the following command and verify that you actually have three running instances of the service web.\ndocker-compose ps\nTry to connect to the application by using the three port numbers indicated in the output of the previous command. Warning\nDon’t forget to shut down the application before you go on.\n2 Pushing an application Once we have built an application, we might want to share it by pushing it to the DockerHub registry or any other container registry, be it private or public.\n2.1 Creating an account on DockerHub You need to create an account on the DockerHub website in order to perform this activity.\n2.2 Renaming the images In order to push your images to the registry, you need to rename them, so as the new name has the following structure:\nyourusername/image-name:image-tag\nFor instance, since my username is quercinigia, I’ll rename the two images pet-store-web and pet-store-db with the docker tag command as follows:\ndocker tag pet-store-web quercinigia/pet-store-web:1.0\ndocker tag pet-store-db quercinigia/pet-store-db:1.0\nI chosen 1.0 as a tag, but feel free to pick another one.\n2.3 Logging in You need to log in to your DockerHub account.\nInstructions for Docker Desktop (macOS and Windows)\nmacOS users: to see how to log in click here.\nWindows users: to see how to log in click here.\n2.4 Pushing the images Once you’re successfully logged in, you can type the following commands in the terminal to push the two images of your application:\ndocker push YOUR-USERNAME/pet-store-web:1.0\ndocker push YOUR-USERNAME/pet-store-db:1.0\nRemember to replace YOUR-USERNAME with your actual username in the commands above.\nAfter the task is completed, verify that the images appear in your Docker registry.\nGood to know\nHere we manually tagged and uploaded the two images. This method becomes quickly annoying when the application consists of more than two images. Another way to go about this task is:\nSpecify the image names with your username in file docker-compose.yml. For instance, in my docker-compose.yml I would write: services: web: build: web image: quercinigia/pet-store-web:1.0 ... db: build: database image: quercinigia/pet-store-db:1.0 ... Build the application with the following command: docker-compose build\nPush the images of the application with the following command: docker-compose push\nThis way, with just two commands we push to the registry all the images of the application, no matter how many they are.\n3 Introduction to Kubernetes Kubernetes is the most popular orchestrator to date. It is used to manage a multi-service application, usually deployed across multiple hosts in a cluster.\nWhile using Docker Compose, a service corresponds to a Docker image and a service instance to a Docker container.\nAs opposed to that, in Kubernetes the computation unit is a pod, that is a collection of containers. In other words, we don’t reason in terms of containers anymore, but in terms of pods. Of course, a pod can also consist of just one container.\nGood to know\nIn practice, we rarely have to manipulate pods directly in Kubernetes, as there are higher-level objects that manage them. We’ll use these objects in the next sections.\n3.1 Activate Kubernetes Instructions for Docker Desktop (macOS and Windows)\nYou need to follow all the instructions documented on this page\nBefore moving on, don’t forget to type the following command to verify that Kubernetes is correctly activated\nkubectl get nodes\nYou should see a node called docker-desktop whose status is set to READY. If the status is NOT READY, just wait few seconds before typing the command again.\nWarning\nThis solution might not be working for you. In that case, disable Kubernetes in Docker Desktop and install minikube, by following these instructions.\n3.2 Deploying a pod In order to deploy a pod, we first have to give its specification, basically its name and the containers that compose it with their settings. Similarly to Docker Compose, a pod is specified in a declarative way with a YAML configuration file.\nConsider the following specification.\napiVersion: v1 kind: Pod metadata: name: web-pod labels: app: web-pod spec: containers: - name: web image: nginx:alpine ports: - containerPort: 80 Here is an explanation of the properties in the specification.\napiVersion. Defines the versioned schema of this representation.\nkind. The type of the resource that we intend to create.\nmetadata. The resource metadata. The list of all metadata is specified here.\nspec. The specification of the desired behaviour of the pod. The list of the possible specifications can be found here.\nEssentially, the previous specification defines a pod with a container that is launched from the image nginx:alpine (a Web server) and listens to ports 80 and 443.\nActivity\nCopy the previous specification to a file named sample-pod.yaml (or any other name of your liking).\nBy using the command cd in the terminal, place yourself in the directory where the file sample-pod.yaml is stored.\nDeploy the pod by typing the following command:\nkubectl create -f sample-pod.yaml\nVerify that the pod is deployed by typing the following command: kubectl get pods\nThe first time you run the last command, you might see that the pod is not ready yet. You need to wait for the image nginx:alpine to be pulled from DockerHub. Wait few seconds and try the command again until the pod is marked as running.\nYou can also get more information on the running pod (e.g., its assigned IP address) by typing the following command:\nkubectl get pod -o wide web-pod\nExercise\nExercise 3.1 Open a web browser and type http://localhost:80.\nWhat do you get? What if you try to use the IP of the pod instead of locahost?\nWhat should we define in order to fix the problem? Solution\nThe connection is refused both when using localhost and the IP address of the pod.\nIn order to connect to the service instance, we need to define a Service object in Kubernetes, that exposes an IP address that client applications can use.\nThe following configuration defines a Service object of type LoadBalancer:\napiVersion: v1 kind: Service metadata: name: nginx-service spec: type: LoadBalancer ports: - port: 8080 targetPort: 80 protocol: TCP name: http selector: app: web-pod Good to know\nA LoadBalancer service is a NodePort service that offers load balancing capabilities. It is intended to expose an IP address that client applications (external to the Kubernetes cluster) can use to access the service.\nExercise\nExercise 3.2 What is the field selector in the definition of the service?\nSolution\nThe selector is used internally by Kubernetes to identify the pods that are the instances of the service. If you look at the specification of the pod above, the field metadata contains a field labels. One of the labels is app: web-pod ; the selector matches this label.\nExercise\nExercise 3.3 In the specification of the service, what port and targetPort mean?\nSolution\nport specifies the port number where the service will be available.\ntargetPort specifies the port number opened at the pods that are instances of the service.\nActivity\nCopy and paste the service specification into a file named sample-service.yaml (or any other name of your liking).\nBy using the command cd in the terminal, place yourself in the directory where the file sample-service.yaml is stored.\nDeploy the service by typing the following command:\nkubectl create -f sample-service.yaml\nVerify that the service is deployed by typing the following command: kubectl get services\nThis command returns all services running in Kubernetes. For each service, you also get a name. In order to only target the service that you’ve just created, simply type the following command:\nkubectl get svc/nginx-service\nYour service’s name is nginx-service (as specified in file sample-service.yml); svc is only the namespace where all Kubernetes services are put.\nExercise\nExercise 3.4 By looking at the output of the command kubectl get svc/nginx-service, which URL do you need to type in the Web browser in order to access the service?\nSolution\nThe CLUSTER-IP is an IP address that is only visible within the cluster. In other words, only the services running within the cluster can connect to our nginx-service by using this IP address.\nWhat we need to use is the IP address specified under EXTERNAL-IP.\nIf you’re using Docker Desktop\nEXTERNAL-IP is likely to be localhost or 127.0.0.1\nIf you’re using Minikube\nEXTERNAL-IP will be whatever IP address assigned to the service.\nAs for the port number, under PORT(S) you should see something like 8080:30548 (the second number is likely different, but it should be something in the range [30000-32767]). The port 30548 is opened on each node of the Kubernetes cluster. In practice, when we type http://localhost:8080, the load balancer replaces it with the IP address of a node in the cluster, followed by port number 30548. When the kube-proxy on that node receives this request, it forwards it to an instance of the service, even if that instance runs in another node.\nWarning\nStop the pod and the service before moving on. Here are the commands to do so:\nkubectl delete po/web-pod\nkubectl delete svc/nginx-service\n4 Kubernetes: deploying an application In this section, we’re going to deploy our pet store in Kubernetes. As a reminder, our application consists of two services: web and db.\nIn order to define an application in Kubernetes, we need to use two types of objects for each service of the application:\nA workload resource that gives the specification of the service, such as the pods that make up the service itself (images, network settings) and metadata, such as the desired number of instances.\nA Service object. As we have seen previously, a service object exposes an IP address that allows client applications, both inside and outside the Kubernetes cluster, to connect to the service.\nWhen we define an application in Kubernetes, we rarely, if ever, need to play directly with pods. Instead, we can resort to higher-level objects, called Controllers, for an easier definition of the desired state of the application itself. The type of the controller that we need to use depends on the nature of the service itself: stateless or stateful.\nExercise\nExercise 4.1 Is the service web of our application stateless or stateful?\nIs the service db of our application stateless of stateful? Solution\nStateful service: one that manages permanent data and is bound to that data. Stateless service: one that is not bound to permanent data. The service web is stateless because it does not have to be tightly bound to the data. In other words, the service web uses the data that is stored in the backend database, but it can be scheduled on any node of the cluster independently of that data.\nThe service db is stateful because it directly manages permanent data.\n4.1 The web service: deployment For stateless application services, we can use a Deployment for the deployment of a set of identical pods that are launched across several nodes in the Kubernetes cluster.\nWe here define the specification of a Deployment corresponding to the service web.\napiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP Here is the explanation of the specification:\nA Deployment named web is created, as indicated by the field metadata.name.\nThe Deployment creates three replicated pods, as indicated by the field spec.replicas.\nThe Deployment considers that the pods with both labels app: pets and service: web are part of the deployment. This is indicated by the field spec.selector.matchLabels. This field is used to let the Deployment know how to find its pods.\nThe configuration of the pods that are part of the Deployment is given in the field spec.template and its subfields.\nEach pod of the Deployment is given labels app: pets and service: web. This is indicated by the field spec.template.metadata.labels. Note that here we specify exactly the same values as in spec.selector.matchLabels. This field is used to give pods labels so that they can be identified and located in Kubernetes.\nEach pod has exactly one container, named web, run from the image quercinigia/pet-store-web:1.0 stored in the DockerHub. The container listens on port 3000 and uses the TCP protocol. This is specified in the field spec.template.spec.containers.\nActivity\nCopy and paste the previous specification to a new file web-deployment.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-web:1.0 with the one that you pushed to your DockerHub registry.\nUse the command cd in the terminal to position yourself in the directory where the file web-deployment.yaml is stored.\nDeploy this Deployment in Kubernetes by typing the following command:\nkubectl create -f web-deployment.yaml\nExercise\nExercise 4.2 Type the following command:\nkubectl get all\nWhich objects have been created following the creation of the Deployment?\nSolution\nWe can clearly see three types of objects:\nThree pods, that correspond to the three replicas that we specified in the configuration.\nOne deployment. Note that the 3/3 indicates that the deployment has 3 identical pods, of which 3 are running.\nOne ReplicaSet. The Deployment creates a ReplicaSet, that is the actual object that controls the three pods. The Deployment provides some additional services (i.e., updates) on top of a ReplicaSet.\nExercise\nExercise 4.3 Get the name of one of the running pods and kill it by using the following command\nkubectl delete name-of-pod\nIf the command hangs in the terminal, feel free to type Ctrl-C to get back control of the terminal.\nType again following command:\nkubectl get all\nHow many pods do you see? Is it surprising?\nSolution\nIf we type the command right after deleting the pod, it is likely that we’ll see four pods, one that is terminating and another one that is running. Try to type the command until you see three running pods.\nThis is not surprising. If we kill the pod, the orchestrator detects a deviation from the desired state (which is, we want three replicas) and reschedules immediately another pod. This is what we mean by a self-healing system. This is one of the major roles of an orchestrator.\n4.2 The web service: service Now we need to define a Service in order to expose the web service to the public. Here is the definition:\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web Exercise\nExercise 4.4 Describe the specification of this service.\nSolution\nWe create a service named web. This indicated in the field metadata.name.\nThe service has type LoadBalancer. This is indicated in the field spec.type.\nThe service listens on port 8080 and uses the TCP protocol. Each service instance (i.e., pod) listens on port 3000. This is indicated in the field spec.ports.\nThe instances of the service (i.e., the pods) are those with labels app: pets and service: web. This is indicated in the field spec.selector.\nActivity\nCopy and paste the previous specification to a new file web-service.yaml (or any name of your liking).\nUse the command cd in the terminal to position yourself in the directory where the file web-service.yaml is stored.\nCreate the service with the following command:\nkubectl create -f web-service.yaml\nVerify that the service has been created with the following command: kubectl get services\nLocate the external IP (let’s call it EXTERNAL-IP) of the web service and type the URL http://EXTERNAL-IP:8080 in your Web browser. You should see a Web page where the phrase Pet store appears. 4.3 The db service: StatefulSet Kubernetes has defined a special type of ReplicaSet for stateful services that is called StatefulSet.\nLet’s define a StatefulSet to give the specification of the db service.\napiVersion: apps/v1 kind: StatefulSet metadata: name: db spec: selector: matchLabels: app: pets service: db serviceName: db template: metadata: labels: app: pets service: db spec: containers: - image: quercinigia/pet-store-db:1.0 name: db ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: pets-data volumeClaimTemplates: - metadata: name: pets-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi By now you shouldn’t have any problem understanding the meaning of the fields in this specification. The fields are also documented in the official Kubernetes documentation. Two points are worth a comment. First, the specification of a StatefulSet needs an attribute serviceName that indicates the name of the service that is responsible for the network identify of the pods in the set. That attribute is mandatory. This is listed as one of the limitations of a StatefulSet. Another novelty is the field volumeClaimTemplates. It describes additional constraints on the volumes defined in the specification, in this case the volume named pets-data (where PostgreSQL keeps the data). In particular, two claims are given:\nThe access mode ReadWriteOnce means that the volume can be mounted as read-write by a single node in the Kubernetes cluster. Access modes are documented here.\nWe request at least 100MB of storage for the database.\nActivity\nCopy and paste the previous specification to a new file db-stateful-set.yaml (or any name of your liking). Make sure that you replace the image quercinigia/pet-store-db:1.0 with the one that you pushed to your DockerHub registry.\nUse the command cd in the terminal to position yourself in the directory where the file db-stateful-set.yaml is stored.\nDeploy this StatefulSet in Kubernetes by typing the following command:\nkubectl create -f db-stateful-set.yaml\nExercise\nExercise 4.5 Type the command:\nkubectl get all\nWhich objects have been created when you deployed the StatefulSet?\nSolution\nA pod (likely to be called pod/db-0)\nA new StatefulSet that governs the pod.\nNote that the StatefulSet has only one pod, which is normal, since we didn’t specify a higher number of replicas.\nExercise\nExercise 4.6 Open the Web browser and type the following URL:\nIf you’re using Docker Desktop\nhttp://localhost:8080/pet\nIf you’re using Minikube\nhttp://minikube-ip:8080/pet\nwhere you’ll replace minikube-ip with the external IP address associated with the web service.\nRight after, type the command kubectl get all. What do you observe? Can you explain the reason?\nSolution\nWe cannot access the application. If you type the command kubectl get all quickly, you should show that the three pods that are instances of the service web are in an error state. If you were too slow, you should see that the restart count of each pod has changed. This means that the pods have been restarted because of an error.\nWhat happens here is that we launched the database, but we didn’t create the Service that lets the clients access the database.\nIn the output of the command get kubectl all, look at the names of the pods that are instances of the web service. Take the name of any these pods, and put it in place of NAME-OF-POD in the following command:\nkubectl logs NAME-OF-POD --previous\nExercise\nExercise 4.7 Does the output of the previous command confirm the explanation given in the previous question?\nSolution\nYes, we can clearly read ENOTFOUND db db:5432. This means that Kubernetes cannot resolve the name db to an IP address.\n5 The db service: Service object From the above observations, we understand that we need to define a service to expose the database to the clients.\nExercise\nExercise 5.1 Should the db service be accessible to client applications that are external to the Kubernetes cluster?\nSolution\nNo, this component is the backend of the application. The only client that needs to access the database is the frontend, that is the web service, that runs inside the cluster.\nExercise\nExercise 5.2 Given the answer to the previous question, what should the type of this service be?\nSolution\nClusterIP; this is the type of a service that does not need to be exposed to the outside world.\nExercise\nExercise 5.3 Write the specification of the db service in a file named db-service.yaml (or any other name of your liking).\nCaution. In the file db-stateful-set.yaml the field spec.serviceName indicates the name that the service must have.\nSolution\napiVersion: v1 kind: Service metadata: name: db spec: type: ClusterIP ports: - port: 5432 protocol: TCP selector: app: pets service: db Activity\nDeploy the service by typing the following command: kubectl create -f db-service.yaml\nVerify that the service has been created with the following command: kubectl get all\nVerify that you can reach the application at http://localhost:8080/pet (Minikube users: replace localhost with the external IP address associated to the web service!). It might happen that the database service is not ready yet, and so you’ll get a connection error. Just wait and retry later. 5.1 Shutting down the application After you’re done with the application, you can shut it down with the following commands:\nkubectl delete svc/web\nkubectl delete svc/db\nkubectl delete deploy/web\nkubectl delete statefulset/db\nThe order in which you type these commands doesn’t matter.\nIf you type multiple times the command:\nkubectl get all\nyou should see that the resources progressively disappear.\n5.2 Conclusion In the previous exercises, we deployed an application with two services, for which we had to create four files and type as many commands. For larger applications this gets a bit annoying. We can write all the definitions in a single file (e.g., pets.yaml) where each specification is terminated by - - -.\nHere is an example, where we write the specification of the Service and Deployment associated with the service web.\napiVersion: v1 kind: Service metadata: name: web spec: type: LoadBalancer ports: - port: 8080 targetPort: 3000 protocol: TCP selector: app: pets service: web --- apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 3 selector: matchLabels: app: pets service: web template: metadata: labels: app: pets service: web spec: containers: - image: quercinigia/pet-store-web:1.0 name: web ports: - containerPort: 3000 protocol: TCP ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"767c81e6fde94a2dd55f1553b2c7ccf2","permalink":"/courses/cloudalbert/tutorials/kube-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/cloudalbert/tutorials/kube-tutorial/","section":"courses","summary":"Kubernetes tutorial","tags":null,"title":"Introduction to Kubernetes","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1 Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\nSuppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2 What is the maximum number of measurements in a year?\nExercise\nExercise 1.3 Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\nExercise\nExercise 1.4 Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 2.1 Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n3 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 3.1 Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\nExercise\nExercise 3.2 Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0cbba47479da04631bd931db00bad3ff","permalink":"/courses/bdia/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1 Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\nSolution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n\\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\). Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2 What is the maximum number of measurements in a year?\nSolution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\nExercise\nExercise 1.3 Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\nSolution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\nExercise\nExercise 1.4 Based on the answer to the previous question, propose a better implementation to handle the CSV file.\nSolution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n\\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\). 2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 2.1 Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\nSolution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n\\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\). We note that the reduce function is the identity.\n3 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 3.1 Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\nSolution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\nExercise\nExercise 3.2 Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year. Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n\\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"45224063bb1ca83ddb072a9416ee9aa7","permalink":"/courses/bdia_old/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1 Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\nSolution\nmap: \\((year, month, temperature) \\rightarrow (year, temperature)\\)\nreduce: \\((year, temps) \\rightarrow\\) \\((year, sum(temps)/len(temps))\\)\n\\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\). Suppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2 What is the maximum number of measurements in a year?\nSolution\nSince we can have up to one measurement per second, the maximum number of measurements \\(M_{max}\\) for a certain year is given by the following formula:\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\nExercise\nExercise 1.3 Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\nSolution\nSince there might be up to 31 million values associated with a key, the bottleneck of the computation would be the shuffle operation, since we need to copy a high number of (key,value) pairs from the mappers to the reducers.\nAlso, a reducer might have to loop over a huge list of values in order to compute their average.\nExercise\nExercise 1.4 Based on the answer to the previous question, propose a better implementation to handle the CSV file.\nSolution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, temps) \\rightarrow\\) \\((year, (sum(temps), len(temps)))\\)\nreduce: \\((year, [(s_i, l_i),\\ i=1\\dots n]) \\rightarrow\\) \\((year, \\frac{\\sum_{i=1}^n s_i}{\\sum_{i=1}^n l_i})\\)\n\\(temps\\) is the list of all temperatures in the same \\(year\\). \\(sum(temps)\\) sums all the elements in the list \\(temps\\). \\(len(temps)\\) gives the length of the list \\(temps\\). 2 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 2.1 Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\nSolution\nThe second equation is more appropriate because it allows the computation of the sum of the elements and of the square of the elements step by step by using map and combine together.\nInstead, if we use the first equation, we need first to compute the average and then use it to compute the variance.\nExercise\nExercise 2.2 Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year. Solution\nmap: \\((year, mo, d, mi, sec, temperature) \\rightarrow (year, temperature)\\)\ncombine: \\((year, T) \\rightarrow\\) \\((year, (sum(T), sum(T^2), len(T)))\\)\nreduce: \\((year, [(s_{i}, sq_{i}, l_{i}),\\ i=1\\dots n]) \\rightarrow\\) \\((year, (\\mu, \\sigma))\\)\nwhere:\n\\(T\\) is the list of all temperatures in the same \\(year\\). \\(sum(T)\\) sums all the elements in the list \\(T\\). \\(T^2 = [x^2 | x\\in T]\\) \\(len(T)\\) gives the length of the list \\(T\\). \\(\\mu = \\sum_{i=1}^n s_{i}/ \\sum_{i=1}^n l_{i}\\) \\(\\sigma = \\sqrt{ (\\sum_{i=1}^n sq_{i}/ \\sum_{i=1}^n l_{i}) - \\mu^2 }\\) 3 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 3.1 Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\nSolution\nmap: \\((x, F) \\rightarrow [((u, v), x)\\ \\forall (u, v) \\in F\\ |\\ u \u0026lt; v ]\\)\nreduce: \\([(u, v), LCF] \\rightarrow [(u, v), LCF]\\)\nwhere:\n\\(x\\) is the first item in a line. \\(F\\) is the list containing the items in a line except the first one (\\(x\\)’s friends). \\(LCF\\) is the list of all individuals that are friends with both \\(u\\) and \\(v\\). We note that the reduce function is the identity.\n4 Creating an inverted index We have a collection of \\(n\\) documents in a directory and we want to create an inverted index, one that associates each word to the list of the files the word occurs in. More precisely, for each word, the inverted index will have a list of the names of the documents that contain the word.\nExercise\nExercise 4.1 Propose a MapReduce implementation to create an inverted index over a collection of documents.\nSolution\nThe input to the map will be a key-value pair, where the key is the name of a file \\(f\\) and the value is the content \\(C\\) of the file.\nmap: \\((f, C) \\rightarrow [(w, f)\\ \\forall w \\in C]\\)\nreduce: \\((w, L) \\rightarrow (w, L)\\)\nwhere \\(L\\) is the list of the files containing the word \\(w\\).\nWe note that the reduce function is the identity.\nNote also that in the map function we can add instructions to preprocess the text. For example, we can eliminate some words that are not useful in the index (e.g., the stopwords) or remove special symbols.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a59fa830506ab61c12736f4ea6f887f4","permalink":"/courses/big-data-marseille/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/big-data-marseille/tutorials/map-reduce/","section":"courses","summary":"Description of the MapReduce tutorial.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Calcul des moyennes On nous donne un ensemble de données qui contient les mesures moyennes des températures mensuelles sur plusieurs années. Plus précisément, l’ensemble de données est stocké dans un fichier CSV, où chaque ligne correspond à une mesure mensuelle et les colonnes contiennent les valeurs suivantes : année, mois, température moyenne du mois.\n1980,1,5 1980,2,2 1981,1,2 1981,2,1 .... 1980,3,10 1980,4,14 1980,5,17 .... 1981,3,3 1981,4,10 .... Nous avons l’intention d’obtenir la température moyenne pour chaque année.\nExercice\nExercise 1.1 Écrire un algorithme MapReduce qui génère des paires clé-valeur \\((annee, temperature\\_moyenne)\\).\nSolution def map(line): values = line.split(\u0026quot;,\u0026quot;) return (values[0], float(values[-1])) def reduce(year, temperatures_in_year): return (year, sum(temperatures_in_year) / len(temperatures_in_year)) Supposons maintenant que nous ayons un grand fichier CSV stocké dans un système de fichiers distribué (par exemple, HDFS), contenant une série de mesures au format « Année,Mois,Jour,Heure,Minute,Seconde,Température ». Pour certaines années, nous pouvons avoir jusqu’à une mesure par seconde. Comme précédemment, nous souhaitons calculer les paires clé-valeur (année, température moyenne) à l’aide d’un algorithme MapReduce.\nExercice\nExercise 1.2 Quel est le nombre maximum de mesures par an ?\nSolution Comme nous pouvons avoir jusqu’à une mesure par seconde, le nombre maximum de mesures \\(M_{max}\\) pour une année est donné par la formule suivante :\n\\[ M_{max} = 365 \\times 24 \\times 60 \\times 60 \\approx 31.5 \\times 10^6 \\]\nExercice\nExercise 1.3 Compte tenu de la réponse à la question précédente, discutez de l’efficacité de la première implémentation de l’algorithme.\nSolution Étant donné qu’il peut y avoir jusqu’à 31 millions de valeurs associées à une clé, le goulot d’étranglement du calcul serait l’opération de shuffle, puisque nous devons copier un grand nombre de paires (clé, valeur) des mappers vers les reducers.\nDe plus, un reducer peut avoir à boucler sur une énorme liste de valeurs afin de calculer leur moyenne.\nExercice\nExercise 1.4 En fonction de la réponse à la question précédente, proposez une meilleure implémentation pour traiter le fichier CSV.\nSolution La fonction map() ne change pas par rapport à l’exercice précédent.\ndef map(line): values = line.split(\u0026quot;,\u0026quot;) return (values[0], float(values[-1])) Maintenant on code la fonction combine() qui agira sur les couples clé-valeur renvoyées par chaque tâche Map. Concrètement, lorsqu’une tâche Map termine d’appliquer la fonction map() sur toutes les lignes de son bloc, une opération de shuffle est réalisée pour rassembler toutes les valeurs à la clé correspondante. Ainsi, la fonction combine() prend en argument un couple, dont la clé (une année) est associée à la liste de toutes ses valeurs (les températures de l’année). La fonction renvoie un couple dont la clé représente toujours une année, mais la valeur est, elle-même, un couple, dont le premier élément est la somme des températures sur l’année et le deuxième élément est la longueur de la liste des températures.\nCe faisant, on enverra aux reducers seulement un couple pour chaque année, réduisant ainsi le nombre de données qui seront transmises via le réseau.\ndef combine(year, temperatures_in_year): return (year, (sum(temperatures_in_year), len(temperatures_in_year))) La fonction reduce() prend en arguments un couple, dont la clé représente une année et la valeur est une liste de couples reçues après l’étape de combine(). Il faudra ainsi sommer toutes les températures et toutes les longueurs, et ensuite calculer la moyenne.\ndef reduce(year, sum_len_tuples): sum_temps = sum(s for (s, _) in sum_len_tuples) nb_temps = sum(l for (_, l) in sum_len_tuples) return (year, sum_temps/nb_temps) 2 Amis communs dans un réseau social Considérons un réseau social décrit par un graphe encodé dans un fichier texte. Chaque ligne du fichier est une liste d’identifiants séparés par des virgules. Par exemple, la ligne \\(A,B,C,D\\) signifie que \\(A\\) est ami avec \\(B\\), \\(C\\) et \\(D\\). Un extrait du fichier ressemble à ce qui suit :\nB,A,D A,D,B,C D,C,A,B C,D,A ... Nous supposons que la relation d’amitié est symétrique : \\((A, B)\\) implique \\((B, A)\\).\nNous voulons obtenir la liste des amis communs pour chaque paire d’individus :\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] Comme contrainte supplémentaire, nous voulons représenter un couple une seule fois et éviter de représenter un couple symétrique. En d’autres termes, si nous produisons \\((A, B)\\), nous ne voulons pas produire \\((B, A)\\).\nExercice\nExercise 2.1 Proposez une implémentation MapReduce pour trouver les amis communs dans un réseau social satisfaisant les contraintes données.\nSolution Le résultat de notre algorithme doit être une une collection de couples, dont le premier élément représente un couple d’individus et le deuxième élément représente la liste des amis communs à ces individus.\nOn en déduit qu’il faudrait choisir un couple d’individus en tant que clé.\nOr, la fonction map() agit ligne par ligne. La question que nous devons nous poser est la suivante : pouvons-nous déduire une information à propos d’amis communs entre deux individus?\nPrenons l’example de la ligne : A,D,B,C. À partir de cette ligne, nous savons que A est l’ami commun à tous les couples de ses amis : (B, D), (C, D) et (B, C).\nCela nous indique comment implémenter la fonction map().\ndef map(line): values = line.split(\u0026quot;,\u0026quot;) return [((x, y), values[0]) for x in values[1:] for y in values[1:] if x \u0026lt; y] À noter que la condition x\u0026lt;y nous garantit que nous ne produirons jamais de couples symétriques.\nLa phase de shuffle associera ensuite à chaque couple d’individus tous leurs amis communs. Ceci étant déjà le résultat voulu, nous n’avons pas besoin de coder une fonction reduce() (fonction identité).\n3 Calcul de la moyenne et de l’écart-type Nous considérons à nouveau le grand fichier CSV contenant une série de mesures au format Année, Mois, Jour, Minute, Seconde, Température. Nous souhaitons maintenant générer une série de paires clé-valeur (année, (temperature_moy, ecart_type)).\nNous pouvons exprimer l’écart-type de \\(n\\) valeurs \\(x_i\\) (\\(1 \\leq i \\leq n\\)) avec deux équations différentes.\nLa première équation est la suivante :\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nLa deuxième équation est la suivante :\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercice\nExercise 3.1 Quelle équation de l’écart-type est plus appropriée dans un algorithme MapReduce ? Pourquoi ?\nSolution La deuxième équation est plus appropriée car elle permet de calculer la somme des éléments et le carré des éléments pas à pas en utilisant conjointement map() et combine().\nAu contraire, si nous utilisons la première équation, nous devons d’abord calculer la moyenne et ensuite l’utiliser pour calculer la variance.\nExercice\nExercise 3.2 Proposez une implémentation MapReduce pour calculer la moyenne et l’écart-type des températures pour chaque année.\nSolution def map(line): values = line.split(\u0026quot;,\u0026quot;) return (values[0], float(values[-1])) def combine(year, temperatures_in_year): return (year, (sum(temperatures_in_year), sum(x**2 for x in temperatures_in_year), len(temperatures_in_year))) def reduce(year, sum_len_tuples): sum_temps = sum(s for (s, _, _) in sum_len_tuples) sum_sq_temps = sum(sq for (_, sq, _) in sum_len_tuples) nb_temps = sum(l for (_, _, l) in sum_len_tuples) return (year, (sum_temps/nb_temps, math.sqrt(sum_sq_temps/nb_temps - (sum_temps/nb_temps)**2))) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52755e7243f2ad30eb8ac78b1ad91ef8","permalink":"/courses/bigdata-mds/labs/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata-mds/labs/map-reduce/","section":"courses","summary":"Introduction to MapReduce programming.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1 Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\nSuppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2 What is the maximum number of measurements in a year?\nExercise\nExercise 1.3 Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\nExercise\nExercise 1.4 Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nB,A,D A,D,B,C D,C,A,B C,D,A ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 2.1 Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n3 Computing average and standard deviation We consider again the large CSV file with a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We now intend to generate a series of key-value pairs (year, (avg_temperature, std_deviation)).\nWe can express the standard deviation of \\(n\\) values \\(x_i\\) (\\(1 \\leq i \\leq n\\)) with two different equations.\nThe first equation is as follows:\n\\[ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} \\]\nThe second equation is as follows:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nExercise\nExercise 3.1 Which equation of the standard deviation is more appropriate in a Map-Reduce algorithm? Why?\nExercise\nExercise 3.2 Propose a MapReduce implementation to compute the average and the standard deviation of the temperatures for each year. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"10c78587578365fc7de9a4d112b1d6ba","permalink":"/courses/bigdata/tutorials/map-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bigdata/tutorials/map-reduce/","section":"courses","summary":"Introduction to MapReduce programming.","tags":null,"title":"MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We are given a dataset that contains the average monthly temperature measurements over the course of some years. More precisely, the dataset is stored in a CSV file, where each row corresponds to a monthly measurement and the columns contain the following values: year, month, average temperature in the month.\n1980,1,5 1980,2,2 1980,3,10 1980,4,14 1980,5,17 .... 1981,1,2 1981,2,1 1981,3,3 1981,4,10 .... We intend to get the average monthly temperature for each year.\nExercise\nExercise 1.1 Write a MapReduce algorithm that generates key-value pairs \\((year, average\\_temperature)\\).\nSuppose now that we have a large CSV file stored in a distributed file system (e.g., HDFS), containing a series of measurements in the format “Year, Month, Day, Minute, Second, Temperature”. We can have up to one measurement per second in some years. Like before, we’d like to compute key-value pairs (year, average_temperature) by using a MapReduce algorithm.\nExercise\nExercise 1.2 What is the maximum number of measurements in a year?\nExercise\nExercise 1.3 Considering the answer to the previous question, discuss the efficiency of the first implementation of the algorithm.\nExercise\nExercise 1.4 Based on the answer to the previous question, propose a better implementation to handle the CSV file.\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nA,B,C,D B,A,D C,A,D D,A,B,C ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(A, B), [D] (A, C), [D] (A, D), [B, C] (B, C), [D] (B, D), [A] (C, D), [A] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nExercise\nExercise 2.1 Propose a MapReduce implementation to find the common friends in a social network satisifying the given constraints.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6aae2091546b9bc97c8eae0bb41a2419","permalink":"/courses/bdalbert/tutorials/mapreduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdalbert/tutorials/mapreduce/","section":"courses","summary":"MapReduce programming","tags":null,"title":"Programming with MapReduce","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, you’ll learn how to use the Spark MLlib library to train, test and evaluate machine learning models. We’ll be using a dataset of AirBnb accommodation in the San Francisco area. The dataset is available in HDFS at the following path:\nhdfs://sar01:9000/cpu_quercini/ml/sf-airbnb-clean.parquet Dataset source\nThe dataset has been obtained from this GitHub repository.\nThe goal of the exercise is to predict the price per night of an apartment given all the features in the dataset.\nChanging Python interpreter of the executors\nThe default Python interpreter used by the executors in the cluster does not have the package numpy installed. In order to use one that has numpy, you need to run the following command in the terminal:\nexport PYSPARK_PYTHON=/usr/bin/python3 1 Obtaining a training and test set In order to build and evaluate a machine learning model, we need to split our dataset into a training and test set.\nActivity\nCopy the file train_test.py to your home folder in the cluster by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/mllib/train_test.py . Complete Line 9, by specifying your directory in HDFS, which is as follows (replace X with your account number). hdfs://sar01:9000/bdiaspark23/bdiaspark23_X Exercise\nExercise 1.1 Complete Line 23. Write the code to read the input file, which is stored in HDFS as a Parquet file, into the DataFrame airbnb_df.\nFrom Line 26, add the instructions necessary to print the schema of airbnb_df and the first 5 rows. Which option should you use to nicely see the values in the columns?\nExecute the code with spark-submit and verify that the data is correctly loaded. We now split airbnb_df into two separate DataFrames, train_df and test_df, containing the training and test instances respectively.\nExercise\nExercise 1.2 Uncomment Line 32. Write the code to split the dataset into a training and test set. 80% of the instances must be taken as training instances, while the remaining 20% will be used a test instances. Looking at the DataFrame API documentation, which function are you going to use?\nFrom Line 36, write the code to print the number of instances in the training and test set.\nExecute the code with spark-submit. You should have 5780 training instances and 1366 test instances. It is time to save the training and test sets to a HDFS file. This way, we can load them whenever we need them to build a machine learning model for this problem.\nExercise\nExercise 1.3 From Line 47. Write the code to write the training and test sets to two Parquet files in HDFS. The paths to the two files are already available in variables training_set_file and test_set_file defined at Lines 40 and 43 respectively.\n2 Preparing the features In both training and test sets, the features correspond to DataFrame columns. Most of the machine learning algorithms in Spark need to have all features in one single vector. We need to use a transformer.\nGood to know\nTransformers take in a DataFrame and return a new DataFrame that has the same columns as the input DataFrame and additional columns that are specified as an argument to the transformer.\nCopy the file train_test_model to your home folder in the cluster by typing the following command:\ncp /usr/users/cpu-prof/cpu_quercini/mllib/train_test_model.py .\nExercise\nExercise 2.1 Implement the function read_training_set at Line 22. The file reads into a Spark DataFrame the training set Parquet file that you generated in the previous section.\nImplement the function read_test_function at Line 39. The file reads into a Spark DataFrame the test set Parquet file that you generated in the previous section.\nModify the variables at Line 105 and 106 so that they point to the two HDFS Parquet files that contain the training and test sets.\nUncomment Lines 109 and 110, so as to print the number of training and test instances.\nExecute the file train_test_model with spark-submit. Check that the number of training and test instances correspond to what you got in the first section.\nIt is now time to learn how to use a transformer to put all the necessary features into a vector. For the time being, we’ll only be using the feature bedrooms to predict the price.\nExercise\nExercise 2.2 Implement the function get_vector at Line 55. The comments in the file describe what the function is supposed to do. You’ll need to use a VectorAssembler object to implement the function.\nUncomment Lines 117 and 118 to call the function get_vector and display the result.\nExecute the file train_test_model with spark-submit. Observe the content of the the DataFrame train_vect. It should have a column named features containing a list with one value (the value of feature bedrooms).\nIt is now time to train a linear regression model on the given training set and the selected features.\nExercise\nExercise 2.3 Implement the function train_linear_regression_model at Line 79. The comments in the file describe what the function is supposed to do. Looking at the documentation, try to identify the object that you need to use to create a linear regressor and the function that you need to invoke to train the linear regressor.\nUncomment Lines 121, 124, 125, 126 to call the function train_linear_regression_model and display the coefficients learned by the linear regression model.\nExecute the file train_test_model with spark-submit. We can now use the trained model to make some predictions. The model returned by the function that you implemented in the previous exercise is an object of type LinearRegressionModel. Looking at the documentation, we learn that there is a function predict that allows us to make a prediction given a single instance. If we want to make a prediction on the whole test set, we should use the function transform. The function takes in a DataFrame with the test set and returns the same DataFrame with an additional column prediction that contains the predicted value.\nExercise\nExercise 2.4 Look at Lines 130 and 131. Which DataFrame do we need to pass the function transform? Is test_df a good choice? Why?\nComplete line 130 and uncomment lines 130, 131, 132.\nExecute the file train_test_model with spark-submit and observe the result. Is the predicted value the one that you expect given that you know the formula of the regression line? 3 Pipelines In the previous section we learned that a training and test set need to go through the same transformation steps in order to be fed to a machine learning model. When we have few transformations, it is easy to remember the ones that we applied to a training set, so as to apply them to the test set too. However, when we need to apply a series of transformations, the order of which is important, it is easy to make mistakes (applying different sets of transformations to the training and test instances, which leads to meaningless predictions).\nA good practice is to use the Pipeline API. A pipeline is composed of stages. Each stage may be a transformer or an estimator. A transformer is an object on which we call the function transform to obtain a new DataFrame from an input DataFrame. An estimator is an object on which we call the function fit to learn a model on a given DataFrame. The learned model is itself a transformer.\nWhen the function fit is called on a Pipeline, the training set goes through all the transformers and the estimators in the order in which they are declared in the pipeline; at last, the estimator specified in the last stage is trained on the training set. The model returned by applying the function fit on the pipeline is itself a transformer. If we invoke the function transform on that model on the test set, we obtain a DataFrame that contains a column named predictions. Implicitly, all the transformations in the pipeline will be applied to the test set too, before making the predictions.\nCopy the file pipeline_example.py to your home folder in the cluster by typing the following command:\ncp /usr/users/cpu-prof/cpu_quercini/mllib/pipeline_example.py . Exercise\nExercise 3.1 Look at the documentation and write a code from Line 34 to create a pipeline that does the same operations as in file train_test_model.py to train and test a linear regression model on the given training and test sets.\nDon’t forget to specify the paths to the training and test set files on HDFS at Lines 27 and 28. 3.1 From categorical to numerical features Many machine learning models do not handle categorical values: they need all features to be numerical. Linear regression is such an example.\nActivity\nCopy the file onehot_playground.py to your home directory in the cluster by typing the following command: cp /usr/users/cpu-prof/cpu_quercini/mllib/onehot_playground.py . Change lines 27 and 28 and write the paths to the files with the training and test sets. First, let’s find out which features are categorical in our dataset.\nExercise\nExercise 3.2 Complete the function get_categorical_columns.\nIn order to get an idea as to how to get the categorical columns, execute the code with spark_submit. This execute the instruction at Line 86.\nOnce you’re done with the implementation, execute the file with spark-submit and observe the output.\nOne example of categorical feature in our dataset is property_type, which takes values such as Apartment, House, Condominium… Each value of a categorical feature is also referred to as a category.\nOne way to turn this feature into a numerical one would be to assign a number to each category (e.g., Apartment corresponds to 0, House to 1….). However, this implicitly introduces an order among the categories: the category House would be worth twice as much as the category Apartment; this would inevitably bias the trained model.\nA commonly used method is one-hot encoding. Let’s find out how it works. Let’s focus only on the feature property_type.\nActivity\nUncomment Lines 40, 41 and 42. These lines instantiate an estimator that is called StringIndexer. This estimator associates numeric indexes to the values of property_type. The indexes will be stored in another column named property_type_index.\nUncomment Line 46. The StringIndexer is applied to the training set to learn how to associate indexes to the values of property_type. The result of the instruction is a new transformer.\nUncomment Line 49. The transformer learned on Line 46 is used to transform the training set into a new DataFrame. This DataFrame contains an additional column called property_type_index.\nExercise\nExercise 3.3 Complete the function count_property_types. Follow the instructions in the file.\nOnce the function is complete, uncomment Line 53 to call the function.\nExecute the file with spark-submit and observe the output. Can you guess how the indexes are assigned to the categories of the feature property_type?\nIt is time to find out how one-hot encoding works.\nActivity\nUncomment Lines 57, 58, 61, 64, 67. These lines instantiate an estimator that is called OneHotEncoder. This estimator uses the indexes in property_type_index and creates a new column property_type_ohe.\nThe estimator is trained on the training set and a new transformer is obtained (line 61).\nThe transformer is used on the training set to transform it into a new DataFrame, where a new column property_type_ohe exists.\nLine 67 prints selected columns of this transformed training set.\nExercise\nExercise 3.4 Execute the file with spark-submit.\nCan you understand how one-hot encoding works? Now, you have all the ingredients to create a full pipeline to train and test a linear regression model by using all features.\nExercise\nExercise 3.5 Create a new file full_pipeline.py where:\nYou select the categorical features.\nSet up a StringIndexer and a OneHotEncoder to apply one-hot encoding to all categorical features.\nObtain the numeric features.\nSet up a VectorAssembler to put into a single vector the one-hot encoded features and the numerical ones.\nSet up a linear regression model that takes in all features and the variable to predict (price).\nMix all these ingredients in a Pipeline.\nUse the Pipeline to train and test the model. 4 Evaluating a model In the previous sections we visually looked at the predictions to get an vague idea of how our estimator performs. In order to quantify the quality of our estimator, we need some evaluation measures.\nExercise\nExercise 4.1 Look at the documentation and play with the code to do model selection based on a grid search of parameters.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"99e158e4ab050341594fbee7e30fcc18","permalink":"/courses/bdia/tutorials/mllib/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/mllib/","section":"courses","summary":"Description of the Spark MLlib tutorial","tags":null,"title":"Spark MLlib","type":"docs"},{"authors":null,"categories":null,"content":" 1 Use case scenario We consider a relational database that holds the data of a chain of DVD stores; the database name is Sakila.\nThe Sakila database is serving an increasing number of queries from staff and customers around the world. A single monolithic database is not sufficient anymore to serve all the requests and the company is thinking of distributing the database across several servers (horizontal scalability). However, a relational database does not handle horizontal scalability very well, due to the fact that the data is scattered across numerous tables, as the result of the normalization process. Hence, the Sakila team is turning to you to help them migrate the database from PostgreSQL to MongoDB.\nFor the migration to happen, it is necessary to conceive a suitable data model. From the first discussions with the Sakila management, you quickly understand that one of the main use of the database is to manage (add, update and read) rental information.\n1.1 Description of the relational model The existing data model is recalled in Figure 1.1.\nFigure 1.1: The logical schema of the Sakila database Here is the description of the tables:\nThe table actor lists information for all actors. Columns: actor_id, first_name, last_name.\nThe table address contains address information for customers, staff and stores. Columns: address_id, address, address2, district, city_id, postal_code, phone.\nThe table category lists the categories that can be assigned to a film. Columns: category_id, name.\nThe table city contains a list of cities. Columns: city_id, city, country_id.\nThe table country contains a list of countries. Columns: country_id, country.\nThe table customer contains a list of all customers. Columns: customer_id, store_id, first_name, last_name, email, address_id, active, create_date.\nThe table film is a list of all films potentially in stock in the stores. The actual in-stock copies of each film are represented in the inventory table. Columns: film_id, title, description, release_year, language_id, original_language_id, rental_duration, rental_rate, length.\nThe table film_actor is used to support a many-to-many relationship between films and actors. Columns: film_id, actor_id.\nThe table film_category is used to support a many-to-many relationship between films and categories. Columns: film_id, category_id.\nThe table inventory contains one row for each copy of a given film in a given store. Columns: inventory_id, film_id, store_id.\nThe table language contains possible languages that films can have for their language and original language values. Columns: language_id, name.\nThe table payment records each payment made by a customer, with information such as the amount and the rental being paid for. Columns: payment_id, customer_id, staff_id, rental_id, amount, payment_date.\nThe table rental contains one row for each rental of each inventory item with information about who rented what item, when it was rented, and when it was returned. Columns: rental_id, rental_date, inventory_id, customer_id, return_date, staff_id.\nThe table staff lists all staff members. Columns: staff_id, first_name, last_name, address_id, picture, email, store_id, active, username, password.\nThe table store lists all stores in the system. Columns: store_id, manager_staff_id, address_id.\n2 Data types in MongDB A MongoDB document is represented as a JSON record. However, internally MongoDB serializes the JSON record into a BSON record. In practice, a BSON record is a binary representation of a JSON record.\nExercise\nExercise 2.1 Looking at the specification of BSON, can you tell how many bytes do you need to represent: an integer, a date, a string and a boolean?\nExercise\nExercise 2.2 The size of a document in MongoDB is limited to 16 MiB. Can you tell why there is such a limit?\nThe table rental has four integer columns (rental_id, inventory_id, customer_id, staff_id) and 2 dates (rental_date, return_date).\nThe table customer has three integer columns (customer_id, store_id, address_id), three strings (first_name, last_name and email), one boolean value (active) and one date (create_date).\nExercise\nExercise 2.3 Suppose that we want to create a MongoDB collection to list all rentals, and a separate collection to list all customers.\nEstimate the size of a document in both collections.\nWe make the following assumptions:\nOn average, each character needs 1.5 bytes.\nAn email address is 20 characters long on average.\nA last name is 8 characters long on average.\nA first name is 6 characters long on average. 3 Considerations for the new model Denormalization in MongoDB is strongly encouraged to read and write a record relative to an entity in one single operation.\nIn the following exercises, we explore different options and analyze advantages and disadvantages.\nExercise\nExercise 3.1 Suppose that we create a collection Customer, where each document includes information about a customer.\nSuppose that we embed in each document the list of rentals for a customer.\nHow many rentals can we store for a given customer, knowing that the size of a document in MongoDB cannot exceed 16 MiB?\nExercise\nExercise 3.2 Consider the two following options:\nA collection customer, where each document contains the information about a customer and an embedded list with the information on all the rentals made by the customer.\nA collection rental, where each document contains the information about a rental and an embedded document with the information on the customer that made the rental.\nCompute the size in byte of a document in the two collections.\nExercise\nExercise 3.3 Suppose that we have in our database\n512 customers.\n16384 rentals.\nOn average, each customer has around 32 rentals.\nCompute the size in byte of the collections customer and rental described in the previous question.\nExercise\nExercise 3.4 Based on the answers in the previous questions, discuss advantages and disadvantages of the two options: having a collection customer (solution 1) or a collection rental (solution 2).\nExercise\nExercise 3.5 Look again at the model in Figure 1.1. A rental document doesn’t only need to include information on the customer who made the rental, but also:\nThe staff member who took care of the rental.\nThe inventory item being rented.\nThe payment information.\nQuestions.\nDiscuss the different ways we can include this information in the collections customer and rental.\nBased on the discussion, which solution would you retain? A collection customer or a collection rental?\n4 The data model in MongoDB In the last question, we chose the collection that we intend to create.\nExercise\nExercise 4.1 Give the complete schema (name and type of the properties) of a document in the collection that you chose in the previous question.\nIf the value of any property is an embedded document:\nSpecify the schema of that document too.\nIf any property of an embedded document is an identifier referencing another entity, use that identifier (don’t try, for now, to further denormalize the schema).\nLet’s take a closer look at the storage requirements of the adopted solution. Consider that:\nThe size in bytes of a document storing the information of a staff member is around 64 KiB (65,536 bytes), because we store a profile picture.\nThe size in bytes of a document storing the information of an inventory item is 12 bytes.\nThe size in bytes of a document storing the information about a payment is 20 bytes.\nExercise\nExercise 4.2 If we denote by \\(N_{rental}\\) the number of rentals, what is the size in bytes of the database for the adopted solution? What do you get if we set \\(N_{rental}\\) to \\(10^4\\), \\(10^5\\) or \\(10^6\\) ?\nAlthough the size that we determined in the previous exercise, may not sound impressive, we still have to store other information (films, actors….). If we could save a bit of space, we would be happy.\nExercise\nExercise 4.3 Discuss how you could save some space in the adopted solution.\nHINT. Do you really need to denormalize all data?\nExercise\nExercise 4.4 Propose a solution for all the entities involved and estimate the savings in terms of storage requirements.\n5 The new model In this section we intend to obtain a complete model of the Sakila database.\nExercise\nExercise 5.1 Consider the model that we obtained at the end of the previous section. Which data can you further denormalize?\nExercise\nExercise 5.2 Complete the diagram obtained in the previous exercise so as to obtain a full data model for the Sakila database.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bbbd127d5a872ded989651aa9b98e2f9","permalink":"/courses/bdia/tutorials/mongodb-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/mongodb-modeling/","section":"courses","summary":"Data modeling in MongoDB","tags":null,"title":"MongoDB - Data modeling","type":"docs"},{"authors":null,"categories":null,"content":" We learn how to write queries in MongoDB on the Sakila database.\n1 Initialization Launch a MongoDB server on your computer.\nOpen MongoDB Compass.\nConnect to the MongoDB server with the following URI: mongodb://localhost:27017.\nDownload the this archive file and extract it. Each file corresponds to a collection.\nIn MongoDB Compass create a new database named sakila.\nCreate the collections customer, film, rental, staff and store.\nImport the data from the downloaded JSON files.\n2 Basic queries We might want to see a list of common operators in this page.\nExercise\nExercise 2.1 Write the following queries in MongoDB:\nReturn all the information on all customers.\nReturn the email of all customers.\nReturn the email of the customers of Canadian stores.\nReturn the identifier of all rentals made by customers from Iran, where the amount paid is strictly greater than 10 dollars.\nReturn the first and last names of the actors who played a role in film 213.\n3 Operations on arrays Useful array operators are listed here.\nExercise\nExercise 3.1 Write the following queries in MongoDB:\nReturn the identifier of the films that have “Behind the Scenes” as special features.\nReturn the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.\nReturn the identifier of all the films where BURT POSEY played a role.\nReturn the identifier of the film that has exactly 15 actors.\n4 Aggregation framework A useful reference for the aggregation pipeline can be found here here.\nExercise\nExercise 4.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)\nCount the total amount of payments across all rentals.\nReturn the number of actors of each film.\nSort the films by the number of actors (decreasing order).\nReturn the average number of actors for each film.\nReturn the identifier of the customer who made the most of rentals.\nReturn the first and last name of the customer who made the most of rentals.\nReturn the country where the customers have rented the most of the films in the category “Music”.\n5 Join Operations The join operation is explained here.\nExercise\nExercise 5.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the language of the film with title “ACE GOLDFINGER”.\nReturn all the information about the staff member who took care of rental 2.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a796fd0d8feb46ee4f88d3c2cff13529","permalink":"/courses/bdia/tutorials/mongodb-queries/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/mongodb-queries/","section":"courses","summary":"Queries in MongoDB","tags":null,"title":"MongoDB - Queries","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn to write basic and advanced queries in MongoDB.\n1 Get the data Download the this archive file and extract it. You’ll find a file for each collection to import into the database.\n2 Setup Download MongoDB Compass and follow the instructions that you find in this video to:\ninstall MongoDB Compass;\nstart a MongoDB cluster on Atlas.\nconnect to the MongoDB cluster.\ncreate a database and import the data.\n3 Basic queries Exercise\nExercise 3.1 Write the following queries in MongoDB:\nReturn all the information on all customers.\nReturn the email of all customers.\nReturn the email of the customers of Canadian stores.\nReturn the identifier of all rentals made by customers from Iran, where the amount paid is strictly greater than 10 dollars.\nReturn the first and last names of the actors who played a role in film 213.\nSolution\ndb.customer.find() db.customer.find({}, {email:1}) db.customer.find({\"store.country\": \"Canada\"}, {email:1}); db.rental.find({\"customer.country\": \"Iran\", amount: {$gt: 10}}, {rental_id: 1, _id:0}); db.film.find({film_id: 213}, {\"actors.first_name\":1, \"actors.last_name\": 1}).sort({\"actors.last_name\": -1}); 4 Operations on arrays Exercise\nExercise 4.1 Write the following queries in MongoDB:\nReturn the identifier of the films that have “Behind the Scenes” as special features.\nReturn the identifier of the films that have as special features all of the following: “Commentaries” and “Deleted Scenes”.\nReturn the identifier of all the films where BURT POSEY played a role.\nReturn the identifier of the film that has exactly 15 actors.\nSolution\ndb.film.find({special_features : {$elemMatch: {$eq: \"Behind the Scenes\"}}}, {film_id: 1, _id:0}); db.film.find({special_features : {$all: [\"Commentaries\", \"Deleted Scenes\"]}}, {film_id: 1, _id:0}); db.film.find({\"actors.first_name\": \"BURT\", \"actors.last_name\": \"POSEY\"}, {film_id: 1, _id:0}); db.film.find({actors: {$size : 15}}, {film_id: 1, _id:0}); 5 Aggregation framework Exercise\nExercise 5.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the title of the films rented by TOMMY COLLAZO (can you also express this query with the function find()?)\nCount the total amount of payments across all rentals.\nReturn the number of actors of each film.\nSort the films by the number of actors (decreasing order).\nReturn the average number of actors for each film.\nReturn the first and last name of the customer who made the most of rentals.\nReturn the country where the customers have rented the most of the films in the category “Music”.\nSolution\ndb.rental.aggregate({$match: {\"customer.first_name\": \"TOMMY\", \"customer.last_name\": \"COLLAZO\"}}, {$project: {\"inventory.film.title\": 1, _id:0}}) One can also express this query with the function find() db.rental.find({\"customer.first_name\": \"TOMMY\", \"customer.last_name\": \"COLLAZO\"}, {\"inventory.film.title\": 1, _id:0}); db.rental.aggregate({$group: {\"_id\": null, total_amount: {$sum: \"$amount\"}}}) db.film.aggregate({$project: {nb_actors: {$size: \"$actors\"}}}) db.films.aggregate({$match: {actors: {$ne: null}}}, {$project: {title: 1, nb_actors: {$size: \"$actors\"}}}, {$sort: {nb_actors:-1}}) db.film.aggregate({$match: {actors: {$elemMatch: {$exists: true}}}}, {$project: {film_id: 1, \"nb_actors\": {$size: \"$actors\"}}}, {$group: {_id: null, avg_actors: {$avg: \"$nb_actors\"}}}) db.rentals.aggregate( {$project: {\"customer.first_name\": 1, \"customer.last_name\": 1}}, {$group: {_id: \"$customer\", nb_rentals:{$sum: 1}}}, {$sort: {nb_rentals:-1}}, {$limit: 1}) db.rental.aggregate( {$match: {\"inventory.film.categories.category\": \"Music\"}}, {$group: {_id: \"$customer.country\", count: {$sum: 1}}}, {$sort:{count: -1}}, {$limit: 1}) 6 Join Operations Exercise\nExercise 6.1 Write the following queries in MongoDB using the aggregation framework:\nReturn the language of the film with title “ACE GOLDFINGER”.\nReturn all the information about the staff member who took care of rental 2.\nSolution\ndb.rental.aggregate({$match: {\"inventory.film.title\": \"ACE GOLDFINGER\"}}, {$lookup: {from: \"film\", localField: \"inventory.film.film_id\", foreignField:\"film_id\", as:\"film\"}}, {$project: {\"film.language\": 1}}, {$limit : 1}) db.rental.aggregate({$match: {rental_id: 2}}, {$lookup: {from: \"staff\", localField: \"staff.staff_id\", foreignField:\"staff_id\", as:\"staff_member\"}}, {$project: {\"staff_member\": 1}}) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a1053a1faac3489255ec57462bccfad","permalink":"/courses/bdalbert/tutorials/mongodb-queries/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdalbert/tutorials/mongodb-queries/","section":"courses","summary":"Queries in MongoDB","tags":null,"title":"MongoDB queries","type":"docs"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b24cdc6cd229bdb2a25dcce9b304ddaf","permalink":"/courses/bdia_old/tutorials/neo4j-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/neo4j-tutorial/","section":"courses","summary":"Description of the Neo4j tutorial.","tags":null,"title":"Neo4j","type":"docs"},{"authors":null,"categories":null,"content":" Ce TP a pour objectif de vous initier à la manipulation et à l’administration d’équipements réseau courants et d’illustrer par la pratique certains concepts abordés en cours concernant les protocoles de la pile TCP/IP.\nInformatios importantes\nCe TP sera évalué.\nRemise du devoir : il faut remettre un seul fichier .zip comprenant :\nUn rapport en PDF, complet d’une page de garde avec vos noms et prénoms et la date. Ecrivez une réponse pour chaque question. Ne vous limitez pas à des phrases courtes et ambiguës. La qualité et l’exhaustivité de vos réponses seront prises en compte dans la note finale.\nUn fichier basic-config.pkt contenant le réseau créé dans Cisco Packet Tracer pour répondre aux questions 2.1 et 2.2.\nUn fichier fullnet.pkt contenant le réseau créé dans Cisco Packet Tracer pour répondre à toutes les autres questions.\n1 Environnement de travail Pour ce TP nous allons utiliser Cisco Packet Tracer. Le logiciel permet de configurer et simuler un réseau. Il utilise des composants de la marque Cisco, un important fabricant sur le marché des équipements réseau professionnels. D’autre importants fabriquants sont : Huawei, HPE, Nokia, Netgear, Dell, Juniper, Aerohive.\n2 Première configuration Dans cette section, nous allons configurer un simple réseau point à point entre deux PC à l’aide d’un câble Ethernet.\nDémarrez avec un document Packet Tracer vierge ;\nAjoutez deux PCs ;\nReliez les deux PC par un câble ; vous pouvez utiliser le bouton vous permettant de choisir automatiquement le cable (bouton avec un éclair orange come icône).\nExercise\nExercise 2.1 Quelles sont les informations nécessaires pour configurer correctement un PC sur ce réseau?\nPrécisez les valeurs numériques de ces informations que vous aurez choisies pour vos deux PCs.\nUne fois que vous aurez configurez vos deux PCs, il faudra penser à vérifier que ces derniers peuvent communiquer correctement. Nous allons utiliser la commande ping.\nActivity\nCliquez sur l’un des PC, sélectionnez l’onglet Bureau et ouvrez une invite de commande.\nTapez la commande ping avec le paramètre approprié pour envoyer un message à l’autre PC.\nAssurez-vous d’obtenir une réponse de l’autre PC.\nCisco Packet Tracer fournit aussi un outil de simulation.\nActivity\nPassez en mode simulation (bouton en bas à droite de l’interface de Packet Tracer) ;\nDepuis l’invite de commande d’un PC, faites un ping vers l’autre PC ;\nDans le panneau de simulation à droite, cliquez sur le bouton d’avancement pour dérouler l’animation. Attendez quelques secondes (jusqu’à ce que vous ne voyiez plus de paquets de type ICMP apparaître), puis cliquez de nouveau sur le bouton pour arrêter la simulation. Vous pouvez maintenant examiner chacun des évènements de la liste en cliquant dessus.\nExercise\nExercise 2.2 Quel est le protocole utilisé par Ping ? Quelle est la signification de son acronyme ?\nSélectionnez l’un des messages échangés par les deux PC dans la fenêtre de simulation.\nD’après les informations qui s’affichent, pouvez-vous dire à quelle couche du modèle TCP/IP appartient ce protocole ?\nPouvez-vous expliquer les informations contenues dans un message de ce protocole ?\nN’oubliez pas de sauvegarder l’espace de travail dans un fichier nommé basic-config.pkt avant de continuer.\n3 Configuration d’un réseau local Le réseau point à point que nous avons configuré dans la section précédente n’est pas couramment utilisé.\nDans cette section, nous allons configurer un exemple réaliste de réseau local.\nActivity\nDémarrez avec un document Packet Tracer vierge ;\nAjoutez un commutateur, par exemple un 2960 (nous le nommerons C1).\nAjoutez quatre PC dans votre réseau (nous les nommerons PC1, PC2, PC3, PC4) ;\nReliez ces PC au commutateur.\nExercise\nExercise 3.1 Quelles sont les informations nécessaires pour configurer correctement un PC sur ce réseau?\nPour chaque PC, donnez les valeurs numériques de ces informations. Maintenant que vous avez configuré le réseau, vous devez tester que les PCs peuvent communiquer.\nActivity\nPassez en mode simulation (bouton en bas à droite de l’interface de Packet Tracer) ;\nDans le panneau de simulation à droite, cliquez sur le bouton d’avancement pour dérouler l’animation.\nDepuis l’invite de commande d’un PC, faites un ping vers un autre PC ;\nDéroulez la simulation jusqu’à ce que vous verrez sur l’invite de commande Reply from…. Seulement à ce moment, mettez en pause la simulation.\nExercise\nExercise 3.2 Quels sont les protocoles qui interviennent dans ces échanges ?\nEn vous aidant de ressources en ligne, pourriez-vous décrire en quelque mots chaque protocole ?\nPourriez-vous expliquer les interactions ARP ? 4 Interconnection de deux réseaux Nous allons maintenant créer un deuxième réseau.\nActivité\nChoisissez une plage d’adresses pour un deuxième réseau IPv4 distinct du premier ;\nCréez un deuxième réseau comprenant au moins deux ordinateurs. N’essayez pas pour le moment de connecter les deux réseaux.\nVérifiez que les ordinateurs du deuxième réseau puissent communiquer entre eux.\nMaintenant il faut connecter les deux réseaux à l’aide d’un routeur.\nActivité\nAjoutez un routeur 1941 (que nous nommerons R1).\nLe routeur a deux interfaces réseaux, nommées GigabitEthernet0/0 et GigabitEthernet0/1. Le routeur permet ainsi l’interconnexion entre deux réseaux.\nAjoutez les cables nécessaires pour connecter le premier réseau au deuxième à l’aide du routeur.\nSi vous avez bien fait le cablage, vous devriez remarquer des triangles rouges sur les cables reliant le routeur aux deux réseaux. Cela indique un problème de connectivité que nous allons résoudre dans l’activité suivante.\nBon à savoir\nSi vous passez la souris sur un cable, une info-bulle s’affiche indiquant les noms des interfaces réseaux reliées par le cable. Cela va vous être utile dans les activités suivantes.\nActivité\nIl faudra attribuer une adresse IPv4 à chacune des interfaces du routeur. Attention, l’adresse IP d’une interface doit être cohérente avec les adresses du réseau auquel l’interface est connectée.\nPour ce faire, cliquez sur le routeur et sélectionnez Config. Vous verrez un onglet pour chaque interface dans le menu à gauche.\nAttribuez une adresse IPv4 à chaque interface.\nN’oubliez pas de cocher la case On à côté de Port status. Cela sert à activer l’interface.\nSi votre configuration est correcte, les triangles rouges devraient changer de couleur et s’afficher en vert. La transition entre les deux couleurs pourrait prendre un certain délai, pendant lequel un cercle orange s’affiche. Cela est normal, il faut juste patienter quelques secondes.\nBon à savoir\nPensez à sauvegarder votre travail !\nExercise\nExercise 4.1 Passez en mode realtime (bouton en bas à droite dans l’interface de Packet Tracer).\nOuvrez l’invite de commande sur un PC du deuxième réseau et essayez de faire un ping vers l’un des ordinateurs du premier réseau.\nQue se passe-t-il ? Pourriez-vous expliquer ce qui manque dans la configuration des PC pour permettre une communication entre les PC des deux réseaux ?\nAjoutez les informations de configuration nécessaires aux PC. Maintenant, on procède à une vérification finale.\nActivité\nVérifiez que vous pouvez toujours faire un ping entre deux PC d’un même réseau (par exemple PC1 et PC2). Si ce n’est pas le cas, cela signifie que votre configuration a regressé par rapport aux exercices précédents.\nVérifiez que vous pouvez faire un ping entre tout PC d’un réseau et tout PC du deuxième réseau.\nPensez également à faire un ping d’un PC vers les deux interfaces du routeur.\nSi à l’issu de l’activité précédente il n’y a pas eu de problèmes de communications, cela veut dire que vous avez correctement configuré votre réseau local !\nIl ne nous reste que comprendre un peu mieux ce qui se passe lorsqu’un PC d’un réseau fait un ping vers un PC de l’autre réseau, ce qui est l’objectif de l’activité suivante.\nActivité\nPassez en mode simulation.\nFaites un ping d’un PC vers un PC de l’autre réseau.\nLaissez dérouler la simulation jusqu’à ce que le ping reply ne revienne à la source. A ce moment, mettez la simulation en pause.\nMaintenant, répondez aux questions suivantes :\nExercise\nExercise 4.2 Dans le panneau de simulation, considérez les messages ICMP (relatifs au ping).\nQuel est le chemin suivi par ces messages ?\nPour chacun de ces messages: quelle est l’adresse MAC source ? À quel dispositif correspond cette adresse ?\nQuelle est l’adresse MAC destination ? À quel dispositif correspond cette adresse ?\nQue pouvez-vous dire à propos des adresses IP source et destination de chacun de ces messages ?\nSur la base des observations faites aux questions précédentes, est-ce qu’un commutateur change les adresses IP/MAC source/destination des messages ?\nSur la base des observations faites aux questions précédentes, est-ce qu’un routeur change les adresses IP/MAC source/destination des messages ? 5 Configuration d’un serveur DHCP Dans les activités précédentes, nous avons manuellement configuré les adresses IP des PC. Dans un réseau domestique ou d’entreprise, cela obligerait toute personne souhaitant se connecter au réseau de rentrer cette configuration à la main.\nDans la pratique, un serveur DHCP (Dynamic Host Configuration Protocol) nous permet de configurer automatiquement les PC.\nNous allons dans un premier temps ajouter un serveur DHCP au premier réseau.\nUn serveur DHCP doit être configuré en lui donnant :\nUne adresse IP statique.\nL’adresse IP de la passerelle.\nUne plage d’adresses IPv4 que le serveur DHCP utilisera pour attribuer automatiquement une adresse IP à tout dispositif se connectant à son réseau.\nExercise\nExercise 5.1 Etant donné que le serveur DHCP sera connecté au premier réseau, répondez aux questions suivantes :\nQuelle adresse IP allez-vous attribuer au serveur DHCP ? Donnez la valeur numérique de cette adresse.\nQuelle est l’adresse IP de la passerelle par défaut du serveur ? Donnez la valeur numérique de cette adresse.\nDonnez les valeurs numérique de la plage d’adresses IPv4 que le serveur utilisera pour attribuer des adresses IP à des PC. Maintenant que la configuration du serveur est faite su papier, nous allons l’implémenter dans l’outil de simulation.\nActivité\nAjoutez au premier réseau un terminal de type Server, que nous nommerons S.\nCliquez sur le serveur et ouvrez l’onglet Config.\nChangez le nom du serveur en S.\nAjoutez les informations concernant le Default gateway et donnez une adresse IP au serveur, selon les réponse que vous avez données à l’exercice précédent.\nOuvrez l’onglet Services et choisissez DHCP.\nConfigurez un pool qui correspond à la plage d’adresses que vous avez précisée à l’exercice précédent (changez le nom Pool name, par ex. en network1). Précisez l’adresse du Default gateway, ignorez le DNS server, précisez la première adresse IP de la plage (Start IP address) et un masque de sous-réseau. Ignorez tous les autres champs.\nImportant. Cliquez sur Save et pensez à activer le service DHCP en cliquant sur On.\nEnfin, connectez le serveur DHCP au premier réseau à l’aide d’un cable. Attendez quelques secondes pour que les triangles vers s’affichent sur ce cable.\nMaintenant que le serveur DHCP est configuré, il faut l’utiliser !\nActivité\nSélectionnez un PC du premier réseau, ouvrez l’onglet Desktop et ensuite un invite de commandes.\nVérifiez que vous arrivez à faire un ping vers le serveur DHCP \\(S\\). Si cela n’est pas le cas, revenez à l’activité précédente et corrigez les erreurs de configuration.\nSi vous avez pu faire un ping vers le serveur DCHP, ouvrez l’onglet Config sur le PC.\nImportant. Pensez à noter l’adresse IP actuelle du PC.\nDans la section Gateway/DNS IPv4 choisissez DHCP au lieu de Static.\nOuvrez à nouveau l’onglet Desktop et ouvrez un invite de commandes.\nSaisissez la commande ipconfig et vérifiez que maintenant votre PC a une nouvelle adresse IP et que la passerelle par défaut est correctement configurée.\nFaites un ping vers un autre PC du premier réseau et vers un autre PC du deuxième réseau pour vous assurer que le PC qui a obtenu l’adresse IP du serveur DHCP puisse encore communiquer avec les autres.\nBon à savoir\nPensez à sauvegarder votre travail !\n5.1 Etude d’un échange DHCP Nous allons maintenant étudier plus en profondeur les échanges entre un PC et un serveur DHCP pour bien apprendre comment le protocole DHCP marche.\nN’hésitez pas à revoir les transparents du cours pour avoir une idée générale des messages que vous devrez vous attendre à trouver.\nActivité\nPassez en mode Simulation et démarrez la simulation.\nSelectionnez un autre PC du premier réseau (pas celui que vous avez configuré dans l’activité précédente )\nActivez le DHCP sur ce PC.\nLaissez dérouler la simulation. Vous la metterez en pause lorsque vous ne verrez plus d’échanges DHCP/ARP.\nExercise\nRegardez le premier message DHCP dans le panneau de simulation.\nExercise 5.2 Quelle est l’adresse MAC source ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse MAC destination ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse IP source ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse IP destination ? A quel dispositif cette adresse correspond-t-elle ?\nRegardez les destinataires des messages DHCP qui suivent immediatement le premier, est-ce que cela est cohérent avec les adresses MAC et IP destination que vous avez notées ?\nQuel est le protocole de la couche transport utilisé dans ces messages DHCP ? Quels sont les ports source et destination ? Maintenant nous allons nous concentrer sur la réponse du serveur.\nExercise\nExercise 5.3 Identifiez le premier message de réponse du DHCP dans le panneau de simulation.\nIgnorez pour le moment les échanges ICMP et ARP.\nQuelle est l’adresse MAC source ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse MAC destination ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse IP source ? A quel dispositif cette adresse correspond-t-elle ?\nQuelle est l’adresse IP destination ? A quel dispositif cette adresse correspond-t-elle ? Ensuite on va voir ce qui se passe après la réponse du serveur.\nExercise\nExercise 5.4 Quel dispositif réagit à la réponse du serveur ?\nQuelle est l’adresse de destination de cette réaction ?\nEst-ce que le serveur répond à cette réaction ?\nVérifiez que l’échange des messages que vous avez identifiés correspond bien aux messages décrits dans les supports de cours. Maintenant on va se préoccuper des messages ICMP.\nExercise\nExercise 5.5 A quoi sert selon vous le message ICMP (ping) envoyé par le serveur ? Analysez le message pour essayer de comprendre.\nAvant de continuer, pensez à terminer l’activité suivante.\nActivité\nPassez en mode Realtime.\nActivez le DHCP sur tous les ordinateurs du premier réseau. Assurez-vous que chaque ordinateur a ainsi une adresse IP valide.\n5.2 Serveur DHCP pour le deuxième réseau Maintenant on va ajouter la fonctionnalité DHCP sur le deuxième réseau. Une solution possible consisterait à ajouter un deuxième serveur DHCP au deuxième réseau et à le configurer avec une plage d’addresses pour le deuxième réseau.\nCette solution étant inefficace (on ne va pas prévoir un serveur DHCP pour chaque réseau), on préfère ajouter une deuxième plage d’addresses sur notre serveur \\(S\\).\nActivité\nAjoutez au serveur \\(S\\) une deuxième plage d’addresses pour le deuxième réseau.\nOn essaie maintenant d’activer le DHCP sur l’un des ordinateurs du deuxième réseau.\nExercise\nExercise 5.6 Passez en mode Simulation et demarrez la simulation.\nChoisissez l’un des ordinateurs du deuxième réseau et activez le DCHP sur celui-ci.\nObservez le chemin pris par les messages DHCP.\nEst-ce que les messages DHCP arrivent au serveur \\(S\\)? Pourquoi ?\nOn vérifie la configuration réseau de l’ordinateur choisi.\nExercise\nExercise 5.7 Cliquez sur l’ordinateur choisi et ouvrez l’onglet Desktop.\nOuvrez un invite de commandes et tapez la commande ipconfig.\nQuelle adresse IP cet ordinateur a-t-il reçu ? Pourriez-vous expliquer en quelques mots ?\nEst-ce que cet ordinateur a reçu l’adresse IP de la passerelle ? Cela vous surprend ?\nPour résoudre les problèmes rencontrés dans les activités précédentes, on procède de la manière suivante :\nActivité\nCliquez sur le routeur \\(R1\\) et ouvrez l’onglet CLI. Cela vous permet d’accèder au terminal (CLI - Command Line Interface) de configuration du routeur.\nLe terminal devrait afficher un invite Router\u0026gt;. Si cela n’est pas le cas, appuyez sur la touche Entrée de votre clavier.\nIl faut maintenant saisir des commandes pour passer en mode configuration du routeur. Les commandes à saisir sont, dans l’ordre (après chaque commande, appuyez sur la touche Entrée):\nenable\nconfigure terminal\nLa commande suivante sert à passer en mode configuration de l’interface du routeur reliée au deuxième réseau. Important. Dans la commande suivante il faut remplacer XXX par le nom de l’interface reliée au premier routeur. Vous pouvez retrouver le nom de cette interface en passant la souris sur le cable reliant le routeur au deuxième réseau. interface XXX\nEnsuite, sasissez la commande suivante en remplaçant IPDHCP par l’adresse IP de votre serveur DHCP. On vous demandera plus tard à quoi cette comment sert. ip helper-address IPDHCP\nPour sortir de la configuration saisissez la commande suivante : end\nPassez en mode Simulation.\nCliquez sur l’ordinateur du deuxième réseau que vous aviez choisi avant, et ouvrez l’invite des commandes. Cochez la case Top que vous trouvez en bas à gauche de l’invite.\nDécalez la fenêtre de l’invite de manière à pouvoir avoir une vision complète des deux réseaux.\nSaissez la commande ipconfig /renew dans l’invite des commandes.\nDémarrez la simulation et observez attentivement le chemin des messages DHCP.\nQuand la commande ipconfig /renew se termine, elle affichera dans l’invite de commande la configuration réseau obtenue du serveur DHCP. Si ce n’est pas le cas, il faut revoir la configuration de votre serveur DHCP et de votre routeur.\nSi la commande ipconfig /renew se termine et que votre ordinateur a obtenu une configuration correcte, vous pouvez mettre en pause la simulation. Ne passez pas en mode Realtime et ne fermez pas l’invite de commandes, car sinon le panneau de simulation sera reinisialisé et vous perdrez tous les messages.\nLe moment est venu de mieux comprendre ce qui s’est passé.\nExercise\nExercise 5.8 Sur la base de ce que vous avez observé, quelle est utilité de la commande ip helper-address que vous avez sasie dans l’activité précédente ?\nPourriez-vous identifier le premier message DHCP (un DHCP discover) reçu par \\(S\\) ? Est-ce que le routeur \\(R1\\) l’a envoyé en broadcast ou bien il a été envoyé directement au serveur DHCP ?\nLe service DHCP en exécution sur \\(S\\) (c’est à dire le logiciel qui est en charge de réaliser le service DHCP) reçoit le message DHCP discover dépourvu des entêtes de la couche 2 et 3. Comment ce logiciel arrive-t-il a déterminer qu’il a reçu ce message du routeur \\(R1\\) ? Coup de pouce. Regardez le contenu du message à la couche DHCP (application).\nMaintenant il faut bien activer le DHCP sur tous les ordinateurs du deuxième réseau.\nActivité\nPassez en mode Realtime.\nActivez le DHCP sur tous les ordinateurs du deuxième réseau. Assurez-vous que chaque ordinateur a ainsi une adresse IP valide.\nBon à savoir\nPensez à sauvegarder votre travail !\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a5428909c55835d44f5f120040ddfff5","permalink":"/courses/network/labs/netbasics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/network/labs/netbasics/","section":"courses","summary":"tutorial-netbasics","tags":null,"title":"Simulation d'un réseau informatique","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nHow to obtain a non-redundant set of functional dependencies. How to determine the candidate keys of a table given its functional dependencies. How to determine the normal form of a table. 1 Question 1 We consider a relational table \\(R\\) with four columns \\(A\\), \\(B\\), \\(C\\) and \\(D\\). Let \\(\\mathcal{F}\\) be the following set of functional dependencies:\n\\(AB \\rightarrow C\\)\n\\(D \\rightarrow BC\\)\n\\(A \\rightarrow B\\)\nExercise\nExercise 1.1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\).\n2 Question 2 Let \\(R\\) be a relational table with five columns \\((A, B, C, D, E)\\). The following set \\(\\mathcal{F}\\) of functional dependencies hold:\n\\(AB \\rightarrow C\\) \\(C \\rightarrow A\\) \\(C \\rightarrow B\\) \\(C \\rightarrow D\\) \\(D \\rightarrow E\\) Exercise\nExercise 2.1 Specify the candidate keys of the table \\(R\\).\nExercise\nExercise 2.2 We assume that \\(R\\) is in 1NF.\nIs table \\(R\\) in 2NF? Justify your answer.\nIs table \\(R\\) in 3NF? Justify your answer.\nIs table \\(R\\) in BCNF? Justify your answer. Exercise\nExercise 2.3 If the table \\(R\\) from the previous exercise is not in BCNF, how would you change the schema so that BCNF is satisfied? For each table, specify the primary key and the foreign keys linking the tables to each other.\n3 Question 3 Let \\(R\\) be a relational table with four columns \\((A, B, C, D)\\). \\(\\{A, B\\}\\) is the only candidate key.\nExercise\nExercise 3.1 Identify a minimal set of functional dependencies. Justify your answer.\nExercise\nExercise 3.2 Add \\(B \\rightarrow D\\) to the set of functional dependencies that you identified in the previous exercise. Modify the minimal set of functional dependencies accordingly. Justify your answer.\nExercise\nExercise 3.3 We assume that \\(R\\) is in 1NF.\nIs table \\(R\\) in 2NF? Justify your answer.\nIs table \\(R\\) in 3NF? Justify your answer.\nIs table \\(R\\) in BCNF? Justify your answer. Exercise\nExercise 3.4 Normalize \\(R\\) to the BCNF form. Justify your choices.\n4 Question 4 We consider the following table:\nPatient (ssn, first_name, last_name, phone_number, insurance_number, insurance_expiration_date) where the following set \\(\\mathcal{F}\\) of functional dependencies holds:\nssn -\u0026gt; first_name, last_name, phone_number, insurance_number, insurance_expiration_date insurance_number -\u0026gt; insurance_expiration_date Exercise\nExercise 4.1 Derive a minimal set \\(\\mathcal{G}\\) of functional dependencies that is equivalent to \\(\\mathcal{F}\\). Exercise\nExercise 4.2 Given \\(\\mathcal{G}\\), identify the candidate keys in the table Patient. Exercise\nExercise 4.3 Specify the normal form of the table Patient. Justify your answer. Exercise\nExercise 4.4 How would you normalize table Patient to BCNF? Justify your answer. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd4b11df4fa072aaae4db3251396b08d","permalink":"/courses/databases/tutorials/normalization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/normalization/","section":"courses","summary":"Description of the normalization tutorial.","tags":null,"title":"Normalization","type":"docs"},{"authors":null,"categories":null,"content":" L’objectif de ce TP est de configurer des tables de transfert.\nLe TP se compose de deux parties :\nPremière partie : configuration statique des tables de transfert.\nDeuxième partie : configuration dynamique des tables de transfert.\nInformations importantes\nCe TP sera évalué.\nRemise du devoir : il faut remettre un seul fichier .zip comprenant :\nUn rapport en PDF, complet d’une page de garde avec vos noms et prénoms et la date. Ecrivez une réponse pour chaque question. Ne vous limitez pas à des phrases courtes et ambiguës. La qualité et l’exhaustivité de vos réponses seront prises en compte dans la note finale.\nUn fichier static-routing.pkt contenant le réseau créé dans Cisco Packet Tracer avec la configuration statique des tables de transfert.\nUn fichier dynamic-routing.pkt contenant le réseau créé dans Cisco Packet Tracer avec la configuration dynamique des tables de transfert.\n1 Environnement de travail Pour ce TP nous allons utiliser Cisco Packet Tracer. Le logiciel permet de configurer et simuler un réseau. Il utilise des composants de la marque Cisco, un important fabricant sur le marché des équipements réseau professionnels. D’autre importants fabriquants sont : Huawei, HPE, Nokia, Netgear, Dell, Juniper, Aerohive.\nPour cette activité, le réseau vous sera fourni sous forme d’un fichier géneré à l’aide de Cisco Packet Tracer.\nTéléchargez le réseau à ce lien.\nFaites un double click sur le fichier téléchargé pour ouvrir le réseau avec Cisco Packet Tracer.\nBon à savoir\nCertains cables sont rouges. Mais cela n’aura aucun impact sur l’activité.\n2 Première partie : routage statique Dans un premier temps, nous allons configurer les tables de transfert des routers de manière statique, c’est à dire manuellement.\nObservez attentivement le réseau et répondez aux questions suivantes.\nExercise\nExercise 2.1 Combien de sous-réseaux a-t-on dans ce réseau ? Pour chaque sous-réseau, précisez les interfaces réseau qui en font partie.\nRestez en mode Realtime et répondez aux questions suivantes.\nExercise\nExercise 2.2 Essayez de faire un ping entre \\(A\\) et \\(B\\). Obtenez-vous une réponse de la part de \\(B\\) ? En regardant la configuration réseau des ordinateurs \\(A\\) et \\(B\\), êtes-vous surpris par ce que vous venez d’observer ?\nExercise\nExercise 2.3 Essayez de faire un ping entre \\(A\\) et \\(C\\). Obtenez-vous une réponse de la part de \\(C\\) ? En regardant la configuration réseau des ordinateurs \\(A\\) et \\(C\\), êtes-vous surpris par ce que vous venez d’observer ?\n2.1 Contenu de la tables de transfert On va maintenant regarder le contenu de la table de transfert du routeur R1.\nCliquez sur le routeur \\(R1\\), sélectionnez l’onglet CLI (IOS Command-Line Interface), tapez Entrée sur votre clavier et ensuite tapez la commande suivante :\nshow ip route\nExercise\nExercise 2.4 En vous aidant des informations que vous trouvez à cette page, expliquez le contenu de la table de transfert de \\(R1\\).\nTout en restant en mode Realtime, essayez de faire un ping entre \\(A\\) et 10.1.5.2. Il s’agit de l’adresse IP de l’interface Gig0/0/0 du routeur \\(R2\\).\nVous ne devriez obtenir aucune réponse. On va maintenant essayer de comprendre pourquoi.\nPour répondre aux questions suivantes, passez en mode Simulation.\nExercise\nExercise 2.5 Est-ce que le message Echo ping request arrive à destination ? Sur la base des informations présentes dans la table de trasfert de \\(R1\\), êtes-vous supris de ce que vous venez d’observer ?\nExercise\nExercise 2.6 Est-ce que le router \\(R2\\) arrive à trouver une route pour répondre à \\(A\\) ? Justifiez votre réponse en vous aidant des informations présentes dans la table de transfert de \\(R2\\).\n2.2 Remplissage des tables de transfert Maintenant nous allons remplir les tables de transfert pour que les ordinateurs et les routeurs de ce réseau puissent communiquer correctement.\nNous commençons par le routeur \\(R2\\).\nComment rajouter une route\nPour rajouter une route à une table de tranfert, il faut suivre la procédure suivante.\nCliquez sur le routeur et sélectionnez l’onglet CLI.\nAssurez-vous que l’invite affiche Router\u0026gt;.\nSaisissez la commande enable. L’invite devrait changer en Router#.\nSaisissez la commande config terminal. L’invite devrait changer en Router (config) #.\nPour ajouter une route, il faut saisir la commande suivante :\nip route PREFIX MASK NEXT-HOP\noù:\nPREFIX spécifie l’adresse IP du sous-réseau de destination.\nMASK spécifie le masque du sous-réseau de destination.\nNEXT-HOP spécifie l’adresse IP de l’interface réseau à laquelle un paquet doit être envoyé pour atteindre la destination.\nPour afficher la table de transfert, il faut saisir deux fois la commande exit (pour quitter le mode configuration du routeur) et taper une fois la touche Entrée pour que l’invite affiche à nouveau Router\u0026gt;. Seulement maintenant on peut saisir la commande que nous avons utilisée tout à l’heure :\nshow ip route Exercise\nExercise 2.7 Ajoutez une route à la table de transfert de \\(R2\\) de manière à ce que le routeur sache comment atteindre le sous-réseau de \\(A\\).\nAffichez le contenu de la table de transfert de \\(R2\\). Voyez-vous la nouvelle route que vous venez de rajouter ? Que signifie l’étiquette S qui s’affiche dans la première colonne à côté de cette route ?\nEssayez de faire un ping de \\(A\\) vers \\(R2\\). Maintenant \\(R2\\) devrait être capable de répondre à \\(A\\). Bon à savoir\nSi le premier ping échoue, ne désespérez pas !\nRéessayez la commande ping une deuxième fois. Cisco Packet Tracer ne gère pas bien les temps de réponse de la commande ping lorsque celle-ci déclenche des requêtes ARP pour obtenir les adresses MAC des appareils concernés.\nMaintenant que vous avez compris le mécanisme, vous devez compléter les tables de tranfert des autres routeurs.\nExercise\nExercise 2.8 Complétez les tables de transfert des autres routeurs afin que tous les appareils puissent communiquer correctement.\nVérifiez que vous pouvez faire un ping entre tous les ordinateurs du réseau.\nSauvegardez votre travail et fermez Cisco Packet Tracer. Renommez le fichier sur lequel vous avez travaillé en static-routing.pkt. 3 Routage dynamique Dans cette section nous allons étudier comment remplir les tables de transfert de manière dynamique, c’est à dire à l’aide d’un algorithme de routage.\nTéléchargez à nouveau le réseau d’origine à ce lien.\nRenommez le fichier en dynamic-routing.pkt.\nOuvrez le fichier avec Cisco Packet Tracer.\n3.1 Présentation du protocole RIP Dans cette section nous allons utiliser le protocole de routage dynamique RIP (Routing Information Protocol). Il s’agit d’un protocole de routage couramment utilisé dans les réseaux TCP/IP de petite et moyenne taille. Il s’agit d’un protocole stable qui utilise un algorithme de vecteur de distance pour calculer les routes.\nLe protocole RIP (Routing Information Protocol) envoie des paquets de données UDP en diffusion pour échanger des informations de routage.\n3.2 Configuration de RIP Pour que les routeurs s’échangent des routes via RIP, il faut activer RIP sur tous les routeurs.\nNous commençons en l’activant sur le routeur R1 à titre d’exemple.\nActivation de RIP\nCliquez sur le routeur et sélectionnez l’onglet CLI.\nSaisissez la commande enable.\nSaisissez la commande config terminal.\nSaisissez la commande router rip.\nMaintenant il faut saisir la commande network pour chaque sous-réseau qui est attaché au routeur. Dans le cas du routeur \\(R1\\) il faudra saisir :\nnetwork 192.168.1.0 network 192.168.2.0 network 10.1.0.0 Saisissez maintenant la commande exit. Si vous regardez le contenu de la table de transfert de \\(R2\\) (le voisin de \\(R1\\)), elle ne devrait pas être changée. En effet, il faut activer le RIP sur \\(R2\\) pour que \\(R2\\) reçoive les routes annoncées par \\(R1\\).\nExercise\nExercise 3.1 En utilisant les commandes détaillées ci-dessus, activez le RIP sur \\(R2\\). Attendez une trentaine de secondes et vérifiez que le contenu de la table de routage de \\(R2\\) contient bien les routes annoncées par \\(R1\\) (les routes vers 192.168.1.0 et 192.168.2.0).\nExercise\nExercise 3.2 Regardez plus attentivement les routes ajoutées dans la table de transfert de \\(R2\\).\nQue signifie l’étiquette R qui apparaît dans la première colonne ?\nQue signifie l’information 120/X qui apparaît dans la troisième colonne ?\nExercise\nExercise 3.3 Activez le RIP sur tous les routeurs.\nEcrivez dans votre rapport le contenu de la table de transfert de chaque routeur.\nVérifiez que chaque routeur connaisse les routes vers les différents sous-réseaux.\nAssurez-vous que tous les ordinateurs arrivent à communiquer à l’aide de la commande ping.\nPassez maintenant en mode Simulation.\nExercise\nExercise 3.4 Faites un ping de la machine \\(A\\) vers la machine \\(G\\). Laissez dérouler la simulation jusqu’à ce que vous ne voyez plus de messages ICMP passer.\nEst-ce que les messages ICMP suivent toujours le même chemin ?\nEst-ce que vous êtes surpris par ce que vous venez d’observer ?\nPassez un mode Realtime et supprimez le lien entre le routeur \\(R2\\) et le routeur \\(R4\\).\nExercise\nExercise 3.5 Comment les tables de tranfert de chaque routeur ont-elles changé ?\nEst-ce que les ordinateurs arrivent toujours à communiquer correctement ?\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e9cb525a5700f2b36d9e0a76cc64a33d","permalink":"/courses/network/labs/routing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/network/labs/routing/","section":"courses","summary":"tutorial-routage","tags":null,"title":"Routage IPv4","type":"docs"},{"authors":null,"categories":null,"content":" 1 Computing averages We consider a collection of CSV files containing temperature measurements in the following format:\nyear,month,day,hours,minutes,seconds,temperature\nyou can find the files under the directory hdfs://sar01:9000/data/temperatures/\nHere are the details for each file:\nFile temperatures_86400.csv contains one measurement per day in the years 1980 - 2018. File temperatures_2880.csv contains one measurement every 2880 seconds in the years 1980 - 2018. File temperatures_86.csv contains one measurement every 86 seconds for the year 1980 alone. File temperatures_10.csv contains one measurement every 10 seconds for the years 1980 - 2018. We intend to implement a Spark algorithm to generate pairs \\((y, t_{avg})\\), where \\(y\\) is the year and \\(t_{avg}\\) is the average temperature in the year.\n1.1 First implementation Copy the file /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py to your home directory by typing the following command:\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py ./avg_temperatures_slow.py\nOpen the file avg_temperatures_slow.py. The file contains the implementation of the function avg_temperature_slow that:\ntakes in an RDD, where each item is a line of the input text file;\nreturns an RDD, where each item is a key-value pair (year, avg_temp).\nIn the same file, locate the two variables input_path and output-path and write the following code:\ninput_path = \u0026quot;hdfs://sar01:9000/data/temperatures/\u0026quot; output_path = \u0026quot;hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/\u0026quot; Replace X with the number corresponding to your account! Don’t forget the / at the end of the file paths!\nExecute the following actions:\nRun the script avg_temperatures_slow.py by using temperatures_86400.csv as an input. To this extent, use the following command: spark-submit --master spark://sar01:7077 avg_temperatures_slow.py temperatures_86400.csv\nYou should find the output of the program under the following folder: hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/temperatures_86400.out\nType the following command to verify it :\nhdfs dfs -ls hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/temperatures_86400.out\nIf you want to read the result of the computation, you can execute the following command: hdfs dfs -cat hdfs://sar01:9000/bdiaspark23/bdiaspark23_X/temperatures_86400.out/*\nExercise\nExercise 1.1 In the output of Spark on the command line you should see a line that reads something similar to the following phrase:\nINFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.478220 s Note the execution time that you obtained.\nRun the same script by using temperatures_2880.csv as an input.\nWhat is the execution time? Does it seem reasonable compared with the execution time that you observed before? Justify your answer.\nExecute the same script by using temperatures_86.csv as an input.\nWhat is the execution time? How would you justify it, knowing that the files temperatures_2880.csv and temperatures_86.csv have a similar size (11 MB the former, 9 MB the latter)? 1.2 Second implementation We want to implement a better version of the program. You can draft your ideas on paper before you write any code.\nWhen you’re ready, create a copy of avg_temperatures_slow.py and rename it as avg_temperatures_fast.py, with the following command:\ncp ./avg_temperatures_slow.py ./avg_temperatures_fast.py\nExercise\nExercise 1.2 Open the file and implement the function avg_temperature_fast.\nNOTE. Remember to comment the call to avg_temperature_slow and to uncomment the call to avg_temperature_fast at the end of the file.\nExercise\nExercise 1.3 Run the script avg_temperatures_fast.py by using temperatures_86.csv as an input.\nWhat’s the execution time? Compare it with the execution time obtained in the previous exercise and comment the difference.\nRun the same script by using temperatures_10.csv (3 GB!) as an input. Do you think that the program takes too long? Why?\n2 Common friends in a social network Consider a social network described by a graph encoded in a text file. Each line of the file is a list of identifiers separated by commas. For instance, the line \\(A,B,C,D\\) means that \\(A\\) is friend with \\(B\\), \\(C\\) and \\(D\\). An excerpt of the file looks like as follows:\nB,A,D A,B,C,D D,A,B,C C,A,D ... We suppose that the friendship relation is symmetric: \\((A, B)\\) implies \\((B, A)\\).\nWe want to obtain the list of the common friends for each pair of individuals:\n(B, C), [A, D] (A, D), [B, C] (C, D), [A] (A, C), [D] (B, D), [A] (A, B), [D] As an additional constraint, we want to represent a couple only once and avoid to represent the symmetric couple. In other words, if we output \\((A, B)\\), we don’t want to output \\((B, A)\\).\nWe use the following input files available in folder hdfs://sar01:9000/data/social-network/:\nsn_tiny.csv. Small social network, that you can use to test your implementation.\nsn_10k_100k.csv. Social network with \\(10^4\\) individuals and \\(10^5\\) links.\nsn_100k_100k.csv. Social network with \\(10^5\\) individuals and \\(10^5\\) links.\nsn_1k_100k.csv. Social network with \\(10^3\\) individuals and \\(10^5\\) links.\nsn_1m_1m.csv. Social network with \\(10^6\\) individuals and \\(10^6\\) links.\n2.1 Implementation Get the code template with the following command:\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_common_friends.py .\nExercise\nExercise 2.1 Write an implementation that uses a groupByKey.\nWrite an implementation that uses a reduceByKey.\nTest both implementations on file sn_tiny.csv. 2.2 Tests and performance measures Exercise\nExercise 2.2 Run both implementations on the other files.\nFill in a table where you indicate: the name and size of each file and the measured running times of both implementations.\n2.3 Minimum, maximum and average degree Exercise\nExercise 2.3 Add a function to file template_common_friends.py that returns a tuple containing the minimum, the maximum and the average degree of a node in the social network. You must use RDDs to do so, don’t try to collect() the content of the RDDs and so compute the values on Python lists.\nExecute the function for all the given input files.\nComplete the table that you created in the previous exercise by adding the minimum, maximum and average number of friends. 2.4 Performance analysis Exercise\nExercise 2.4 We suppose that each node has a number of friends that is equal to the average number of friends. Compute (with pencil or paper, no need to write a code for that) the number of intermediate pairs \\(((A, B), X)\\) generated by your code.\nComplete the table by writing down the number of intermediate pairs for each file.\nPlot three graphs, where the y-axis has the program running times and the x-axis has: the number of intermediate pairs, the average degree and the file size respectively.\nWhich graphs best predict the evolution of the computational time? 3 Average and standard deviation We use the same files as in the first question. Our objective is to write a Spark program that produces triples \\((y, t_{\\mu}, t_{\\sigma})\\), where \\(y\\), \\(t_{\\mu}\\) and \\(t_{\\sigma}\\) are the year, the average temperature in the year and the standard deviation respectively.\nWe can express the standard deviation of \\(n\\) values \\(x_1 \\ldots x_n\\) with the following formula:\n\\[ \\sigma = \\sqrt{\\overline{x^2} - \\overline{x}^2} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i)^2}{n} - \\Bigg(\\frac{\\sum_{i=1}^n x_i}{n}\\Bigg)^2} \\]\nType the following command to get a code template:\ncp /usr/users/cpu-prof/cpu_vialle/DCE-Spark/template_temperatures.py ./avg_stddev_temp.py\nExercise\nExercise 3.1 Define a new function avg_stddev_temperature in file avg_stddev_temp.py.\nExecute the script and observe the results.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67f3bc33b5d1da6975469e8a8b62caae","permalink":"/courses/bdia/tutorials/spark-programming-dce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia/tutorials/spark-programming-dce/","section":"courses","summary":"Description of the tutorial on Spark programming on a cluster","tags":null,"title":"Running Spark programs on a cluster","type":"docs"},{"authors":null,"categories":null,"content":" Refer to this documentation to learn how to connect and interact with the cluster.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b9dfc136ad31db433d3400a02f460d5","permalink":"/courses/bdia_old/tutorials/spark-programming-dce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdia_old/tutorials/spark-programming-dce/","section":"courses","summary":"Description of the tutorial on Spark programming on a cluster","tags":null,"title":"Introduction to Spark programming","type":"docs"},{"authors":null,"categories":null,"content":" Exercise\nExercise 1 Write the following SQL queries:\nWhich last names are not repeated in table actor?\nIs a copy of the movie ACADEMY DINOSAUR available for rent from store 1?\nReturn the title and the release year of all films in one of the following categories: Family, Animation, Documentary.\nTip You can use the operator IN Find all customers (id, last name, first name) whose last name starts with the letter L. Tip You can use the operator LIKE Return the total paid by each customer. For each customer, display a single column containing first and last name and another column with the total amount paid. Order the result alphabetically by last name Tip You can use the operator CONCAT Return the total revenue from the rentals across the stores in each country. Order by descending revenue.\nThe first and last name of the actor that played in the most films. If two or more actors are tied, the query must return the names of all of them. Solution\nselect last_name from actor group by last_name having count(*) = 1 select distinct i.inventory_id from film f join inventory i using(film_id) join rental r using(inventory_id) where f.title='ACADEMY DINOSAUR' and i.store_id=1 and r.return_date is not null select distinct f.title, f.release_year from film f join film_category using(film_id) join category cat using(category_id) where cat.name in ('Family', 'Animation', 'Documentary') select customer_id, first_name, last_name from customer where last_name LIKE 'L%' select concat(first_name, ' ', last_name), sum(amount) from customer join payment using (customer_id) group by customer_id order by last_name asc select country, sum(amount) as revenue from payment join rental using (rental_id) join inventory using (inventory_id) join store using (store_id) join address using (address_id) join city using (city_id) join country using (country_id) group by country_id order by revenue desc select actor_id, first_name, last_name, count() from film_actor join actor using(actor_id) group by actor_id having count() = (select max(nb_films) from (select count(*) as nb_films from film_actor group by actor_id) t) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4c83776bb54db4b55b2d0739132d465","permalink":"/courses/bdalbert/tutorials/sql-advanced/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdalbert/tutorials/sql-advanced/","section":"courses","summary":"SQL advanced queries","tags":null,"title":"Advanced SQL queries","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nThe key integrity constraints in a relational database. Basic SQL queries. 1 Description of the data We consider the database of a DVD rental store chain containing data on films, actors, customers and the transactions of the store.\nThis database goes by the name Sakila and was developed by Mike Hillyer, a former member of the MySQL team.\nFigure 1.1: The conceptual schema of the Sakila database The tables of the database are documented on this page.\nIn this lab, we’ll use SQLite as a relational DBMS, due to the unavailability of the PostgreSQL servers. SQLite is a simple relational DBMS; data is stored in just one file.\nDownload and install DB Browser for SQLite.\nDownload the database file here.\nOpen DB Browser for SQLite and select “Open Database”.\nSelect the file that you’ve just downloaded. In the “Database Structure” tab you should see the tables of the database.\nClick on the tab “Execute SQL” to access a text area where you can type SQL queries.\n2 Integrity constraints Exercise\nExercise 2.1 Execute the following statement:\ninsert into film_actor (actor_id, film_id) values(1, 25) What is this statement supposed to do? What is the reaction of the DBMS?\nSolution\nThis statement should insert a new row in table film_actor, where the value of actor_id is 1 and the value of film_id is 25.\nThe DBMS returns an error because there is already a row with these values and the two columns film_actor, film_id form the primary key.\nExercise\nExercise 2.2 Write the statement to delete the film with film_id=1 from the table Film.\nExecute the command. What happens?\nSolution\ndelete from film where film_id=1 The statement is rejected because there are rows in other tables that reference the row that we intend to delete. This is the effect of the foreign key constraint.\nExercise\nExercise 2.3 Look at the definition of the foreign key constraints in table film_actor (see in the Database structure tab).\nIs the definition of the foreign key constraint to table film coherent with the behavior observed in the previous exercise?\nSolution\nThe foreign key linking table film_actor to table film is defined with the option RESTRICT on delete. This is coherent with the behavior that we observed in the previous exercise. Referenced rows cannot be deleted if there are still referencing rows.\nExercise\nExercise 2.4 Execute the following query:\nSELECT f.film_id as film_id_in_table_film, fa.film_id AS film_id_in_table_film_actor, fa.actor_id as actor_id, f.title as title FROM film f JOIN film_actor fa USING (film_id) WHERE f.title='ACE GOLDFINGER' What does the query? Note down the identifier of the film in both tables film and film_actor. (columns film_id_in_table_film and film_id_in_table_film_actor).\nSolution\nThe query returns the list of all actors in the film titled Ace Goldfinger. We note that the identifier of the film in both tables is identical (2), as it should because the query joins the two tables on the equality of these two values.\nExercise\nExercise 2.5 Write and execute a statement to set the value 10000 to the identifier of the film ACE GOLDFINGER in table Film.\nAfter the modification, execute the query of the previous exercise. What changed? Explain in details what happened.\nSolution\nThe statement to modify the film_id of the given film is as follows:\nUPDATE film SET film_id=10000 WHERE title=‘ACE GOLDFINGER’ After executing the same query as the previous exercise, we see that the identifier of the film has changed in the table film_actor too. This is expected, because the foreign key constraint between the colum film_id in table film_actor and the column film_id in table film has the option ON UPDATE CASCADE. This means that if we modify the identifier of the film in table film, the modification is propagated to all the referencing columns.\nExercise\nExercise 2.6 Execute the following statement:\nUPDATE film_actor SET film_id=2 WHERE film_id=10000 What does it? What happens? Explain.\nSolution\nThe statement intends to set the identifier of the film titled Ace Goldfinger (in the previous exercise we gave it the identifier 10000) back to its original value. However, we execute the statement on the table film_actor. The action is not allowed, as the identifier 2 does not correspond to any film in table film.\nThe foreign key enforces the referential integrity constraint. A row cannot refer to a non-existing entity in the referenced table.\n3 Basic queries Exercise\nExercise 3.1 Write the following SQL queries:\nReturn all the information on all customers.\nReturn the first and last name of all customers.\nReturn the first and last name of all customers of the store with identifier 1. Solution\nselect * from customer select first_name, last_name from customer select first_name, last_name from customer where store_id=1 4 Sorting and paginating Exercise\nExercise 4.1 Write the following SQL queries:\nReturn the last and first name of all customers. Sort by last name in ascending order.\nSame as in 1., but only return the first 100 customers.\nReturn the last and first name of all customers of the store with identifier 1. Sort by last name in ascending order and by first name in descending order. Solution\nselect last_name, first_name from customer order by last_name asc select last_name, first_name from customer order by last_name asc, first_name DESC limit 100 select first_name, last_name from customer where store_id=1 5 Aggregating queries Exercise\nExercise 5.1 Write the following SQL queries:\nCount the number of films in the database (expected result: 1000).\nHow many distinct actor last names are there?\nCompute the total amount of payments across all rentals (expected result: 67416.51).\nCompute the average, minimum and maximum duration of rental across all films (expected result: 4.9850000000000000, 3, 7).\nReturn the number of actors for each film.\nReturn the number of copies of each film in each store (table inventory).\nSame as 6., but only returns the pairs (film, store) if the number of copies is greater than or equal to 3.\nSolution\nselect count(*) from film select count (distinct last_name) from actor select sum(amount) from payment select avg(rental_duration), min(rental_duration), max(rental_duration) from film select film_id, count(*) as nb_actors from film_actor group by film_id select film_id, store_id, count(*) as nb_films from inventory group by film_id, store_id select film_id, store_id, count(*) as nb_films from inventory group by film_id, store_id having count(*) \u003e=3 6 Join queries Exercise\nExercise 6.1 Write the following SQL queries:\nReturn the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).\nReturn the first and last name of the actors in the film ACE GOLDFINGER.\nReturn first and last name of each actor and the number of films in which they played a role.\nSame as in 3., but order by number of films in descending order.\nSame as in 4., but only return actors who played a role in at least 10 films.\nReturn the identifier, the first and family name of the customers who have rented between 5 and 10 movies in the category Family. Solution\nselect first_name, last_name from staff join store using(store_id) where store_id=1 select first_name, last_name from film join film_actor using(film_id) join actor using(actor_id) where title='ACE GOLDFINGER' select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id order by nb_films desc select first_name, last_name, count(*) as nb_films from actor join film_actor using(actor_id) group by actor_id having count(*) \u003e= 10 order by nb_films desc select cust.customer_id, first_name, last_name, count(*) as nb_films from customer cust join rental using(customer_id) join inventory using(inventory_id) join film_category using(film_id) join category cat using(category_id) where cat.name='Family' group by customer_id having count(*) between 5 and 10 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f9bf24ed764f61d19e1b0ae0d194aab0","permalink":"/courses/bdalbert/tutorials/sql-intro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/bdalbert/tutorials/sql-intro/","section":"courses","summary":"Introduction to SQL queries","tags":null,"title":"Introduction to SQL","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial you’ll learn:\nThe key integrity constraints in a relational database. Basic SQL queries. 1 Description of the data We consider the database of a DVD rental store chain containing data on films, actors, customers and the transactions of the store.\nThis database goes by the name Sakila and was developed by Mike Hillyer, a former member of the MySQL team.\nFigure 1.1: The conceptual schema of the Sakila database The tables of the database are documented on this page.\nSakila has been ported from MySQL to PostgreSQL under the name pagila. In pgAdmin, Open a new query tool to do the exercises.\n2 Integrity constraints Exercise\nExercise 2.1 Execute the following statement:\ninsert into film_actor (actor_id, film_id) values(1, 25) What is this statement supposed to do? What is the reaction of the DBMS?\nExercise\nExercise 2.2 Write the statement to delete the film with film_id=1 from the table Film.\nExecute the command. What happens?\nExercise\nExercise 2.3 Look at the definition of the foreign key constraints in table film_actor (see in the Database structure tab).\nIs the definition of the foreign key constraint to table film coherent with the behavior observed in the previous exercise?\nExercise\nExercise 2.4 Execute the following query:\nSELECT f.film_id as film_id_in_table_film, fa.film_id AS film_id_in_table_film_actor, fa.actor_id as actor_id, f.title as title FROM film f JOIN film_actor fa USING (film_id) WHERE f.title='ACE GOLDFINGER' What does the query? Note down the identifier of the film in both tables film and film_actor. (columns film_id_in_table_film and film_id_in_table_film_actor).\nExercise\nExercise 2.5 Write and execute a statement to set the value 10000 to the identifier of the film ACE GOLDFINGER in table Film.\nAfter the modification, execute the query of the previous exercise. What changed? Explain in details what happened.\nExercise\nExercise 2.6 Execute the following statement:\nUPDATE film_actor SET film_id=2 WHERE film_id=10000 What does it? What happens? Explain.\n3 Basic queries Exercise\nExercise 3.1 Write the following SQL queries:\nReturn all the information on all customers.\nReturn the first and last name of all customers.\nReturn the first and last name of all customers of the store with identifier 1.\n4 Sorting and paginating Exercise\nExercise 4.1 Write the following SQL queries:\nReturn the last and first name of all customers. Sort by last name in ascending order.\nSame as in 1., but only return the first 100 customers.\nReturn the last and first name of all customers of the store with identifier 1. Sort by last name in ascending order and by first name in descending order.\n5 Aggregating queries Exercise\nExercise 5.1 Write the following SQL queries:\nCount the number of films in the database (expected result: 1000).\nHow many distinct actor last names are there?\nCompute the total amount of payments across all rentals (expected result: 67416.51).\nCompute the average, minimum and maximum duration of rental across all films (expected result: 4.9850000000000000, 3, 7).\nReturn the number of actors for each film.\nReturn the number of copies of each film in each store (table inventory).\nSame as 6., but only returns the pairs (film, store) if the number of copies is greater than or equal to 3.\n6 Join queries Exercise\nExercise 6.1 Write the following SQL queries:\nReturn the first and last name of the manager of the store with identifier 1 (expected result: Mike Hillyer).\nReturn the first and last name of the actors in the film ACE GOLDFINGER.\nReturn first and last name of each actor and the number of films in which they played a role.\nSame as in 3., but order by number of films in descending order.\nSame as in 4., but only return actors who played a role in at least 10 films.\nReturn the identifier, the first and family name of the customers who have rented between 5 and 10 movies in the category Family.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbe104d364885f55187f1d8090d0fa37","permalink":"/courses/databases/tutorials/sql-intro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sql-intro/","section":"courses","summary":"Description of the SQLite tutorial.","tags":null,"title":"Learning SQL queries","type":"docs"},{"authors":null,"categories":null,"content":" Exercise\nExercise 1 Write the following SQL queries:\nWhich last names are not repeated in table actor?\nIs a copy of the movie ACADEMY DINOSAUR available for rent from store 1?\nReturn the title and the release year of all films in one of the following categories: Family, Animation, Documentary.\nTip You can use the operator IN Find all customers (id, last name, first name) whose last name starts with the letter L. Tip You can use the operator LIKE Return the total paid by each customer. For each customer, display a single column containing first and last name and another column with the total amount paid. Order the result alphabetically by last name Tip You can use the operator CONCAT Return the total revenue from the rentals across the stores in each country. Order by descending revenue.\nThe first and last name of the actor that played in the most films. If two or more actors are tied, the query must return the names of all of them. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c06fcd64991e971604cc6f448ab6d9db","permalink":"/courses/databases/tutorials/sql-advanced/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/databases/tutorials/sql-advanced/","section":"courses","summary":"Advanced SQL queries.","tags":null,"title":"Learning SQL queries","type":"docs"}]